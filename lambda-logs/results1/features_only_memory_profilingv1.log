wandb: Tracking run with wandb version 0.9.3
wandb: Run data is saved locally in wandb/run-20200718_162533-agmv1xk3
wandb: Syncing run feature_only_memory_profiling_v1
wandb: ‚≠êÔ∏è View project at https://app.wandb.ai/apappu97/molecule-metalearning-chemprop
wandb: üöÄ View run at https://app.wandb.ai/apappu97/molecule-metalearning-chemprop/runs/agmv1xk3
wandb: Run `wandb off` to turn off syncing.

Fold 0
Command line
python chemprop/train.py --data_path filtered_chembl/chembl_less_64_instances.csv --dataset_type classification --features_only --features_generator morgan --split_type scaffold_balanced --batch_size 32 --results_save_dir results1/ --experiment_name feature_only_memory_profiling_v1
Args
{'ANIL': False,
 'FO_MAML': False,
 'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 32,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'chembl_assay_metadata_pickle_path': None,
 'class_balance': False,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': 'filtered_chembl/chembl_less_64_instances.csv',
 'dataset_type': 'classification',
 'depth': 3,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'dummy': False,
 'ensemble_size': 1,
 'epochs': 30,
 'experiment_name': 'feature_only_memory_profiling_v1',
 'features_generator': ['morgan'],
 'features_only': True,
 'features_path': None,
 'features_scaling': True,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 300,
 'init_lr': 0.0001,
 'inner_loop_lr': 0.05,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'meta_batch_size': 3,
 'meta_learning': False,
 'meta_test_split_sizes': (0.8, 0.1, 0.1),
 'meta_train_split_sizes': (0.8, 0.2, 0),
 'metric': 'auc',
 'minimize_score': False,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': False,
 'num_folds': 1,
 'num_inner_gradient_steps': 2,
 'num_lrs': 1,
 'num_tasks': None,
 'num_workers': 0,
 'outer_loop_lr': 0.003,
 'pytorch_seed': 0,
 'quiet': False,
 'results_save_dir': 'results1/',
 'save_dir': '/tmp/tmpbe6l1foj/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': None,
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'scaffold_balanced',
 'target_columns': None,
 'task_names': None,
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': True,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
0it [00:00, ?it/s]114it [00:00, 1138.24it/s]228it [00:00, 1135.80it/s]343it [00:00, 1137.15it/s]343it [00:00, 1132.59it/s]
  0% 0/343 [00:00<?, ?it/s] 36% 124/343 [00:00<00:00, 1235.03it/s] 72% 247/343 [00:00<00:00, 1232.75it/s]100% 343/343 [00:00<00:00, 1220.68it/s]
  0% 0/343 [00:00<?, ?it/s]100% 343/343 [00:00<00:00, 26040.73it/s]
Number of tasks = 8
Splitting data with seed 0
  0% 0/343 [00:00<?, ?it/s]100% 343/343 [00:00<00:00, 3858.05it/s]
Total scaffolds = 230 | train scaffolds = 170 | val scaffolds = 28 | test scaffolds = 32
/home/ubuntu/molecule-metalearning/chemprop/chemprop/data/scaffold.py:154: RuntimeWarning: Mean of empty slice
  target_avgs.append(np.nanmean(targets, axis=0))
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan, 0.52,  nan,  nan]), array([ 0,  0,  0,  0,  0, 25,  0,  0])), (array([       nan, 0.66666667,        nan,        nan,        nan,
              nan,        nan,        nan]), array([ 0, 18,  0,  0,  0,  0,  0,  0])), (array([nan, nan, nan, nan,  1., nan, nan, nan]), array([0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 1])), (array([ 0., nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 1, 0, 0, 0]))]
Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/data/utils.py

Line #    Mem usage    Increment   Line Contents
================================================
    98    306.2 MiB    306.2 MiB   @profile
    99                             def get_data(path: str,
   100                                          smiles_column: str = None,
   101                                          target_columns: List[str] = None,
   102                                          skip_invalid_smiles: bool = True,
   103                                          args: Union[PredictArgs, TrainArgs] = None,
   104                                          features_path: List[str] = None,
   105                                          features_generator: List[str] = None,
   106                                          max_data_size: int = None,
   107                                          logger: Logger = None) -> MoleculeDataset:
   108                                 """
   109                                 Gets smiles string and target values (and optionally compound names if provided) from a CSV file.
   110                             
   111                                 :param path: Path to a CSV file.
   112                                 :param smiles_column: The name of the column containing SMILES strings. By default, uses the first column.
   113                                 :param target_columns: Name of the columns containing target values. By default, uses all columns except the SMILES column.
   114                                 :param skip_invalid_smiles: Whether to skip and filter out invalid smiles.
   115                                 :param args: Arguments.
   116                                 :param features_path: A list of paths to files containing features. If provided, it is used
   117                                 in place of args.features_path.
   118                                 :param features_generator: A list of features generators to use. If provided, it is used
   119                                 in place of args.features_generator.
   120                                 :param max_data_size: The maximum number of data points to load.
   121                                 :param logger: Logger.
   122                                 :return: A MoleculeDataset containing smiles strings and target values along
   123                                 with other info such as additional features and compound names when desired.
   124                                 """
   125    306.2 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
   126                             
   127    306.2 MiB      0.0 MiB       if args is not None:
   128                                     # Prefer explicit function arguments but default to args if not provided
   129    306.2 MiB      0.0 MiB           smiles_column = smiles_column if smiles_column is not None else args.smiles_column
   130    306.2 MiB      0.0 MiB           target_columns = target_columns if target_columns is not None else args.target_columns
   131    306.2 MiB      0.0 MiB           features_path = features_path if features_path is not None else args.features_path
   132    306.2 MiB      0.0 MiB           features_generator = features_generator if features_generator is not None else args.features_generator
   133    306.2 MiB      0.0 MiB           max_data_size = max_data_size if max_data_size is not None else args.max_data_size
   134                             
   135    306.2 MiB      0.0 MiB       max_data_size = max_data_size or float('inf')
   136                             
   137                                 # Load features
   138    306.2 MiB      0.0 MiB       if features_path is not None:
   139                                     features_data = []
   140                                     for feat_path in features_path:
   141                                         features_data.append(load_features(feat_path))  # each is num_data x num_features
   142                                     features_data = np.concatenate(features_data, axis=1)
   143                                 else:
   144    306.2 MiB      0.0 MiB           features_data = None
   145                             
   146    306.2 MiB      0.0 MiB       skip_smiles = set()
   147                             
   148                                 # Load data
   149    306.2 MiB      0.0 MiB       with open(path) as f:
   150    306.2 MiB      0.0 MiB           reader = csv.DictReader(f)
   151    306.2 MiB      0.0 MiB           columns = reader.fieldnames
   152                             
   153                                     # By default, the SMILES column is the first column
   154    306.2 MiB      0.0 MiB           if smiles_column is None:
   155    306.2 MiB      0.0 MiB               smiles_column = columns[0]
   156                             
   157                                     # By default, the targets columns are all the columns except the SMILES column
   158    306.2 MiB      0.0 MiB           if target_columns is None:
   159    306.2 MiB      0.0 MiB               target_columns = [column for column in columns if column != smiles_column]
   160                             
   161    306.2 MiB      0.0 MiB           all_smiles, all_targets, all_rows = [], [], []
   162    306.3 MiB      0.1 MiB           for row in tqdm(reader):
   163    306.3 MiB      0.0 MiB               smiles = row[smiles_column]
   164                             
   165    306.3 MiB      0.0 MiB               if smiles in skip_smiles:
   166                                             continue
   167                             
   168    306.3 MiB      0.0 MiB               targets = [float(row[column]) if row[column] != '' else None for column in target_columns]
   169                             
   170    306.3 MiB      0.0 MiB               all_smiles.append(smiles)
   171    306.3 MiB      0.0 MiB               all_targets.append(targets)
   172    306.3 MiB      0.0 MiB               all_rows.append(row)
   173                             
   174    306.3 MiB      0.0 MiB               if len(all_smiles) >= max_data_size:
   175                                             break
   176                             
   177    306.3 MiB      0.0 MiB           data = MoleculeDataset([
   178    317.6 MiB      0.0 MiB               MoleculeDatapoint(
   179                                             smiles=smiles,
   180                                             targets=targets,
   181                                             row=row,
   182                                             features_generator=features_generator,
   183                                             features=features_data[i] if features_data is not None else None
   184    317.6 MiB      0.3 MiB               ) for i, (smiles, targets, row) in tqdm(enumerate(zip(all_smiles, all_targets, all_rows)),
   185    317.6 MiB      0.0 MiB                                                       total=len(all_smiles))
   186                                     ])
   187                             
   188                                 # Filter out invalid SMILES
   189    317.6 MiB      0.0 MiB       if skip_invalid_smiles:
   190    317.6 MiB      0.0 MiB           original_data_len = len(data)
   191    317.8 MiB      0.1 MiB           data = filter_invalid_smiles(data)
   192                             
   193    317.8 MiB      0.0 MiB           if len(data) < original_data_len:
   194                                         debug(f'Warning: {original_data_len - len(data)} SMILES are invalid.')
   195                             
   196    317.8 MiB      0.0 MiB       return data


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/data/scaffold.py

Line #    Mem usage    Increment   Line Contents
================================================
    49    317.8 MiB    317.8 MiB   @profile
    50                             def scaffold_split(data: MoleculeDataset,
    51                                                sizes: Tuple[float, float, float] = (0.8, 0.1, 0.1),
    52                                                balanced: bool = False,
    53                                                seed: int = 0,
    54                                                logger: logging.Logger = None) -> Tuple[MoleculeDataset,
    55                                                                                        MoleculeDataset,
    56                                                                                        MoleculeDataset]:
    57                                 """
    58                                 Split a dataset by scaffold so that no molecules sharing a scaffold are in the same split.
    59                             
    60                                 :param data: A MoleculeDataset.
    61                                 :param sizes: A length-3 tuple with the proportions of data in the
    62                                 train, validation, and test sets.
    63                                 :param balanced: Try to balance sizes of scaffolds in each set, rather than just putting smallest in test set.
    64                                 :param seed: Seed for shuffling when doing balanced splitting.
    65                                 :param logger: A logger.
    66                                 :return: A tuple containing the train, validation, and test splits of the data.
    67                                 """
    68    317.8 MiB      0.0 MiB       assert sum(sizes) == 1
    69                             
    70                                 # Split
    71    317.8 MiB      0.0 MiB       train_size, val_size, test_size = sizes[0] * len(data), sizes[1] * len(data), sizes[2] * len(data)
    72    317.8 MiB      0.0 MiB       train, val, test = [], [], []
    73    317.8 MiB      0.0 MiB       train_scaffold_count, val_scaffold_count, test_scaffold_count = 0, 0, 0
    74                             
    75                                 # Map from scaffold to index in the data
    76    320.9 MiB      3.1 MiB       scaffold_to_indices = scaffold_to_smiles(data.mols(), use_indices=True)
    77                             
    78                                 # Seed randomness
    79    320.9 MiB      0.0 MiB       random = Random(seed)
    80                             
    81    320.9 MiB      0.0 MiB       if balanced:  # Put stuff that's bigger than half the val/test size into train, rest just order randomly
    82    320.9 MiB      0.0 MiB           index_sets = list(scaffold_to_indices.values())
    83    320.9 MiB      0.0 MiB           big_index_sets = []
    84    320.9 MiB      0.0 MiB           small_index_sets = []
    85    320.9 MiB      0.0 MiB           for index_set in index_sets:
    86    320.9 MiB      0.0 MiB               if len(index_set) > val_size / 2 or len(index_set) > test_size / 2:
    87    320.9 MiB      0.0 MiB                   big_index_sets.append(index_set)
    88                                         else:
    89    320.9 MiB      0.0 MiB                   small_index_sets.append(index_set)
    90    320.9 MiB      0.0 MiB           random.seed(seed)
    91    320.9 MiB      0.0 MiB           random.shuffle(big_index_sets)
    92    320.9 MiB      0.0 MiB           random.shuffle(small_index_sets)
    93    320.9 MiB      0.0 MiB           index_sets = big_index_sets + small_index_sets
    94                                 else:  # Sort from largest to smallest scaffold sets
    95                                     index_sets = sorted(list(scaffold_to_indices.values()),
    96                                                         key=lambda index_set: len(index_set),
    97                                                         reverse=True)
    98                             
    99    320.9 MiB      0.0 MiB       for index_set in index_sets:
   100    320.9 MiB      0.0 MiB           if len(train) + len(index_set) <= train_size:
   101    320.9 MiB      0.0 MiB               train += index_set
   102    320.9 MiB      0.0 MiB               train_scaffold_count += 1
   103    320.9 MiB      0.0 MiB           elif len(val) + len(index_set) <= val_size:
   104    320.9 MiB      0.0 MiB               val += index_set
   105    320.9 MiB      0.0 MiB               val_scaffold_count += 1
   106                                     else:
   107    320.9 MiB      0.0 MiB               test += index_set
   108    320.9 MiB      0.0 MiB               test_scaffold_count += 1
   109                             
   110    320.9 MiB      0.0 MiB       if logger is not None:
   111    320.9 MiB      0.0 MiB           logger.debug(f'Total scaffolds = {len(scaffold_to_indices):,} | '
   112                                                  f'train scaffolds = {train_scaffold_count:,} | '
   113                                                  f'val scaffolds = {val_scaffold_count:,} | '
   114                                                  f'test scaffolds = {test_scaffold_count:,}')
   115                             
   116    320.9 MiB      0.0 MiB       if logger is not None:
   117    321.1 MiB      0.2 MiB           log_scaffold_stats(data, index_sets, logger=logger)
   118                             
   119                                 # Map from indices to data
   120    321.1 MiB      0.0 MiB       train = [data[i] for i in train]
   121    321.1 MiB      0.0 MiB       val = [data[i] for i in val]
   122    321.1 MiB      0.0 MiB       test = [data[i] for i in test]
   123                             
   124    321.1 MiB      0.0 MiB       train_dataset = MoleculeDataset(train)
   125    321.1 MiB      0.0 MiB       val_dataset = MoleculeDataset(val)
   126    321.1 MiB      0.0 MiB       test_dataset = MoleculeDataset(test)
   127    321.1 MiB      0.0 MiB       return train_dataset, val_dataset, test_dataset


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/data/utils.py

Line #    Mem usage    Increment   Line Contents
================================================
   232    317.8 MiB    317.8 MiB   @profile
   233                             def split_data(data: MoleculeDataset,
   234                                            split_type: str = 'random',
   235                                            sizes: Tuple[float, float, float] = (0.8, 0.1, 0.1),
   236                                            seed: int = 0,
   237                                            args: TrainArgs = None,
   238                                            logger: Logger = None) -> Tuple[MoleculeDataset,
   239                                                                            MoleculeDataset,
   240                                                                            MoleculeDataset]:
   241                                 """
   242                                 Splits data into training, validation, and test splits.
   243                             
   244                                 :param data: A MoleculeDataset.
   245                                 :param split_type: Split type.
   246                                 :param sizes: A length-3 tuple with the proportions of data in the
   247                                 train, validation, and test sets.
   248                                 :param seed: The random seed to use before shuffling data.
   249                                 :param args: Arguments.
   250                                 :param logger: A logger.
   251                                 :return: A tuple containing the train, validation, and test splits of the data.
   252                                 """
   253    317.8 MiB      0.0 MiB       if not (len(sizes) == 3 and sum(sizes) == 1):
   254                                     raise ValueError('Valid split sizes must sum to 1 and must have three sizes: train, validation, and test.')
   255                             
   256    317.8 MiB      0.0 MiB       random = Random(seed)
   257                             
   258    317.8 MiB      0.0 MiB       if args is not None:
   259                                     folds_file, val_fold_index, test_fold_index = \
   260    317.8 MiB      0.0 MiB               args.folds_file, args.val_fold_index, args.test_fold_index
Class sizes
CHEMBL1024480 0: 89.58%, 1: 10.42%
CHEMBL1243971 0: 48.89%, 1: 51.11%
CHEMBL1614070 0: 18.75%, 1: 81.25%
CHEMBL1614338 0: 15.00%, 1: 85.00%
CHEMBL1614405 0: 50.94%, 1: 49.06%
CHEMBL1820680 0: 46.81%, 1: 53.19%
CHEMBL2303649 0: 68.09%, 1: 31.91%
CHEMBL806152 0: 48.08%, 1: 51.92%
   261                                 else:
   262                                     folds_file = val_fold_index = test_fold_index = None
   263                                 
   264    317.8 MiB      0.0 MiB       if split_type == 'crossval':
   265                                     index_set = args.crossval_index_sets[args.seed]
   266                                     data_split = []
   267                                     for split in range(3):
   268                                         split_indices = []
   269                                         for index in index_set[split]:
   270                                             with open(os.path.join(args.crossval_index_dir, f'{index}.pkl'), 'rb') as rf:
   271                                                 split_indices.extend(pickle.load(rf))
   272                                         data_split.append([data[i] for i in split_indices])
   273                                     train, val, test = tuple(data_split)
   274                                     return MoleculeDataset(train), MoleculeDataset(val), MoleculeDataset(test)
   275                                 
   276    317.8 MiB      0.0 MiB       elif split_type == 'index_predetermined':
   277                                     split_indices = args.crossval_index_sets[args.seed]
   278                             
   279                                     if len(split_indices) != 3:
   280                                         raise ValueError('Split indices must have three splits: train, validation, and test')
   281                             
   282                                     data_split = []
   283                                     for split in range(3):
   284                                         data_split.append([data[i] for i in split_indices[split]])
   285                                     train, val, test = tuple(data_split)
   286                                     return MoleculeDataset(train), MoleculeDataset(val), MoleculeDataset(test)
   287                             
   288    317.8 MiB      0.0 MiB       elif split_type == 'predetermined':
   289                                     if not val_fold_index and sizes[2] != 0:
   290                                         raise ValueError('Test size must be zero since test set is created separately '
   291                                                          'and we want to put all other data in train and validation')
   292                             
   293                                     assert folds_file is not None
   294                                     assert test_fold_index is not None
   295                             
   296                                     try:
   297                                         with open(folds_file, 'rb') as f:
   298                                             all_fold_indices = pickle.load(f)
   299                                     except UnicodeDecodeError:
   300                                         with open(folds_file, 'rb') as f:
   301                                             all_fold_indices = pickle.load(f, encoding='latin1')  # in case we're loading indices from python2
   302                             
   303                                     log_scaffold_stats(data, all_fold_indices, logger=logger)
   304                             
   305                                     folds = [[data[i] for i in fold_indices] for fold_indices in all_fold_indices]
   306                             
   307                                     test = folds[test_fold_index]
   308                                     if val_fold_index is not None:
   309                                         val = folds[val_fold_index]
   310                             
   311                                     train_val = []
   312                                     for i in range(len(folds)):
   313                                         if i != test_fold_index and (val_fold_index is None or i != val_fold_index):
   314                                             train_val.extend(folds[i])
   315                             
   316                                     if val_fold_index is not None:
   317                                         train = train_val
   318                                     else:
   319                                         random.shuffle(train_val)
   320                                         train_size = int(sizes[0] * len(train_val))
   321                                         train = train_val[:train_size]
   322                                         val = train_val[train_size:]
   323                             
   324                                     return MoleculeDataset(train), MoleculeDataset(val), MoleculeDataset(test)
   325                                 
   326    317.8 MiB      0.0 MiB       elif split_type == 'scaffold_balanced':
   327    321.1 MiB      3.3 MiB           return scaffold_split(data, sizes=sizes, balanced=True, seed=seed, logger=logger)
   328                             
   329                                 elif split_type == 'random':
   330                                     data.shuffle(seed=seed)
   331                             
   332                                     train_size = int(sizes[0] * len(data))
   333                                     train_val_size = int((sizes[0] + sizes[1]) * len(data))
   334                             
   335                                     train = data[:train_size]
   336                                     val = data[train_size:train_val_size]
   337                                     test = data[train_val_size:]
   338                             
   339                                     return MoleculeDataset(train), MoleculeDataset(val), MoleculeDataset(test)
   340                             
   341                                 else:
   342                                     raise ValueError(f'split_type "{split_type}" not supported.')


shape of X array:  (274, 2048)
4489328
Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/data/scaler.py

Line #    Mem usage    Increment   Line Contents
================================================
    26    325.4 MiB    325.4 MiB       @profile
    27                                 def fit(self, X: List[List[float]]) -> 'StandardScaler':
    28                                     """
    29                                     Learns means and standard deviations across the 0th axis.
    30                             
    31                                     :param X: A list of lists of floats.
    32                                     :return: The fitted StandardScaler.
    33                                     """
    34    329.7 MiB      4.3 MiB           X = np.array(X).astype(float)
    35    329.7 MiB      0.0 MiB           print('shape of X array: ', X.shape)
    36    329.7 MiB      0.0 MiB           print(sys.getsizeof(X))
    37    335.0 MiB      5.3 MiB           self.stds = np.nanstd(X, axis=0)
    38    335.0 MiB      0.0 MiB           self.means = np.nanmean(X, axis=0)
    39    335.0 MiB      0.0 MiB           self.means = np.where(np.isnan(self.means), np.zeros(self.means.shape), self.means)
    40    335.0 MiB      0.0 MiB           self.stds = np.where(np.isnan(self.stds), np.ones(self.stds.shape), self.stds)
    41    335.0 MiB      0.0 MiB           self.stds = np.where(self.stds == 0, np.ones(self.stds.shape), self.stds)
    42                                     # all_objects = muppy.get_objects()
    43                                     # sum1 = summary.summarize(all_objects)
    44                                     # summary.print_(sum1)
    45    335.0 MiB      0.0 MiB           return self


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/data/data.py

Line #    Mem usage    Increment   Line Contents
================================================
   194    321.1 MiB    321.1 MiB       @profile
   195                                 def normalize_features(self, scaler: StandardScaler = None, replace_nan_token: int = 0) -> StandardScaler:
Total size = 343 | train size = 274 | val size = 34 | test size = 35
Building model 0
MoleculeModel(
  (sigmoid): Sigmoid()
  (encoder): MPN(
    (encoder): MPNEncoder()
  )
  (ffn): Sequential(
    (0): Linear(in_features=2048, out_features=300, bias=True)
    (1): ReLU()
    (2): Dropout(p=0.0, inplace=False)
    (3): Linear(in_features=300, out_features=8, bias=True)
  )
)
Number of parameters = 617,108
Moving model to cuda
  0% 0/30 [00:00<?, ?it/s]Epoch 0

  0% 0/9 [00:00<?, ?it/s][A
 11% 1/9 [00:00<00:06,  1.26it/s][A
 22% 2/9 [00:01<00:04,  1.46it/s][A
 33% 3/9 [00:01<00:03,  1.69it/s][A
 44% 4/9 [00:01<00:02,  1.95it/s][A
 56% 5/9 [00:02<00:01,  2.07it/s][A
 67% 6/9 [00:02<00:01,  2.14it/s][A
 78% 7/9 [00:03<00:00,  2.38it/s][A
 89% 8/9 [00:03<00:00,  2.53it/s][A
100% 9/9 [00:03<00:00,  2.83it/s][A100% 9/9 [00:03<00:00,  2.45it/s]   196                                     """
   197                                     Normalizes the features of the dataset using a StandardScaler (subtract mean, divide by standard deviation).
   198                             
   199                                     If a scaler is provided, uses that scaler to perform the normalization. Otherwise fits a scaler to the
   200                                     features in the dataset and then performs the normalization.
   201                             
   202                                     :param scaler: A fitted StandardScaler. Used if provided. Otherwise a StandardScaler is fit on
   203                                     this dataset and is then used.
   204                                     :param replace_nan_token: What to replace nans with.
   205                                     :return: A fitted StandardScaler. If a scaler is provided, this is the same scaler. Otherwise, this is
   206                                     a scaler fit on this dataset.
   207                                     """
   208    321.1 MiB      0.0 MiB           if len(self._data) == 0 or self._data[0].features is None:
   209                                         return None
   210                             
   211    321.1 MiB      0.0 MiB           if scaler is not None:
   212                                         self._scaler = scaler
   213                             
   214    321.1 MiB      0.0 MiB           elif self._scaler is None:
   215    325.4 MiB      4.4 MiB               features = np.vstack([d.features for d in self._data])
   216    325.4 MiB      0.0 MiB               self._scaler = StandardScaler(replace_nan_token=replace_nan_token)
   217    330.9 MiB      5.5 MiB               self._scaler.fit(features)
   218                             
   219                                     # for d in self._data:
   220                                     #     d.set_features(self._scaler.transform(d.features.reshape(1, -1))[0])
   221                             
   222    330.9 MiB      0.0 MiB           return self._scaler


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/data/data.py

Line #    Mem usage    Increment   Line Contents
================================================
   194    326.6 MiB    326.6 MiB       @profile
   195                                 def normalize_features(self, scaler: StandardScaler = None, replace_nan_token: int = 0) -> StandardScaler:
   196                                     """
   197                                     Normalizes the features of the dataset using a StandardScaler (subtract mean, divide by standard deviation).
   198                             
   199                                     If a scaler is provided, uses that scaler to perform the normalization. Otherwise fits a scaler to the
   200                                     features in the dataset and then performs the normalization.
   201                             
   202                                     :param scaler: A fitted StandardScaler. Used if provided. Otherwise a StandardScaler is fit on
   203                                     this dataset and is then used.
   204                                     :param replace_nan_token: What to replace nans with.
   205                                     :return: A fitted StandardScaler. If a scaler is provided, this is the same scaler. Otherwise, this is
   206                                     a scaler fit on this dataset.
   207                                     """
   208    326.6 MiB      0.0 MiB           if len(self._data) == 0 or self._data[0].features is None:
   209                                         return None
   210                             
   211    326.6 MiB      0.0 MiB           if scaler is not None:
   212    326.6 MiB      0.0 MiB               self._scaler = scaler
   213                             
   214                                     elif self._scaler is None:
   215                                         features = np.vstack([d.features for d in self._data])
   216                                         self._scaler = StandardScaler(replace_nan_token=replace_nan_token)
   217                                         self._scaler.fit(features)
   218                             
   219                                     # for d in self._data:
   220                                     #     d.set_features(self._scaler.transform(d.features.reshape(1, -1))[0])
   221                             
   222    326.6 MiB      0.0 MiB           return self._scaler


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/data/data.py

Line #    Mem usage    Increment   Line Contents
================================================
   194    326.6 MiB    326.6 MiB       @profile
   195                                 def normalize_features(self, scaler: StandardScaler = None, replace_nan_token: int = 0) -> StandardScaler:
   196                                     """
   197                                     Normalizes the features of the dataset using a StandardScaler (subtract mean, divide by standard deviation).
   198                             
   199                                     If a scaler is provided, uses that scaler to perform the normalization. Otherwise fits a scaler to the
   200                                     features in the dataset and then performs the normalization.
   201                             
   202                                     :param scaler: A fitted StandardScaler. Used if provided. Otherwise a StandardScaler is fit on
   203                                     this dataset and is then used.
   204                                     :param replace_nan_token: What to replace nans with.
   205                                     :return: A fitted StandardScaler. If a scaler is provided, this is the same scaler. Otherwise, this is
   206                                     a scaler fit on this dataset.
   207                                     """
   208    326.6 MiB      0.0 MiB           if len(self._data) == 0 or self._data[0].features is None:
   209                                         return None
   210                             
   211    326.6 MiB      0.0 MiB           if scaler is not None:
   212    326.6 MiB      0.0 MiB               self._scaler = scaler
   213                             
   214                                     elif self._scaler is None:
   215                                         features = np.vstack([d.features for d in self._data])
   216                                         self._scaler = StandardScaler(replace_nan_token=replace_nan_token)
   217                                         self._scaler.fit(features)
   218                             
   219                                     # for d in self._data:
   220                                     #     d.set_features(self._scaler.transform(d.features.reshape(1, -1))[0])
   221                             
   222    326.6 MiB      0.0 MiB           return self._scaler


save dir for model is /tmp/tmpbe6l1foj/fold_0/model_0
Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/train.py

Line #    Mem usage    Increment   Line Contents
================================================
    17   2004.7 MiB   2004.7 MiB   @profile
    18                             def train(model: nn.Module,
    19                                       data_loader: MoleculeDataLoader,
    20                                       loss_func: Callable,
    21                                       optimizer: Optimizer,
    22                                       scheduler: _LRScheduler,
    23                                       args: TrainArgs,
    24                                       n_iter: int = 0,
    25                                       logger: logging.Logger = None,
    26                                       writer: SummaryWriter = None) -> int:
    27                                 """
    28                                 Trains a model for an epoch.
    29                             


  0% 0/2 [00:00<?, ?it/s][A
 50% 1/2 [00:00<00:00,  3.62it/s][A100% 2/2 [00:00<00:00,  6.26it/s]
Validation auc = 0.562500
  3% 1/30 [00:04<02:22,  4.92s/it]Epoch 1

  0% 0/9 [00:00<?, ?it/s][A
 11% 1/9 [00:00<00:00,  9.01it/s][ALoss = 1.7432e-02, PNorm = 23.2192, GNorm = 0.4926, lr_0 = 7.7500e-04

 22% 2/9 [00:00<00:00,  7.45it/s][A
 33% 3/9 [00:00<00:00,  6.40it/s][A
 44% 4/9 [00:00<00:00,  5.73it/s][A
 56% 5/9 [00:00<00:00,  5.79it/s][A
 67% 6/9 [00:01<00:00,  5.50it/s][A
 78% 7/9 [00:01<00:00,  5.06it/s][A
 89% 8/9 [00:01<00:00,  5.10it/s][A
100% 9/9 [00:01<00:00,  5.28it/s][A100% 9/9 [00:01<00:00,  5.30it/s]    30                                 :param model: Model.
    31                                 :param data_loader: A MoleculeDataLoader.
    32                                 :param loss_func: Loss function.
    33                                 :param optimizer: An Optimizer.
    34                                 :param scheduler: A learning rate scheduler.
    35                                 :param args: Arguments.
    36                                 :param n_iter: The number of iterations (training examples) trained on so far.
    37                                 :param logger: A logger for printing intermediate results.
    38                                 :param writer: A tensorboardX SummaryWriter.
    39                                 :return: The total number of iterations (training examples) trained on so far.
    40                                 """
    41   2004.7 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
    42                                 
    43   2004.7 MiB      0.0 MiB       model.train()
    44   2004.7 MiB      0.0 MiB       loss_sum, iter_count = 0, 0
    45                             
    46   2230.5 MiB      3.9 MiB       for batch in tqdm(data_loader, total=len(data_loader)):
    47                                     # Prepare batch
    48                                     batch: MoleculeDataset
    49   2230.5 MiB      0.0 MiB           mol_batch, features_batch, target_batch = batch.batch_graph(), batch.features(), batch.targets()
    50   2230.5 MiB      0.0 MiB           mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])
    51   2230.5 MiB      0.0 MiB           targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])
    52                             
    53                                     # Run model
    54   2230.5 MiB      0.0 MiB           model.zero_grad()
    55   2230.5 MiB    198.5 MiB           preds = model(mol_batch, features_batch)
    56                             
    57                                     # Move tensors to correct device
    58   2230.5 MiB      0.0 MiB           mask = mask.to(preds.device)
    59   2230.5 MiB      0.0 MiB           targets = targets.to(preds.device)
    60   2230.5 MiB      0.0 MiB           class_weights = torch.ones(targets.shape, device=preds.device)
    61                             
    62   2230.5 MiB      0.0 MiB           if args.dataset_type == 'multiclass':
    63                                         targets = targets.long()
    64                                         loss = torch.cat([loss_func(preds[:, target_index, :], targets[:, target_index]).unsqueeze(1) for target_index in range(preds.size(1))], dim=1) * class_weights * mask
    65                                     else:
    66   2230.5 MiB      1.9 MiB               loss = loss_func(preds, targets) * class_weights * mask
    67   2230.5 MiB      0.0 MiB           loss = loss.sum() / mask.sum()
    68                             
    69   2230.5 MiB      0.0 MiB           loss_sum += loss.item()
    70   2230.5 MiB      0.0 MiB           iter_count += len(batch)
    71                             
    72   2230.5 MiB      1.2 MiB           loss.backward()
    73   2230.5 MiB      1.9 MiB           optimizer.step()
    74                             
    75   2230.5 MiB      0.0 MiB           if isinstance(scheduler, NoamLR):
    76   2230.5 MiB      0.0 MiB               scheduler.step()
    77                             
    78   2230.5 MiB      0.0 MiB           n_iter += len(batch)
    79                             
    80                                     # Log and/or add to tensorboard
    81   2230.5 MiB      0.0 MiB           wandb.log({'n_iter': n_iter, 'loss_avg': loss_sum/iter_count})
    82   2230.5 MiB      0.0 MiB           if (n_iter // args.batch_size) % args.log_frequency == 0:
    83                                         lrs = scheduler.get_lr()
    84                                         pnorm = compute_pnorm(model)
    85                                         gnorm = compute_gnorm(model)
    86                                         loss_avg = loss_sum / iter_count
    87                                         loss_sum, iter_count = 0, 0
    88                             
    89                                         lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))
    90                                         debug(f'Loss = {loss_avg:.4e}, PNorm = {pnorm:.4f}, GNorm = {gnorm:.4f}, {lrs_str}')
    91                             
    92                                         if writer is not None:
    93                                             writer.add_scalar('train_loss', loss_avg, n_iter)
    94                                             writer.add_scalar('param_norm', pnorm, n_iter)
    95                                             writer.add_scalar('gradient_norm', gnorm, n_iter)
    96                                             for i, lr in enumerate(lrs):
    97                                                 writer.add_scalar(f'learning_rate_{i}', lr, n_iter)
    98                             
    99   2230.5 MiB      0.0 MiB       return n_iter


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/train.py

Line #    Mem usage    Increment   Line Contents
================================================
    17   2236.6 MiB   2236.6 MiB   @profile
    18                             def train(model: nn.Module,
    19                                       data_loader: MoleculeDataLoader,
    20                                       loss_func: Callable,
    21                                       optimizer: Optimizer,
    22                                       scheduler: _LRScheduler,
    23                                       args: TrainArgs,
    24                                       n_iter: int = 0,
    25                                       logger: logging.Logger = None,
    26                                       writer: SummaryWriter = None) -> int:
    27                                 """
    28                                 Trains a model for an epoch.
    29                             
    30                                 :param model: Model.
    31                                 :param data_loader: A MoleculeDataLoader.
    32                                 :param loss_func: Loss function.
    33                                 :param optimizer: An Optimizer.
    34                                 :param scheduler: A learning rate scheduler.
    35                                 :param args: Arguments.
    36                                 :param n_iter: The number of iterations (training examples) trained on so far.
    37                                 :param logger: A logger for printing intermediate results.
    38                                 :param writer: A tensorboardX SummaryWriter.
    39                                 :return: The total number of iterations (training examples) trained on so far.
    40                                 """
    41   2236.6 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
    42                                 
    43   2236.6 MiB      0.0 MiB       model.train()
    44   2236.6 MiB      0.0 MiB       loss_sum, iter_count = 0, 0
    45                             
    46   2237.1 MiB      0.5 MiB       for batch in tqdm(data_loader, total=len(data_loader)):
    47                                     # Prepare batch
    48                                     batch: MoleculeDataset
    49   2237.1 MiB      0.0 MiB           mol_batch, features_batch, target_batch = batch.batch_graph(), batch.features(), batch.targets()
    50   2237.1 MiB      0.0 MiB           mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])
    51   2237.1 MiB      0.0 MiB           targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])
    52                             
    53                                     # Run model
    54   2237.1 MiB      0.0 MiB           model.zero_grad()


  0% 0/2 [00:00<?, ?it/s][A100% 2/2 [00:00<00:00, 25.45it/s]
Validation auc = 0.504167
  7% 2/30 [00:06<01:53,  4.04s/it]Epoch 2

  0% 0/9 [00:00<?, ?it/s][A
 11% 1/9 [00:00<00:01,  6.90it/s][A
 22% 2/9 [00:00<00:01,  6.22it/s][ALoss = 1.1982e-02, PNorm = 23.3528, GNorm = 0.4333, lr_0 = 9.4019e-04

 33% 3/9 [00:00<00:01,  5.64it/s][A
 44% 4/9 [00:00<00:00,  5.48it/s][A
 56% 5/9 [00:00<00:00,  5.29it/s][A
 67% 6/9 [00:01<00:00,  5.38it/s][A
 78% 7/9 [00:01<00:00,  5.47it/s][A
 89% 8/9 [00:01<00:00,  5.46it/s][A
100% 9/9 [00:01<00:00,  5.98it/s][A100% 9/9 [00:01<00:00,  5.53it/s]    55   2237.1 MiB      0.0 MiB           preds = model(mol_batch, features_batch)
    56                             
    57                                     # Move tensors to correct device
    58   2237.1 MiB      0.0 MiB           mask = mask.to(preds.device)
    59   2237.1 MiB      0.0 MiB           targets = targets.to(preds.device)
    60   2237.1 MiB      0.0 MiB           class_weights = torch.ones(targets.shape, device=preds.device)
    61                             
    62   2237.1 MiB      0.0 MiB           if args.dataset_type == 'multiclass':
    63                                         targets = targets.long()
    64                                         loss = torch.cat([loss_func(preds[:, target_index, :], targets[:, target_index]).unsqueeze(1) for target_index in range(preds.size(1))], dim=1) * class_weights * mask
    65                                     else:
    66   2237.1 MiB      0.0 MiB               loss = loss_func(preds, targets) * class_weights * mask
    67   2237.1 MiB      0.0 MiB           loss = loss.sum() / mask.sum()
    68                             
    69   2237.1 MiB      0.0 MiB           loss_sum += loss.item()
    70   2237.1 MiB      0.0 MiB           iter_count += len(batch)
    71                             
    72   2237.1 MiB      0.0 MiB           loss.backward()
    73   2237.1 MiB      0.0 MiB           optimizer.step()
    74                             
    75   2237.1 MiB      0.0 MiB           if isinstance(scheduler, NoamLR):
    76   2237.1 MiB      0.0 MiB               scheduler.step()
    77                             
    78   2237.1 MiB      0.0 MiB           n_iter += len(batch)
    79                             
    80                                     # Log and/or add to tensorboard
    81   2237.1 MiB      0.0 MiB           wandb.log({'n_iter': n_iter, 'loss_avg': loss_sum/iter_count})
    82   2237.1 MiB      0.0 MiB           if (n_iter // args.batch_size) % args.log_frequency == 0:
    83   2236.6 MiB      0.0 MiB               lrs = scheduler.get_lr()
    84   2236.6 MiB      0.0 MiB               pnorm = compute_pnorm(model)
    85   2236.6 MiB      0.0 MiB               gnorm = compute_gnorm(model)
    86   2236.6 MiB      0.0 MiB               loss_avg = loss_sum / iter_count
    87   2236.6 MiB      0.0 MiB               loss_sum, iter_count = 0, 0
    88                             
    89   2236.6 MiB      0.0 MiB               lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))
    90   2236.6 MiB      0.0 MiB               debug(f'Loss = {loss_avg:.4e}, PNorm = {pnorm:.4f}, GNorm = {gnorm:.4f}, {lrs_str}')
    91                             
    92   2236.6 MiB      0.0 MiB               if writer is not None:
    93   2236.6 MiB      0.0 MiB                   writer.add_scalar('train_loss', loss_avg, n_iter)
    94   2236.6 MiB      0.0 MiB                   writer.add_scalar('param_norm', pnorm, n_iter)
    95   2236.6 MiB      0.0 MiB                   writer.add_scalar('gradient_norm', gnorm, n_iter)
    96   2236.6 MiB      0.0 MiB                   for i, lr in enumerate(lrs):
    97   2236.6 MiB      0.0 MiB                       writer.add_scalar(f'learning_rate_{i}', lr, n_iter)
    98                             
    99   2237.1 MiB      0.0 MiB       return n_iter


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/train.py

Line #    Mem usage    Increment   Line Contents
================================================
    17   2237.1 MiB   2237.1 MiB   @profile
    18                             def train(model: nn.Module,
    19                                       data_loader: MoleculeDataLoader,
    20                                       loss_func: Callable,
    21                                       optimizer: Optimizer,
    22                                       scheduler: _LRScheduler,
    23                                       args: TrainArgs,
    24                                       n_iter: int = 0,
    25                                       logger: logging.Logger = None,
    26                                       writer: SummaryWriter = None) -> int:
    27                                 """
    28                                 Trains a model for an epoch.
    29                             
    30                                 :param model: Model.
    31                                 :param data_loader: A MoleculeDataLoader.
    32                                 :param loss_func: Loss function.
    33                                 :param optimizer: An Optimizer.
    34                                 :param scheduler: A learning rate scheduler.
    35                                 :param args: Arguments.
    36                                 :param n_iter: The number of iterations (training examples) trained on so far.
    37                                 :param logger: A logger for printing intermediate results.
    38                                 :param writer: A tensorboardX SummaryWriter.
    39                                 :return: The total number of iterations (training examples) trained on so far.
    40                                 """
    41   2237.1 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
    42                                 
    43   2237.1 MiB      0.0 MiB       model.train()
    44   2237.1 MiB      0.0 MiB       loss_sum, iter_count = 0, 0
    45                             
    46   2237.9 MiB      0.5 MiB       for batch in tqdm(data_loader, total=len(data_loader)):
    47                                     # Prepare batch
    48                                     batch: MoleculeDataset
    49   2237.9 MiB      0.0 MiB           mol_batch, features_batch, target_batch = batch.batch_graph(), batch.features(), batch.targets()
    50   2237.9 MiB      0.0 MiB           mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])
    51   2237.9 MiB      0.0 MiB           targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])
    52                             
    53                                     # Run model
    54   2237.9 MiB      0.0 MiB           model.zero_grad()
    55   2237.9 MiB      0.0 MiB           preds = model(mol_batch, features_batch)
    56                             
    57                                     # Move tensors to correct device
    58   2237.9 MiB      0.0 MiB           mask = mask.to(preds.device)
    59   2237.9 MiB      0.0 MiB           targets = targets.to(preds.device)
    60   2237.9 MiB      0.0 MiB           class_weights = torch.ones(targets.shape, device=preds.device)
    61                             
    62   2237.9 MiB      0.0 MiB           if args.dataset_type == 'multiclass':
    63                                         targets = targets.long()
    64                                         loss = torch.cat([loss_func(preds[:, target_index, :], targets[:, target_index]).unsqueeze(1) for target_index in range(preds.size(1))], dim=1) * class_weights * mask
    65                                     else:
    66   2237.9 MiB      0.0 MiB               loss = loss_func(preds, targets) * class_weights * mask
    67   2237.9 MiB      0.0 MiB           loss = loss.sum() / mask.sum()
    68                             
    69   2237.9 MiB      0.0 MiB           loss_sum += loss.item()
    70   2237.9 MiB      0.0 MiB           iter_count += len(batch)
    71                             
    72   2237.9 MiB      0.0 MiB           loss.backward()
    73   2237.9 MiB      0.0 MiB           optimizer.step()
    74                             
    75   2237.9 MiB      0.0 MiB           if isinstance(scheduler, NoamLR):
    76   2237.9 MiB      0.0 MiB               scheduler.step()
    77                             
    78   2237.9 MiB      0.0 MiB           n_iter += len(batch)
    79                             
    80                                     # Log and/or add to tensorboard
    81   2237.9 MiB      0.0 MiB           wandb.log({'n_iter': n_iter, 'loss_avg': loss_sum/iter_count})
    82   2237.9 MiB      0.0 MiB           if (n_iter // args.batch_size) % args.log_frequency == 0:


  0% 0/2 [00:00<?, ?it/s][A100% 2/2 [00:00<00:00, 25.35it/s]
Validation auc = 0.504167
 10% 3/30 [00:08<01:32,  3.41s/it]Epoch 3

  0% 0/9 [00:00<?, ?it/s][A
 22% 2/9 [00:00<00:00,  7.54it/s][A
 33% 3/9 [00:00<00:00,  6.89it/s][A
 44% 4/9 [00:00<00:00,  6.40it/s][ALoss = 8.8407e-03, PNorm = 23.5774, GNorm = 0.3587, lr_0 = 8.3967e-04

 56% 5/9 [00:00<00:00,  5.96it/s][A
 67% 6/9 [00:01<00:00,  5.46it/s][A
 78% 7/9 [00:01<00:00,  5.03it/s][A
 89% 8/9 [00:01<00:00,  5.22it/s][A
100% 9/9 [00:01<00:00,  6.02it/s][A100% 9/9 [00:01<00:00,  5.79it/s]

  0% 0/2 [00:00<?, ?it/s][A100% 2/2 [00:00<00:00, 25.26it/s]
Validation auc = 0.472917
 13% 4/30 [00:10<01:16,  2.93s/it]Epoch 4

  0% 0/9 [00:00<?, ?it/s][A
 22% 2/9 [00:00<00:00,  7.01it/s][A
 33% 3/9 [00:00<00:00,  6.53it/s][A
 44% 4/9 [00:00<00:00,  6.00it/s][A
 56% 5/9 [00:00<00:00,  5.50it/s][ALoss = 6.2871e-03, PNorm = 23.7819, GNorm = 0.2861, lr_0 = 7.5764e-04

 67% 6/9 [00:01<00:00,  4.74it/s][A
 78% 7/9 [00:01<00:00,  4.91it/s][A
 89% 8/9 [00:01<00:00,  5.13it/s][A
100% 9/9 [00:01<00:00,  5.80it/s][A100% 9/9 [00:01<00:00,  5.49it/s]    83   2237.3 MiB      0.0 MiB               lrs = scheduler.get_lr()
    84   2237.3 MiB      0.0 MiB               pnorm = compute_pnorm(model)
    85   2237.3 MiB      0.0 MiB               gnorm = compute_gnorm(model)
    86   2237.3 MiB      0.0 MiB               loss_avg = loss_sum / iter_count
    87   2237.3 MiB      0.0 MiB               loss_sum, iter_count = 0, 0
    88                             
    89   2237.3 MiB      0.0 MiB               lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))
    90   2237.3 MiB      0.0 MiB               debug(f'Loss = {loss_avg:.4e}, PNorm = {pnorm:.4f}, GNorm = {gnorm:.4f}, {lrs_str}')
    91                             
    92   2237.3 MiB      0.0 MiB               if writer is not None:
    93   2237.3 MiB      0.0 MiB                   writer.add_scalar('train_loss', loss_avg, n_iter)
    94   2237.3 MiB      0.0 MiB                   writer.add_scalar('param_norm', pnorm, n_iter)
    95   2237.3 MiB      0.0 MiB                   writer.add_scalar('gradient_norm', gnorm, n_iter)
    96   2237.3 MiB      0.0 MiB                   for i, lr in enumerate(lrs):
    97   2237.3 MiB      0.0 MiB                       writer.add_scalar(f'learning_rate_{i}', lr, n_iter)
    98                             
    99   2237.9 MiB      0.0 MiB       return n_iter


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/train.py

Line #    Mem usage    Increment   Line Contents
================================================
    17   2237.9 MiB   2237.9 MiB   @profile
    18                             def train(model: nn.Module,
    19                                       data_loader: MoleculeDataLoader,
    20                                       loss_func: Callable,
    21                                       optimizer: Optimizer,
    22                                       scheduler: _LRScheduler,
    23                                       args: TrainArgs,
    24                                       n_iter: int = 0,
    25                                       logger: logging.Logger = None,
    26                                       writer: SummaryWriter = None) -> int:
    27                                 """
    28                                 Trains a model for an epoch.
    29                             
    30                                 :param model: Model.
    31                                 :param data_loader: A MoleculeDataLoader.
    32                                 :param loss_func: Loss function.
    33                                 :param optimizer: An Optimizer.
    34                                 :param scheduler: A learning rate scheduler.
    35                                 :param args: Arguments.
    36                                 :param n_iter: The number of iterations (training examples) trained on so far.
    37                                 :param logger: A logger for printing intermediate results.
    38                                 :param writer: A tensorboardX SummaryWriter.
    39                                 :return: The total number of iterations (training examples) trained on so far.
    40                                 """
    41   2237.9 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
    42                                 
    43   2237.9 MiB      0.0 MiB       model.train()
    44   2237.9 MiB      0.0 MiB       loss_sum, iter_count = 0, 0
    45                             
    46   2238.1 MiB      0.3 MiB       for batch in tqdm(data_loader, total=len(data_loader)):
    47                                     # Prepare batch
    48                                     batch: MoleculeDataset
    49   2238.1 MiB      0.0 MiB           mol_batch, features_batch, target_batch = batch.batch_graph(), batch.features(), batch.targets()
    50   2238.1 MiB      0.0 MiB           mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])
    51   2238.1 MiB      0.0 MiB           targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])
    52                             
    53                                     # Run model
    54   2238.1 MiB      0.0 MiB           model.zero_grad()
    55   2238.1 MiB      0.0 MiB           preds = model(mol_batch, features_batch)
    56                             
    57                                     # Move tensors to correct device
    58   2238.1 MiB      0.0 MiB           mask = mask.to(preds.device)
    59   2238.1 MiB      0.0 MiB           targets = targets.to(preds.device)
    60   2238.1 MiB      0.0 MiB           class_weights = torch.ones(targets.shape, device=preds.device)
    61                             
    62   2238.1 MiB      0.0 MiB           if args.dataset_type == 'multiclass':
    63                                         targets = targets.long()
    64                                         loss = torch.cat([loss_func(preds[:, target_index, :], targets[:, target_index]).unsqueeze(1) for target_index in range(preds.size(1))], dim=1) * class_weights * mask
    65                                     else:
    66   2238.1 MiB      0.0 MiB               loss = loss_func(preds, targets) * class_weights * mask
    67   2238.1 MiB      0.0 MiB           loss = loss.sum() / mask.sum()
    68                             
    69   2238.1 MiB      0.0 MiB           loss_sum += loss.item()
    70   2238.1 MiB      0.0 MiB           iter_count += len(batch)
    71                             
    72   2238.1 MiB      0.0 MiB           loss.backward()
    73   2238.1 MiB      0.0 MiB           optimizer.step()
    74                             
    75   2238.1 MiB      0.0 MiB           if isinstance(scheduler, NoamLR):
    76   2238.1 MiB      0.0 MiB               scheduler.step()
    77                             
    78   2238.1 MiB      0.0 MiB           n_iter += len(batch)
    79                             
    80                                     # Log and/or add to tensorboard
    81   2238.1 MiB      0.0 MiB           wandb.log({'n_iter': n_iter, 'loss_avg': loss_sum/iter_count})
    82   2238.1 MiB      0.0 MiB           if (n_iter // args.batch_size) % args.log_frequency == 0:
    83   2238.1 MiB      0.0 MiB               lrs = scheduler.get_lr()
    84   2238.1 MiB      0.0 MiB               pnorm = compute_pnorm(model)
    85   2238.1 MiB      0.0 MiB               gnorm = compute_gnorm(model)
    86   2238.1 MiB      0.0 MiB               loss_avg = loss_sum / iter_count
    87   2238.1 MiB      0.0 MiB               loss_sum, iter_count = 0, 0
    88                             
    89   2238.1 MiB      0.0 MiB               lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))
    90   2238.1 MiB      0.0 MiB               debug(f'Loss = {loss_avg:.4e}, PNorm = {pnorm:.4f}, GNorm = {gnorm:.4f}, {lrs_str}')
    91                             
    92   2238.1 MiB      0.0 MiB               if writer is not None:
    93   2238.1 MiB      0.0 MiB                   writer.add_scalar('train_loss', loss_avg, n_iter)
    94   2238.1 MiB      0.0 MiB                   writer.add_scalar('param_norm', pnorm, n_iter)
    95   2238.1 MiB      0.0 MiB                   writer.add_scalar('gradient_norm', gnorm, n_iter)
    96   2238.1 MiB      0.0 MiB                   for i, lr in enumerate(lrs):
    97   2238.1 MiB      0.0 MiB                       writer.add_scalar(f'learning_rate_{i}', lr, n_iter)
    98                             
    99   2238.1 MiB      0.0 MiB       return n_iter


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/train.py

Line #    Mem usage    Increment   Line Contents
================================================
    17   2238.1 MiB   2238.1 MiB   @profile
    18                             def train(model: nn.Module,
    19                                       data_loader: MoleculeDataLoader,
    20                                       loss_func: Callable,
    21                                       optimizer: Optimizer,
    22                                       scheduler: _LRScheduler,


  0% 0/2 [00:00<?, ?it/s][A100% 2/2 [00:00<00:00, 25.07it/s]
Validation auc = 0.504167
 17% 5/30 [00:12<01:05,  2.63s/it]Epoch 5

  0% 0/9 [00:00<?, ?it/s][A
 22% 2/9 [00:00<00:00,  7.62it/s][A
 33% 3/9 [00:00<00:00,  6.26it/s][A
 44% 4/9 [00:00<00:00,  5.88it/s][A
 56% 5/9 [00:00<00:00,  5.79it/s][A
 67% 6/9 [00:01<00:00,  5.79it/s][A
 78% 7/9 [00:01<00:00,  5.73it/s][ALoss = 4.8472e-03, PNorm = 23.9859, GNorm = 0.2339, lr_0 = 6.7664e-04

 89% 8/9 [00:01<00:00,  5.39it/s][A
100% 9/9 [00:01<00:00,  6.21it/s][A100% 9/9 [00:01<00:00,  5.88it/s]    23                                       args: TrainArgs,
    24                                       n_iter: int = 0,
    25                                       logger: logging.Logger = None,
    26                                       writer: SummaryWriter = None) -> int:
    27                                 """
    28                                 Trains a model for an epoch.
    29                             
    30                                 :param model: Model.
    31                                 :param data_loader: A MoleculeDataLoader.
    32                                 :param loss_func: Loss function.
    33                                 :param optimizer: An Optimizer.
    34                                 :param scheduler: A learning rate scheduler.
    35                                 :param args: Arguments.
    36                                 :param n_iter: The number of iterations (training examples) trained on so far.
    37                                 :param logger: A logger for printing intermediate results.
    38                                 :param writer: A tensorboardX SummaryWriter.
    39                                 :return: The total number of iterations (training examples) trained on so far.
    40                                 """
    41   2238.1 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
    42                                 
    43   2238.1 MiB      0.0 MiB       model.train()
    44   2238.1 MiB      0.0 MiB       loss_sum, iter_count = 0, 0
    45                             
    46   2238.1 MiB      0.0 MiB       for batch in tqdm(data_loader, total=len(data_loader)):
    47                                     # Prepare batch
    48                                     batch: MoleculeDataset
    49   2238.1 MiB      0.0 MiB           mol_batch, features_batch, target_batch = batch.batch_graph(), batch.features(), batch.targets()
    50   2238.1 MiB      0.0 MiB           mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])
    51   2238.1 MiB      0.0 MiB           targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])
    52                             
    53                                     # Run model
    54   2238.1 MiB      0.0 MiB           model.zero_grad()
    55   2238.1 MiB      0.0 MiB           preds = model(mol_batch, features_batch)
    56                             
    57                                     # Move tensors to correct device
    58   2238.1 MiB      0.0 MiB           mask = mask.to(preds.device)
    59   2238.1 MiB      0.0 MiB           targets = targets.to(preds.device)
    60   2238.1 MiB      0.0 MiB           class_weights = torch.ones(targets.shape, device=preds.device)
    61                             
    62   2238.1 MiB      0.0 MiB           if args.dataset_type == 'multiclass':
    63                                         targets = targets.long()
    64                                         loss = torch.cat([loss_func(preds[:, target_index, :], targets[:, target_index]).unsqueeze(1) for target_index in range(preds.size(1))], dim=1) * class_weights * mask
    65                                     else:
    66   2238.1 MiB      0.0 MiB               loss = loss_func(preds, targets) * class_weights * mask
    67   2238.1 MiB      0.0 MiB           loss = loss.sum() / mask.sum()
    68                             
    69   2238.1 MiB      0.0 MiB           loss_sum += loss.item()
    70   2238.1 MiB      0.0 MiB           iter_count += len(batch)
    71                             
    72   2238.1 MiB      0.0 MiB           loss.backward()
    73   2238.1 MiB      0.0 MiB           optimizer.step()
    74                             
    75   2238.1 MiB      0.0 MiB           if isinstance(scheduler, NoamLR):
    76   2238.1 MiB      0.0 MiB               scheduler.step()
    77                             
    78   2238.1 MiB      0.0 MiB           n_iter += len(batch)
    79                             
    80                                     # Log and/or add to tensorboard
    81   2238.1 MiB      0.0 MiB           wandb.log({'n_iter': n_iter, 'loss_avg': loss_sum/iter_count})
    82   2238.1 MiB      0.0 MiB           if (n_iter // args.batch_size) % args.log_frequency == 0:
    83   2238.1 MiB      0.0 MiB               lrs = scheduler.get_lr()
    84   2238.1 MiB      0.0 MiB               pnorm = compute_pnorm(model)
    85   2238.1 MiB      0.0 MiB               gnorm = compute_gnorm(model)
    86   2238.1 MiB      0.0 MiB               loss_avg = loss_sum / iter_count
    87   2238.1 MiB      0.0 MiB               loss_sum, iter_count = 0, 0
    88                             
    89   2238.1 MiB      0.0 MiB               lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))
    90   2238.1 MiB      0.0 MiB               debug(f'Loss = {loss_avg:.4e}, PNorm = {pnorm:.4f}, GNorm = {gnorm:.4f}, {lrs_str}')
    91                             
    92   2238.1 MiB      0.0 MiB               if writer is not None:
    93   2238.1 MiB      0.0 MiB                   writer.add_scalar('train_loss', loss_avg, n_iter)
    94   2238.1 MiB      0.0 MiB                   writer.add_scalar('param_norm', pnorm, n_iter)
    95   2238.1 MiB      0.0 MiB                   writer.add_scalar('gradient_norm', gnorm, n_iter)
    96   2238.1 MiB      0.0 MiB                   for i, lr in enumerate(lrs):
    97   2238.1 MiB      0.0 MiB                       writer.add_scalar(f'learning_rate_{i}', lr, n_iter)
    98                             
    99   2238.1 MiB      0.0 MiB       return n_iter


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/train.py

Line #    Mem usage    Increment   Line Contents
================================================
    17   2238.1 MiB   2238.1 MiB   @profile
    18                             def train(model: nn.Module,
    19                                       data_loader: MoleculeDataLoader,
    20                                       loss_func: Callable,
    21                                       optimizer: Optimizer,
    22                                       scheduler: _LRScheduler,
    23                                       args: TrainArgs,
    24                                       n_iter: int = 0,
    25                                       logger: logging.Logger = None,
    26                                       writer: SummaryWriter = None) -> int:
    27                                 """
    28                                 Trains a model for an epoch.
    29                             
    30                                 :param model: Model.
    31                                 :param data_loader: A MoleculeDataLoader.
    32                                 :param loss_func: Loss function.
    33                                 :param optimizer: An Optimizer.
    34                                 :param scheduler: A learning rate scheduler.
    35                                 :param args: Arguments.
    36                                 :param n_iter: The number of iterations (training examples) trained on so far.
    37                                 :param logger: A logger for printing intermediate results.
    38                                 :param writer: A tensorboardX SummaryWriter.
    39                                 :return: The total number of iterations (training examples) trained on so far.
    40                                 """
    41   2238.1 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
    42                                 
    43   2238.1 MiB      0.0 MiB       model.train()
    44   2238.1 MiB      0.0 MiB       loss_sum, iter_count = 0, 0
    45                             
    46   2238.1 MiB      0.0 MiB       for batch in tqdm(data_loader, total=len(data_loader)):
    47                                     # Prepare batch
    48                                     batch: MoleculeDataset
    49   2238.1 MiB      0.0 MiB           mol_batch, features_batch, target_batch = batch.batch_graph(), batch.features(), batch.targets()


  0% 0/2 [00:00<?, ?it/s][A100% 2/2 [00:00<00:00, 24.74it/s]
Validation auc = 0.472917
 20% 6/30 [00:14<00:57,  2.39s/it]Epoch 6

  0% 0/9 [00:00<?, ?it/s][A
 22% 2/9 [00:00<00:00,  7.25it/s][A
 33% 3/9 [00:00<00:00,  6.66it/s][A
 44% 4/9 [00:00<00:00,  6.32it/s][A
 56% 5/9 [00:00<00:00,  6.03it/s][A
 67% 6/9 [00:00<00:00,  5.95it/s][A
 78% 7/9 [00:01<00:00,  5.82it/s][A
 89% 8/9 [00:01<00:00,  5.80it/s][A
100% 9/9 [00:01<00:00,  6.52it/s][A100% 9/9 [00:01<00:00,  6.19it/s]    50   2238.1 MiB      0.0 MiB           mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])
    51   2238.1 MiB      0.0 MiB           targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])
    52                             
    53                                     # Run model
    54   2238.1 MiB      0.0 MiB           model.zero_grad()
    55   2238.1 MiB      0.0 MiB           preds = model(mol_batch, features_batch)
    56                             
    57                                     # Move tensors to correct device
    58   2238.1 MiB      0.0 MiB           mask = mask.to(preds.device)
    59   2238.1 MiB      0.0 MiB           targets = targets.to(preds.device)
    60   2238.1 MiB      0.0 MiB           class_weights = torch.ones(targets.shape, device=preds.device)
    61                             
    62   2238.1 MiB      0.0 MiB           if args.dataset_type == 'multiclass':
    63                                         targets = targets.long()
    64                                         loss = torch.cat([loss_func(preds[:, target_index, :], targets[:, target_index]).unsqueeze(1) for target_index in range(preds.size(1))], dim=1) * class_weights * mask
    65                                     else:
    66   2238.1 MiB      0.0 MiB               loss = loss_func(preds, targets) * class_weights * mask
    67   2238.1 MiB      0.0 MiB           loss = loss.sum() / mask.sum()
    68                             
    69   2238.1 MiB      0.0 MiB           loss_sum += loss.item()
    70   2238.1 MiB      0.0 MiB           iter_count += len(batch)
    71                             
    72   2238.1 MiB      0.0 MiB           loss.backward()
    73   2238.1 MiB      0.0 MiB           optimizer.step()
    74                             
    75   2238.1 MiB      0.0 MiB           if isinstance(scheduler, NoamLR):
    76   2238.1 MiB      0.0 MiB               scheduler.step()
    77                             
    78   2238.1 MiB      0.0 MiB           n_iter += len(batch)
    79                             
    80                                     # Log and/or add to tensorboard
    81   2238.1 MiB      0.0 MiB           wandb.log({'n_iter': n_iter, 'loss_avg': loss_sum/iter_count})
    82   2238.1 MiB      0.0 MiB           if (n_iter // args.batch_size) % args.log_frequency == 0:
    83   2238.1 MiB      0.0 MiB               lrs = scheduler.get_lr()
    84   2238.1 MiB      0.0 MiB               pnorm = compute_pnorm(model)
    85   2238.1 MiB      0.0 MiB               gnorm = compute_gnorm(model)
    86   2238.1 MiB      0.0 MiB               loss_avg = loss_sum / iter_count
    87   2238.1 MiB      0.0 MiB               loss_sum, iter_count = 0, 0
    88                             
    89   2238.1 MiB      0.0 MiB               lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))
    90   2238.1 MiB      0.0 MiB               debug(f'Loss = {loss_avg:.4e}, PNorm = {pnorm:.4f}, GNorm = {gnorm:.4f}, {lrs_str}')
    91                             
    92   2238.1 MiB      0.0 MiB               if writer is not None:
    93   2238.1 MiB      0.0 MiB                   writer.add_scalar('train_loss', loss_avg, n_iter)
    94   2238.1 MiB      0.0 MiB                   writer.add_scalar('param_norm', pnorm, n_iter)
    95   2238.1 MiB      0.0 MiB                   writer.add_scalar('gradient_norm', gnorm, n_iter)
    96   2238.1 MiB      0.0 MiB                   for i, lr in enumerate(lrs):
    97   2238.1 MiB      0.0 MiB                       writer.add_scalar(f'learning_rate_{i}', lr, n_iter)
    98                             
    99   2238.1 MiB      0.0 MiB       return n_iter


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/train.py

Line #    Mem usage    Increment   Line Contents
================================================
    17   2238.1 MiB   2238.1 MiB   @profile
    18                             def train(model: nn.Module,
    19                                       data_loader: MoleculeDataLoader,
    20                                       loss_func: Callable,
    21                                       optimizer: Optimizer,
    22                                       scheduler: _LRScheduler,
    23                                       args: TrainArgs,
    24                                       n_iter: int = 0,
    25                                       logger: logging.Logger = None,
    26                                       writer: SummaryWriter = None) -> int:
    27                                 """
    28                                 Trains a model for an epoch.
    29                             
    30                                 :param model: Model.
    31                                 :param data_loader: A MoleculeDataLoader.
    32                                 :param loss_func: Loss function.
    33                                 :param optimizer: An Optimizer.
    34                                 :param scheduler: A learning rate scheduler.
    35                                 :param args: Arguments.
    36                                 :param n_iter: The number of iterations (training examples) trained on so far.
    37                                 :param logger: A logger for printing intermediate results.
    38                                 :param writer: A tensorboardX SummaryWriter.
    39                                 :return: The total number of iterations (training examples) trained on so far.
    40                                 """
    41   2238.1 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
    42                                 
    43   2238.1 MiB      0.0 MiB       model.train()
    44   2238.1 MiB      0.0 MiB       loss_sum, iter_count = 0, 0
    45                             
    46   2238.1 MiB      0.0 MiB       for batch in tqdm(data_loader, total=len(data_loader)):
    47                                     # Prepare batch
    48                                     batch: MoleculeDataset
    49   2238.1 MiB      0.0 MiB           mol_batch, features_batch, target_batch = batch.batch_graph(), batch.features(), batch.targets()
    50   2238.1 MiB      0.0 MiB           mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])
    51   2238.1 MiB      0.0 MiB           targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])
    52                             
    53                                     # Run model
    54   2238.1 MiB      0.0 MiB           model.zero_grad()
    55   2238.1 MiB      0.0 MiB           preds = model(mol_batch, features_batch)
    56                             
    57                                     # Move tensors to correct device
    58   2238.1 MiB      0.0 MiB           mask = mask.to(preds.device)
    59   2238.1 MiB      0.0 MiB           targets = targets.to(preds.device)
    60   2238.1 MiB      0.0 MiB           class_weights = torch.ones(targets.shape, device=preds.device)
    61                             
    62   2238.1 MiB      0.0 MiB           if args.dataset_type == 'multiclass':
    63                                         targets = targets.long()
    64                                         loss = torch.cat([loss_func(preds[:, target_index, :], targets[:, target_index]).unsqueeze(1) for target_index in range(preds.size(1))], dim=1) * class_weights * mask
    65                                     else:
    66   2238.1 MiB      0.0 MiB               loss = loss_func(preds, targets) * class_weights * mask
    67   2238.1 MiB      0.0 MiB           loss = loss.sum() / mask.sum()
    68                             
    69   2238.1 MiB      0.0 MiB           loss_sum += loss.item()
    70   2238.1 MiB      0.0 MiB           iter_count += len(batch)
    71                             
    72   2238.1 MiB      0.0 MiB           loss.backward()
    73   2238.1 MiB      0.0 MiB           optimizer.step()
    74                             
    75   2238.1 MiB      0.0 MiB           if isinstance(scheduler, NoamLR):


  0% 0/2 [00:00<?, ?it/s][A100% 2/2 [00:00<00:00, 20.19it/s]
Validation auc = 0.472917
 23% 7/30 [00:16<00:50,  2.21s/it]Epoch 7

  0% 0/9 [00:00<?, ?it/s][ALoss = 3.5100e-03, PNorm = 24.1648, GNorm = 0.3166, lr_0 = 6.0430e-04

 11% 1/9 [00:00<00:01,  7.84it/s][A
 22% 2/9 [00:00<00:01,  6.89it/s][A
 33% 3/9 [00:00<00:00,  6.48it/s][A
 44% 4/9 [00:00<00:00,  6.18it/s][A
 56% 5/9 [00:00<00:00,  5.52it/s][A
 67% 6/9 [00:01<00:00,  5.32it/s][A
 78% 7/9 [00:01<00:00,  5.41it/s][A
 89% 8/9 [00:01<00:00,  5.06it/s][A
100% 9/9 [00:01<00:00,  5.81it/s][A100% 9/9 [00:01<00:00,  5.56it/s]

  0% 0/2 [00:00<?, ?it/s][A100% 2/2 [00:00<00:00, 22.82it/s]
Validation auc = 0.497917
 27% 8/30 [00:18<00:46,  2.12s/it]Epoch 8

  0% 0/9 [00:00<?, ?it/s][ALoss = 2.8624e-03, PNorm = 24.3077, GNorm = 0.1816, lr_0 = 5.4526e-04

 22% 2/9 [00:00<00:00,  7.03it/s][A
 33% 3/9 [00:00<00:00,  6.61it/s][A
 44% 4/9 [00:00<00:00,  5.74it/s][A
 56% 5/9 [00:00<00:00,  5.35it/s][A
 67% 6/9 [00:01<00:00,  5.44it/s][A
 78% 7/9 [00:01<00:00,  5.36it/s][A
 89% 8/9 [00:01<00:00,  5.47it/s][A
100% 9/9 [00:01<00:00,  5.46it/s][A100% 9/9 [00:01<00:00,  5.52it/s]    76   2238.1 MiB      0.0 MiB               scheduler.step()
    77                             
    78   2238.1 MiB      0.0 MiB           n_iter += len(batch)
    79                             
    80                                     # Log and/or add to tensorboard
    81   2238.1 MiB      0.0 MiB           wandb.log({'n_iter': n_iter, 'loss_avg': loss_sum/iter_count})
    82   2238.1 MiB      0.0 MiB           if (n_iter // args.batch_size) % args.log_frequency == 0:
    83                                         lrs = scheduler.get_lr()
    84                                         pnorm = compute_pnorm(model)
    85                                         gnorm = compute_gnorm(model)
    86                                         loss_avg = loss_sum / iter_count
    87                                         loss_sum, iter_count = 0, 0
    88                             
    89                                         lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))
    90                                         debug(f'Loss = {loss_avg:.4e}, PNorm = {pnorm:.4f}, GNorm = {gnorm:.4f}, {lrs_str}')
    91                             
    92                                         if writer is not None:
    93                                             writer.add_scalar('train_loss', loss_avg, n_iter)
    94                                             writer.add_scalar('param_norm', pnorm, n_iter)
    95                                             writer.add_scalar('gradient_norm', gnorm, n_iter)
    96                                             for i, lr in enumerate(lrs):
    97                                                 writer.add_scalar(f'learning_rate_{i}', lr, n_iter)
    98                             
    99   2238.1 MiB      0.0 MiB       return n_iter


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/train.py

Line #    Mem usage    Increment   Line Contents
================================================
    17   2238.1 MiB   2238.1 MiB   @profile
    18                             def train(model: nn.Module,
    19                                       data_loader: MoleculeDataLoader,
    20                                       loss_func: Callable,
    21                                       optimizer: Optimizer,
    22                                       scheduler: _LRScheduler,
    23                                       args: TrainArgs,
    24                                       n_iter: int = 0,
    25                                       logger: logging.Logger = None,
    26                                       writer: SummaryWriter = None) -> int:
    27                                 """
    28                                 Trains a model for an epoch.
    29                             
    30                                 :param model: Model.
    31                                 :param data_loader: A MoleculeDataLoader.
    32                                 :param loss_func: Loss function.
    33                                 :param optimizer: An Optimizer.
    34                                 :param scheduler: A learning rate scheduler.
    35                                 :param args: Arguments.
    36                                 :param n_iter: The number of iterations (training examples) trained on so far.
    37                                 :param logger: A logger for printing intermediate results.
    38                                 :param writer: A tensorboardX SummaryWriter.
    39                                 :return: The total number of iterations (training examples) trained on so far.
    40                                 """
    41   2238.1 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
    42                                 
    43   2238.1 MiB      0.0 MiB       model.train()
    44   2238.1 MiB      0.0 MiB       loss_sum, iter_count = 0, 0
    45                             
    46   2238.1 MiB      0.0 MiB       for batch in tqdm(data_loader, total=len(data_loader)):
    47                                     # Prepare batch
    48                                     batch: MoleculeDataset
    49   2238.1 MiB      0.0 MiB           mol_batch, features_batch, target_batch = batch.batch_graph(), batch.features(), batch.targets()
    50   2238.1 MiB      0.0 MiB           mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])
    51   2238.1 MiB      0.0 MiB           targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])
    52                             
    53                                     # Run model
    54   2238.1 MiB      0.0 MiB           model.zero_grad()
    55   2238.1 MiB      0.0 MiB           preds = model(mol_batch, features_batch)
    56                             
    57                                     # Move tensors to correct device
    58   2238.1 MiB      0.0 MiB           mask = mask.to(preds.device)
    59   2238.1 MiB      0.0 MiB           targets = targets.to(preds.device)
    60   2238.1 MiB      0.0 MiB           class_weights = torch.ones(targets.shape, device=preds.device)
    61                             
    62   2238.1 MiB      0.0 MiB           if args.dataset_type == 'multiclass':
    63                                         targets = targets.long()
    64                                         loss = torch.cat([loss_func(preds[:, target_index, :], targets[:, target_index]).unsqueeze(1) for target_index in range(preds.size(1))], dim=1) * class_weights * mask
    65                                     else:
    66   2238.1 MiB      0.0 MiB               loss = loss_func(preds, targets) * class_weights * mask
    67   2238.1 MiB      0.0 MiB           loss = loss.sum() / mask.sum()
    68                             
    69   2238.1 MiB      0.0 MiB           loss_sum += loss.item()
    70   2238.1 MiB      0.0 MiB           iter_count += len(batch)
    71                             
    72   2238.1 MiB      0.0 MiB           loss.backward()
    73   2238.1 MiB      0.0 MiB           optimizer.step()
    74                             
    75   2238.1 MiB      0.0 MiB           if isinstance(scheduler, NoamLR):
    76   2238.1 MiB      0.0 MiB               scheduler.step()
    77                             
    78   2238.1 MiB      0.0 MiB           n_iter += len(batch)
    79                             
    80                                     # Log and/or add to tensorboard
    81   2238.1 MiB      0.0 MiB           wandb.log({'n_iter': n_iter, 'loss_avg': loss_sum/iter_count})
    82   2238.1 MiB      0.0 MiB           if (n_iter // args.batch_size) % args.log_frequency == 0:
    83   2238.1 MiB      0.0 MiB               lrs = scheduler.get_lr()
    84   2238.1 MiB      0.0 MiB               pnorm = compute_pnorm(model)
    85   2238.1 MiB      0.0 MiB               gnorm = compute_gnorm(model)
    86   2238.1 MiB      0.0 MiB               loss_avg = loss_sum / iter_count
    87   2238.1 MiB      0.0 MiB               loss_sum, iter_count = 0, 0
    88                             
    89   2238.1 MiB      0.0 MiB               lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))
    90   2238.1 MiB      0.0 MiB               debug(f'Loss = {loss_avg:.4e}, PNorm = {pnorm:.4f}, GNorm = {gnorm:.4f}, {lrs_str}')
    91                             
    92   2238.1 MiB      0.0 MiB               if writer is not None:
    93   2238.1 MiB      0.0 MiB                   writer.add_scalar('train_loss', loss_avg, n_iter)
    94   2238.1 MiB      0.0 MiB                   writer.add_scalar('param_norm', pnorm, n_iter)
    95   2238.1 MiB      0.0 MiB                   writer.add_scalar('gradient_norm', gnorm, n_iter)
    96   2238.1 MiB      0.0 MiB                   for i, lr in enumerate(lrs):
    97   2238.1 MiB      0.0 MiB                       writer.add_scalar(f'learning_rate_{i}', lr, n_iter)
    98                             
    99   2238.1 MiB      0.0 MiB       return n_iter


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/train.py

Line #    Mem usage    Increment   Line Contents


  0% 0/2 [00:00<?, ?it/s][A100% 2/2 [00:00<00:00, 24.54it/s]
Validation auc = 0.522917
 30% 9/30 [00:20<00:43,  2.06s/it]Epoch 9

  0% 0/9 [00:00<?, ?it/s][A
 22% 2/9 [00:00<00:00,  7.54it/s][ALoss = 3.0537e-03, PNorm = 24.4324, GNorm = 0.3198, lr_0 = 4.9200e-04

 33% 3/9 [00:00<00:01,  5.83it/s][A
 44% 4/9 [00:00<00:00,  5.39it/s][A
 56% 5/9 [00:00<00:00,  5.47it/s][A
 67% 6/9 [00:01<00:00,  5.15it/s][A
 78% 7/9 [00:01<00:00,  5.29it/s][A
 89% 8/9 [00:01<00:00,  5.04it/s][A
100% 9/9 [00:01<00:00,  5.84it/s][A100% 9/9 [00:01<00:00,  5.45it/s]================================================
    17   2238.1 MiB   2238.1 MiB   @profile
    18                             def train(model: nn.Module,
    19                                       data_loader: MoleculeDataLoader,
    20                                       loss_func: Callable,
    21                                       optimizer: Optimizer,
    22                                       scheduler: _LRScheduler,
    23                                       args: TrainArgs,
    24                                       n_iter: int = 0,
    25                                       logger: logging.Logger = None,
    26                                       writer: SummaryWriter = None) -> int:
    27                                 """
    28                                 Trains a model for an epoch.
    29                             
    30                                 :param model: Model.
    31                                 :param data_loader: A MoleculeDataLoader.
    32                                 :param loss_func: Loss function.
    33                                 :param optimizer: An Optimizer.
    34                                 :param scheduler: A learning rate scheduler.
    35                                 :param args: Arguments.
    36                                 :param n_iter: The number of iterations (training examples) trained on so far.
    37                                 :param logger: A logger for printing intermediate results.
    38                                 :param writer: A tensorboardX SummaryWriter.
    39                                 :return: The total number of iterations (training examples) trained on so far.
    40                                 """
    41   2238.1 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
    42                                 
    43   2238.1 MiB      0.0 MiB       model.train()
    44   2238.1 MiB      0.0 MiB       loss_sum, iter_count = 0, 0
    45                             
    46   2238.1 MiB      0.0 MiB       for batch in tqdm(data_loader, total=len(data_loader)):
    47                                     # Prepare batch
    48                                     batch: MoleculeDataset
    49   2238.1 MiB      0.0 MiB           mol_batch, features_batch, target_batch = batch.batch_graph(), batch.features(), batch.targets()
    50   2238.1 MiB      0.0 MiB           mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])
    51   2238.1 MiB      0.0 MiB           targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])
    52                             
    53                                     # Run model
    54   2238.1 MiB      0.0 MiB           model.zero_grad()
    55   2238.1 MiB      0.0 MiB           preds = model(mol_batch, features_batch)
    56                             
    57                                     # Move tensors to correct device
    58   2238.1 MiB      0.0 MiB           mask = mask.to(preds.device)
    59   2238.1 MiB      0.0 MiB           targets = targets.to(preds.device)
    60   2238.1 MiB      0.0 MiB           class_weights = torch.ones(targets.shape, device=preds.device)
    61                             
    62   2238.1 MiB      0.0 MiB           if args.dataset_type == 'multiclass':
    63                                         targets = targets.long()
    64                                         loss = torch.cat([loss_func(preds[:, target_index, :], targets[:, target_index]).unsqueeze(1) for target_index in range(preds.size(1))], dim=1) * class_weights * mask
    65                                     else:
    66   2238.1 MiB      0.0 MiB               loss = loss_func(preds, targets) * class_weights * mask
    67   2238.1 MiB      0.0 MiB           loss = loss.sum() / mask.sum()
    68                             
    69   2238.1 MiB      0.0 MiB           loss_sum += loss.item()
    70   2238.1 MiB      0.0 MiB           iter_count += len(batch)
    71                             
    72   2238.1 MiB      0.0 MiB           loss.backward()
    73   2238.1 MiB      0.0 MiB           optimizer.step()
    74                             
    75   2238.1 MiB      0.0 MiB           if isinstance(scheduler, NoamLR):
    76   2238.1 MiB      0.0 MiB               scheduler.step()
    77                             
    78   2238.1 MiB      0.0 MiB           n_iter += len(batch)
    79                             
    80                                     # Log and/or add to tensorboard
    81   2238.1 MiB      0.0 MiB           wandb.log({'n_iter': n_iter, 'loss_avg': loss_sum/iter_count})
    82   2238.1 MiB      0.0 MiB           if (n_iter // args.batch_size) % args.log_frequency == 0:
    83   2238.1 MiB      0.0 MiB               lrs = scheduler.get_lr()
    84   2238.1 MiB      0.0 MiB               pnorm = compute_pnorm(model)
    85   2238.1 MiB      0.0 MiB               gnorm = compute_gnorm(model)
    86   2238.1 MiB      0.0 MiB               loss_avg = loss_sum / iter_count
    87   2238.1 MiB      0.0 MiB               loss_sum, iter_count = 0, 0
    88                             
    89   2238.1 MiB      0.0 MiB               lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))
    90   2238.1 MiB      0.0 MiB               debug(f'Loss = {loss_avg:.4e}, PNorm = {pnorm:.4f}, GNorm = {gnorm:.4f}, {lrs_str}')
    91                             
    92   2238.1 MiB      0.0 MiB               if writer is not None:
    93   2238.1 MiB      0.0 MiB                   writer.add_scalar('train_loss', loss_avg, n_iter)
    94   2238.1 MiB      0.0 MiB                   writer.add_scalar('param_norm', pnorm, n_iter)
    95   2238.1 MiB      0.0 MiB                   writer.add_scalar('gradient_norm', gnorm, n_iter)
    96   2238.1 MiB      0.0 MiB                   for i, lr in enumerate(lrs):
    97   2238.1 MiB      0.0 MiB                       writer.add_scalar(f'learning_rate_{i}', lr, n_iter)
    98                             
    99   2238.1 MiB      0.0 MiB       return n_iter


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/train.py

Line #    Mem usage    Increment   Line Contents
================================================
    17   2238.1 MiB   2238.1 MiB   @profile
    18                             def train(model: nn.Module,
    19                                       data_loader: MoleculeDataLoader,
    20                                       loss_func: Callable,
    21                                       optimizer: Optimizer,
    22                                       scheduler: _LRScheduler,
    23                                       args: TrainArgs,
    24                                       n_iter: int = 0,
    25                                       logger: logging.Logger = None,
    26                                       writer: SummaryWriter = None) -> int:
    27                                 """
    28                                 Trains a model for an epoch.
    29                             
    30                                 :param model: Model.
    31                                 :param data_loader: A MoleculeDataLoader.
    32                                 :param loss_func: Loss function.
    33                                 :param optimizer: An Optimizer.
    34                                 :param scheduler: A learning rate scheduler.
    35                                 :param args: Arguments.
    36                                 :param n_iter: The number of iterations (training examples) trained on so far.
    37                                 :param logger: A logger for printing intermediate results.
    38                                 :param writer: A tensorboardX SummaryWriter.
    39                                 :return: The total number of iterations (training examples) trained on so far.
    40                                 """
    41   2238.1 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
    42                                 
    43   2238.1 MiB      0.0 MiB       model.train()


  0% 0/2 [00:00<?, ?it/s][A100% 2/2 [00:00<00:00, 25.32it/s]
Validation auc = 0.547917
 33% 10/30 [00:22<00:40,  2.03s/it]Epoch 10

  0% 0/9 [00:00<?, ?it/s][A
 22% 2/9 [00:00<00:00,  7.52it/s][A
 33% 3/9 [00:00<00:00,  6.35it/s][A
 44% 4/9 [00:00<00:00,  6.23it/s][ALoss = 2.3860e-03, PNorm = 24.5508, GNorm = 0.1922, lr_0 = 4.3940e-04

 56% 5/9 [00:00<00:00,  5.96it/s][A
 67% 6/9 [00:01<00:00,  5.87it/s][A
 78% 7/9 [00:01<00:00,  5.78it/s][A
 89% 8/9 [00:01<00:00,  5.56it/s][A
100% 9/9 [00:01<00:00,  6.18it/s][A100% 9/9 [00:01<00:00,  5.98it/s]    44   2238.1 MiB      0.0 MiB       loss_sum, iter_count = 0, 0
    45                             
    46   2238.1 MiB      0.0 MiB       for batch in tqdm(data_loader, total=len(data_loader)):
    47                                     # Prepare batch
    48                                     batch: MoleculeDataset
    49   2238.1 MiB      0.0 MiB           mol_batch, features_batch, target_batch = batch.batch_graph(), batch.features(), batch.targets()
    50   2238.1 MiB      0.0 MiB           mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])
    51   2238.1 MiB      0.0 MiB           targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])
    52                             
    53                                     # Run model
    54   2238.1 MiB      0.0 MiB           model.zero_grad()
    55   2238.1 MiB      0.0 MiB           preds = model(mol_batch, features_batch)
    56                             
    57                                     # Move tensors to correct device
    58   2238.1 MiB      0.0 MiB           mask = mask.to(preds.device)
    59   2238.1 MiB      0.0 MiB           targets = targets.to(preds.device)
    60   2238.1 MiB      0.0 MiB           class_weights = torch.ones(targets.shape, device=preds.device)
    61                             
    62   2238.1 MiB      0.0 MiB           if args.dataset_type == 'multiclass':
    63                                         targets = targets.long()
    64                                         loss = torch.cat([loss_func(preds[:, target_index, :], targets[:, target_index]).unsqueeze(1) for target_index in range(preds.size(1))], dim=1) * class_weights * mask
    65                                     else:
    66   2238.1 MiB      0.0 MiB               loss = loss_func(preds, targets) * class_weights * mask
    67   2238.1 MiB      0.0 MiB           loss = loss.sum() / mask.sum()
    68                             
    69   2238.1 MiB      0.0 MiB           loss_sum += loss.item()
    70   2238.1 MiB      0.0 MiB           iter_count += len(batch)
    71                             
    72   2238.1 MiB      0.0 MiB           loss.backward()
    73   2238.1 MiB      0.0 MiB           optimizer.step()
    74                             
    75   2238.1 MiB      0.0 MiB           if isinstance(scheduler, NoamLR):
    76   2238.1 MiB      0.0 MiB               scheduler.step()
    77                             
    78   2238.1 MiB      0.0 MiB           n_iter += len(batch)
    79                             
    80                                     # Log and/or add to tensorboard
    81   2238.1 MiB      0.0 MiB           wandb.log({'n_iter': n_iter, 'loss_avg': loss_sum/iter_count})
    82   2238.1 MiB      0.0 MiB           if (n_iter // args.batch_size) % args.log_frequency == 0:
    83   2238.1 MiB      0.0 MiB               lrs = scheduler.get_lr()
    84   2238.1 MiB      0.0 MiB               pnorm = compute_pnorm(model)
    85   2238.1 MiB      0.0 MiB               gnorm = compute_gnorm(model)
    86   2238.1 MiB      0.0 MiB               loss_avg = loss_sum / iter_count
    87   2238.1 MiB      0.0 MiB               loss_sum, iter_count = 0, 0
    88                             
    89   2238.1 MiB      0.0 MiB               lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))
    90   2238.1 MiB      0.0 MiB               debug(f'Loss = {loss_avg:.4e}, PNorm = {pnorm:.4f}, GNorm = {gnorm:.4f}, {lrs_str}')
    91                             
    92   2238.1 MiB      0.0 MiB               if writer is not None:
    93   2238.1 MiB      0.0 MiB                   writer.add_scalar('train_loss', loss_avg, n_iter)
    94   2238.1 MiB      0.0 MiB                   writer.add_scalar('param_norm', pnorm, n_iter)
    95   2238.1 MiB      0.0 MiB                   writer.add_scalar('gradient_norm', gnorm, n_iter)
    96   2238.1 MiB      0.0 MiB                   for i, lr in enumerate(lrs):
    97   2238.1 MiB      0.0 MiB                       writer.add_scalar(f'learning_rate_{i}', lr, n_iter)
    98                             
    99   2238.1 MiB      0.0 MiB       return n_iter


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/train.py

Line #    Mem usage    Increment   Line Contents
================================================
    17   2238.1 MiB   2238.1 MiB   @profile
    18                             def train(model: nn.Module,
    19                                       data_loader: MoleculeDataLoader,
    20                                       loss_func: Callable,
    21                                       optimizer: Optimizer,
    22                                       scheduler: _LRScheduler,
    23                                       args: TrainArgs,
    24                                       n_iter: int = 0,
    25                                       logger: logging.Logger = None,
    26                                       writer: SummaryWriter = None) -> int:
    27                                 """
    28                                 Trains a model for an epoch.
    29                             
    30                                 :param model: Model.
    31                                 :param data_loader: A MoleculeDataLoader.
    32                                 :param loss_func: Loss function.
    33                                 :param optimizer: An Optimizer.
    34                                 :param scheduler: A learning rate scheduler.
    35                                 :param args: Arguments.
    36                                 :param n_iter: The number of iterations (training examples) trained on so far.
    37                                 :param logger: A logger for printing intermediate results.
    38                                 :param writer: A tensorboardX SummaryWriter.
    39                                 :return: The total number of iterations (training examples) trained on so far.
    40                                 """
    41   2238.1 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
    42                                 
    43   2238.1 MiB      0.0 MiB       model.train()
    44   2238.1 MiB      0.0 MiB       loss_sum, iter_count = 0, 0
    45                             
    46   2238.1 MiB      0.0 MiB       for batch in tqdm(data_loader, total=len(data_loader)):
    47                                     # Prepare batch
    48                                     batch: MoleculeDataset
    49   2238.1 MiB      0.0 MiB           mol_batch, features_batch, target_batch = batch.batch_graph(), batch.features(), batch.targets()
    50   2238.1 MiB      0.0 MiB           mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])
    51   2238.1 MiB      0.0 MiB           targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])
    52                             
    53                                     # Run model
    54   2238.1 MiB      0.0 MiB           model.zero_grad()
    55   2238.1 MiB      0.0 MiB           preds = model(mol_batch, features_batch)
    56                             
    57                                     # Move tensors to correct device
    58   2238.1 MiB      0.0 MiB           mask = mask.to(preds.device)
    59   2238.1 MiB      0.0 MiB           targets = targets.to(preds.device)
    60   2238.1 MiB      0.0 MiB           class_weights = torch.ones(targets.shape, device=preds.device)
    61                             
    62   2238.1 MiB      0.0 MiB           if args.dataset_type == 'multiclass':
    63                                         targets = targets.long()
    64                                         loss = torch.cat([loss_func(preds[:, target_index, :], targets[:, target_index]).unsqueeze(1) for target_index in range(preds.size(1))], dim=1) * class_weights * mask
    65                                     else:
    66   2238.1 MiB      0.0 MiB               loss = loss_func(preds, targets) * class_weights * mask
    67   2238.1 MiB      0.0 MiB           loss = loss.sum() / mask.sum()


  0% 0/2 [00:00<?, ?it/s][A100% 2/2 [00:00<00:00, 25.38it/s]
Validation auc = 0.547917
 37% 11/30 [00:23<00:37,  1.96s/it]Epoch 11

  0% 0/9 [00:00<?, ?it/s][A
 11% 1/9 [00:00<00:00,  9.13it/s][A
 22% 2/9 [00:00<00:00,  7.15it/s][A
 33% 3/9 [00:00<00:00,  6.13it/s][A
 44% 4/9 [00:00<00:00,  5.73it/s][A
 56% 5/9 [00:00<00:00,  5.66it/s][ALoss = 1.9979e-03, PNorm = 24.6447, GNorm = 0.2138, lr_0 = 3.9647e-04

 67% 6/9 [00:01<00:00,  5.40it/s][A
 78% 7/9 [00:01<00:00,  5.16it/s][A
 89% 8/9 [00:01<00:00,  5.01it/s][A
100% 9/9 [00:01<00:00,  5.19it/s][A100% 9/9 [00:01<00:00,  5.20it/s]    68                             
    69   2238.1 MiB      0.0 MiB           loss_sum += loss.item()
    70   2238.1 MiB      0.0 MiB           iter_count += len(batch)
    71                             
    72   2238.1 MiB      0.0 MiB           loss.backward()
    73   2238.1 MiB      0.0 MiB           optimizer.step()
    74                             
    75   2238.1 MiB      0.0 MiB           if isinstance(scheduler, NoamLR):
    76   2238.1 MiB      0.0 MiB               scheduler.step()
    77                             
    78   2238.1 MiB      0.0 MiB           n_iter += len(batch)
    79                             
    80                                     # Log and/or add to tensorboard
    81   2238.1 MiB      0.0 MiB           wandb.log({'n_iter': n_iter, 'loss_avg': loss_sum/iter_count})
    82   2238.1 MiB      0.0 MiB           if (n_iter // args.batch_size) % args.log_frequency == 0:
    83   2238.1 MiB      0.0 MiB               lrs = scheduler.get_lr()
    84   2238.1 MiB      0.0 MiB               pnorm = compute_pnorm(model)
    85   2238.1 MiB      0.0 MiB               gnorm = compute_gnorm(model)
    86   2238.1 MiB      0.0 MiB               loss_avg = loss_sum / iter_count
    87   2238.1 MiB      0.0 MiB               loss_sum, iter_count = 0, 0
    88                             
    89   2238.1 MiB      0.0 MiB               lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))
    90   2238.1 MiB      0.0 MiB               debug(f'Loss = {loss_avg:.4e}, PNorm = {pnorm:.4f}, GNorm = {gnorm:.4f}, {lrs_str}')
    91                             
    92   2238.1 MiB      0.0 MiB               if writer is not None:
    93   2238.1 MiB      0.0 MiB                   writer.add_scalar('train_loss', loss_avg, n_iter)
    94   2238.1 MiB      0.0 MiB                   writer.add_scalar('param_norm', pnorm, n_iter)
    95   2238.1 MiB      0.0 MiB                   writer.add_scalar('gradient_norm', gnorm, n_iter)
    96   2238.1 MiB      0.0 MiB                   for i, lr in enumerate(lrs):
    97   2238.1 MiB      0.0 MiB                       writer.add_scalar(f'learning_rate_{i}', lr, n_iter)
    98                             
    99   2238.1 MiB      0.0 MiB       return n_iter


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/train.py

Line #    Mem usage    Increment   Line Contents
================================================
    17   2238.1 MiB   2238.1 MiB   @profile
    18                             def train(model: nn.Module,
    19                                       data_loader: MoleculeDataLoader,
    20                                       loss_func: Callable,
    21                                       optimizer: Optimizer,
    22                                       scheduler: _LRScheduler,
    23                                       args: TrainArgs,
    24                                       n_iter: int = 0,
    25                                       logger: logging.Logger = None,
    26                                       writer: SummaryWriter = None) -> int:
    27                                 """
    28                                 Trains a model for an epoch.
    29                             
    30                                 :param model: Model.
    31                                 :param data_loader: A MoleculeDataLoader.
    32                                 :param loss_func: Loss function.
    33                                 :param optimizer: An Optimizer.
    34                                 :param scheduler: A learning rate scheduler.
    35                                 :param args: Arguments.
    36                                 :param n_iter: The number of iterations (training examples) trained on so far.
    37                                 :param logger: A logger for printing intermediate results.
    38                                 :param writer: A tensorboardX SummaryWriter.
    39                                 :return: The total number of iterations (training examples) trained on so far.
    40                                 """
    41   2238.1 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
    42                                 
    43   2238.1 MiB      0.0 MiB       model.train()
    44   2238.1 MiB      0.0 MiB       loss_sum, iter_count = 0, 0
    45                             
    46   2240.7 MiB      1.0 MiB       for batch in tqdm(data_loader, total=len(data_loader)):
    47                                     # Prepare batch
    48                                     batch: MoleculeDataset
    49   2240.7 MiB      0.0 MiB           mol_batch, features_batch, target_batch = batch.batch_graph(), batch.features(), batch.targets()
    50   2240.7 MiB      0.0 MiB           mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])
    51   2240.7 MiB      0.0 MiB           targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])
    52                             
    53                                     # Run model
    54   2240.7 MiB      0.0 MiB           model.zero_grad()
    55   2240.7 MiB      0.0 MiB           preds = model(mol_batch, features_batch)
    56                             
    57                                     # Move tensors to correct device
    58   2240.7 MiB      0.0 MiB           mask = mask.to(preds.device)
    59   2240.7 MiB      0.0 MiB           targets = targets.to(preds.device)
    60   2240.7 MiB      0.0 MiB           class_weights = torch.ones(targets.shape, device=preds.device)
    61                             
    62   2240.7 MiB      0.0 MiB           if args.dataset_type == 'multiclass':
    63                                         targets = targets.long()
    64                                         loss = torch.cat([loss_func(preds[:, target_index, :], targets[:, target_index]).unsqueeze(1) for target_index in range(preds.size(1))], dim=1) * class_weights * mask
    65                                     else:
    66   2240.7 MiB      0.0 MiB               loss = loss_func(preds, targets) * class_weights * mask
    67   2240.7 MiB      0.0 MiB           loss = loss.sum() / mask.sum()
    68                             
    69   2240.7 MiB      0.0 MiB           loss_sum += loss.item()
    70   2240.7 MiB      0.0 MiB           iter_count += len(batch)
    71                             
    72   2240.7 MiB      1.3 MiB           loss.backward()
    73   2240.7 MiB      0.0 MiB           optimizer.step()
    74                             
    75   2240.7 MiB      0.0 MiB           if isinstance(scheduler, NoamLR):
    76   2240.7 MiB      0.0 MiB               scheduler.step()
    77                             
    78   2240.7 MiB      0.0 MiB           n_iter += len(batch)
    79                             
    80                                     # Log and/or add to tensorboard
    81   2240.7 MiB      0.0 MiB           wandb.log({'n_iter': n_iter, 'loss_avg': loss_sum/iter_count})
    82   2240.7 MiB      0.0 MiB           if (n_iter // args.batch_size) % args.log_frequency == 0:
    83   2240.7 MiB      0.0 MiB               lrs = scheduler.get_lr()
    84   2240.7 MiB      0.0 MiB               pnorm = compute_pnorm(model)
    85   2240.7 MiB      0.0 MiB               gnorm = compute_gnorm(model)
    86   2240.7 MiB      0.0 MiB               loss_avg = loss_sum / iter_count
    87   2240.7 MiB      0.0 MiB               loss_sum, iter_count = 0, 0
    88                             
    89   2240.7 MiB      0.0 MiB               lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))
    90   2240.7 MiB      0.0 MiB               debug(f'Loss = {loss_avg:.4e}, PNorm = {pnorm:.4f}, GNorm = {gnorm:.4f}, {lrs_str}')
    91                             
    92   2240.7 MiB      0.0 MiB               if writer is not None:
    93   2240.7 MiB      0.0 MiB                   writer.add_scalar('train_loss', loss_avg, n_iter)
    94   2240.7 MiB      0.0 MiB                   writer.add_scalar('param_norm', pnorm, n_iter)
    95   2240.7 MiB      0.0 MiB                   writer.add_scalar('gradient_norm', gnorm, n_iter)


  0% 0/2 [00:00<?, ?it/s][A
 50% 1/2 [00:00<00:00,  4.67it/s][A100% 2/2 [00:00<00:00,  8.98it/s]
Validation auc = 0.547917
 40% 12/30 [00:25<00:36,  2.00s/it]Epoch 12

  0% 0/9 [00:00<?, ?it/s][A
 22% 2/9 [00:00<00:01,  6.74it/s][A
 33% 3/9 [00:00<00:01,  5.88it/s][A
 44% 4/9 [00:00<00:00,  5.96it/s][A
 56% 5/9 [00:00<00:00,  5.90it/s][A
 67% 6/9 [00:01<00:00,  5.31it/s][A
 78% 7/9 [00:01<00:00,  5.33it/s][ALoss = 1.8695e-03, PNorm = 24.7349, GNorm = 0.1270, lr_0 = 3.5408e-04

 89% 8/9 [00:01<00:00,  5.30it/s][A
100% 9/9 [00:01<00:00,  5.56it/s][A100% 9/9 [00:01<00:00,  5.54it/s]

  0% 0/2 [00:00<?, ?it/s][A100% 2/2 [00:00<00:00, 25.03it/s]
Validation auc = 0.485417
 43% 13/30 [00:27<00:33,  1.97s/it]Epoch 13

  0% 0/9 [00:00<?, ?it/s][A
 22% 2/9 [00:00<00:00,  7.38it/s][A
 33% 3/9 [00:00<00:00,  6.42it/s][A
 44% 4/9 [00:00<00:00,  5.70it/s][A
 56% 5/9 [00:00<00:00,  5.34it/s][A
 67% 6/9 [00:01<00:00,  5.11it/s][A
 78% 7/9 [00:01<00:00,  5.17it/s][A
 89% 8/9 [00:01<00:00,  5.33it/s][A
100% 9/9 [00:01<00:00,  5.25it/s][A100% 9/9 [00:01<00:00,  5.34it/s]    96   2240.7 MiB      0.0 MiB                   for i, lr in enumerate(lrs):
    97   2240.7 MiB      0.0 MiB                       writer.add_scalar(f'learning_rate_{i}', lr, n_iter)
    98                             
    99   2240.7 MiB      0.0 MiB       return n_iter


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/train.py

Line #    Mem usage    Increment   Line Contents
================================================
    17   2240.7 MiB   2240.7 MiB   @profile
    18                             def train(model: nn.Module,
    19                                       data_loader: MoleculeDataLoader,
    20                                       loss_func: Callable,
    21                                       optimizer: Optimizer,
    22                                       scheduler: _LRScheduler,
    23                                       args: TrainArgs,
    24                                       n_iter: int = 0,
    25                                       logger: logging.Logger = None,
    26                                       writer: SummaryWriter = None) -> int:
    27                                 """
    28                                 Trains a model for an epoch.
    29                             
    30                                 :param model: Model.
    31                                 :param data_loader: A MoleculeDataLoader.
    32                                 :param loss_func: Loss function.
    33                                 :param optimizer: An Optimizer.
    34                                 :param scheduler: A learning rate scheduler.
    35                                 :param args: Arguments.
    36                                 :param n_iter: The number of iterations (training examples) trained on so far.
    37                                 :param logger: A logger for printing intermediate results.
    38                                 :param writer: A tensorboardX SummaryWriter.
    39                                 :return: The total number of iterations (training examples) trained on so far.
    40                                 """
    41   2240.7 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
    42                                 
    43   2240.7 MiB      0.0 MiB       model.train()
    44   2240.7 MiB      0.0 MiB       loss_sum, iter_count = 0, 0
    45                             
    46   2240.7 MiB      0.0 MiB       for batch in tqdm(data_loader, total=len(data_loader)):
    47                                     # Prepare batch
    48                                     batch: MoleculeDataset
    49   2240.7 MiB      0.0 MiB           mol_batch, features_batch, target_batch = batch.batch_graph(), batch.features(), batch.targets()
    50   2240.7 MiB      0.0 MiB           mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])
    51   2240.7 MiB      0.0 MiB           targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])
    52                             
    53                                     # Run model
    54   2240.7 MiB      0.0 MiB           model.zero_grad()
    55   2240.7 MiB      0.0 MiB           preds = model(mol_batch, features_batch)
    56                             
    57                                     # Move tensors to correct device
    58   2240.7 MiB      0.0 MiB           mask = mask.to(preds.device)
    59   2240.7 MiB      0.0 MiB           targets = targets.to(preds.device)
    60   2240.7 MiB      0.0 MiB           class_weights = torch.ones(targets.shape, device=preds.device)
    61                             
    62   2240.7 MiB      0.0 MiB           if args.dataset_type == 'multiclass':
    63                                         targets = targets.long()
    64                                         loss = torch.cat([loss_func(preds[:, target_index, :], targets[:, target_index]).unsqueeze(1) for target_index in range(preds.size(1))], dim=1) * class_weights * mask
    65                                     else:
    66   2240.7 MiB      0.0 MiB               loss = loss_func(preds, targets) * class_weights * mask
    67   2240.7 MiB      0.0 MiB           loss = loss.sum() / mask.sum()
    68                             
    69   2240.7 MiB      0.0 MiB           loss_sum += loss.item()
    70   2240.7 MiB      0.0 MiB           iter_count += len(batch)
    71                             
    72   2240.7 MiB      0.0 MiB           loss.backward()
    73   2240.7 MiB      0.0 MiB           optimizer.step()
    74                             
    75   2240.7 MiB      0.0 MiB           if isinstance(scheduler, NoamLR):
    76   2240.7 MiB      0.0 MiB               scheduler.step()
    77                             
    78   2240.7 MiB      0.0 MiB           n_iter += len(batch)
    79                             
    80                                     # Log and/or add to tensorboard
    81   2240.7 MiB      0.0 MiB           wandb.log({'n_iter': n_iter, 'loss_avg': loss_sum/iter_count})
    82   2240.7 MiB      0.0 MiB           if (n_iter // args.batch_size) % args.log_frequency == 0:
    83   2240.7 MiB      0.0 MiB               lrs = scheduler.get_lr()
    84   2240.7 MiB      0.0 MiB               pnorm = compute_pnorm(model)
    85   2240.7 MiB      0.0 MiB               gnorm = compute_gnorm(model)
    86   2240.7 MiB      0.0 MiB               loss_avg = loss_sum / iter_count
    87   2240.7 MiB      0.0 MiB               loss_sum, iter_count = 0, 0
    88                             
    89   2240.7 MiB      0.0 MiB               lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))
    90   2240.7 MiB      0.0 MiB               debug(f'Loss = {loss_avg:.4e}, PNorm = {pnorm:.4f}, GNorm = {gnorm:.4f}, {lrs_str}')
    91                             
    92   2240.7 MiB      0.0 MiB               if writer is not None:
    93   2240.7 MiB      0.0 MiB                   writer.add_scalar('train_loss', loss_avg, n_iter)
    94   2240.7 MiB      0.0 MiB                   writer.add_scalar('param_norm', pnorm, n_iter)
    95   2240.7 MiB      0.0 MiB                   writer.add_scalar('gradient_norm', gnorm, n_iter)
    96   2240.7 MiB      0.0 MiB                   for i, lr in enumerate(lrs):
    97   2240.7 MiB      0.0 MiB                       writer.add_scalar(f'learning_rate_{i}', lr, n_iter)
    98                             
    99   2240.7 MiB      0.0 MiB       return n_iter


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/train.py

Line #    Mem usage    Increment   Line Contents
================================================
    17   2240.7 MiB   2240.7 MiB   @profile
    18                             def train(model: nn.Module,
    19                                       data_loader: MoleculeDataLoader,
    20                                       loss_func: Callable,
    21                                       optimizer: Optimizer,
    22                                       scheduler: _LRScheduler,
    23                                       args: TrainArgs,
    24                                       n_iter: int = 0,
    25                                       logger: logging.Logger = None,
    26                                       writer: SummaryWriter = None) -> int:
    27                                 """
    28                                 Trains a model for an epoch.
    29                             
    30                                 :param model: Model.
    31                                 :param data_loader: A MoleculeDataLoader.
    32                                 :param loss_func: Loss function.
    33                                 :param optimizer: An Optimizer.
    34                                 :param scheduler: A learning rate scheduler.
    35                                 :param args: Arguments.
    36                                 :param n_iter: The number of iterations (training examples) trained on so far.
    37                                 :param logger: A logger for printing intermediate results.


  0% 0/2 [00:00<?, ?it/s][A100% 2/2 [00:00<00:00, 25.42it/s]
Validation auc = 0.485417
 47% 14/30 [00:29<00:31,  1.97s/it]Epoch 14

  0% 0/9 [00:00<?, ?it/s][ALoss = 1.9338e-03, PNorm = 24.8140, GNorm = 0.2109, lr_0 = 3.1623e-04

 22% 2/9 [00:00<00:00,  7.47it/s][A
 33% 3/9 [00:00<00:00,  6.70it/s][A
 44% 4/9 [00:00<00:00,  6.30it/s][A
 56% 5/9 [00:00<00:00,  5.99it/s][A
 67% 6/9 [00:01<00:00,  5.43it/s][A
 78% 7/9 [00:01<00:00,  5.53it/s][A
 89% 8/9 [00:01<00:00,  5.11it/s][A
100% 9/9 [00:01<00:00,  5.35it/s][A100% 9/9 [00:01<00:00,  5.57it/s]    38                                 :param writer: A tensorboardX SummaryWriter.
    39                                 :return: The total number of iterations (training examples) trained on so far.
    40                                 """
    41   2240.7 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
    42                                 
    43   2240.7 MiB      0.0 MiB       model.train()
    44   2240.7 MiB      0.0 MiB       loss_sum, iter_count = 0, 0
    45                             
    46   2240.7 MiB      0.0 MiB       for batch in tqdm(data_loader, total=len(data_loader)):
    47                                     # Prepare batch
    48                                     batch: MoleculeDataset
    49   2240.7 MiB      0.0 MiB           mol_batch, features_batch, target_batch = batch.batch_graph(), batch.features(), batch.targets()
    50   2240.7 MiB      0.0 MiB           mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])
    51   2240.7 MiB      0.0 MiB           targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])
    52                             
    53                                     # Run model
    54   2240.7 MiB      0.0 MiB           model.zero_grad()
    55   2240.7 MiB      0.0 MiB           preds = model(mol_batch, features_batch)
    56                             
    57                                     # Move tensors to correct device
    58   2240.7 MiB      0.0 MiB           mask = mask.to(preds.device)
    59   2240.7 MiB      0.0 MiB           targets = targets.to(preds.device)
    60   2240.7 MiB      0.0 MiB           class_weights = torch.ones(targets.shape, device=preds.device)
    61                             
    62   2240.7 MiB      0.0 MiB           if args.dataset_type == 'multiclass':
    63                                         targets = targets.long()
    64                                         loss = torch.cat([loss_func(preds[:, target_index, :], targets[:, target_index]).unsqueeze(1) for target_index in range(preds.size(1))], dim=1) * class_weights * mask
    65                                     else:
    66   2240.7 MiB      0.0 MiB               loss = loss_func(preds, targets) * class_weights * mask
    67   2240.7 MiB      0.0 MiB           loss = loss.sum() / mask.sum()
    68                             
    69   2240.7 MiB      0.0 MiB           loss_sum += loss.item()
    70   2240.7 MiB      0.0 MiB           iter_count += len(batch)
    71                             
    72   2240.7 MiB      0.0 MiB           loss.backward()
    73   2240.7 MiB      0.0 MiB           optimizer.step()
    74                             
    75   2240.7 MiB      0.0 MiB           if isinstance(scheduler, NoamLR):
    76   2240.7 MiB      0.0 MiB               scheduler.step()
    77                             
    78   2240.7 MiB      0.0 MiB           n_iter += len(batch)
    79                             
    80                                     # Log and/or add to tensorboard
    81   2240.7 MiB      0.0 MiB           wandb.log({'n_iter': n_iter, 'loss_avg': loss_sum/iter_count})
    82   2240.7 MiB      0.0 MiB           if (n_iter // args.batch_size) % args.log_frequency == 0:
    83                                         lrs = scheduler.get_lr()
    84                                         pnorm = compute_pnorm(model)
    85                                         gnorm = compute_gnorm(model)
    86                                         loss_avg = loss_sum / iter_count
    87                                         loss_sum, iter_count = 0, 0
    88                             
    89                                         lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))
    90                                         debug(f'Loss = {loss_avg:.4e}, PNorm = {pnorm:.4f}, GNorm = {gnorm:.4f}, {lrs_str}')
    91                             
    92                                         if writer is not None:
    93                                             writer.add_scalar('train_loss', loss_avg, n_iter)
    94                                             writer.add_scalar('param_norm', pnorm, n_iter)
    95                                             writer.add_scalar('gradient_norm', gnorm, n_iter)
    96                                             for i, lr in enumerate(lrs):
    97                                                 writer.add_scalar(f'learning_rate_{i}', lr, n_iter)
    98                             
    99   2240.7 MiB      0.0 MiB       return n_iter


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/train.py

Line #    Mem usage    Increment   Line Contents
================================================
    17   2240.7 MiB   2240.7 MiB   @profile
    18                             def train(model: nn.Module,
    19                                       data_loader: MoleculeDataLoader,
    20                                       loss_func: Callable,
    21                                       optimizer: Optimizer,
    22                                       scheduler: _LRScheduler,
    23                                       args: TrainArgs,
    24                                       n_iter: int = 0,
    25                                       logger: logging.Logger = None,
    26                                       writer: SummaryWriter = None) -> int:
    27                                 """
    28                                 Trains a model for an epoch.
    29                             
    30                                 :param model: Model.
    31                                 :param data_loader: A MoleculeDataLoader.
    32                                 :param loss_func: Loss function.
    33                                 :param optimizer: An Optimizer.
    34                                 :param scheduler: A learning rate scheduler.
    35                                 :param args: Arguments.
    36                                 :param n_iter: The number of iterations (training examples) trained on so far.
    37                                 :param logger: A logger for printing intermediate results.
    38                                 :param writer: A tensorboardX SummaryWriter.
    39                                 :return: The total number of iterations (training examples) trained on so far.
    40                                 """
    41   2240.7 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
    42                                 
    43   2240.7 MiB      0.0 MiB       model.train()
    44   2240.7 MiB      0.0 MiB       loss_sum, iter_count = 0, 0
    45                             
    46   2240.7 MiB      0.0 MiB       for batch in tqdm(data_loader, total=len(data_loader)):
    47                                     # Prepare batch
    48                                     batch: MoleculeDataset
    49   2240.7 MiB      0.0 MiB           mol_batch, features_batch, target_batch = batch.batch_graph(), batch.features(), batch.targets()
    50   2240.7 MiB      0.0 MiB           mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])
    51   2240.7 MiB      0.0 MiB           targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])
    52                             
    53                                     # Run model
    54   2240.7 MiB      0.0 MiB           model.zero_grad()
    55   2240.7 MiB      0.0 MiB           preds = model(mol_batch, features_batch)
    56                             
    57                                     # Move tensors to correct device
    58   2240.7 MiB      0.0 MiB           mask = mask.to(preds.device)
    59   2240.7 MiB      0.0 MiB           targets = targets.to(preds.device)
    60   2240.7 MiB      0.0 MiB           class_weights = torch.ones(targets.shape, device=preds.device)
    61                             
    62   2240.7 MiB      0.0 MiB           if args.dataset_type == 'multiclass':
    63                                         targets = targets.long()


  0% 0/2 [00:00<?, ?it/s][A100% 2/2 [00:00<00:00, 25.51it/s]
Validation auc = 0.485417
 50% 15/30 [00:31<00:29,  1.95s/it]Epoch 15

  0% 0/9 [00:00<?, ?it/s][ALoss = 1.6256e-03, PNorm = 24.8770, GNorm = 0.2440, lr_0 = 2.8534e-04

 22% 2/9 [00:00<00:01,  6.70it/s][A
 33% 3/9 [00:00<00:00,  6.40it/s][A
 44% 4/9 [00:00<00:00,  6.16it/s][A
 56% 5/9 [00:00<00:00,  5.52it/s][A
 67% 6/9 [00:01<00:00,  5.48it/s][A
 78% 7/9 [00:01<00:00,  5.44it/s][A
 89% 8/9 [00:01<00:00,  5.28it/s][A
100% 9/9 [00:01<00:00,  6.07it/s][A100% 9/9 [00:01<00:00,  5.79it/s]    64                                         loss = torch.cat([loss_func(preds[:, target_index, :], targets[:, target_index]).unsqueeze(1) for target_index in range(preds.size(1))], dim=1) * class_weights * mask
    65                                     else:
    66   2240.7 MiB      0.0 MiB               loss = loss_func(preds, targets) * class_weights * mask
    67   2240.7 MiB      0.0 MiB           loss = loss.sum() / mask.sum()
    68                             
    69   2240.7 MiB      0.0 MiB           loss_sum += loss.item()
    70   2240.7 MiB      0.0 MiB           iter_count += len(batch)
    71                             
    72   2240.7 MiB      0.0 MiB           loss.backward()
    73   2240.7 MiB      0.0 MiB           optimizer.step()
    74                             
    75   2240.7 MiB      0.0 MiB           if isinstance(scheduler, NoamLR):
    76   2240.7 MiB      0.0 MiB               scheduler.step()
    77                             
    78   2240.7 MiB      0.0 MiB           n_iter += len(batch)
    79                             
    80                                     # Log and/or add to tensorboard
    81   2240.7 MiB      0.0 MiB           wandb.log({'n_iter': n_iter, 'loss_avg': loss_sum/iter_count})
    82   2240.7 MiB      0.0 MiB           if (n_iter // args.batch_size) % args.log_frequency == 0:
    83   2240.7 MiB      0.0 MiB               lrs = scheduler.get_lr()
    84   2240.7 MiB      0.0 MiB               pnorm = compute_pnorm(model)
    85   2240.7 MiB      0.0 MiB               gnorm = compute_gnorm(model)
    86   2240.7 MiB      0.0 MiB               loss_avg = loss_sum / iter_count
    87   2240.7 MiB      0.0 MiB               loss_sum, iter_count = 0, 0
    88                             
    89   2240.7 MiB      0.0 MiB               lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))
    90   2240.7 MiB      0.0 MiB               debug(f'Loss = {loss_avg:.4e}, PNorm = {pnorm:.4f}, GNorm = {gnorm:.4f}, {lrs_str}')
    91                             
    92   2240.7 MiB      0.0 MiB               if writer is not None:
    93   2240.7 MiB      0.0 MiB                   writer.add_scalar('train_loss', loss_avg, n_iter)
    94   2240.7 MiB      0.0 MiB                   writer.add_scalar('param_norm', pnorm, n_iter)
    95   2240.7 MiB      0.0 MiB                   writer.add_scalar('gradient_norm', gnorm, n_iter)
    96   2240.7 MiB      0.0 MiB                   for i, lr in enumerate(lrs):
    97   2240.7 MiB      0.0 MiB                       writer.add_scalar(f'learning_rate_{i}', lr, n_iter)
    98                             
    99   2240.7 MiB      0.0 MiB       return n_iter


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/train.py

Line #    Mem usage    Increment   Line Contents
================================================
    17   2240.7 MiB   2240.7 MiB   @profile
    18                             def train(model: nn.Module,
    19                                       data_loader: MoleculeDataLoader,
    20                                       loss_func: Callable,
    21                                       optimizer: Optimizer,
    22                                       scheduler: _LRScheduler,
    23                                       args: TrainArgs,
    24                                       n_iter: int = 0,
    25                                       logger: logging.Logger = None,
    26                                       writer: SummaryWriter = None) -> int:
    27                                 """
    28                                 Trains a model for an epoch.
    29                             
    30                                 :param model: Model.
    31                                 :param data_loader: A MoleculeDataLoader.
    32                                 :param loss_func: Loss function.
    33                                 :param optimizer: An Optimizer.
    34                                 :param scheduler: A learning rate scheduler.
    35                                 :param args: Arguments.
    36                                 :param n_iter: The number of iterations (training examples) trained on so far.
    37                                 :param logger: A logger for printing intermediate results.
    38                                 :param writer: A tensorboardX SummaryWriter.
    39                                 :return: The total number of iterations (training examples) trained on so far.
    40                                 """
    41   2240.7 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
    42                                 
    43   2240.7 MiB      0.0 MiB       model.train()
    44   2240.7 MiB      0.0 MiB       loss_sum, iter_count = 0, 0
    45                             
    46   2240.7 MiB      0.0 MiB       for batch in tqdm(data_loader, total=len(data_loader)):
    47                                     # Prepare batch
    48                                     batch: MoleculeDataset
    49   2240.7 MiB      0.0 MiB           mol_batch, features_batch, target_batch = batch.batch_graph(), batch.features(), batch.targets()
    50   2240.7 MiB      0.0 MiB           mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])
    51   2240.7 MiB      0.0 MiB           targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])
    52                             
    53                                     # Run model
    54   2240.7 MiB      0.0 MiB           model.zero_grad()
    55   2240.7 MiB      0.0 MiB           preds = model(mol_batch, features_batch)
    56                             
    57                                     # Move tensors to correct device
    58   2240.7 MiB      0.0 MiB           mask = mask.to(preds.device)
    59   2240.7 MiB      0.0 MiB           targets = targets.to(preds.device)
    60   2240.7 MiB      0.0 MiB           class_weights = torch.ones(targets.shape, device=preds.device)
    61                             
    62   2240.7 MiB      0.0 MiB           if args.dataset_type == 'multiclass':
    63                                         targets = targets.long()
    64                                         loss = torch.cat([loss_func(preds[:, target_index, :], targets[:, target_index]).unsqueeze(1) for target_index in range(preds.size(1))], dim=1) * class_weights * mask
    65                                     else:
    66   2240.7 MiB      0.0 MiB               loss = loss_func(preds, targets) * class_weights * mask
    67   2240.7 MiB      0.0 MiB           loss = loss.sum() / mask.sum()
    68                             
    69   2240.7 MiB      0.0 MiB           loss_sum += loss.item()
    70   2240.7 MiB      0.0 MiB           iter_count += len(batch)
    71                             
    72   2240.7 MiB      0.0 MiB           loss.backward()
    73   2240.7 MiB      0.0 MiB           optimizer.step()
    74                             
    75   2240.7 MiB      0.0 MiB           if isinstance(scheduler, NoamLR):
    76   2240.7 MiB      0.0 MiB               scheduler.step()
    77                             
    78   2240.7 MiB      0.0 MiB           n_iter += len(batch)
    79                             
    80                                     # Log and/or add to tensorboard
    81   2240.7 MiB      0.0 MiB           wandb.log({'n_iter': n_iter, 'loss_avg': loss_sum/iter_count})
    82   2240.7 MiB      0.0 MiB           if (n_iter // args.batch_size) % args.log_frequency == 0:
    83   2240.7 MiB      0.0 MiB               lrs = scheduler.get_lr()
    84   2240.7 MiB      0.0 MiB               pnorm = compute_pnorm(model)
    85   2240.7 MiB      0.0 MiB               gnorm = compute_gnorm(model)
    86   2240.7 MiB      0.0 MiB               loss_avg = loss_sum / iter_count
    87   2240.7 MiB      0.0 MiB               loss_sum, iter_count = 0, 0
    88                             
    89   2240.7 MiB      0.0 MiB               lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))
    90   2240.7 MiB      0.0 MiB               debug(f'Loss = {loss_avg:.4e}, PNorm = {pnorm:.4f}, GNorm = {gnorm:.4f}, {lrs_str}')


  0% 0/2 [00:00<?, ?it/s][A
 50% 1/2 [00:00<00:00,  6.86it/s][A100% 2/2 [00:00<00:00, 12.95it/s]
Validation auc = 0.485417
 53% 16/30 [00:33<00:27,  1.96s/it]Epoch 16

  0% 0/9 [00:00<?, ?it/s][A
 22% 2/9 [00:00<00:00,  7.54it/s][ALoss = 1.4408e-03, PNorm = 24.9334, GNorm = 0.1517, lr_0 = 2.5746e-04

 33% 3/9 [00:00<00:01,  5.96it/s][A
 44% 4/9 [00:00<00:00,  5.76it/s][A
 56% 5/9 [00:00<00:00,  5.78it/s][A
 67% 6/9 [00:01<00:00,  5.29it/s][A
 78% 7/9 [00:01<00:00,  5.22it/s][A
 89% 8/9 [00:01<00:00,  5.35it/s][A
100% 9/9 [00:01<00:00,  6.09it/s][A100% 9/9 [00:01<00:00,  5.67it/s]

  0% 0/2 [00:00<?, ?it/s][A100% 2/2 [00:00<00:00, 25.30it/s]
Validation auc = 0.485417
 57% 17/30 [00:35<00:25,  1.94s/it]Epoch 17

  0% 0/9 [00:00<?, ?it/s][A
 11% 1/9 [00:00<00:00,  9.69it/s][A
 22% 2/9 [00:00<00:00,  8.13it/s][A
 33% 3/9 [00:00<00:00,  6.35it/s][A
 44% 4/9 [00:00<00:00,  5.65it/s][ALoss = 1.6598e-03, PNorm = 24.9885, GNorm = 0.1411, lr_0 = 2.2994e-04

 56% 5/9 [00:00<00:00,  5.24it/s][A
 67% 6/9 [00:01<00:00,  5.29it/s][A
 78% 7/9 [00:01<00:00,  5.41it/s][A
 89% 8/9 [00:01<00:00,  5.40it/s][A    91                             
    92   2240.7 MiB      0.0 MiB               if writer is not None:
    93   2240.7 MiB      0.0 MiB                   writer.add_scalar('train_loss', loss_avg, n_iter)
    94   2240.7 MiB      0.0 MiB                   writer.add_scalar('param_norm', pnorm, n_iter)
    95   2240.7 MiB      0.0 MiB                   writer.add_scalar('gradient_norm', gnorm, n_iter)
    96   2240.7 MiB      0.0 MiB                   for i, lr in enumerate(lrs):
    97   2240.7 MiB      0.0 MiB                       writer.add_scalar(f'learning_rate_{i}', lr, n_iter)
    98                             
    99   2240.7 MiB      0.0 MiB       return n_iter


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/train.py

Line #    Mem usage    Increment   Line Contents
================================================
    17   2240.7 MiB   2240.7 MiB   @profile
    18                             def train(model: nn.Module,
    19                                       data_loader
: MoleculeDataLoader,
    20                                       loss_func: Callable,
    21                                       optimizer: Optimizer,
    22                                       scheduler: _LRScheduler,
    23                                       args: TrainArgs,
    24                                       n_iter: int = 0,
    25                                       logger: logging.Logger = None,
    26                                       writer: SummaryWriter = None) -> int:
    27                                 """
    28                                 Trains a model for an epoch.
    29                             
    30                                 :param model: Model.
    31                                 :param data_loader: A MoleculeDataLoader.
    32                                 :param loss_func: Loss function.
    33                                 :param optimizer: An Optimizer.
    34                                 :param scheduler: A learning rate scheduler.
 100% 9/9 [00:01<00:00,  6.05it/s][A100% 9/9 [00:01<00:00,  5.54it/s]   35                                 :param args: Arguments.
    36                                 :param n_iter: The number of iterations (training examples) trained on so far.
    37                                 :param logger: A logger for printing intermediate results.
    38                                 :param writer: A tensorboardX SummaryWriter.
    39                                 :return: The total number of iterations (training examples) trained on so far.
    40                                 """
    41   2240.7 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
    42                                 
    43   2240.7 MiB      0.0 MiB       model.train()
    44   2240.7 MiB      0.0 MiB       loss_sum, iter_count = 0, 0
    45                             
    46   2240.7 MiB      0.0 MiB       for batch in tqdm(data_loader, total=len(data_loader)):
    47                                     # Prepare batch
    48                                     batch: MoleculeDataset
    49   2240.7 MiB      0.0 MiB           mol_batch, features_batch, target_batch = batch.batch_graph(), batch.features(), batch.targets()
    50   2240.7 MiB      0.0 MiB           mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])
    51   2240.7 MiB      0.0 MiB           targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])
    52                             
    53                                     # Run model
    54   2240.7 MiB      0.0 MiB           model.zero_grad()
    55   2240.7 MiB      0.0 MiB           preds = model(mol_batch, features_batch)
    56                             
    57                                     # Move tensors to correct device
    58   2240.7 MiB      0.0 MiB           mask = mask.to(preds.device)
    59   2240.7 MiB      0.0 MiB           targets = targets.to(preds.device)
    60   2240.7 MiB      0.0 MiB           class_weights = torch.ones(targets.shape, device=preds.device)
    61                             
    62   2240.7 MiB      0.0 MiB           if args.dataset_type == 'multiclass':
    63                                         targets = targets.long()
    64                                         loss = torch.cat([loss_func(preds[:, target_index, :], targets[:, target_index]).unsqueeze(1) for target_index in range(preds.size(1))], dim=1) * class_weights * mask
    65                                     else:
    66   2240.7 MiB      0.0 MiB               loss = loss_func(preds, targets) * class_weights * mask
    67   2240.7 MiB      0.0 MiB           loss = loss.sum() / mask.sum()
    68                             
    69   2240.7 MiB      0.0 MiB           loss_sum += loss.item()
    70   2240.7 MiB      0.0 MiB           iter_count += len(batch)
    71                             
    72   2240.7 MiB      0.0 MiB           loss.backward()
    73   2240.7 MiB      0.0 MiB           optimizer.step()
    74                             
    75   2240.7 MiB      0.0 MiB           if isinstance(scheduler, NoamLR):
    76   2240.7 MiB      0.0 MiB               scheduler.step()
    77                             
    78   2240.7 MiB      0.0 MiB           n_iter += len(batch)
    79                             
    80                                     # Log and/or add to tensorboard
    81   2240.7 MiB      0.0 MiB           wandb.log({'n_iter': n_iter, 'loss_avg': loss_sum/iter_count})
    82   2240.7 MiB      0.0 MiB           if (n_iter // args.batch_size) % args.log_frequency == 0:
    83   2240.7 MiB      0.0 MiB               lrs = scheduler.get_lr()
    84   2240.7 MiB      0.0 MiB               pnorm = compute_pnorm(model)
    85   2240.7 MiB      0.0 MiB               gnorm = compute_gnorm(model)
    86   2240.7 MiB      0.0 MiB               loss_avg = loss_sum / iter_count
    87   2240.7 MiB      0.0 MiB               loss_sum, iter_count = 0, 0
    88                             
    89   2240.7 MiB      0.0 MiB               lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))
    90   2240.7 MiB      0.0 MiB               debug(f'Loss = {loss_avg:.4e}, PNorm = {pnorm:.4f}, GNorm = {gnorm:.4f}, {lrs_str}')
    91                             
    92   2240.7 MiB      0.0 MiB               if writer is not None:
    93   2240.7 MiB      0.0 MiB                   writer.add_scalar('train_loss', loss_avg, n_iter)
    94   2240.7 MiB      0.0 MiB                   writer.add_scalar('param_norm', pnorm, n_iter)
    95   2240.7 MiB      0.0 MiB                   writer.add_scalar('gradient_norm', gnorm, n_iter)
    96   2240.7 MiB      0.0 MiB                   for i, lr in enumerate(lrs):
    97   2240.7 MiB      0.0 MiB                       writer.add_scalar(f'learning_rate_{i}', lr, n_iter)
    98                             
    99   2240.7 MiB      0.0 MiB       return n_iter


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/train.py

Line #    Mem usage    Increment   Line Contents
================================================
    17   2240.7 MiB   2240.7 MiB   @profile
    18                             def train(model: nn.Module,
    19                                       data_loader: MoleculeDataLoader,
    20                                       loss_func: Callable,
    21                                       optimizer: Optimizer,
    22                                       scheduler: _LRScheduler,
    23                                       args: TrainArgs,
    24                                       n_iter: int = 0,
    25                                       logger: logging.Logger = None,
    26                                       writer: SummaryWriter = None) -> int:
    27                                 """
    28                                 Trains a model for an epoch.
    29                             
    30                                 :param model: Model.
    31                                 :param data_loader: A MoleculeDataLoader.
    32                                 :param loss_func: Loss function.
    33                                 :param optimizer: An Optimizer.


  0% 0/2 [00:00<?, ?it/s][A
 50% 1/2 [00:00<00:00,  7.48it/s][A100% 2/2 [00:00<00:00, 14.05it/s]
Validation auc = 0.485417
 60% 18/30 [00:37<00:23,  1.95s/it]Epoch 18

  0% 0/9 [00:00<?, ?it/s][A
 22% 2/9 [00:00<00:00,  7.42it/s][A
 33% 3/9 [00:00<00:00,  6.11it/s][A
 44% 4/9 [00:00<00:00,  5.56it/s][A
 56% 5/9 [00:00<00:00,  5.23it/s][ALoss = 1.4344e-03, PNorm = 25.0333, GNorm = 0.2025, lr_0 = 2.0747e-04

 67% 6/9 [00:01<00:00,  4.99it/s][A
 78% 7/9 [00:01<00:00,  5.24it/s][A
 89% 8/9 [00:01<00:00,  5.35it/s][A
100% 9/9 [00:01<00:00,  6.12it/s][A100% 9/9 [00:01<00:00,  5.57it/s]    34                                 :param scheduler: A learning rate scheduler.
    35                                 :param args: Arguments.
    36                                 :param n_iter: The number of iterations (training examples) trained on so far.
    37                                 :param logger: A logger for printing intermediate results.
    38                                 :param writer: A tensorboardX SummaryWriter.
    39                                 :return: The total number of iterations (training examples) trained on so far.
    40                                 """
    41   2240.7 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
    42                                 
    43   2240.7 MiB      0.0 MiB       model.train()
    44   2240.7 MiB      0.0 MiB       loss_sum, iter_count = 0, 0
    45                             
    46   2240.7 MiB      0.0 MiB       for batch in tqdm(data_loader, total=len(data_loader)):
    47                                     # Prepare batch
    48                                     batch: MoleculeDataset
    49   2240.7 MiB      0.0 MiB           mol_batch, features_batch, target_batch = batch.batch_graph(), batch.features(), batch.targets()
    50   2240.7 MiB      0.0 MiB           mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])
    51   2240.7 MiB      0.0 MiB           targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])
    52                             
    53                                     # Run model
    54   2240.7 MiB      0.0 MiB           model.zero_grad()
    55   2240.7 MiB      0.0 MiB           preds = model(mol_batch, features_batch)
    56                             
    57                                     # Move tensors to correct device
    58   2240.7 MiB      0.0 MiB           mask = mask.to(preds.device)
    59   2240.7 MiB      0.0 MiB           targets = targets.to(preds.device)
    60   2240.7 MiB      0.0 MiB           class_weights = torch.ones(targets.shape, device=preds.device)
    61                             
    62   2240.7 MiB      0.0 MiB           if args.dataset_type == 'multiclass':
    63                                         targets = targets.long()
    64                                         loss = torch.cat([loss_func(preds[:, target_index, :], targets[:, target_index]).unsqueeze(1) for target_index in range(preds.size(1))], dim=1) * class_weights * mask
    65                                     else:
    66   2240.7 MiB      0.0 MiB               loss = loss_func(preds, targets) * class_weights * mask
    67   2240.7 MiB      0.0 MiB           loss = loss.sum() / mask.sum()
    68                             
    69   2240.7 MiB      0.0 MiB           loss_sum += loss.item()
    70   2240.7 MiB      0.0 MiB           iter_count += len(batch)
    71                             
    72   2240.7 MiB      0.0 MiB           loss.backward()
    73   2240.7 MiB      0.0 MiB           optimizer.step()
    74                             
    75   2240.7 MiB      0.0 MiB           if isinstance(scheduler, NoamLR):
    76   2240.7 MiB      0.0 MiB               scheduler.step()
    77                             
    78   2240.7 MiB      0.0 MiB           n_iter += len(batch)
    79                             
    80                                     # Log and/or add to tensorboard
    81   2240.7 MiB      0.0 MiB           wandb.log({'n_iter': n_iter, 'loss_avg': loss_sum/iter_count})
    82   2240.7 MiB      0.0 MiB           if (n_iter // args.batch_size) % args.log_frequency == 0:
    83   2240.7 MiB      0.0 MiB               lrs = scheduler.get_lr()
    84   2240.7 MiB      0.0 MiB               pnorm = compute_pnorm(model)
    85   2240.7 MiB      0.0 MiB               gnorm = compute_gnorm(model)
    86   2240.7 MiB      0.0 MiB               loss_avg = loss_sum / iter_count
    87   2240.7 MiB      0.0 MiB               loss_sum, iter_count = 0, 0
    88                             
    89   2240.7 MiB      0.0 MiB               lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))
    90   2240.7 MiB      0.0 MiB               debug(f'Loss = {loss_avg:.4e}, PNorm = {pnorm:.4f}, GNorm = {gnorm:.4f}, {lrs_str}')
    91                             
    92   2240.7 MiB      0.0 MiB               if writer is not None:
    93   2240.7 MiB      0.0 MiB                   writer.add_scalar('train_loss', loss_avg, n_iter)
    94   2240.7 MiB      0.0 MiB                   writer.add_scalar('param_norm', pnorm, n_iter)
    95   2240.7 MiB      0.0 MiB                   writer.add_scalar('gradient_norm', gnorm, n_iter)
    96   2240.7 MiB      0.0 MiB                   for i, lr in enumerate(lrs):
    97   2240.7 MiB      0.0 MiB                       writer.add_scalar(f'learning_rate_{i}', lr, n_iter)
    98                             
    99   2240.7 MiB      0.0 MiB       return n_iter


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/train.py

Line #    Mem usage    Increment   Line Contents
================================================
    17   2240.7 MiB   2240.7 MiB   @profile
    18                             def train(model: nn.Module,
    19                                       data_loader: MoleculeDataLoader,
    20                                       loss_func: Callable,
    21                                       optimizer: Optimizer,
    22                                       scheduler: _LRScheduler,
    23                                       args: TrainArgs,
    24                                       n_iter: int = 0,
    25                                       logger: logging.Logger = None,
    26                                       writer: SummaryWriter = None) -> int:
    27                                 """
    28                                 Trains a model for an epoch.
    29                             
    30                                 :param model: Model.
    31                                 :param data_loader: A MoleculeDataLoader.
    32                                 :param loss_func: Loss function.
    33                                 :param optimizer: An Optimizer.
    34                                 :param scheduler: A learning rate scheduler.
    35                                 :param args: Arguments.
    36                                 :param n_iter: The number of iterations (training examples) trained on so far.
    37                                 :param logger: A logger for printing intermediate results.
    38                                 :param writer: A tensorboardX SummaryWriter.
    39                                 :return: The total number of iterations (training examples) trained on so far.
    40                                 """
    41   2240.7 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
    42                                 
    43   2240.7 MiB      0.0 MiB       model.train()
    44   2240.7 MiB      0.0 MiB       loss_sum, iter_count = 0, 0
    45                             
    46   2240.7 MiB      0.0 MiB       for batch in tqdm(data_loader, total=len(data_loader)):
    47                                     # Prepare batch
    48                                     batch: MoleculeDataset
    49   2240.7 MiB      0.0 MiB           mol_batch, features_batch, target_batch = batch.batch_graph(), batch.features(), batch.targets()
    50   2240.7 MiB      0.0 MiB           mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])
    51   2240.7 MiB      0.0 MiB           targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])
    52                             
    53                                     # Run model
    54   2240.7 MiB      0.0 MiB           model.zero_grad()
    55   2240.7 MiB      0.0 MiB           preds = model(mol_batch, features_batch)
    56                             
    57                                     # Move tensors to correct device
    58   2240.7 MiB      0.0 MiB           mask = mask.to(preds.device)


  0% 0/2 [00:00<?, ?it/s][A
 50% 1/2 [00:00<00:00,  6.83it/s][A100% 2/2 [00:00<00:00, 12.14it/s]
Validation auc = 0.422917
 63% 19/30 [00:39<00:21,  1.97s/it]Epoch 19

  0% 0/9 [00:00<?, ?it/s][A
 11% 1/9 [00:00<00:00,  9.07it/s][A
 22% 2/9 [00:00<00:00,  7.83it/s][A
 33% 3/9 [00:00<00:00,  6.93it/s][A
 44% 4/9 [00:00<00:00,  6.48it/s][A
 56% 5/9 [00:00<00:00,  6.18it/s][A
 67% 6/9 [00:01<00:00,  5.36it/s][A
 78% 7/9 [00:01<00:00,  5.11it/s][ALoss = 1.2975e-03, PNorm = 25.0777, GNorm = 0.0858, lr_0 = 1.8529e-04

 89% 8/9 [00:01<00:00,  4.88it/s][A
100% 9/9 [00:01<00:00,  5.24it/s][A100% 9/9 [00:01<00:00,  5.40it/s]    59   2240.7 MiB      0.0 MiB           targets = targets.to(preds.device)
    60   2240.7 MiB      0.0 MiB           class_weights = torch.ones(targets.shape, device=preds.device)
    61                             
    62   2240.7 MiB      0.0 MiB           if args.dataset_type == 'multiclass':
    63                                         targets = targets.long()
    64                                         loss = torch.cat([loss_func(preds[:, target_index, :], targets[:, target_index]).unsqueeze(1) for target_index in range(preds.size(1))], dim=1) * class_weights * mask
    65                                     else:
    66   2240.7 MiB      0.0 MiB               loss = loss_func(preds, targets) * class_weights * mask
    67   2240.7 MiB      0.0 MiB           loss = loss.sum() / mask.sum()
    68                             
    69   2240.7 MiB      0.0 MiB           loss_sum += loss.item()
    70   2240.7 MiB      0.0 MiB           iter_count += len(batch)
    71                             
    72   2240.7 MiB      0.0 MiB           loss.backward()
    73   2240.7 MiB      0.0 MiB           optimizer.step()
    74                             
    75   2240.7 MiB      0.0 MiB           if isinstance(scheduler, NoamLR):
    76   2240.7 MiB      0.0 MiB               scheduler.step()
    77                             
    78   2240.7 MiB      0.0 MiB           n_iter += len(batch)
    79                             
    80                                     # Log and/or add to tensorboard
    81   2240.7 MiB      0.0 MiB           wandb.log({'n_iter': n_iter, 'loss_avg': loss_sum/iter_count})
    82   2240.7 MiB      0.0 MiB           if (n_iter // args.batch_size) % args.log_frequency == 0:
    83   2240.7 MiB      0.0 MiB               lrs = scheduler.get_lr()
    84   2240.7 MiB      0.0 MiB               pnorm = compute_pnorm(model)
    85   2240.7 MiB      0.0 MiB               gnorm = compute_gnorm(model)
    86   2240.7 MiB      0.0 MiB               loss_avg = loss_sum / iter_count
    87   2240.7 MiB      0.0 MiB               loss_sum, iter_count = 0, 0
    88                             
    89   2240.7 MiB      0.0 MiB               lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))
    90   2240.7 MiB      0.0 MiB               debug(f'Loss = {loss_avg:.4e}, PNorm = {pnorm:.4f}, GNorm = {gnorm:.4f}, {lrs_str}')
    91                             
    92   2240.7 MiB      0.0 MiB               if writer is not None:
    93   2240.7 MiB      0.0 MiB                   writer.add_scalar('train_loss', loss_avg, n_iter)
    94   2240.7 MiB      0.0 MiB                   writer.add_scalar('param_norm', pnorm, n_iter)
    95   2240.7 MiB      0.0 MiB                   writer.add_scalar('gradient_norm', gnorm, n_iter)
    96   2240.7 MiB      0.0 MiB                   for i, lr in enumerate(lrs):
    97   2240.7 MiB      0.0 MiB                       writer.add_scalar(f'learning_rate_{i}', lr, n_iter)
    98                             
    99   2240.7 MiB      0.0 MiB       return n_iter


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/train.py

Line #    Mem usage    Increment   Line Contents
================================================
    17   2240.7 MiB   2240.7 MiB   @profile
    18                             def train(model: nn.Module,
    19                                       data_loader: MoleculeDataLoader,
    20                                       loss_func: Callable,
    21                                       optimizer: Optimizer,
    22                                       scheduler: _LRScheduler,
    23                                       args: TrainArgs,
    24                                       n_iter: int = 0,
    25                                       logger: logging.Logger = None,
    26                                       writer: SummaryWriter = None) -> int:
    27                                 """
    28                                 Trains a model for an epoch.
    29                             
    30                                 :param model: Model.
    31                                 :param data_loader: A MoleculeDataLoader.
    32                                 :param loss_func: Loss function.
    33                                 :param optimizer: An Optimizer.
    34                                 :param scheduler: A learning rate scheduler.
    35                                 :param args: Arguments.
    36                                 :param n_iter: The number of iterations (training examples) trained on so far.
    37                                 :param logger: A logger for printing intermediate results.
    38                                 :param writer: A tensorboardX SummaryWriter.
    39                                 :return: The total number of iterations (training examples) trained on so far.
    40                                 """
    41   2240.7 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
    42                                 
    43   2240.7 MiB      0.0 MiB       model.train()
    44   2240.7 MiB      0.0 MiB       loss_sum, iter_count = 0, 0
    45                             
    46   2240.7 MiB      0.0 MiB       for batch in tqdm(data_loader, total=len(data_loader)):
    47                                     # Prepare batch
    48                                     batch: MoleculeDataset
    49   2240.7 MiB      0.0 MiB           mol_batch, features_batch, target_batch = batch.batch_graph(), batch.features(), batch.targets()
    50   2240.7 MiB      0.0 MiB           mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])
    51   2240.7 MiB      0.0 MiB           targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])
    52                             
    53                                     # Run model
    54   2240.7 MiB      0.0 MiB           model.zero_grad()
    55   2240.7 MiB      0.0 MiB           preds = model(mol_batch, features_batch)
    56                             
    57                                     # Move tensors to correct device
    58   2240.7 MiB      0.0 MiB           mask = mask.to(preds.device)
    59   2240.7 MiB      0.0 MiB           targets = targets.to(preds.device)
    60   2240.7 MiB      0.0 MiB           class_weights = torch.ones(targets.shape, device=preds.device)
    61                             
    62   2240.7 MiB      0.0 MiB           if args.dataset_type == 'multiclass':
    63                                         targets = targets.long()
    64                                         loss = torch.cat([loss_func(preds[:, target_index, :], targets[:, target_index]).unsqueeze(1) for target_index in range(preds.size(1))], dim=1) * class_weights * mask
    65                                     else:
    66   2240.7 MiB      0.0 MiB               loss = loss_func(preds, targets) * class_weights * mask
    67   2240.7 MiB      0.0 MiB           loss = loss.sum() / mask.sum()
    68                             
    69   2240.7 MiB      0.0 MiB           loss_sum += loss.item()
    70   2240.7 MiB      0.0 MiB           iter_count += len(batch)
    71                             
    72   2240.7 MiB      0.0 MiB           loss.backward()
    73   2240.7 MiB      0.0 MiB           optimizer.step()
    74                             
    75   2240.7 MiB      0.0 MiB           if isinstance(scheduler, NoamLR):
    76   2240.7 MiB      0.0 MiB               scheduler.step()
    77                             
    78   2240.7 MiB      0.0 MiB           n_iter += len(batch)
    79                             
    80                                     # Log and/or add to tensorboard
    81   2240.7 MiB      0.0 MiB           wandb.log({'n_iter': n_iter, 'loss_avg': loss_sum/iter_count})
    82   2240.7 MiB      0.0 MiB           if (n_iter // args.batch_size) % args.log_frequency == 0:
    83   2240.7 MiB      0.0 MiB               lrs = scheduler.get_lr()
    84   2240.7 MiB      0.0 MiB               pnorm = compute_pnorm(model)
    85   2240.7 MiB      0.0 MiB               gnorm = compute_gnorm(model)


  0% 0/2 [00:00<?, ?it/s][A
 50% 1/2 [00:00<00:00,  7.59it/s][A100% 2/2 [00:00<00:00, 14.22it/s]
Validation auc = 0.422917
 67% 20/30 [00:41<00:19,  2.00s/it]Epoch 20

  0% 0/9 [00:00<?, ?it/s][A
 11% 1/9 [00:00<00:01,  7.18it/s][A
 22% 2/9 [00:00<00:01,  6.46it/s][A
 33% 3/9 [00:00<00:01,  5.92it/s][A
 44% 4/9 [00:00<00:00,  5.74it/s][A
 56% 5/9 [00:00<00:00,  5.75it/s][A
 67% 6/9 [00:01<00:00,  5.41it/s][A
 78% 7/9 [00:01<00:00,  5.48it/s][A
 89% 8/9 [00:01<00:00,  5.30it/s][A
100% 9/9 [00:01<00:00,  6.00it/s][A100% 9/9 [00:01<00:00,  5.62it/s]

  0% 0/2 [00:00<?, ?it/s][A100% 2/2 [00:00<00:00, 24.05it/s]
Validation auc = 0.422917
 70% 21/30 [00:43<00:17,  1.96s/it]Epoch 21

  0% 0/9 [00:00<?, ?it/s][ALoss = 1.0995e-03, PNorm = 25.1169, GNorm = 0.0720, lr_0 = 1.6548e-04

 11% 1/9 [00:00<00:01,  6.51it/s][A
 22% 2/9 [00:00<00:01,  6.27it/s][A
 33% 3/9 [00:00<00:00,  6.25it/s][A
 44% 4/9 [00:00<00:00,  6.02it/s][A
 56% 5/9 [00:00<00:00,  5.88it/s][A
 67% 6/9 [00:01<00:00,  5.74it/s][A
 78% 7/9 [00:01<00:00,  5.28it/s][A
 89% 8/9 [00:01<00:00,  5.07it/s][A
100% 9/9 [00:01<00:00,  5.82it/s][A100% 9/9 [00:01<00:00,  5.67it/s]    86   2240.7 MiB      0.0 MiB               loss_avg = loss_sum / iter_count
    87   2240.7 MiB      0.0 MiB               loss_sum, iter_count = 0, 0
    88                             
    89   2240.7 MiB      0.0 MiB               lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))
    90   2240.7 MiB      0.0 MiB               debug(f'Loss = {loss_avg:.4e}, PNorm = {pnorm:.4f}, GNorm = {gnorm:.4f}, {lrs_str}')
    91                             
    92   2240.7 MiB      0.0 MiB               if writer is not None:
    93   2240.7 MiB      0.0 MiB                   writer.add_scalar('train_loss', loss_avg, n_iter)
    94   2240.7 MiB      0.0 MiB                   writer.add_scalar('param_norm', pnorm, n_iter)
    95   2240.7 MiB      0.0 MiB                   writer.add_scalar('gradient_norm', gnorm, n_iter)
    96   2240.7 MiB      0.0 MiB                   for i, lr in enumerate(lrs):
    97   2240.7 MiB      0.0 MiB                       writer.add_scalar(f'learning_rate_{i}', lr, n_iter)
    98                             
    99   2240.7 MiB      0.0 MiB       return n_iter


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/train.py

Line #    Mem usage    Increment   Line Contents
================================================
    17   2240.7 MiB   2240.7 MiB   @profile
    18                             def train(model: nn.Module,
    19                                       data_loader: MoleculeDataLoader,
    20                                       loss_func: Callable,
    21                                       optimizer: Optimizer,
    22                                       scheduler: _LRScheduler,
    23                                       args: TrainArgs,
    24                                       n_iter: int = 0,
    25                                       logger: logging.Logger = None,
    26                                       writer: SummaryWriter = None) -> int:
    27                                 """
    28                                 Trains a model for an epoch.
    29                             
    30                                 :param model: Model.
    31                                 :param data_loader: A MoleculeDataLoader.
    32                                 :param loss_func: Loss function.
    33                                 :param optimizer: An Optimizer.
    34                                 :param scheduler: A learning rate scheduler.
    35                                 :param args: Arguments.
    36                                 :param n_iter: The number of iterations (training examples) trained on so far.
    37                                 :param logger: A logger for printing intermediate results.
    38                                 :param writer: A tensorboardX SummaryWriter.
    39                                 :return: The total number of iterations (training examples) trained on so far.
    40                                 """
    41   2240.7 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
    42                                 
    43   2240.7 MiB      0.0 MiB       model.train()
    44   2240.7 MiB      0.0 MiB       loss_sum, iter_count = 0, 0
    45                             
    46   2240.7 MiB      0.0 MiB       for batch in tqdm(data_loader, total=len(data_loader)):
    47                                     # Prepare batch
    48                                     batch: MoleculeDataset
    49   2240.7 MiB      0.0 MiB           mol_batch, features_batch, target_batch = batch.batch_graph(), batch.features(), batch.targets()
    50   2240.7 MiB      0.0 MiB           mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])
    51   2240.7 MiB      0.0 MiB           targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])
    52                             
    53                                     # Run model
    54   2240.7 MiB      0.0 MiB           model.zero_grad()
    55   2240.7 MiB      0.0 MiB           preds = model(mol_batch, features_batch)
    56                             
    57                                     # Move tensors to correct device
    58   2240.7 MiB      0.0 MiB           mask = mask.to(preds.device)
    59   2240.7 MiB      0.0 MiB           targets = targets.to(preds.device)
    60   2240.7 MiB      0.0 MiB           class_weights = torch.ones(targets.shape, device=preds.device)
    61                             
    62   2240.7 MiB      0.0 MiB           if args.dataset_type == 'multiclass':
    63                                         targets = targets.long()
    64                                         loss = torch.cat([loss_func(preds[:, target_index, :], targets[:, target_index]).unsqueeze(1) for target_index in range(preds.size(1))], dim=1) * class_weights * mask
    65                                     else:
    66   2240.7 MiB      0.0 MiB               loss = loss_func(preds, targets) * class_weights * mask
    67   2240.7 MiB      0.0 MiB           loss = loss.sum() / mask.sum()
    68                             
    69   2240.7 MiB      0.0 MiB           loss_sum += loss.item()
    70   2240.7 MiB      0.0 MiB           iter_count += len(batch)
    71                             
    72   2240.7 MiB      0.0 MiB           loss.backward()
    73   2240.7 MiB      0.0 MiB           optimizer.step()
    74                             
    75   2240.7 MiB      0.0 MiB           if isinstance(scheduler, NoamLR):
    76   2240.7 MiB      0.0 MiB               scheduler.step()
    77                             
    78   2240.7 MiB      0.0 MiB           n_iter += len(batch)
    79                             
    80                                     # Log and/or add to tensorboard
    81   2240.7 MiB      0.0 MiB           wandb.log({'n_iter': n_iter, 'loss_avg': loss_sum/iter_count})
    82   2240.7 MiB      0.0 MiB           if (n_iter // args.batch_size) % args.log_frequency == 0:
    83                                         lrs = scheduler.get_lr()
    84                                         pnorm = compute_pnorm(model)
    85                                         gnorm = compute_gnorm(model)
    86                                         loss_avg = loss_sum / iter_count
    87                                         loss_sum, iter_count = 0, 0
    88                             
    89                                         lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))
    90                                         debug(f'Loss = {loss_avg:.4e}, PNorm = {pnorm:.4f}, GNorm = {gnorm:.4f}, {lrs_str}')
    91                             
    92                                         if writer is not None:
    93                                             writer.add_scalar('train_loss', loss_avg, n_iter)
    94                                             writer.add_scalar('param_norm', pnorm, n_iter)
    95                                             writer.add_scalar('gradient_norm', gnorm, n_iter)
    96                                             for i, lr in enumerate(lrs):
    97                                                 writer.add_scalar(f'learning_rate_{i}', lr, n_iter)
    98                             
    99   2240.7 MiB      0.0 MiB       return n_iter


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/train.py

Line #    Mem usage    Increment   Line Contents
================================================
    17   2240.7 MiB   2240.7 MiB   @profile
    18                             def train(model: nn.Module,
    19                                       data_loader: MoleculeDataLoader,
    20                                       loss_func: Callable,
    21                                       optimizer: Optimizer,
    22                                       scheduler: _LRScheduler,
    23                                       args: TrainArgs,
    24                                       n_iter: int = 0,
    25                                       logger: logging.Logger = None,
    26                                       writer: SummaryWriter = None) -> int:


  0% 0/2 [00:00<?, ?it/s][A100% 2/2 [00:00<00:00, 25.16it/s]
Validation auc = 0.422917
 73% 22/30 [00:45<00:15,  1.94s/it]Epoch 22

  0% 0/9 [00:00<?, ?it/s][A
 11% 1/9 [00:00<00:00,  8.72it/s][ALoss = 1.5886e-03, PNorm = 25.1492, GNorm = 0.1221, lr_0 = 1.4932e-04

 22% 2/9 [00:00<00:01,  6.66it/s][A
 33% 3/9 [00:00<00:00,  6.28it/s][A
 44% 4/9 [00:00<00:00,  5.39it/s][A
 56% 5/9 [00:00<00:00,  5.81it/s][A
 67% 6/9 [00:01<00:00,  5.54it/s][A
 78% 7/9 [00:01<00:00,  5.28it/s][A
 89% 8/9 [00:01<00:00,  5.28it/s][A
100% 9/9 [00:01<00:00,  5.88it/s][A100% 9/9 [00:01<00:00,  5.48it/s]    27                                 """
    28                                 Trains a model for an epoch.
    29                             
    30                                 :param model: Model.
    31                                 :param data_loader: A MoleculeDataLoader.
    32                                 :param loss_func: Loss function.
    33                                 :param optimizer: An Optimizer.
    34                                 :param scheduler: A learning rate scheduler.
    35                                 :param args: Arguments.
    36                                 :param n_iter: The number of iterations (training examples) trained on so far.
    37                                 :param logger: A logger for printing intermediate results.
    38                                 :param writer: A tensorboardX SummaryWriter.
    39                                 :return: The total number of iterations (training examples) trained on so far.
    40                                 """
    41   2240.7 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
    42                                 
    43   2240.7 MiB      0.0 MiB       model.train()
    44   2240.7 MiB      0.0 MiB       loss_sum, iter_count = 0, 0
    45                             
    46   2240.7 MiB      0.0 MiB       for batch in tqdm(data_loader, total=len(data_loader)):
    47                                     # Prepare batch
    48                                     batch: MoleculeDataset
    49   2240.7 MiB      0.0 MiB           mol_batch, features_batch, target_batch = batch.batch_graph(), batch.features(), batch.targets()
    50   2240.7 MiB      0.0 MiB           mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])
    51   2240.7 MiB      0.0 MiB           targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])
    52                             
    53                                     # Run model
    54   2240.7 MiB      0.0 MiB           model.zero_grad()
    55   2240.7 MiB      0.0 MiB           preds = model(mol_batch, features_batch)
    56                             
    57                                     # Move tensors to correct device
    58   2240.7 MiB      0.0 MiB           mask = mask.to(preds.device)
    59   2240.7 MiB      0.0 MiB           targets = targets.to(preds.device)
    60   2240.7 MiB      0.0 MiB           class_weights = torch.ones(targets.shape, device=preds.device)
    61                             
    62   2240.7 MiB      0.0 MiB           if args.dataset_type == 'multiclass':
    63                                         targets = targets.long()
    64                                         loss = torch.cat([loss_func(preds[:, target_index, :], targets[:, target_index]).unsqueeze(1) for target_index in range(preds.size(1))], dim=1) * class_weights * mask
    65                                     else:
    66   2240.7 MiB      0.0 MiB               loss = loss_func(preds, targets) * class_weights * mask
    67   2240.7 MiB      0.0 MiB           loss = loss.sum() / mask.sum()
    68                             
    69   2240.7 MiB      0.0 MiB           loss_sum += loss.item()
    70   2240.7 MiB      0.0 MiB           iter_count += len(batch)
    71                             
    72   2240.7 MiB      0.0 MiB           loss.backward()
    73   2240.7 MiB      0.0 MiB           optimizer.step()
    74                             
    75   2240.7 MiB      0.0 MiB           if isinstance(scheduler, NoamLR):
    76   2240.7 MiB      0.0 MiB               scheduler.step()
    77                             
    78   2240.7 MiB      0.0 MiB           n_iter += len(batch)
    79                             
    80                                     # Log and/or add to tensorboard
    81   2240.7 MiB      0.0 MiB           wandb.log({'n_iter': n_iter, 'loss_avg': loss_sum/iter_count})
    82   2240.7 MiB      0.0 MiB           if (n_iter // args.batch_size) % args.log_frequency == 0:
    83   2240.7 MiB      0.0 MiB               lrs = scheduler.get_lr()
    84   2240.7 MiB      0.0 MiB               pnorm = compute_pnorm(model)
    85   2240.7 MiB      0.0 MiB               gnorm = compute_gnorm(model)
    86   2240.7 MiB      0.0 MiB               loss_avg = loss_sum / iter_count
    87   2240.7 MiB      0.0 MiB               loss_sum, iter_count = 0, 0
    88                             
    89   2240.7 MiB      0.0 MiB               lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))
    90   2240.7 MiB      0.0 MiB               debug(f'Loss = {loss_avg:.4e}, PNorm = {pnorm:.4f}, GNorm = {gnorm:.4f}, {lrs_str}')
    91                             
    92   2240.7 MiB      0.0 MiB               if writer is not None:
    93   2240.7 MiB      0.0 MiB                   writer.add_scalar('train_loss', loss_avg, n_iter)
    94   2240.7 MiB      0.0 MiB                   writer.add_scalar('param_norm', pnorm, n_iter)
    95   2240.7 MiB      0.0 MiB                   writer.add_scalar('gradient_norm', gnorm, n_iter)
    96   2240.7 MiB      0.0 MiB                   for i, lr in enumerate(lrs):
    97   2240.7 MiB      0.0 MiB                       writer.add_scalar(f'learning_rate_{i}', lr, n_iter)
    98                             
    99   2240.7 MiB      0.0 MiB       return n_iter


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/train.py

Line #    Mem usage    Increment   Line Contents
================================================
    17   2240.7 MiB   2240.7 MiB   @profile
    18                             def train(model: nn.Module,
    19                                       data_loader: MoleculeDataLoader,
    20                                       loss_func: Callable,
    21                                       optimizer: Optimizer,
    22                                       scheduler: _LRScheduler,
    23                                       args: TrainArgs,
    24                                       n_iter: int = 0,
    25                                       logger: logging.Logger = None,
    26                                       writer: SummaryWriter = None) -> int:
    27                                 """
    28                                 Trains a model for an epoch.
    29                             
    30                                 :param model: Model.
    31                                 :param data_loader: A MoleculeDataLoader.
    32                                 :param loss_func: Loss function.
    33                                 :param optimizer: An Optimizer.
    34                                 :param scheduler: A learning rate scheduler.
    35                                 :param args: Arguments.
    36                                 :param n_iter: The number of iterations (training examples) trained on so far.
    37                                 :param logger: A logger for printing intermediate results.
    38                                 :param writer: A tensorboardX SummaryWriter.
    39                                 :return: The total number of iterations (training examples) trained on so far.
    40                                 """
    41   2240.7 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
    42                                 
    43   2240.7 MiB      0.0 MiB       model.train()
    44   2240.7 MiB      0.0 MiB       loss_sum, iter_count = 0, 0
    45                             
    46   2240.7 MiB      0.0 MiB       for batch in tqdm(data_loader, total=len(data_loader)):
    47                                     # Prepare batch
    48                                     batch: MoleculeDataset
    49   2240.7 MiB      0.0 MiB           mol_batch, features_batch, target_batch = batch.batch_graph(), batch.features(), batch.targets()
    50   2240.7 MiB      0.0 MiB           mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])
    51   2240.7 MiB      0.0 MiB           targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])


  0% 0/2 [00:00<?, ?it/s][A
 50% 1/2 [00:00<00:00,  7.40it/s][A100% 2/2 [00:00<00:00, 13.01it/s]
Validation auc = 0.422917
 77% 23/30 [00:47<00:13,  1.97s/it]Epoch 23

  0% 0/9 [00:00<?, ?it/s][A
 22% 2/9 [00:00<00:01,  6.37it/s][A
 33% 3/9 [00:00<00:01,  5.85it/s][ALoss = 1.0084e-03, PNorm = 25.1807, GNorm = 0.0491, lr_0 = 1.3335e-04

 44% 4/9 [00:00<00:00,  5.64it/s][A
 56% 5/9 [00:00<00:00,  5.69it/s][A
 67% 6/9 [00:01<00:00,  5.54it/s][A
 78% 7/9 [00:01<00:00,  5.53it/s][A
 89% 8/9 [00:01<00:00,  5.37it/s][A
100% 9/9 [00:01<00:00,  5.40it/s][A100% 9/9 [00:01<00:00,  5.50it/s]    52                             
    53                                     # Run model
    54   2240.7 MiB      0.0 MiB           model.zero_grad()
    55   2240.7 MiB      0.0 MiB           preds = model(mol_batch, features_batch)
    56                             
    57                                     # Move tensors to correct device
    58   2240.7 MiB      0.0 MiB           mask = mask.to(preds.device)
    59   2240.7 MiB      0.0 MiB           targets = targets.to(preds.device)
    60   2240.7 MiB      0.0 MiB           class_weights = torch.ones(targets.shape, device=preds.device)
    61                             
    62   2240.7 MiB      0.0 MiB           if args.dataset_type == 'multiclass':
    63                                         targets = targets.long()
    64                                         loss = torch.cat([loss_func(preds[:, target_index, :], targets[:, target_index]).unsqueeze(1) for target_index in range(preds.size(1))], dim=1) * class_weights * mask
    65                                     else:
    66   2240.7 MiB      0.0 MiB               loss = loss_func(preds, targets) * class_weights * mask
    67   2240.7 MiB      0.0 MiB           loss = loss.sum() / mask.sum()
    68                             
    69   2240.7 MiB      0.0 MiB           loss_sum += loss.item()
    70   2240.7 MiB      0.0 MiB           iter_count += len(batch)
    71                             
    72   2240.7 MiB      0.0 MiB           loss.backward()
    73   2240.7 MiB      0.0 MiB           optimizer.step()
    74                             
    75   2240.7 MiB      0.0 MiB           if isinstance(scheduler, NoamLR):
    76   2240.7 MiB      0.0 MiB               scheduler.step()
    77                             
    78   2240.7 MiB      0.0 MiB           n_iter += len(batch)
    79                             
    80                                     # Log and/or add to tensorboard
    81   2240.7 MiB      0.0 MiB           wandb.log({'n_iter': n_iter, 'loss_avg': loss_sum/iter_count})
    82   2240.7 MiB      0.0 MiB           if (n_iter // args.batch_size) % args.log_frequency == 0:
    83   2240.7 MiB      0.0 MiB               lrs = scheduler.get_lr()
    84   2240.7 MiB      0.0 MiB               pnorm = compute_pnorm(model)
    85   2240.7 MiB      0.0 MiB               gnorm = compute_gnorm(model)
    86   2240.7 MiB      0.0 MiB               loss_avg = loss_sum / iter_count
    87   2240.7 MiB      0.0 MiB               loss_sum, iter_count = 0, 0
    88                             
    89   2240.7 MiB      0.0 MiB               lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))
    90   2240.7 MiB      0.0 MiB               debug(f'Loss = {loss_avg:.4e}, PNorm = {pnorm:.4f}, GNorm = {gnorm:.4f}, {lrs_str}')
    91                             
    92   2240.7 MiB      0.0 MiB               if writer is not None:
    93   2240.7 MiB      0.0 MiB                   writer.add_scalar('train_loss', loss_avg, n_iter)
    94   2240.7 MiB      0.0 MiB                   writer.add_scalar('param_norm', pnorm, n_iter)
    95   2240.7 MiB      0.0 MiB                   writer.add_scalar('gradient_norm', gnorm, n_iter)
    96   2240.7 MiB      0.0 MiB                   for i, lr in enumerate(lrs):
    97   2240.7 MiB      0.0 MiB                       writer.add_scalar(f'learning_rate_{i}', lr, n_iter)
    98                             
    99   2240.7 MiB      0.0 MiB       return n_iter


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/train.py

Line #    Mem usage    Increment   Line Contents
================================================
    17   2240.7 MiB   2240.7 MiB   @profile
    18                             def train(model: nn.Module,
    19                                       data_loader: MoleculeDataLoader,
    20                                       loss_func: Callable,
    21                                       optimizer: Optimizer,
    22                                       scheduler: _LRScheduler,
    23                                       args: TrainArgs,
    24                                       n_iter: int = 0,
    25                                       logger: logging.Logger = None,
    26                                       writer: SummaryWriter = None) -> int:
    27                                 """
    28                                 Trains a model for an epoch.
    29                             
    30                                 :param model: Model.
    31                                 :param data_loader: A MoleculeDataLoader.
    32                                 :param loss_func: Loss function.
    33                                 :param optimizer: An Optimizer.
    34                                 :param scheduler: A learning rate scheduler.
    35                                 :param args: Arguments.
    36                                 :param n_iter: The number of iterations (training examples) trained on so far.
    37                                 :param logger: A logger for printing intermediate results.
    38                                 :param writer: A tensorboardX SummaryWriter.
    39                                 :return: The total number of iterations (training examples) trained on so far.
    40                                 """
    41   2240.7 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
    42                                 
    43   2240.7 MiB      0.0 MiB       model.train()
    44   2240.7 MiB      0.0 MiB       loss_sum, iter_count = 0, 0
    45                             
    46   2240.7 MiB      0.0 MiB       for batch in tqdm(data_loader, total=len(data_loader)):
    47                                     # Prepare batch
    48                                     batch: MoleculeDataset
    49   2240.7 MiB      0.0 MiB           mol_batch, features_batch, target_batch = batch.batch_graph(), batch.features(), batch.targets()
    50   2240.7 MiB      0.0 MiB           mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])
    51   2240.7 MiB      0.0 MiB           targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])
    52                             
    53                                     # Run model
    54   2240.7 MiB      0.0 MiB           model.zero_grad()
    55   2240.7 MiB      0.0 MiB           preds = model(mol_batch, features_batch)
    56                             
    57                                     # Move tensors to correct device
    58   2240.7 MiB      0.0 MiB           mask = mask.to(preds.device)
    59   2240.7 MiB      0.0 MiB           targets = targets.to(preds.device)
    60   2240.7 MiB      0.0 MiB           class_weights = torch.ones(targets.shape, device=preds.device)
    61                             
    62   2240.7 MiB      0.0 MiB           if args.dataset_type == 'multiclass':
    63                                         targets = targets.long()
    64                                         loss = torch.cat([loss_func(preds[:, target_index, :], targets[:, target_index]).unsqueeze(1) for target_index in range(preds.size(1))], dim=1) * class_weights * mask
    65                                     else:
    66   2240.7 MiB      0.0 MiB               loss = loss_func(preds, targets) * class_weights * mask
    67   2240.7 MiB      0.0 MiB           loss = loss.sum() / mask.sum()
    68                             
    69   2240.7 MiB      0.0 MiB           loss_sum += loss.item()
    70   2240.7 MiB      0.0 MiB           iter_count += len(batch)
    71                             
    72   2240.7 MiB      0.0 MiB           loss.backward()
    73   2240.7 MiB      0.0 MiB           optimizer.step()
    74                             
    75   2240.7 MiB      0.0 MiB           if isinstance(scheduler, NoamLR):
    76   2240.7 MiB      0.0 MiB               scheduler.step()
    77                             
    78   2240.7 MiB      0.0 MiB           n_iter += len(batch)
    79                             
    80                                     # Log and/or add to tensorboard


  0% 0/2 [00:00<?, ?it/s][A100% 2/2 [00:00<00:00, 24.00it/s]
Validation auc = 0.422917
 80% 24/30 [00:49<00:11,  1.95s/it]Epoch 24

  0% 0/9 [00:00<?, ?it/s][A
 22% 2/9 [00:00<00:01,  6.44it/s][A
 33% 3/9 [00:00<00:01,  5.91it/s][A
 44% 4/9 [00:00<00:00,  5.83it/s][ALoss = 1.0462e-03, PNorm = 25.2062, GNorm = 0.1990, lr_0 = 1.2033e-04

 56% 5/9 [00:00<00:00,  5.19it/s][A
 67% 6/9 [00:01<00:00,  5.41it/s][A
 78% 7/9 [00:01<00:00,  4.98it/s][A
 89% 8/9 [00:01<00:00,  5.10it/s][A
100% 9/9 [00:01<00:00,  5.68it/s][A100% 9/9 [00:01<00:00,  5.45it/s]

  0% 0/2 [00:00<?, ?it/s][A
 50% 1/2 [00:00<00:00,  7.34it/s][A100% 2/2 [00:00<00:00, 13.80it/s]
Validation auc = 0.422917
 83% 25/30 [00:51<00:09,  1.99s/it]Epoch 25

  0% 0/9 [00:00<?, ?it/s][A
 22% 2/9 [00:00<00:00,  7.58it/s][A
 33% 3/9 [00:00<00:00,  6.90it/s][A
 44% 4/9 [00:00<00:00,  6.14it/s][A
 56% 5/9 [00:00<00:00,  5.94it/s][ALoss = 1.0379e-03, PNorm = 25.2294, GNorm = 0.1087, lr_0 = 1.0857e-04

 67% 6/9 [00:01<00:00,  5.66it/s][A
 78% 7/9 [00:01<00:00,  5.56it/s][A
 89% 8/9 [00:01<00:00,  5.26it/s][A
100% 9/9 [00:01<00:00,  5.48it/s][A100% 9/9 [00:01<00:00,  5.66it/s]    81   2240.7 MiB      0.0 MiB           wandb.log({'n_iter': n_iter, 'loss_avg': loss_sum/iter_count})
    82   2240.7 MiB      0.0 MiB           if (n_iter // args.batch_size) % args.log_frequency == 0:
    83   2240.7 MiB      0.0 MiB               lrs = scheduler.get_lr()
    84   2240.7 MiB      0.0 MiB               pnorm = compute_pnorm(model)
    85   2240.7 MiB      0.0 MiB               gnorm = compute_gnorm(model)
    86   2240.7 MiB      0.0 MiB               loss_avg = loss_sum / iter_count
    87   2240.7 MiB      0.0 MiB               loss_sum, iter_count = 0, 0
    88                             
    89   2240.7 MiB      0.0 MiB               lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))
    90   2240.7 MiB      0.0 MiB               debug(f'Loss = {loss_avg:.4e}, PNorm = {pnorm:.4f}, GNorm = {gnorm:.4f}, {lrs_str}')
    91                             
    92   2240.7 MiB      0.0 MiB               if writer is not None:
    93   2240.7 MiB      0.0 MiB                   writer.add_scalar('train_loss', loss_avg, n_iter)
    94   2240.7 MiB      0.0 MiB                   writer.add_scalar('param_norm', pnorm, n_iter)
    95   2240.7 MiB      0.0 MiB                   writer.add_scalar('gradient_norm', gnorm, n_iter)
    96   2240.7 MiB      0.0 MiB                   for i, lr in enumerate(lrs):
    97   2240.7 MiB      0.0 MiB                       writer.add_scalar(f'learning_rate_{i}', lr, n_iter)
    98                             
    99   2240.7 MiB      0.0 MiB       return n_iter


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/train.py

Line #    Mem usage    Increment   Line Contents
================================================
    17   2240.7 MiB   2240.7 MiB   @profile
    18                             def train(model: nn.Module,
    19                                       data_loader: MoleculeDataLoader,
    20                                       loss_func: Callable,
    21                                       optimizer: Optimizer,
    22                                       scheduler: _LRScheduler,
    23                                       args: TrainArgs,
    24                                       n_iter: int = 0,
    25                                       logger: logging.Logger = None,
    26                                       writer: SummaryWriter = None) -> int:
    27                                 """
    28                                 Trains a model for an epoch.
    29                             
    30                                 :param model: Model.
    31                                 :param data_loader: A MoleculeDataLoader.
    32                                 :param loss_func: Loss function.
    33                                 :param optimizer: An Optimizer.
    34                                 :param scheduler: A learning rate scheduler.
    35                                 :param args: Arguments.
    36                                 :param n_iter: The number of iterations (training examples) trained on so far.
    37                                 :param logger: A logger for printing intermediate results.
    38                                 :param writer: A tensorboardX SummaryWriter.
    39                                 :return: The total number of iterations (training examples) trained on so far.
    40                                 """
    41   2240.7 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
    42                                 
    43   2240.7 MiB      0.0 MiB       model.train()
    44   2240.7 MiB      0.0 MiB       loss_sum, iter_count = 0, 0
    45                             
    46   2240.7 MiB      0.0 MiB       for batch in tqdm(data_loader, total=len(data_loader)):
    47                                     # Prepare batch
    48                                     batch: MoleculeDataset
    49   2240.7 MiB      0.0 MiB           mol_batch, features_batch, target_batch = batch.batch_graph(), batch.features(), batch.targets()
    50   2240.7 MiB      0.0 MiB           mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])
    51   2240.7 MiB      0.0 MiB           targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])
    52                             
    53                                     # Run model
    54   2240.7 MiB      0.0 MiB           model.zero_grad()
    55   2240.7 MiB      0.0 MiB           preds = model(mol_batch, features_batch)
    56                             
    57                                     # Move tensors to correct device
    58   2240.7 MiB      0.0 MiB           mask = mask.to(preds.device)
    59   2240.7 MiB      0.0 MiB           targets = targets.to(preds.device)
    60   2240.7 MiB      0.0 MiB           class_weights = torch.ones(targets.shape, device=preds.device)
    61                             
    62   2240.7 MiB      0.0 MiB           if args.dataset_type == 'multiclass':
    63                                         targets = targets.long()
    64                                         loss = torch.cat([loss_func(preds[:, target_index, :], targets[:, target_index]).unsqueeze(1) for target_index in range(preds.size(1))], dim=1) * class_weights * mask
    65                                     else:
    66   2240.7 MiB      0.0 MiB               loss = loss_func(preds, targets) * class_weights * mask
    67   2240.7 MiB      0.0 MiB           loss = loss.sum() / mask.sum()
    68                             
    69   2240.7 MiB      0.0 MiB           loss_sum += loss.item()
    70   2240.7 MiB      0.0 MiB           iter_count += len(batch)
    71                             
    72   2240.7 MiB      0.0 MiB           loss.backward()
    73   2240.7 MiB      0.0 MiB           optimizer.step()
    74                             
    75   2240.7 MiB      0.0 MiB           if isinstance(scheduler, NoamLR):
    76   2240.7 MiB      0.0 MiB               scheduler.step()
    77                             
    78   2240.7 MiB      0.0 MiB           n_iter += len(batch)
    79                             
    80                                     # Log and/or add to tensorboard
    81   2240.7 MiB      0.0 MiB           wandb.log({'n_iter': n_iter, 'loss_avg': loss_sum/iter_count})
    82   2240.7 MiB      0.0 MiB           if (n_iter // args.batch_size) % args.log_frequency == 0:
    83   2240.7 MiB      0.0 MiB               lrs = scheduler.get_lr()
    84   2240.7 MiB      0.0 MiB               pnorm = compute_pnorm(model)
    85   2240.7 MiB      0.0 MiB               gnorm = compute_gnorm(model)
    86   2240.7 MiB      0.0 MiB               loss_avg = loss_sum / iter_count
    87   2240.7 MiB      0.0 MiB               loss_sum, iter_count = 0, 0
    88                             
    89   2240.7 MiB      0.0 MiB               lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))
    90   2240.7 MiB      0.0 MiB               debug(f'Loss = {loss_avg:.4e}, PNorm = {pnorm:.4f}, GNorm = {gnorm:.4f}, {lrs_str}')
    91                             
    92   2240.7 MiB      0.0 MiB               if writer is not None:
    93   2240.7 MiB      0.0 MiB                   writer.add_scalar('train_loss', loss_avg, n_iter)
    94   2240.7 MiB      0.0 MiB                   writer.add_scalar('param_norm', pnorm, n_iter)
    95   2240.7 MiB      0.0 MiB                   writer.add_scalar('gradient_norm', gnorm, n_iter)
    96   2240.7 MiB      0.0 MiB                   for i, lr in enumerate(lrs):
    97   2240.7 MiB      0.0 MiB                       writer.add_scalar(f'learning_rate_{i}', lr, n_iter)
    98                             
    99   2240.7 MiB      0.0 MiB       return n_iter


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/train.py

Line #    Mem usage    Increment   Line Contents
================================================
    17   2240.7 MiB   2240.7 MiB   @profile
    18                             def train(model: nn.Module,
    19                                       data_loader: MoleculeDataLoader,


  0% 0/2 [00:00<?, ?it/s][A100% 2/2 [00:00<00:00, 25.63it/s]
Validation auc = 0.422917
 87% 26/30 [00:53<00:07,  1.95s/it]Epoch 26

  0% 0/9 [00:00<?, ?it/s][A
 22% 2/9 [00:00<00:00,  7.60it/s][A
 33% 3/9 [00:00<00:00,  6.23it/s][A
 44% 4/9 [00:00<00:00,  6.02it/s][A
 56% 5/9 [00:00<00:00,  5.88it/s][A
 67% 6/9 [00:01<00:00,  5.83it/s][A
 78% 7/9 [00:01<00:00,  5.44it/s][ALoss = 9.4802e-04, PNorm = 25.2524, GNorm = 0.1476, lr_0 = 1.0000e-04

 89% 8/9 [00:01<00:00,  5.35it/s][A
100% 9/9 [00:01<00:00,  5.46it/s][A100% 9/9 [00:01<00:00,  5.60it/s]    20                                       loss_func: Callable,
    21                                       optimizer: Optimizer,
    22                                       scheduler: _LRScheduler,
    23                                       args: TrainArgs,
    24                                       n_iter: int = 0,
    25                                       logger: logging.Logger = None,
    26                                       writer: SummaryWriter = None) -> int:
    27                                 """
    28                                 Trains a model for an epoch.
    29                             
    30                                 :param model: Model.
    31                                 :param data_loader: A MoleculeDataLoader.
    32                                 :param loss_func: Loss function.
    33                                 :param optimizer: An Optimizer.
    34                                 :param scheduler: A learning rate scheduler.
    35                                 :param args: Arguments.
    36                                 :param n_iter: The number of iterations (training examples) trained on so far.
    37                                 :param logger: A logger for printing intermediate results.
    38                                 :param writer: A tensorboardX SummaryWriter.
    39                                 :return: The total number of iterations (training examples) trained on so far.
    40                                 """
    41   2240.7 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
    42                                 
    43   2240.7 MiB      0.0 MiB       model.train()
    44   2240.7 MiB      0.0 MiB       loss_sum, iter_count = 0, 0
    45                             
    46   2240.7 MiB      0.0 MiB       for batch in tqdm(data_loader, total=len(data_loader)):
    47                                     # Prepare batch
    48                                     batch: MoleculeDataset
    49   2240.7 MiB      0.0 MiB           mol_batch, features_batch, target_batch = batch.batch_graph(), batch.features(), batch.targets()
    50   2240.7 MiB      0.0 MiB           mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])
    51   2240.7 MiB      0.0 MiB           targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])
    52                             
    53                                     # Run model
    54   2240.7 MiB      0.0 MiB           model.zero_grad()
    55   2240.7 MiB      0.0 MiB           preds = model(mol_batch, features_batch)
    56                             
    57                                     # Move tensors to correct device
    58   2240.7 MiB      0.0 MiB           mask = mask.to(preds.device)
    59   2240.7 MiB      0.0 MiB           targets = targets.to(preds.device)
    60   2240.7 MiB      0.0 MiB           class_weights = torch.ones(targets.shape, device=preds.device)
    61                             
    62   2240.7 MiB      0.0 MiB           if args.dataset_type == 'multiclass':
    63                                         targets = targets.long()
    64                                         loss = torch.cat([loss_func(preds[:, target_index, :], targets[:, target_index]).unsqueeze(1) for target_index in range(preds.size(1))], dim=1) * class_weights * mask
    65                                     else:
    66   2240.7 MiB      0.0 MiB               loss = loss_func(preds, targets) * class_weights * mask
    67   2240.7 MiB      0.0 MiB           loss = loss.sum() / mask.sum()
    68                             
    69   2240.7 MiB      0.0 MiB           loss_sum += loss.item()
    70   2240.7 MiB      0.0 MiB           iter_count += len(batch)
    71                             
    72   2240.7 MiB      0.0 MiB           loss.backward()
    73   2240.7 MiB      0.0 MiB           optimizer.step()
    74                             
    75   2240.7 MiB      0.0 MiB           if isinstance(scheduler, NoamLR):
    76   2240.7 MiB      0.0 MiB               scheduler.step()
    77                             
    78   2240.7 MiB      0.0 MiB           n_iter += len(batch)
    79                             
    80                                     # Log and/or add to tensorboard
    81   2240.7 MiB      0.0 MiB           wandb.log({'n_iter': n_iter, 'loss_avg': loss_sum/iter_count})
    82   2240.7 MiB      0.0 MiB           if (n_iter // args.batch_size) % args.log_frequency == 0:
    83   2240.7 MiB      0.0 MiB               lrs = scheduler.get_lr()
    84   2240.7 MiB      0.0 MiB               pnorm = compute_pnorm(model)
    85   2240.7 MiB      0.0 MiB               gnorm = compute_gnorm(model)
    86   2240.7 MiB      0.0 MiB               loss_avg = loss_sum / iter_count
    87   2240.7 MiB      0.0 MiB               loss_sum, iter_count = 0, 0
    88                             
    89   2240.7 MiB      0.0 MiB               lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))
    90   2240.7 MiB      0.0 MiB               debug(f'Loss = {loss_avg:.4e}, PNorm = {pnorm:.4f}, GNorm = {gnorm:.4f}, {lrs_str}')
    91                             
    92   2240.7 MiB      0.0 MiB               if writer is not None:
    93   2240.7 MiB      0.0 MiB                   writer.add_scalar('train_loss', loss_avg, n_iter)
    94   2240.7 MiB      0.0 MiB                   writer.add_scalar('param_norm', pnorm, n_iter)
    95   2240.7 MiB      0.0 MiB                   writer.add_scalar('gradient_norm', gnorm, n_iter)
    96   2240.7 MiB      0.0 MiB                   for i, lr in enumerate(lrs):
    97   2240.7 MiB      0.0 MiB                       writer.add_scalar(f'learning_rate_{i}', lr, n_iter)
    98                             
    99   2240.7 MiB      0.0 MiB       return n_iter


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/train.py

Line #    Mem usage    Increment   Line Contents
================================================
    17   2240.7 MiB   2240.7 MiB   @profile
    18                             def train(model: nn.Module,
    19                                       data_loader: MoleculeDataLoader,
    20                                       loss_func: Callable,
    21                                       optimizer: Optimizer,
    22                                       scheduler: _LRScheduler,
    23                                       args: TrainArgs,
    24                                       n_iter: int = 0,
    25                                       logger: logging.Logger = None,
    26                                       writer: SummaryWriter = None) -> int:
    27                                 """
    28                                 Trains a model for an epoch.
    29                             
    30                                 :param model: Model.
    31                                 :param data_loader: A MoleculeDataLoader.
    32                                 :param loss_func: Loss function.
    33                                 :param optimizer: An Optimizer.
    34                                 :param scheduler: A learning rate scheduler.
    35                                 :param args: Arguments.
    36                                 :param n_iter: The number of iterations (training examples) trained on so far.
    37                                 :param logger: A logger for printing intermediate results.
    38                                 :param writer: A tensorboardX SummaryWriter.
    39                                 :return: The total number of iterations (training examples) trained on so far.
    40                                 """
    41   2240.7 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
    42                                 
    43   2240.7 MiB      0.0 MiB       model.train()
    44   2240.7 MiB      0.0 MiB       loss_sum, iter_count = 0, 0
    45                             
    46   2240.7 MiB      0.0 MiB       for batch in tqdm(data_loader, total=len(data_loader)):
    47                                     # Prepare batch


  0% 0/2 [00:00<?, ?it/s][A100% 2/2 [00:00<00:00, 25.42it/s]
Validation auc = 0.422917
 90% 27/30 [00:55<00:05,  1.93s/it]Epoch 27

  0% 0/9 [00:00<?, ?it/s][A
 22% 2/9 [00:00<00:00,  7.26it/s][A
 33% 3/9 [00:00<00:00,  6.69it/s][A
 44% 4/9 [00:00<00:00,  6.31it/s][A
 56% 5/9 [00:00<00:00,  5.56it/s][A
 67% 6/9 [00:01<00:00,  5.50it/s][A
 78% 7/9 [00:01<00:00,  5.22it/s][A
 89% 8/9 [00:01<00:00,  5.40it/s][A
100% 9/9 [00:01<00:00,  6.01it/s][A100% 9/9 [00:01<00:00,  5.78it/s]    48                                     batch: MoleculeDataset
    49   2240.7 MiB      0.0 MiB           mol_batch, features_batch, target_batch = batch.batch_graph(), batch.features(), batch.targets()
    50   2240.7 MiB      0.0 MiB           mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])
    51   2240.7 MiB      0.0 MiB           targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])
    52                             
    53                                     # Run model
    54   2240.7 MiB      0.0 MiB           model.zero_grad()
    55   2240.7 MiB      0.0 MiB           preds = model(mol_batch, features_batch)
    56                             
    57                                     # Move tensors to correct device
    58   2240.7 MiB      0.0 MiB           mask = mask.to(preds.device)
    59   2240.7 MiB      0.0 MiB           targets = targets.to(preds.device)
    60   2240.7 MiB      0.0 MiB           class_weights = torch.ones(targets.shape, device=preds.device)
    61                             
    62   2240.7 MiB      0.0 MiB           if args.dataset_type == 'multiclass':
    63                                         targets = targets.long()
    64                                         loss = torch.cat([loss_func(preds[:, target_index, :], targets[:, target_index]).unsqueeze(1) for target_index in range(preds.size(1))], dim=1) * class_weights * mask
    65                                     else:
    66   2240.7 MiB      0.0 MiB               loss = loss_func(preds, targets) * class_weights * mask
    67   2240.7 MiB      0.0 MiB           loss = loss.sum() / mask.sum()
    68                             
    69   2240.7 MiB      0.0 MiB           loss_sum += loss.item()
    70   2240.7 MiB      0.0 MiB           iter_count += len(batch)
    71                             
    72   2240.7 MiB      0.0 MiB           loss.backward()
    73   2240.7 MiB      0.0 MiB           optimizer.step()
    74                             
    75   2240.7 MiB      0.0 MiB           if isinstance(scheduler, NoamLR):
    76   2240.7 MiB      0.0 MiB               scheduler.step()
    77                             
    78   2240.7 MiB      0.0 MiB           n_iter += len(batch)
    79                             
    80                                     # Log and/or add to tensorboard
    81   2240.7 MiB      0.0 MiB           wandb.log({'n_iter': n_iter, 'loss_avg': loss_sum/iter_count})
    82   2240.7 MiB      0.0 MiB           if (n_iter // args.batch_size) % args.log_frequency == 0:
    83   2240.7 MiB      0.0 MiB               lrs = scheduler.get_lr()
    84   2240.7 MiB      0.0 MiB               pnorm = compute_pnorm(model)
    85   2240.7 MiB      0.0 MiB               gnorm = compute_gnorm(model)
    86   2240.7 MiB      0.0 MiB               loss_avg = loss_sum / iter_count
    87   2240.7 MiB      0.0 MiB               loss_sum, iter_count = 0, 0
    88                             
    89   2240.7 MiB      0.0 MiB               lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))
    90   2240.7 MiB      0.0 MiB               debug(f'Loss = {loss_avg:.4e}, PNorm = {pnorm:.4f}, GNorm = {gnorm:.4f}, {lrs_str}')
    91                             
    92   2240.7 MiB      0.0 MiB               if writer is not None:
    93   2240.7 MiB      0.0 MiB                   writer.add_scalar('train_loss', loss_avg, n_iter)
    94   2240.7 MiB      0.0 MiB                   writer.add_scalar('param_norm', pnorm, n_iter)
    95   2240.7 MiB      0.0 MiB                   writer.add_scalar('gradient_norm', gnorm, n_iter)
    96   2240.7 MiB      0.0 MiB                   for i, lr in enumerate(lrs):
    97   2240.7 MiB      0.0 MiB                       writer.add_scalar(f'learning_rate_{i}', lr, n_iter)
    98                             
    99   2240.7 MiB      0.0 MiB       return n_iter


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/train.py

Line #    Mem usage    Increment   Line Contents
================================================
    17   2240.7 MiB   2240.7 MiB   @profile
    18                             def train(model: nn.Module,
    19                                       data_loader: MoleculeDataLoader,
    20                                       loss_func: Callable,
    21                                       optimizer: Optimizer,
    22                                       scheduler: _LRScheduler,
    23                                       args: TrainArgs,
    24                                       n_iter: int = 0,
    25                                       logger: logging.Logger = None,
    26                                       writer: SummaryWriter = None) -> int:
    27                                 """
    28                                 Trains a model for an epoch.
    29                             
    30                                 :param model: Model.
    31                                 :param data_loader: A MoleculeDataLoader.
    32                                 :param loss_func: Loss function.
    33                                 :param optimizer: An Optimizer.
    34                                 :param scheduler: A learning rate scheduler.
    35                                 :param args: Arguments.
    36                                 :param n_iter: The number of iterations (training examples) trained on so far.
    37                                 :param logger: A logger for printing intermediate results.
    38                                 :param writer: A tensorboardX SummaryWriter.
    39                                 :return: The total number of iterations (training examples) trained on so far.
    40                                 """
    41   2240.7 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
    42                                 
    43   2240.7 MiB      0.0 MiB       model.train()
    44   2240.7 MiB      0.0 MiB       loss_sum, iter_count = 0, 0
    45                             
    46   2240.7 MiB      0.0 MiB       for batch in tqdm(data_loader, total=len(data_loader)):
    47                                     # Prepare batch
    48                                     batch: MoleculeDataset
    49   2240.7 MiB      0.0 MiB           mol_batch, features_batch, target_batch = batch.batch_graph(), batch.features(), batch.targets()
    50   2240.7 MiB      0.0 MiB           mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])
    51   2240.7 MiB      0.0 MiB           targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])
    52                             
    53                                     # Run model
    54   2240.7 MiB      0.0 MiB           model.zero_grad()
    55   2240.7 MiB      0.0 MiB           preds = model(mol_batch, features_batch)
    56                             
    57                                     # Move tensors to correct device
    58   2240.7 MiB      0.0 MiB           mask = mask.to(preds.device)
    59   2240.7 MiB      0.0 MiB           targets = targets.to(preds.device)
    60   2240.7 MiB      0.0 MiB           class_weights = torch.ones(targets.shape, device=preds.device)
    61                             
    62   2240.7 MiB      0.0 MiB           if args.dataset_type == 'multiclass':
    63                                         targets = targets.long()
    64                                         loss = torch.cat([loss_func(preds[:, target_index, :], targets[:, target_index]).unsqueeze(1) for target_index in range(preds.size(1))], dim=1) * class_weights * mask
    65                                     else:
    66   2240.7 MiB      0.0 MiB               loss = loss_func(preds, targets) * class_weights * mask
    67   2240.7 MiB      0.0 MiB           loss = loss.sum() / mask.sum()
    68                             
    69   2240.7 MiB      0.0 MiB           loss_sum += loss.item()
    70   2240.7 MiB      0.0 MiB           iter_count += len(batch)
    71                             
    72   2240.7 MiB      0.0 MiB           loss.backward()


  0% 0/2 [00:00<?, ?it/s][A100% 2/2 [00:00<00:00, 25.39it/s]
Validation auc = 0.422917
 93% 28/30 [00:57<00:03,  1.91s/it]Epoch 28

  0% 0/9 [00:00<?, ?it/s][ALoss = 8.7800e-04, PNorm = 25.2747, GNorm = 0.0882, lr_0 = 1.0000e-04

 11% 1/9 [00:00<00:01,  6.51it/s][A
 22% 2/9 [00:00<00:01,  6.21it/s][A
 33% 3/9 [00:00<00:01,  5.78it/s][A
 44% 4/9 [00:00<00:00,  5.13it/s][A
 56% 5/9 [00:00<00:00,  5.19it/s][A
 67% 6/9 [00:01<00:00,  5.24it/s][A
 78% 7/9 [00:01<00:00,  5.33it/s][A
 89% 8/9 [00:01<00:00,  4.95it/s][A
100% 9/9 [00:01<00:00,  5.57it/s][A100% 9/9 [00:01<00:00,  5.30it/s]    73   2240.7 MiB      0.0 MiB           optimizer.step()
    74                             
    75   2240.7 MiB      0.0 MiB           if isinstance(scheduler, NoamLR):
    76   2240.7 MiB      0.0 MiB               scheduler.step()
    77                             
    78   2240.7 MiB      0.0 MiB           n_iter += len(batch)
    79                             
    80                                     # Log and/or add to tensorboard
    81   2240.7 MiB      0.0 MiB           wandb.log({'n_iter': n_iter, 'loss_avg': loss_sum/iter_count})
    82   2240.7 MiB      0.0 MiB           if (n_iter // args.batch_size) % args.log_frequency == 0:
    83                                         lrs = scheduler.get_lr()
    84                                         pnorm = compute_pnorm(model)
    85                                         gnorm = compute_gnorm(model)
    86                                         loss_avg = loss_sum / iter_count
    87                                         loss_sum, iter_count = 0, 0
    88                             
    89                                         lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))
    90                                         debug(f'Loss = {loss_avg:.4e}, PNorm = {pnorm:.4f}, GNorm = {gnorm:.4f}, {lrs_str}')
    91                             
    92                                         if writer is not None:
    93                                             writer.add_scalar('train_loss', loss_avg, n_iter)
    94                                             writer.add_scalar('param_norm', pnorm, n_iter)
    95                                             writer.add_scalar('gradient_norm', gnorm, n_iter)
    96                                             for i, lr in enumerate(lrs):
    97                                                 writer.add_scalar(f'learning_rate_{i}', lr, n_iter)
    98                             
    99   2240.7 MiB      0.0 MiB       return n_iter


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/train.py

Line #    Mem usage    Increment   Line Contents
================================================
    17   2240.7 MiB   2240.7 MiB   @profile
    18                             def train(model: nn.Module,
    19                                       data_loader: MoleculeDataLoader,
    20                                       loss_func: Callable,
    21                                       optimizer: Optimizer,
    22                                       scheduler: _LRScheduler,
    23                                       args: TrainArgs,
    24                                       n_iter: int = 0,
    25                                       logger: logging.Logger = None,
    26                                       writer: SummaryWriter = None) -> int:
    27                                 """
    28                                 Trains a model for an epoch.
    29                             
    30                                 :param model: Model.
    31                                 :param data_loader: A MoleculeDataLoader.
    32                                 :param loss_func: Loss function.
    33                                 :param optimizer: An Optimizer.
    34                                 :param scheduler: A learning rate scheduler.
    35                                 :param args: Arguments.
    36                                 :param n_iter: The number of iterations (training examples) trained on so far.
    37                                 :param logger: A logger for printing intermediate results.
    38                                 :param writer: A tensorboardX SummaryWriter.
    39                                 :return: The total number of iterations (training examples) trained on so far.
    40                                 """
    41   2240.7 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
    42                                 
    43   2240.7 MiB      0.0 MiB       model.train()
    44   2240.7 MiB      0.0 MiB       loss_sum, iter_count = 0, 0
    45                             
    46   2240.7 MiB      0.0 MiB       for batch in tqdm(data_loader, total=len(data_loader)):
    47                                     # Prepare batch
    48                                     batch: MoleculeDataset
    49   2240.7 MiB      0.0 MiB           mol_batch, features_batch, target_batch = batch.batch_graph(), batch.features(), batch.targets()
    50   2240.7 MiB      0.0 MiB           mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])
    51   2240.7 MiB      0.0 MiB           targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])
    52                             
    53                                     # Run model
    54   2240.7 MiB      0.0 MiB           model.zero_grad()
    55   2240.7 MiB      0.0 MiB           preds = model(mol_batch, features_batch)
    56                             
    57                                     # Move tensors to correct device
    58   2240.7 MiB      0.0 MiB           mask = mask.to(preds.device)
    59   2240.7 MiB      0.0 MiB           targets = targets.to(preds.device)
    60   2240.7 MiB      0.0 MiB           class_weights = torch.ones(targets.shape, device=preds.device)
    61                             
    62   2240.7 MiB      0.0 MiB           if args.dataset_type == 'multiclass':
    63                                         targets = targets.long()
    64                                         loss = torch.cat([loss_func(preds[:, target_index, :], targets[:, target_index]).unsqueeze(1) for target_index in range(preds.size(1))], dim=1) * class_weights * mask
    65                                     else:
    66   2240.7 MiB      0.0 MiB               loss = loss_func(preds, targets) * class_weights * mask
    67   2240.7 MiB      0.0 MiB           loss = loss.sum() / mask.sum()
    68                             
    69   2240.7 MiB      0.0 MiB           loss_sum += loss.item()
    70   2240.7 MiB      0.0 MiB           iter_count += len(batch)
    71                             
    72   2240.7 MiB      0.0 MiB           loss.backward()
    73   2240.7 MiB      0.0 MiB           optimizer.step()
    74                             
    75   2240.7 MiB      0.0 MiB           if isinstance(scheduler, NoamLR):
    76   2240.7 MiB      0.0 MiB               scheduler.step()
    77                             
    78   2240.7 MiB      0.0 MiB           n_iter += len(batch)
    79                             
    80                                     # Log and/or add to tensorboard
    81   2240.7 MiB      0.0 MiB           wandb.log({'n_iter': n_iter, 'loss_avg': loss_sum/iter_count})
    82   2240.7 MiB      0.0 MiB           if (n_iter // args.batch_size) % args.log_frequency == 0:
    83   2240.7 MiB      0.0 MiB               lrs = scheduler.get_lr()
    84   2240.7 MiB      0.0 MiB               pnorm = compute_pnorm(model)
    85   2240.7 MiB      0.0 MiB               gnorm = compute_gnorm(model)
    86

   2240.7 MiB      0.0 MiB               loss_avg = loss_sum / iter_count
    87   2240.7 MiB      0.0 MiB               loss_sum, iter_count = 0, 0
    88                             
    89   2240.7 MiB      0.0 MiB               lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))
    90   2240.7 MiB      0.0 MiB               debug(f'Loss = {loss_avg:.4e}, PNorm = {pnorm:.4f}, GNorm = {gnorm:.4f}, {lrs_str}')
    91                             
    92   2240.7 MiB      0.0 MiB               if writer is not None:
    93   2240.7 MiB      0.0 MiB                   writer.add_scalar('train_loss', loss_avg, n_iter)
    94   2240.7 MiB      0.0 MiB                   writer.add_scalar('param_norm', pnorm, n_iter)
    95   2240.7 MiB      0.0 MiB                   writer.add_scalar('gradient_norm', gnorm, n_iter)
    96   2240.7 MiB      0.0 MiB                   for i, lr in enumerate(lrs):
    97   2240.7 MiB      0.0 MiB                       writer.add_scalar(f'learning_rate_{i}', lr, n_iter)
  0% 0/2 [00:00<?, ?it/s][A100% 2/2 [00:00<00:00, 25.08it/s]
Validation auc = 0.422917
 97% 29/30 [00:59<00:01,  1.93s/it]Epoch 29

  0% 0/9 [00:00<?, ?it/s][A
 11% 1/9 [00:00<00:00,  9.24it/s][ALoss = 7.5075e-04, PNorm = 25.2950, GNorm = 0.0744, lr_0 = 1.0000e-04

 22% 2/9 [00:00<00:01,  6.99it/s][A
 33% 3/9 [00:00<00:01,  5.31it/s][A
 44% 4/9 [00:00<00:00,  5.46it/s][A
 56% 5/9 [00:01<00:00,  4.96it/s][A
 67% 6/9 [00:01<00:00,  4.92it/s][A
 78% 7/9 [00:01<00:00,  4.86it/s][A
 89% 8/9 [00:01<00:00,  4.56it/s][A
100% 9/9 [00:01<00:00,  4.92it/s][A100% 9/9 [00:01<00:00,  4.79it/s]

  0% 0/2 [00:00<?, ?it/s][A
 50% 1/2 [00:00<00:00,  7.52it/s][A100% 2/2 [00:00<00:00, 14.22it/s]
Validation auc = 0.422917
100% 30/30 [01:01<00:00,  2.02s/it]100% 30/30 [01:01<00:00,  2.04s/it]
Model 0 best validation auc = 0.562500 on epoch 0
Loading pretrained parameter "ffn.0.weight".
Loading pretrained parameter "ffn.0.bias".
Loading pretrained parameter "ffn.3.weight".
Loading pretrained parameter "ffn.3.bias".
Moving model to cuda
  0% 0/2 [00:00<?, ?it/s] 50% 1/2 [00:00<00:00,  3.19it/s]100% 2/2 [00:00<00:00,  5.55it/s]
Warning: Found a task with targets all 0s or all 1s
Warning: Found a task with targets all 0s or all 1s
Warning: Found a task with targets all 0s or all 1s
Warning: Found a task with targets all 0s or all 1s
Model 0 test auc = 0.833333
Warning: Found a task with targets all 0s or all 1s
Warning: Found a task with targets all 0s or all 1s
Warning: Found a task with targets all 0s or all 1s
Warning: Found a task with targets all 0s or all 1s
Ensemble test auc = 0.833333
    98                             
    99   2240.7 MiB      0.0 MiB       return n_iter


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/train.py

Line #    Mem usage    Increment   Line Contents
================================================
    17   2240.7 MiB   2240.7 MiB   @profile
    18                             def train(model: nn.Module,
    19                                       data_loader: MoleculeDataLoader,
    20                                       loss_func: Callable,
    21                                       optimizer: Optimizer,
    22                                       scheduler: _LRScheduler,
    23                                       args: TrainArgs,
    24                                       n_iter: int = 0,
    25                                       logger: logging.Logger = None,
    26                                       writer: SummaryWriter = None) -> int:
    27                                 """
    28                                 Trains a model for an epoch.
    29                             
    30                                 :param model: Model.
    31                                 :param data_loader: A MoleculeDataLoader.
    32                                 :param loss_func: Loss function.
    33                                 :param optimizer: An Optimizer.
    34                                 :param scheduler: A learning rate scheduler.
    35                                 :param args: Arguments.
    36                                 :param n_iter: The number of iterations (training examples) trained on so far.
    37                                 :param logger: A logger for printing intermediate results.
    38                                 :param writer: A tensorboardX SummaryWriter.
    39                                 :return: The total number of iterations (training examples) trained on so far.
    40                                 """
    41   2240.7 MiB      0.0 MiB       debug = logger.debug if logger is not None else print
    42                                 
    43   2240.7 MiB      0.0 MiB       model.train()
    44   2240.7 MiB      0.0 MiB       loss_sum, iter_count = 0, 0
    45                             
    46   2240.7 MiB      0.0 MiB       for batch in tqdm(data_loader, total=len(data_loader)):
    47                                     # Prepare batch
    48                                     batch: MoleculeDataset
    49   2240.7 MiB      0.0 MiB           mol_batch, features_batch, target_batch = batch.batch_graph(), batch.features(), batch.targets()
    50   2240.7 MiB      0.0 MiB           mask = torch.Tensor([[x is not None for x in tb] for tb in target_batch])
    51   2240.7 MiB      0.0 MiB           targets = torch.Tensor([[0 if x is None else x for x in tb] for tb in target_batch])
    52                             
    53                                     # Run model
    54   2240.7 MiB      0.0 MiB           model.zero_grad()
    55   2240.7 MiB      0.0 MiB           preds = model(mol_batch, features_batch)
    56                             
    57                                     # Move tensors to correct device
    58   2240.7 MiB      0.0 MiB           mask = mask.to(preds.device)
    59   2240.7 MiB      0.0 MiB           targets = targets.to(preds.device)
    60   2240.7 MiB      0.0 MiB           class_weights = torch.ones(targets.shape, device=preds.device)
    61                             
    62   2240.7 MiB      0.0 MiB           if args.dataset_type == 'multiclass':
    63                                         targets = targets.long()
    64                                         loss = torch.cat([loss_func(preds[:, target_index, :], targets[:, target_index]).unsqueeze(1) for target_index in range(preds.size(1))], dim=1) * class_weights * mask
    65                                     else:
    66   2240.7 MiB      0.0 MiB               loss = loss_func(preds, targets) * class_weights * mask
    67   2240.7 MiB      0.0 MiB           loss = loss.sum() / mask.sum()
    68                             
    69   2240.7 MiB      0.0 MiB           loss_sum += loss.item()
    70   2240.7 MiB      0.0 MiB           iter_count += len(batch)
    71                             
    72   2240.7 MiB      0.0 MiB           loss.backward()
    73   2240.7 MiB      0.0 MiB           optimizer.step()
    74                             
    75   2240.7 MiB      0.0 MiB           if isinstance(scheduler, NoamLR):
    76   2240.7 MiB      0.0 MiB               scheduler.step()
    77                             
    78   2240.7 MiB      0.0 MiB           n_iter += len(batch)
    79                             
    80                                     # Log and/or add to tensorboard
    81   2240.7 MiB      0.0 MiB           wandb.log({'n_iter': n_iter, 'loss_avg': loss_sum/iter_count})
    82   2240.7 MiB      0.0 MiB           if (n_iter // args.batch_size) % args.log_frequency == 0:
    83   2240.7 MiB      0.0 MiB               lrs = scheduler.get_lr()
    84   2240.7 MiB      0.0 MiB               pnorm = compute_pnorm(model)
    85   2240.7 MiB      0.0 MiB               gnorm = compute_gnorm(model)
    86   2240.7 MiB      0.0 MiB               loss_avg = loss_sum / iter_count
    87   2240.7 MiB      0.0 MiB               loss_sum, iter_count = 0, 0
    88                             
    89   2240.7 MiB      0.0 MiB               lrs_str = ', '.join(f'lr_{i} = {lr:.4e}' for i, lr in enumerate(lrs))
    90   2240.7 MiB      0.0 MiB               debug(f'Loss = {loss_avg:.4e}, PNorm = {pnorm:.4f}, GNorm = {gnorm:.4f}, {lrs_str}')
    91                             
    92   2240.7 MiB      0.0 MiB               if writer is not None:
    93   2240.7 MiB      0.0 MiB                   writer.add_scalar('train_loss', loss_avg, n_iter)
    94   2240.7 MiB      0.0 MiB                   writer.add_scalar('param_norm', pnorm, n_iter)
    95   2240.7 MiB      0.0 MiB                   writer.add_scalar('gradient_norm', gnorm, n_iter)
    96   2240.7 MiB      0.0 MiB                   for i, lr in enumerate(lrs):
    97   2240.7 MiB      0.0 MiB                       writer.add_scalar(f'learning_rate_{i}', lr, n_iter)
    98                             
    99   2240.7 MiB      0.0 MiB       return n_iter


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/run_training.py

Line #    Mem usage    Increment   Line Contents
================================================
    27    305.2 MiB    305.2 MiB   @profile
    28                             def run_training(args: TrainArgs, logger: Logger = None) -> List[float]:
    29                                 """
    30                                 Trains a model and returns test scores on the model checkpoint with the highest validation score.
    31                             
    32                                 :param args: Arguments.
    33                                 :param logger: Logger.
    34                                 :return: A list of ensemble scores for each task.
    35                                 """
    36    305.2 MiB      0.0 MiB       if logger is not None:
    37    305.2 MiB      0.0 MiB           debug, info = logger.debug, logger.info
    38                                 else:
    39                                     debug = info = print
    40                             
    41                                 # Print command line
    42    305.2 MiB      0.0 MiB       debug('Command line')
    43    305.2 MiB      0.0 MiB       debug(f'python {" ".join(sys.argv)}')
    44                             
    45                                 # Print args
    46    305.2 MiB      0.0 MiB       debug('Args')
    47    306.0 MiB      0.7 MiB       debug(args)
    48                             
    49                                 # Save args
    50    306.2 MiB      0.2 MiB       args.save(os.path.join(args.save_dir, 'args.json'))
    51                             
    52                                 # Set pytorch seed for random initial weights
    53    306.2 MiB      0.0 MiB       torch.manual_seed(args.pytorch_seed)
    54                             
    55                                 # Get data
    56    306.2 MiB      0.0 MiB       debug('Loading data')
    57    306.2 MiB      0.0 MiB       args.task_names = args.target_columns or get_task_names(args.data_path)
    58    317.8 MiB     11.6 MiB       data = get_data(path=args.data_path, args=args, logger=logger)
    59    317.8 MiB      0.0 MiB       args.num_tasks = data.num_tasks()
    60    317.8 MiB      0.0 MiB       args.features_size = data.features_size()
    61    317.8 MiB      0.0 MiB       debug(f'Number of tasks = {args.num_tasks}')
    62                             
    63                                 # Split data
    64    317.8 MiB      0.0 MiB       debug(f'Splitting data with seed {args.seed}')
    65    317.8 MiB      0.0 MiB       if args.separate_test_path:
    66                                     test_data = get_data(path=args.separate_test_path, args=args, features_path=args.separate_test_features_path, logger=logger)
    67    317.8 MiB      0.0 MiB       if args.separate_val_path:
    68                                     val_data = get_data(path=args.separate_val_path, args=args, features_path=args.separate_val_features_path, logger=logger)
    69                             
    70    317.8 MiB      0.0 MiB       if args.separate_val_path and args.separate_test_path:
    71                                     train_data = data
    72    317.8 MiB      0.0 MiB       elif args.separate_val_path:
    73                                     train_data, _, test_data = split_data(data=data, split_type=args.split_type, sizes=(0.8, 0.0, 0.2), seed=args.seed, args=args, logger=logger)
    74    317.8 MiB      0.0 MiB       elif args.separate_test_path:
    75                                     train_data, val_data, _ = split_data(data=data, split_type=args.split_type, sizes=(0.8, 0.2, 0.0), seed=args.seed, args=args, logger=logger)
    76                                 else:
    77    321.1 MiB      3.3 MiB           train_data, val_data, test_data = split_data(data=data, split_type=args.split_type, sizes=args.split_sizes, seed=args.seed, args=args, logger=logger)
    78                             
    79    321.1 MiB      0.0 MiB       if args.dataset_type == 'classification':
    80    321.1 MiB      0.0 MiB           class_sizes = get_class_sizes(data)
    81    321.1 MiB      0.0 MiB           debug('Class sizes')
    82    321.1 MiB      0.0 MiB           for i, task_class_sizes in enumerate(class_sizes):
    83    321.1 MiB      0.0 MiB               debug(f'{args.task_names[i]} '
    84                                               f'{", ".join(f"{cls}: {size * 100:.2f}%" for cls, size in enumerate(task_class_sizes))}')
    85                             
    86    321.1 MiB      0.0 MiB       if args.save_smiles_splits:
    87                                     save_smiles_splits(
    88                                         train_data=train_data,
    89                                         val_data=val_data,
    90                                         test_data=test_data,
    91                                         data_path=args.data_path,
    92                                         save_dir=args.save_dir
    93                                     )
    94                             
    95    321.1 MiB      0.0 MiB       if args.features_scaling:
    96    326.6 MiB      5.6 MiB           features_scaler = train_data.normalize_features(replace_nan_token=0)
    97    326.6 MiB      0.0 MiB           val_data.normalize_features(features_scaler)
    98    326.6 MiB      0.0 MiB           test_data.normalize_features(features_scaler)
    99                                 else:
   100                                     features_scaler = None
   101    326.6 MiB      0.0 MiB       args.train_data_size = len(train_data)
   102                                 
   103    326.6 MiB      0.0 MiB       debug(f'Total size = {len(data):,} | '
   104                                       f'train size = {len(train_data):,} | val size = {len(val_data):,} | test size = {len(test_data):,}')
   105                             
   106                                 # Initialize scaler and scale training targets by subtracting mean and dividing standard deviation (regression only)
   107    326.6 MiB      0.0 MiB       if args.dataset_type == 'regression':
   108                                     debug('Fitting scaler')
   109                                     train_smiles, train_targets = train_data.smiles(), train_data.targets()
   110                                     scaler = StandardScaler().fit(train_targets)
   111                                     scaled_targets = scaler.transform(train_targets).tolist()
   112                                     train_data.set_targets(scaled_targets)
   113                                 else:
   114    326.6 MiB      0.0 MiB           scaler = None
   115                             
   116                                 # Get loss and metric functions
   117    326.6 MiB      0.0 MiB       loss_func = get_loss_func(args)
   118    326.6 MiB      0.0 MiB       metric_func = get_metric_func(metric=args.metric)
   119                             
   120                                 # Set up test set evaluation
   121    326.6 MiB      0.0 MiB       test_smiles, test_targets = test_data.smiles(), test_data.targets()
   122    326.6 MiB      0.0 MiB       if args.dataset_type == 'multiclass':
   123                                     sum_test_preds = np.zeros((len(test_smiles), args.num_tasks, args.multiclass_num_classes))
   124                                 else:
   125    326.6 MiB      0.0 MiB           sum_test_preds = np.zeros((len(test_smiles), args.num_tasks))
   126                             
   127                                 # Automatically determine whether to cache
   128    326.6 MiB      0.0 MiB       if len(data) <= args.cache_cutoff:
   129    326.6 MiB      0.0 MiB           cache = True
   130    326.6 MiB      0.0 MiB           num_workers = 0
   131                                 else:
   132                                     cache = False
   133                                     num_workers = args.num_workers
   134                             
   135                                 # Create data loaders
   136    326.6 MiB      0.0 MiB       train_data_loader = MoleculeDataLoader(
   137    326.6 MiB      0.0 MiB           dataset=train_data,
   138    326.6 MiB      0.0 MiB           batch_size=args.batch_size,
   139    326.6 MiB      0.0 MiB           num_workers=num_workers,
   140    326.6 MiB      0.0 MiB           cache=cache,
   141    326.6 MiB      0.0 MiB           class_balance=args.class_balance,
   142    326.6 MiB      0.0 MiB           shuffle=True,
   143    326.6 MiB      0.0 MiB           seed=args.seed
   144                                 )
   145    326.6 MiB      0.0 MiB       val_data_loader = MoleculeDataLoader(
   146    326.6 MiB      0.0 MiB           dataset=val_data,
   147    326.6 MiB      0.0 MiB           batch_size=args.batch_size,
   148    326.6 MiB      0.0 MiB           num_workers=num_workers,
   149    326.6 MiB      0.0 MiB           cache=cache
   150                                 )
   151    326.6 MiB      0.0 MiB       test_data_loader = MoleculeDataLoader(
   152    326.6 MiB      0.0 MiB           dataset=test_data,
   153    326.6 MiB      0.0 MiB           batch_size=args.batch_size,
   154    326.6 MiB      0.0 MiB           num_workers=num_workers,
   155    326.6 MiB      0.0 MiB           cache=cache
   156                                 )
   157                             
   158                                 # Train ensemble of models
   159   2241.6 MiB      0.0 MiB       for model_idx in range(args.ensemble_size):
   160                                     # Tensorboard writer
   161    326.6 MiB      0.0 MiB           save_dir = os.path.join(args.save_dir, f'model_{model_idx}')
   162    326.6 MiB      0.0 MiB           print("save dir for model is {}".format(save_dir))
   163    326.6 MiB      0.0 MiB           makedirs(save_dir)
   164    326.6 MiB      0.0 MiB           try:
   165    326.6 MiB      0.0 MiB               writer = SummaryWriter(log_dir=save_dir)
   166                                     except:
   167                                         writer = SummaryWriter(logdir=save_dir)
   168                             
   169                                     # Load/build model
   170    326.6 MiB      0.0 MiB           if args.checkpoint_paths is not None:
   171                                         debug(f'Loading model {model_idx} from {args.checkpoint_paths[model_idx]}')
   172                                         model = load_checkpoint(args.checkpoint_paths[model_idx], logger=logger)
   173                                     else:
   174    326.6 MiB      0.0 MiB               debug(f'Building model {model_idx}')
   175    327.8 MiB      1.1 MiB               model = MoleculeModel(args)
   176                                     
   177    328.3 MiB      0.5 MiB           wandb.watch(model)
   178    328.3 MiB      0.0 MiB           debug(model)
   179    328.3 MiB      0.0 MiB           debug(f'Number of parameters = {param_count(model):,}')
   180    328.3 MiB      0.0 MiB           if args.cuda:
   181    328.3 MiB      0.0 MiB               debug('Moving model to cuda')
   182   2003.4 MiB   1675.1 MiB           model = model.to(args.device)
   183                             
   184                                     # Ensure that model is saved in correct location for evaluation if 0 epochs
   185   2004.5 MiB      1.1 MiB           save_checkpoint(os.path.join(save_dir, 'model.pt'), model, scaler, features_scaler, args)
   186                             
   187                                     # Optimizers
   188   2004.5 MiB      0.0 MiB           optimizer = build_optimizer(model, args)
   189                             
   190                                     # Learning rate schedulers
   191   2004.5 MiB      0.0 MiB           scheduler = build_lr_scheduler(optimizer, args)
   192                             
   193                                     # Run training
   194   2004.5 MiB      0.0 MiB           best_score = float('inf') if args.minimize_score else -float('inf')
   195   2004.5 MiB      0.0 MiB           best_epoch, n_iter = 0, 0
   196   2240.7 MiB      0.2 MiB           for epoch in trange(args.epochs):
   197   2240.7 MiB      0.0 MiB               debug(f'Epoch {epoch}')
   198                             
   199   2240.7 MiB      0.0 MiB               n_iter = train(
   200   2240.7 MiB      0.0 MiB                   model=model,
   201   2240.7 MiB      0.0 MiB                   data_loader=train_data_loader,
   202   2240.7 MiB      0.0 MiB                   loss_func=loss_func,
   203   2240.7 MiB      0.0 MiB                   optimizer=optimizer,
   204   2240.7 MiB      0.0 MiB                   scheduler=scheduler,
   205   2240.7 MiB      0.0 MiB                   args=args,
   206   2240.7 MiB      0.0 MiB                   n_iter=n_iter,
   207   2240.7 MiB      0.0 MiB                   logger=logger,
   208   2240.7 MiB    225.7 MiB                   writer=writer
   209                                         )
   210   2240.7 MiB      0.0 MiB               if isinstance(scheduler, ExponentialLR):
   211                                             scheduler.step()
   212   2240.7 MiB      0.0 MiB               val_scores = evaluate(
   213   2240.7 MiB      0.0 MiB                   model=model,
   214   2240.7 MiB      0.0 MiB                   data_loader=val_data_loader,
   215   2240.7 MiB      0.0 MiB                   num_tasks=args.num_tasks,
   216   2240.7 MiB      0.0 MiB                   metric_func=metric_func,
   217   2240.7 MiB      0.0 MiB                   dataset_type=args.dataset_type,
   218   2240.7 MiB      0.0 MiB                   scaler=scaler,
   219   2240.7 MiB      3.9 MiB                   logger=logger
   220                                         )
   221                             
   222                                         # Average validation score
   223   2240.7 MiB      0.0 MiB               avg_val_score = np.nanmean(val_scores)
   224   2240.7 MiB      0.0 MiB               debug(f'Validation {args.metric} = {avg_val_score:.6f}')
   225   2240.7 MiB      0.1 MiB               writer.add_scalar(f'validation_{args.metric}', avg_val_score, n_iter)
   226   2240.7 MiB      0.0 MiB               wandb.log({'epoch': epoch, 'val_auc': avg_val_score})
   227                             
   228   2240.7 MiB      0.0 MiB               if args.show_individual_scores:
   229                                             # Individual validation scores
   230                                             for task_name, val_score in zip(args.task_names, val_scores):
   231                                                 debug(f'Validation {task_name} {args.metric} = {val_score:.6f}')
   232                                                 writer.add_scalar(f'validation_{task_name}_{args.metric}', val_score, n_iter)
   233                             
   234                                         # Save model checkpoint if improved validation score
   235   2240.7 MiB      0.0 MiB               if args.minimize_score and avg_val_score < best_score or \
   236   2240.7 MiB      0.0 MiB                       not args.minimize_score and avg_val_score > best_score:
   237   2234.5 MiB      0.0 MiB                   best_score, best_epoch = avg_val_score, epoch
   238   2236.6 MiB      2.1 MiB                   save_checkpoint(os.path.join(save_dir, 'model.pt'), model, scaler, features_scaler, args)        
   239                             
   240                                     # Evaluate on test set using model with best validation score
   241   2240.7 MiB      0.0 MiB           info(f'Model {model_idx} best validation {args.metric} = {best_score:.6f} on epoch {best_epoch}')
   242   2241.5 MiB      0.8 MiB           model = load_checkpoint(os.path.join(save_dir, 'model.pt'), device=args.device, logger=logger)
   243                                     
   244   2241.5 MiB      0.0 MiB           test_preds = predict(
   245   2241.5 MiB      0.0 MiB               model=model,
   246   2241.5 MiB      0.0 MiB               data_loader=test_data_loader,
   247   2241.5 MiB      0.0 MiB               scaler=scaler
   248                                     )
   249   2241.5 MiB      0.0 MiB           test_scores = evaluate_predictions(
   250   2241.5 MiB      0.0 MiB               preds=test_preds,
   251   2241.5 MiB      0.0 MiB               targets=test_targets,
   252   2241.5 MiB      0.0 MiB               num_tasks=args.num_tasks,
   253   2241.5 MiB      0.0 MiB               metric_func=metric_func,
   254   2241.5 MiB      0.0 MiB               dataset_type=args.dataset_type,
   255   2241.5 MiB      0.0 MiB               logger=logger
   256                                     )
   257                             
   258   2241.5 MiB      0.0 MiB           if len(test_preds) != 0:
   259   2241.5 MiB      0.0 MiB               sum_test_preds += np.array(test_preds)
   260                             
   261                                     # Average test score
   262   2241.5 MiB      0.0 MiB           avg_test_score = np.nanmean(test_scores)
   263   2241.5 MiB      0.0 MiB           info(f'Model {model_idx} test {args.metric} = {avg_test_score:.6f}')
   264   2241.5 MiB      0.0 MiB           writer.add_scalar(f'test_{args.metric}', avg_test_score, 0)
   265                             
   266   2241.5 MiB      0.0 MiB           if args.show_individual_scores:
   267                                         # Individual test scores
   268                                         for task_name, test_score in zip(args.task_names, test_scores):
   269                                             info(f'Model {model_idx} test {task_name} {args.metric} = {test_score:.6f}')
   270                                             writer.add_scalar(f'test_{task_name}_{args.metric}', test_score, n_iter)
   271   2241.6 MiB      0.0 MiB           writer.close()
1-fold cross validation
Seed 0 ==> test auc = 0.833333
Overall test auc = 0.833333 +/- 0.000000
/home/ubuntu/molecule-metalearning/chemprop/chemprop/utils.py:50: RuntimeWarning: Mean of empty slice
  results_dict[task_name]['avg_score'] = np.nanmean(all_scores[:, task_num])
/home/ubuntu/miniconda3/envs/chemprop/lib/python3.7/site-packages/numpy/lib/nanfunctions.py:1665: RuntimeWarning: Degrees of freedom <= 0 for slice.
  keepdims=keepdims)
Total running time was 70.41559338569641 seconds

wandb: Waiting for W&B process to finish, PID 7697
   272                             
   273                                 # Evaluate ensemble on test set
   274   2241.6 MiB      0.0 MiB       avg_test_preds = (sum_test_preds / args.ensemble_size).tolist()
   275                             
   276   2241.6 MiB      0.0 MiB       ensemble_scores = evaluate_predictions(
   277   2241.6 MiB      0.0 MiB           preds=avg_test_preds,
   278   2241.6 MiB      0.0 MiB           targets=test_targets,
   279   2241.6 MiB      0.0 MiB           num_tasks=args.num_tasks,
   280   2241.6 MiB      0.0 MiB           metric_func=metric_func,
   281   2241.6 MiB      0.0 MiB           dataset_type=args.dataset_type,
   282   2241.6 MiB      0.0 MiB           logger=logger
   283                                 )
   284                             
   285                                 # Average ensemble score
   286   2241.6 MiB      0.0 MiB       avg_ensemble_test_score = np.nanmean(ensemble_scores)
   287   2241.6 MiB      0.0 MiB       info(f'Ensemble test {args.wandb: Program ended successfully.
metric} = {avg_ensemble_test_score:.6f}')
   288                             
   289                                 # Individual ensemble scores
   290   2241.6 MiB      0.0 MiB       if args.show_individual_scores:
   291                                     for task_name, ensemble_score in zip(args.task_names, ensemble_scores):
   292                                         info(f'Ensemble test {task_name} {args.metric} = {ensemble_score:.6f}')
   293                             
   294   2241.6 MiB      0.0 MiB       return ensemble_scores, best_epoch


Filename: /home/ubuntu/molecule-metalearning/chemprop/chemprop/train/cross_validate.py

Line #    Mem usage    Increment   Line Contents
================================================
    13    305.2 MiB    305.2 MiB   @profile
    14                             def cross_validate(args: TrainArgs, logger: Logger = None) -> Tuple[float, float]:
    15                                 """k-fold cross validation"""
    16    305.2 MiB      0.0 MiB       info = logger.info if logger is not None else print
    17                             
    18                                 # Initialize relevant variables
    19    305.2 MiB      0.0 MiB       init_seed = args.seed
    20    305.2 MiB      0.0 MiB       save_dir = args.save_dir
    21    305.2 MiB      0.0 MiB       task_names = args.target_columns or get_task_names(args.data_path)
    22                             
    23                                 # Run training on different random seeds for each fold
    24    305.2 MiB      0.0 MiB       all_scores = []
    25    305.2 MiB      0.0 MiB       best_epochs = []
    26   2241.6 MiB      0.0 MiB       for fold_num in range(args.num_folds):
    27    305.2 MiB      0.0 MiB           info(f'Fold {fold_num}')
    28    305.2 MiB      0.0 MiB           args.seed = init_seed + fold_num
    29    305.2 MiB      0.0 MiB           args.save_dir = os.path.join(save_dir, f'fold_{fold_num}')
    30    305.2 MiB      0.0 MiB           makedirs(args.save_dir)
    31   2241.6 MiB   1936.3 MiB           model_scores, best_epoch = run_training(args, logger)
    32   2241.6 MiB      0.0 MiB           all_scores.append(model_scores)
    33   2241.6 MiB      0.0 MiB           best_epochs.append(best_epoch)
    34   2241.6 MiB      0.0 MiB       all_scores = np.array(all_scores)
    35                             
    36                                 # Report results
    37   2241.6 MiB      0.0 MiB       info(f'{args.num_folds}-fold cross validation')
    38                             
    39                                 # Report scores for each fold
    40   2241.6 MiB      0.0 MiB       for fold_num, scores in enumerate(all_scores):
    41   2241.6 MiB      0.0 MiB           info(f'Seed {init_seed + fold_num} ==> test {args.metric} = {np.nanmean(scores):.6f}')
    42                             
    43   2241.6 MiB      0.0 MiB           if args.show_individual_scores:
    44                                         for task_name, score in zip(task_names, scores):
    45                                             info(f'Seed {init_seed + fold_num} ==> test {task_name} {args.metric} = {score:.6f}')
    46                             
    47                                 # Report scores across models
    48   2241.6 MiB      0.0 MiB       avg_scores = np.nanmean(all_scores, axis=1)  # average score for each model across tasks
    49   2241.6 MiB      0.0 MiB       mean_score, std_score = np.nanmean(avg_scores), np.nanstd(avg_scores)
    50   2241.6 MiB      0.0 MiB       info(f'Overall test {args.metric} = {mean_score:.6f} +/- {std_score:.6f}')
    51                             
    52                                 # Save results for later analysis
    53   2241.6 MiB      0.0 MiB       save_results(all_scores, best_epochs, task_names, args)
    54                             
    55   2241.6 MiB      0.0 MiB       if args.show_individual_scores:
    56                                     for task_num, task_name in enumerate(task_names):
    57                                         info(f'Overall test {task_name} {args.metric} = '
    58                                              f'{np.nanmean(all_scores[:, task_num]):.6f} +/- {np.nanstd(all_scores[:, task_num]):.6f}')
    59                             
    60   2241.6 MiB      0.0 MiB       return mean_score, std_score


wandb: Run summary:
wandb:                    _step 301
wandb:               start_time 1595114740.741817
wandb:                 _runtime 78.08667039871216
wandb:               _timestamp 1595114811.1575403
wandb:                   n_iter 8220
wandb:                 loss_avg 0.0011294248292133921
wandb:                  val_auc 0.4229166666666666
wandb:                    epoch 29
wandb:             elapsed_time 70.41559338569641
wandb:                 end_time 1595114811.1574104
wandb: Syncing 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: - 0.27MB of 0.27MB uploaded (0.00MB deduped)wandb: \ 0.27MB of 0.27MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Synced feature_only_memory_profiling_v1: https://app.wandb.ai/apappu97/molecule-metalearning-chemprop/runs/agmv1xk3
