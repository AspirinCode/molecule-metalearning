wandb: Tracking run with wandb version 0.9.4
wandb: Run data is saved locally in wandb/run-20200728_025539-10lguxd9
wandb: Syncing run maml_test
wandb: ‚≠êÔ∏è View project at https://app.wandb.ai/apappu97/molecule-metalearning-chemprop
wandb: üöÄ View run at https://app.wandb.ai/apappu97/molecule-metalearning-chemprop/runs/10lguxd9
wandb: Run `wandb off` to turn off syncing.
Setting args.meta_learning to True as we are meta learning

Fold 0
Command line
python chemprop/meta_train.py --meta_learning --data_path filtered_chembl/chembl_less_1024_more_128_645_tasks.csv --dataset_type classification --split_type scaffold_balanced --chembl_assay_metadata_pickle_path filtered_chembl/ --save_dir checkpoints/ --results_save_dir results/maml/ --experiment_name maml_test
Args
{'ANIL': False,
 'FO_MAML': False,
 'activation': 'ReLU',
 'atom_messages': False,
 'batch_size': 50,
 'bias': False,
 'cache_cutoff': 10000,
 'checkpoint_dir': None,
 'checkpoint_path': None,
 'checkpoint_paths': None,
 'chembl_assay_metadata_pickle_path': 'filtered_chembl/',
 'class_balance': False,
 'config_path': None,
 'crossval_index_dir': None,
 'crossval_index_file': None,
 'crossval_index_sets': None,
 'cuda': True,
 'data_path': 'filtered_chembl/chembl_less_1024_more_128_645_tasks.csv',
 'dataset_type': 'classification',
 'depth': 3,
 'device': device(type='cuda'),
 'dropout': 0.0,
 'dummy': False,
 'ensemble_size': 1,
 'epochs': 30,
 'experiment_name': 'maml_test',
 'features_generator': None,
 'features_only': False,
 'features_path': None,
 'features_scaling': True,
 'features_size': None,
 'ffn_hidden_size': 300,
 'ffn_num_layers': 2,
 'final_lr': 0.0001,
 'folds_file': None,
 'gpu': None,
 'hidden_size': 300,
 'init_lr': 0.0001,
 'inner_loop_lr': 0.05,
 'log_frequency': 10,
 'max_data_size': None,
 'max_lr': 0.001,
 'meta_batch_size': 32,
 'meta_learning': True,
 'meta_test_epochs': 30,
 'meta_test_lr': 0.001,
 'meta_test_split_sizes': (0.8, 0.1, 0.1),
 'meta_train_split_sizes': (0.8, 0.2, 0),
 'metric': 'auc',
 'minimize_score': False,
 'multiclass_num_classes': 3,
 'no_cuda': False,
 'no_features_scaling': False,
 'num_folds': 1,
 'num_inner_gradient_steps': 2,
 'num_lrs': 1,
 'num_tasks': None,
 'num_workers': 0,
 'outer_loop_lr': 0.003,
 'pytorch_seed': 0,
 'quiet': False,
 'results_save_dir': 'results/maml/',
 'save_dir': 'checkpoints/fold_0',
 'save_smiles_splits': False,
 'seed': 0,
 'separate_test_features_path': None,
 'separate_test_path': None,
 'separate_val_features_path': None,
 'separate_val_path': None,
 'show_individual_scores': False,
 'smiles_column': None,
 'split_sizes': (0.8, 0.1, 0.1),
 'split_type': 'scaffold_balanced',
 'target_columns': None,
 'task_names': None,
 'test': False,
 'test_fold_index': None,
 'train_data_size': None,
 'undirected': False,
 'use_input_features': False,
 'val_fold_index': None,
 'warmup_epochs': 2.0}
Loading data
0it [00:00, ?it/s]565it [00:00, 5646.14it/s]1115it [00:00, 5600.41it/s]1695it [00:00, 5597.74it/s]2274it [00:00, 5653.96it/s]2819it [00:00, 5590.15it/s]3391it [00:00, 5628.15it/s]3888it [00:00, 4827.21it/s]4434it [00:00, 5000.64it/s]4981it [00:00, 5130.52it/s]5539it [00:01, 5257.17it/s]6117it [00:01, 5401.30it/s]6682it [00:01, 5471.60it/s]7266it [00:01, 5575.93it/s]7823it [00:01, 4854.88it/s]8373it [00:01, 5031.22it/s]8955it [00:01, 5243.52it/s]9497it [00:01, 5294.97it/s]10070it [00:01, 5387.53it/s]10656it [00:01, 5520.10it/s]11216it [00:02, 5543.43it/s]11803it [00:02, 5637.49it/s]12370it [00:02, 4918.93it/s]12931it [00:02, 5105.61it/s]13515it [00:02, 5305.46it/s]14058it [00:02, 5306.31it/s]14623it [00:02, 5403.59it/s]15209it [00:02, 5530.30it/s]15771it [00:02, 5556.56it/s]16331it [00:03, 4914.82it/s]16897it [00:03, 5116.42it/s]17461it [00:03, 5261.54it/s]18050it [00:03, 5433.15it/s]18614it [00:03, 5492.92it/s]19179it [00:03, 5536.97it/s]19768it [00:03, 5634.12it/s]20335it [00:03, 4900.61it/s]20900it [00:03, 5102.54it/s]21490it [00:04, 5317.29it/s]22057it [00:04, 5417.09it/s]22634it [00:04, 5480.47it/s]23219it [00:04, 5583.99it/s]23785it [00:04, 5606.42it/s]24375it [00:04, 5690.92it/s]24948it [00:05, 2424.14it/s]25507it [00:05, 2920.15it/s]26094it [00:05, 3437.99it/s]26660it [00:05, 3895.97it/s]27227it [00:05, 4297.77it/s]27810it [00:05, 4664.95it/s]28370it [00:05, 4909.63it/s]28923it [00:05, 4498.19it/s]29512it [00:05, 4840.72it/s]30069it [00:06, 5038.21it/s]30645it [00:06, 5202.70it/s]31235it [00:06, 5392.65it/s]31800it [00:06, 5466.01it/s]32387it [00:06, 5518.54it/s]32972it [00:06, 5613.76it/s]33541it [00:06, 4891.49it/s]34129it [00:06, 5099.34it/s]34717it [00:06, 5310.27it/s]35279it [00:07, 5399.00it/s]35865it [00:07, 5528.52it/s]36431it [00:07, 5565.71it/s]36998it [00:07, 5594.15it/s]37562it [00:07, 4933.48it/s]38122it [00:07, 5114.68it/s]38685it [00:07, 5257.88it/s]39275it [00:07, 5433.15it/s]39837it [00:07, 5485.47it/s]40400it [00:07, 5525.24it/s]40986it [00:08, 5618.96it/s]41552it [00:08, 4913.00it/s]42135it [00:08, 5155.15it/s]42694it [00:08, 5278.29it/s]43258it [00:08, 5381.02it/s]43846it [00:08, 5520.32it/s]44410it [00:08, 5555.65it/s]44972it [00:08, 5572.73it/s]45557it [00:08, 5652.28it/s]46125it [00:09, 4933.96it/s]46686it [00:09, 5118.59it/s]47270it [00:09, 5315.20it/s]47831it [00:09, 5398.35it/s]48415it [00:09, 5463.20it/s]49002it [00:09, 5577.25it/s]49565it [00:09, 5579.50it/s]50127it [00:09, 4909.99it/s]50691it [00:09, 5108.26it/s]51253it [00:10, 5251.02it/s]51839it [00:10, 5418.74it/s]52403it [00:10, 5481.49it/s]52961it [00:10, 5508.04it/s]53528it [00:10, 5554.30it/s]54087it [00:10, 4821.56it/s]54647it [00:10, 5030.90it/s]55188it [00:10, 5138.11it/s]55739it [00:10, 5224.76it/s]56315it [00:10, 5372.55it/s]56874it [00:11, 5434.72it/s]57462it [00:11, 5559.79it/s]58026it [00:11, 5583.44it/s]58588it [00:11, 4885.02it/s]59177it [00:11, 5146.83it/s]59738it [00:11, 5276.00it/s]60300it [00:11, 5373.04it/s]60888it [00:11, 5515.58it/s]61454it [00:11, 5555.46it/s]61865it [00:11, 5164.81it/s]
  0% 0/61865 [00:00<?, ?it/s] 15% 9287/61865 [00:00<00:05, 9847.89it/s] 90% 55898/61865 [00:01<00:00, 12942.97it/s]100% 61865/61865 [00:01<00:00, 32344.13it/s]
  0% 0/61865 [00:00<?, ?it/s]  1% 691/61865 [00:00<00:08, 6902.26it/s]  2% 1359/61865 [00:00<00:08, 6831.85it/s]  3% 1984/61865 [00:00<00:09, 6645.95it/s]  4% 2588/61865 [00:00<00:09, 6447.52it/s]  5% 3251/61865 [00:00<00:09, 6499.41it/s]  6% 3878/61865 [00:00<00:09, 6428.00it/s]  7% 4476/61865 [00:00<00:09, 6284.89it/s]  8% 5060/61865 [00:00<00:09, 6143.63it/s]  9% 5637/61865 [00:00<00:09, 5843.87it/s] 10% 6198/61865 [00:01<00:09, 5740.10it/s] 11% 6756/61865 [00:01<00:09, 5581.96it/s] 12% 7315/61865 [00:01<00:09, 5584.18it/s] 13% 7867/61865 [00:01<00:09, 5481.44it/s] 14% 8411/61865 [00:01<00:09, 5423.73it/s] 15% 8980/61865 [00:01<00:09, 5500.35it/s] 15% 9536/61865 [00:01<00:09, 5516.14it/s] 16% 10089/61865 [00:01<00:09, 5518.54it/s] 17% 10640/61865 [00:01<00:09, 5460.45it/s] 18% 11193/61865 [00:01<00:09, 5478.68it/s] 19% 11752/61865 [00:02<00:09, 5507.57it/s] 20% 12315/61865 [00:02<00:08, 5540.71it/s] 21% 12874/61865 [00:02<00:08, 5553.38it/s] 22% 13441/61865 [00:02<00:08, 5585.55it/s] 23% 14004/61865 [00:02<00:08, 5595.73it/s] 24% 14564/61865 [00:02<00:08, 5517.83it/s] 24% 15117/61865 [00:02<00:08, 5489.06it/s] 25% 15673/61865 [00:02<00:08, 5507.01it/s] 26% 16238/61865 [00:02<00:08, 5547.14it/s] 27% 16806/61865 [00:02<00:08, 5584.48it/s] 28% 17388/61865 [00:03<00:07, 5652.67it/s] 29% 17964/61865 [00:03<00:07, 5680.18it/s] 30% 18534/61865 [00:03<00:07, 5685.25it/s] 31% 19103/61865 [00:03<00:07, 5621.55it/s] 32% 19672/61865 [00:03<00:07, 5640.30it/s] 33% 20237/61865 [00:03<00:07, 5593.79it/s] 34% 20797/61865 [00:03<00:07, 5566.60it/s] 35% 21354/61865 [00:03<00:07, 5550.70it/s] 35% 21913/61865 [00:03<00:07, 5562.25it/s] 36% 22480/61865 [00:03<00:07, 5594.09it/s] 37% 23047/61865 [00:04<00:06, 5616.31it/s] 38% 23623/61865 [00:04<00:06, 5655.98it/s] 39% 24195/61865 [00:04<00:06, 5674.77it/s] 40% 24764/61865 [00:04<00:06, 5676.69it/s] 41% 25332/61865 [00:04<00:06, 5653.76it/s] 42% 25898/61865 [00:04<00:06, 5624.32it/s] 43% 26461/61865 [00:04<00:06, 5605.92it/s] 44% 27026/61865 [00:04<00:06, 5615.71it/s] 45% 27595/61865 [00:04<00:06, 5635.61it/s] 46% 28159/61865 [00:04<00:05, 5632.89it/s] 46% 28723/61865 [00:05<00:05, 5584.13it/s] 47% 29282/61865 [00:05<00:05, 5554.75it/s] 48% 29848/61865 [00:05<00:05, 5584.61it/s] 49% 30411/61865 [00:05<00:05, 5596.27it/s] 50% 30979/61865 [00:05<00:05, 5620.03it/s] 51% 31542/61865 [00:05<00:05, 5612.63it/s] 52% 32111/61865 [00:05<00:05, 5635.30it/s] 53% 32675/61865 [00:05<00:05, 5582.68it/s] 54% 33237/61865 [00:05<00:05, 5591.80it/s] 55% 33797/61865 [00:05<00:05, 5593.99it/s] 56% 34368/61865 [00:06<00:04, 5627.78it/s] 56% 34931/61865 [00:06<00:04, 5613.32it/s] 57% 35493/61865 [00:06<00:04, 5532.59it/s] 58% 36047/61865 [00:06<00:04, 5520.11it/s] 59% 36600/61865 [00:06<00:04, 5517.54it/s] 60% 37171/61865 [00:06<00:04, 5572.87it/s] 61% 37749/61865 [00:06<00:04, 5632.36it/s] 62% 38315/61865 [00:06<00:04, 5639.46it/s] 63% 38880/61865 [00:06<00:04, 5615.72it/s] 64% 39448/61865 [00:06<00:03, 5630.64it/s] 65% 40012/61865 [00:07<00:03, 5623.99it/s] 66% 40575/61865 [00:07<00:03, 5572.53it/s] 66% 41134/61865 [00:07<00:03, 5577.11it/s] 67% 41695/61865 [00:07<00:03, 5585.29it/s] 68% 42254/61865 [00:07<00:03, 5580.94it/s] 69% 42813/61865 [00:07<00:03, 5528.17it/s] 70% 43373/61865 [00:07<00:03, 5547.98it/s] 71% 43935/61865 [00:07<00:03, 5569.33it/s] 72% 44504/61865 [00:07<00:03, 5603.02it/s] 73% 45066/61865 [00:07<00:02, 5606.92it/s] 74% 45627/61865 [00:08<00:02, 5593.70it/s] 75% 46187/61865 [00:08<00:02, 5556.88it/s] 76% 46743/61865 [00:08<00:02, 5539.34it/s] 76% 47301/61865 [00:08<00:02, 5548.82it/s] 77% 47856/61865 [00:08<00:02, 5546.52it/s] 78% 48411/61865 [00:08<00:02, 5507.90it/s] 79% 48962/61865 [00:08<00:02, 5481.93it/s] 80% 49511/61865 [00:08<00:02, 5440.09it/s] 81% 50056/61865 [00:08<00:02, 5381.97it/s] 82% 50595/61865 [00:08<00:02, 5337.82it/s] 83% 51130/61865 [00:09<00:02, 5298.01it/s] 84% 51661/61865 [00:09<00:01, 5263.87it/s] 84% 52188/61865 [00:09<00:01, 5253.31it/s] 85% 52714/61865 [00:09<00:01, 5241.21it/s] 86% 53239/61865 [00:09<00:01, 5198.91it/s] 87% 53789/61865 [00:09<00:01, 5285.29it/s] 88% 54318/61865 [00:09<00:01, 5267.94it/s] 89% 54862/61865 [00:09<00:01, 5316.45it/s] 90% 55394/61865 [00:09<00:01, 5184.31it/s] 90% 55914/61865 [00:10<00:01, 5179.18it/s] 91% 56474/61865 [00:10<00:01, 5297.93it/s] 92% 57005/61865 [00:10<00:00, 5068.72it/s] 93% 57515/61865 [00:10<00:00, 4681.51it/s] 94% 57992/61865 [00:10<00:00, 4333.69it/s] 94% 58437/61865 [00:10<00:00, 4142.57it/s] 95% 58942/61865 [00:10<00:00, 4377.31it/s] 96% 59534/61865 [00:10<00:00, 4746.82it/s] 97% 60031/61865 [00:10<00:00, 4810.63it/s] 98% 60577/61865 [00:11<00:00, 4988.39it/s] 99% 61160/61865 [00:11<00:00, 5212.62it/s]100% 61753/61865 [00:11<00:00, 5408.07it/s]100% 61865/61865 [00:11<00:00, 5508.74it/s]
Number of tasks = 645
Class sizes
CHEMBL1033994 0: 41.98%, 1: 58.02%
CHEMBL1119333 0: 9.59%, 1: 90.41%
CHEMBL1217000 0: 52.41%, 1: 47.59%
CHEMBL1243965 0: 17.11%, 1: 82.89%
CHEMBL1243966 0: 41.96%, 1: 58.04%
CHEMBL1243967 0: 10.17%, 1: 89.83%
CHEMBL1243968 0: 15.06%, 1: 84.94%
CHEMBL1243970 0: 6.91%, 1: 93.09%
CHEMBL1243972 0: 9.92%, 1: 90.08%
CHEMBL1243976 0: 28.99%, 1: 71.01%
CHEMBL1246087 0: 33.88%, 1: 66.12%
CHEMBL1246088 0: 48.37%, 1: 51.63%
CHEMBL1613762 0: 2.45%, 1: 97.55%
CHEMBL1613779 0: 58.47%, 1: 41.53%
CHEMBL1613785 0: 43.22%, 1: 56.78%
CHEMBL1613787 0: 69.08%, 1: 30.92%
CHEMBL1613807 0: 95.99%, 1: 4.01%
CHEMBL1613813 0: 44.20%, 1: 55.80%
CHEMBL1613814 0: 53.85%, 1: 46.15%
CHEMBL1613817 0: 84.86%, 1: 15.14%
CHEMBL1613853 0: 41.86%, 1: 58.14%
CHEMBL1613861 0: 14.47%, 1: 85.53%
CHEMBL1613864 0: 5.19%, 1: 94.81%
CHEMBL1613867 0: 97.13%, 1: 2.87%
CHEMBL1613870 0: 15.80%, 1: 84.20%
CHEMBL1613871 0: 6.15%, 1: 93.85%
CHEMBL1613874 0: 65.78%, 1: 34.22%
CHEMBL1613876 0: 54.35%, 1: 45.65%
CHEMBL1613884 0: 21.51%, 1: 78.49%
CHEMBL1613890 0: 34.85%, 1: 65.15%
CHEMBL1613897 0: 46.64%, 1: 53.36%
CHEMBL1613898 0: 75.45%, 1: 24.55%
CHEMBL1613904 0: 50.77%, 1: 49.23%
CHEMBL1613907 0: 5.34%, 1: 94.66%
CHEMBL1613926 0: 30.15%, 1: 69.85%
CHEMBL1613928 0: 45.90%, 1: 54.10%
CHEMBL1613929 0: 81.25%, 1: 18.75%
CHEMBL1613941 0: 32.58%, 1: 67.42%
CHEMBL1613942 0: 41.43%, 1: 58.57%
CHEMBL1613947 0: 46.01%, 1: 53.99%
CHEMBL1613949 0: 60.13%, 1: 39.87%
CHEMBL1613950 0: 43.05%, 1: 56.95%
CHEMBL1613955 0: 54.04%, 1: 45.96%
CHEMBL1613962 0: 24.26%, 1: 75.74%
CHEMBL1613967 0: 44.59%, 1: 55.41%
CHEMBL1613981 0: 58.85%, 1: 41.15%
CHEMBL1613991 0: 14.98%, 1: 85.02%
CHEMBL1613997 0: 7.73%, 1: 92.27%
CHEMBL1614001 0: 98.28%, 1: 1.72%
CHEMBL1614004 0: 14.66%, 1: 85.34%
CHEMBL1614016 0: 12.22%, 1: 87.78%
CHEMBL1614030 0: 19.96%, 1: 80.04%
CHEMBL1614034 0: 2.63%, 1: 97.37%
CHEMBL1614035 0: 77.14%, 1: 22.86%
CHEMBL1614049 0: 39.63%, 1: 60.37%
CHEMBL1614053 0: 42.48%, 1: 57.52%
CHEMBL1614063 0: 95.18%, 1: 4.82%
CHEMBL1614065 0: 9.71%, 1: 90.29%
CHEMBL1614066 0: 94.74%, 1: 5.26%
CHEMBL1614069 0: 56.82%, 1: 43.18%
CHEMBL1614072 0: 50.36%, 1: 49.64%
CHEMBL1614084 0: 13.26%, 1: 86.74%
CHEMBL1614091 0: 4.43%, 1: 95.57%
CHEMBL1614092 0: 47.50%, 1: 52.50%
CHEMBL1614097 0: 46.99%, 1: 53.01%
CHEMBL1614098 0: 97.25%, 1: 2.75%
CHEMBL1614104 0: 7.74%, 1: 92.26%
CHEMBL1614105 0: 6.12%, 1: 93.88%
CHEMBL1614109 0: 59.87%, 1: 40.13%
CHEMBL1614128 0: 10.35%, 1: 89.65%
CHEMBL1614131 0: 97.14%, 1: 2.86%
CHEMBL1614132 0: 42.07%, 1: 57.93%
CHEMBL1614138 0: 67.83%, 1: 32.17%
CHEMBL1614155 0: 25.40%, 1: 74.60%
CHEMBL1614158 0: 74.52%, 1: 25.48%
CHEMBL1614167 0: 16.17%, 1: 83.83%
CHEMBL1614170 0: 68.86%, 1: 31.14%
CHEMBL1614171 0: 93.77%, 1: 6.23%
CHEMBL1614175 0: 80.66%, 1: 19.34%
CHEMBL1614185 0: 26.17%, 1: 73.83%
CHEMBL1614197 0: 5.26%, 1: 94.74%
CHEMBL1614199 0: 49.66%, 1: 50.34%
CHEMBL1614202 0: 7.32%, 1: 92.68%
CHEMBL1614215 0: 52.76%, 1: 47.24%
CHEMBL1614216 0: 48.52%, 1: 51.48%
CHEMBL1614218 0: 22.36%, 1: 77.64%
CHEMBL1614225 0: 62.50%, 1: 37.50%
CHEMBL1614244 0: 38.85%, 1: 61.15%
CHEMBL1614247 0: 44.34%, 1: 55.66%
CHEMBL1614252 0: 60.11%, 1: 39.89%
CHEMBL1614255 0: 61.26%, 1: 38.74%
CHEMBL1614259 0: 32.82%, 1: 67.18%
CHEMBL1614272 0: 67.71%, 1: 32.29%
CHEMBL1614276 0: 51.74%, 1: 48.26%
CHEMBL1614287 0: 85.53%, 1: 14.47%
CHEMBL1614288 0: 72.51%, 1: 27.49%
CHEMBL1614290 0: 4.51%, 1: 95.49%
CHEMBL1614295 0: 2.29%, 1: 97.71%
CHEMBL1614301 0: 19.74%, 1: 80.26%
CHEMBL1614304 0: 63.76%, 1: 36.24%
CHEMBL1614309 0: 27.66%, 1: 72.34%
CHEMBL1614311 0: 53.76%, 1: 46.24%
CHEMBL1614314 0: 94.70%, 1: 5.30%
CHEMBL1614319 0: 72.16%, 1: 27.84%
CHEMBL1614320 0: 29.79%, 1: 70.21%
CHEMBL1614321 0: 60.16%, 1: 39.84%
CHEMBL1614328 0: 46.64%, 1: 53.36%
CHEMBL1614329 0: 40.69%, 1: 59.31%
CHEMBL1614336 0: 47.01%, 1: 52.99%
CHEMBL1614344 0: 67.72%, 1: 32.28%
CHEMBL1614356 0: 43.90%, 1: 56.10%
CHEMBL1614359 0: 50.26%, 1: 49.74%
CHEMBL1614363 0: 53.19%, 1: 46.81%
CHEMBL1614385 0: 93.27%, 1: 6.73%
CHEMBL1614388 0: 56.19%, 1: 43.81%
CHEMBL1614393 0: 45.00%, 1: 55.00%
CHEMBL1614395 0: 11.32%, 1: 88.68%
CHEMBL1614403 0: 88.15%, 1: 11.85%
CHEMBL1614423 0: 57.84%, 1: 42.16%
CHEMBL1614425 0: 38.24%, 1: 61.76%
CHEMBL1614433 0: 60.77%, 1: 39.23%
CHEMBL1614434 0: 80.25%, 1: 19.75%
CHEMBL1614456 0: 3.36%, 1: 96.64%
CHEMBL1614466 0: 10.32%, 1: 89.68%
CHEMBL1614469 0: 24.90%, 1: 75.10%
CHEMBL1614477 0: 43.75%, 1: 56.25%
CHEMBL1614478 0: 12.50%, 1: 87.50%
CHEMBL1614480 0: 85.71%, 1: 14.29%
CHEMBL1614484 0: 9.01%, 1: 90.99%
CHEMBL1614492 0: 40.54%, 1: 59.46%
CHEMBL1614499 0: 44.47%, 1: 55.53%
CHEMBL1614503 0: 40.30%, 1: 59.70%
CHEMBL1614504 0: 17.80%, 1: 82.20%
CHEMBL1614509 0: 79.43%, 1: 20.57%
CHEMBL1614512 0: 62.79%, 1: 37.21%
CHEMBL1614514 0: 83.07%, 1: 16.93%
CHEMBL1614515 0: 19.61%, 1: 80.39%
CHEMBL1614516 0: 96.81%, 1: 3.19%
CHEMBL1614522 0: 28.55%, 1: 71.45%
CHEMBL1614524 0: 12.98%, 1: 87.02%
CHEMBL1614528 0: 40.91%, 1: 59.09%
CHEMBL1614547 0: 72.51%, 1: 27.49%
CHEMBL1614548 0: 17.95%, 1: 82.05%
CHEMBL1614549 0: 84.25%, 1: 15.75%
CHEMBL1614550 0: 9.30%, 1: 90.70%
CHEMBL1614554 0: 71.64%, 1: 28.36%
CHEMBL1676103 0: 39.85%, 1: 60.15%
CHEMBL1737860 0: 3.60%, 1: 96.40%
CHEMBL1737863 0: 36.27%, 1: 63.73%
CHEMBL1737865 0: 61.01%, 1: 38.99%
CHEMBL1737868 0: 79.41%, 1: 20.59%
CHEMBL1737910 0: 45.71%, 1: 54.29%
CHEMBL1737912 0: 26.51%, 1: 73.49%
CHEMBL1737942 0: 44.52%, 1: 55.48%
CHEMBL1737951 0: 51.83%, 1: 48.17%
CHEMBL1737961 0: 93.34%, 1: 6.66%
CHEMBL1737966 0: 21.36%, 1: 78.64%
CHEMBL1737967 0: 89.17%, 1: 10.83%
CHEMBL1737977 0: 13.87%, 1: 86.13%
CHEMBL1737978 0: 18.18%, 1: 81.82%
CHEMBL1737979 0: 8.94%, 1: 91.06%
CHEMBL1738019 0: 24.85%, 1: 75.15%
CHEMBL1738021 0: 10.87%, 1: 89.13%
CHEMBL1738025 0: 33.80%, 1: 66.20%
CHEMBL1738040 0: 73.48%, 1: 26.52%
CHEMBL1738043 0: 46.43%, 1: 53.57%
CHEMBL1738079 0: 3.88%, 1: 96.12%
CHEMBL1738080 0: 76.43%, 1: 23.57%
CHEMBL1738091 0: 89.77%, 1: 10.23%
CHEMBL1738097 0: 72.41%, 1: 27.59%
CHEMBL1738131 0: 68.59%, 1: 31.41%
CHEMBL1738164 0: 79.79%, 1: 20.21%
CHEMBL1738171 0: 85.45%, 1: 14.55%
CHEMBL1738183 0: 59.62%, 1: 40.38%
CHEMBL1738197 0: 36.90%, 1: 63.10%
CHEMBL1738202 0: 3.47%, 1: 96.53%
CHEMBL1738242 0: 16.55%, 1: 83.45%
CHEMBL1738249 0: 86.53%, 1: 13.47%
CHEMBL1738253 0: 7.33%, 1: 92.67%
CHEMBL1738319 0: 59.17%, 1: 40.83%
CHEMBL1738325 0: 96.86%, 1: 3.14%
CHEMBL1738362 0: 21.43%, 1: 78.57%
CHEMBL1738369 0: 56.34%, 1: 43.66%
CHEMBL1738371 0: 81.97%, 1: 18.03%
CHEMBL1738391 0: 68.38%, 1: 31.62%
CHEMBL1738400 0: 77.86%, 1: 22.14%
CHEMBL1738402 0: 54.19%, 1: 45.81%
CHEMBL1738407 0: 85.58%, 1: 14.42%
CHEMBL1738408 0: 2.18%, 1: 97.82%
CHEMBL1738414 0: 5.11%, 1: 94.89%
CHEMBL1738418 0: 68.85%, 1: 31.15%
CHEMBL1738422 0: 4.29%, 1: 95.71%
CHEMBL1738424 0: 77.50%, 1: 22.50%
CHEMBL1738430 0: 25.54%, 1: 74.46%
CHEMBL1738438 0: 2.76%, 1: 97.24%
CHEMBL1738482 0: 7.69%, 1: 92.31%
CHEMBL1738485 0: 17.65%, 1: 82.35%
CHEMBL1738494 0: 97.26%, 1: 2.74%
CHEMBL1738495 0: 1.70%, 1: 98.30%
CHEMBL1738497 0: 64.50%, 1: 35.50%
CHEMBL1738502 0: 15.20%, 1: 84.80%
CHEMBL1738510 0: 54.41%, 1: 45.59%
CHEMBL1738512 0: 3.05%, 1: 96.95%
CHEMBL1738513 0: 28.64%, 1: 71.36%
CHEMBL1738552 0: 64.70%, 1: 35.30%
CHEMBL1738575 0: 65.65%, 1: 34.35%
CHEMBL1738578 0: 23.30%, 1: 76.70%
CHEMBL1738579 0: 22.79%, 1: 77.21%
CHEMBL1738593 0: 17.65%, 1: 82.35%
CHEMBL1738599 0: 16.07%, 1: 83.93%
CHEMBL1738602 0: 42.70%, 1: 57.30%
CHEMBL1738610 0: 42.07%, 1: 57.93%
CHEMBL1738611 0: 2.23%, 1: 97.77%
CHEMBL1738632 0: 86.32%, 1: 13.68%
CHEMBL1738633 0: 4.05%, 1: 95.95%
CHEMBL1738639 0: 64.47%, 1: 35.53%
CHEMBL1738642 0: 34.82%, 1: 65.18%
CHEMBL1738670 0: 9.18%, 1: 90.82%
CHEMBL1738673 0: 12.98%, 1: 87.02%
CHEMBL1738679 0: 63.95%, 1: 36.05%
CHEMBL1738682 0: 62.45%, 1: 37.55%
CHEMBL1794296 0: 44.02%, 1: 55.98%
CHEMBL1794303 0: 2.71%, 1: 97.29%
CHEMBL1794320 0: 96.68%, 1: 3.32%
CHEMBL1794327 0: 91.59%, 1: 8.41%
CHEMBL1794336 0: 53.07%, 1: 46.93%
CHEMBL1794350 0: 3.44%, 1: 96.56%
CHEMBL1794355 0: 5.26%, 1: 94.74%
CHEMBL1794356 0: 13.92%, 1: 86.08%
CHEMBL1794358 0: 94.14%, 1: 5.86%
CHEMBL1794365 0: 7.12%, 1: 92.88%
CHEMBL1794383 0: 6.90%, 1: 93.10%
CHEMBL1794387 0: 65.04%, 1: 34.96%
CHEMBL1794393 0: 67.55%, 1: 32.45%
CHEMBL1794396 0: 3.96%, 1: 96.04%
CHEMBL1794410 0: 23.98%, 1: 76.02%
CHEMBL1794413 0: 6.02%, 1: 93.98%
CHEMBL1794438 0: 40.17%, 1: 59.83%
CHEMBL1794445 0: 87.84%, 1: 12.16%
CHEMBL1794452 0: 94.30%, 1: 5.70%
CHEMBL1794457 0: 92.98%, 1: 7.02%
CHEMBL1794460 0: 28.63%, 1: 71.37%
CHEMBL1794467 0: 18.39%, 1: 81.61%
CHEMBL1794475 0: 56.02%, 1: 43.98%
CHEMBL1794484 0: 44.93%, 1: 55.07%
CHEMBL1794494 0: 5.33%, 1: 94.67%
CHEMBL1794497 0: 8.47%, 1: 91.53%
CHEMBL1794499 0: 73.90%, 1: 26.10%
CHEMBL1794508 0: 94.63%, 1: 5.37%
CHEMBL1794516 0: 3.21%, 1: 96.79%
CHEMBL1794522 0: 58.65%, 1: 41.35%
CHEMBL1794528 0: 79.86%, 1: 20.14%
CHEMBL1794531 0: 92.03%, 1: 7.97%
CHEMBL1794548 0: 88.74%, 1: 11.26%
CHEMBL1794566 0: 6.05%, 1: 93.95%
CHEMBL1794567 0: 9.61%, 1: 90.39%
CHEMBL1794570 0: 92.79%, 1: 7.21%
CHEMBL1794571 0: 53.55%, 1: 46.45%
CHEMBL1794573 0: 10.54%, 1: 89.46%
CHEMBL1794574 0: 1.09%, 1: 98.91%
CHEMBL1794578 0: 46.99%, 1: 53.01%
CHEMBL1794581 0: 19.65%, 1: 80.35%
CHEMBL1863510 0: 2.14%, 1: 97.86%
CHEMBL1863512 0: 2.79%, 1: 97.21%
CHEMBL1909084 0: 99.28%, 1: 0.72%
CHEMBL1909085 0: 92.10%, 1: 7.90%
CHEMBL1909086 0: 91.52%, 1: 8.48%
CHEMBL1909087 0: 91.86%, 1: 8.14%
CHEMBL1909088 0: 87.97%, 1: 12.03%
CHEMBL1909089 0: 88.69%, 1: 11.31%
CHEMBL1909090 0: 90.81%, 1: 9.19%
CHEMBL1909091 0: 97.24%, 1: 2.76%
CHEMBL1909092 0: 97.14%, 1: 2.86%
CHEMBL1909093 0: 98.42%, 1: 1.58%
CHEMBL1909094 0: 90.85%, 1: 9.15%
CHEMBL1909095 0: 98.56%, 1: 1.44%
CHEMBL1909097 0: 99.28%, 1: 0.72%
CHEMBL1909102 0: 92.94%, 1: 7.06%
CHEMBL1909103 0: 97.23%, 1: 2.77%
CHEMBL1909104 0: 87.06%, 1: 12.94%
CHEMBL1909105 0: 88.70%, 1: 11.30%
CHEMBL1909106 0: 98.57%, 1: 1.43%
CHEMBL1909107 0: 97.86%, 1: 2.14%
CHEMBL1909108 0: 93.04%, 1: 6.96%
CHEMBL1909109 0: 90.35%, 1: 9.65%
CHEMBL1909110 0: 92.47%, 1: 7.53%
CHEMBL1909111 0: 94.03%, 1: 5.97%
CHEMBL1909112 0: 90.91%, 1: 9.09%
CHEMBL1909114 0: 96.50%, 1: 3.50%
CHEMBL1909115 0: 96.00%, 1: 4.00%
CHEMBL1909116 0: 96.98%, 1: 3.02%
CHEMBL1909121 0: 92.80%, 1: 7.20%
CHEMBL1909123 0: 99.16%, 1: 0.84%
CHEMBL1909124 0: 99.40%, 1: 0.60%
CHEMBL1909130 0: 95.56%, 1: 4.44%
CHEMBL1909131 0: 97.01%, 1: 2.99%
CHEMBL1909132 0: 96.15%, 1: 3.85%
CHEMBL1909134 0: 95.19%, 1: 4.81%
CHEMBL1909135 0: 95.76%, 1: 4.24%
CHEMBL1909136 0: 93.20%, 1: 6.80%
CHEMBL1909138 0: 96.16%, 1: 3.84%
CHEMBL1909139 0: 93.84%, 1: 6.16%
CHEMBL1909140 0: 93.87%, 1: 6.13%
CHEMBL1909141 0: 89.73%, 1: 10.27%
CHEMBL1909142 0: 99.04%, 1: 0.96%
CHEMBL1909143 0: 93.01%, 1: 6.99%
CHEMBL1909145 0: 97.97%, 1: 2.03%
CHEMBL1909148 0: 98.57%, 1: 1.43%
CHEMBL1909150 0: 95.10%, 1: 4.90%
CHEMBL1909156 0: 94.28%, 1: 5.72%
CHEMBL1909157 0: 96.38%, 1: 3.62%
CHEMBL1909158 0: 99.05%, 1: 0.95%
CHEMBL1909159 0: 93.18%, 1: 6.82%
CHEMBL1909165 0: 97.79%, 1: 2.21%
CHEMBL1909169 0: 98.32%, 1: 1.68%
CHEMBL1909170 0: 92.35%, 1: 7.65%
CHEMBL1909171 0: 93.29%, 1: 6.71%
CHEMBL1909172 0: 92.82%, 1: 7.18%
CHEMBL1909173 0: 92.38%, 1: 7.62%
CHEMBL1909174 0: 92.71%, 1: 7.29%
CHEMBL1909180 0: 98.20%, 1: 1.80%
CHEMBL1909181 0: 97.71%, 1: 2.29%
CHEMBL1909182 0: 96.99%, 1: 3.01%
CHEMBL1909184 0: 98.93%, 1: 1.07%
CHEMBL1909186 0: 99.52%, 1: 0.48%
CHEMBL1909190 0: 98.66%, 1: 1.34%
CHEMBL1909191 0: 94.99%, 1: 5.01%
CHEMBL1909192 0: 99.52%, 1: 0.48%
CHEMBL1909200 0: 97.97%, 1: 2.03%
CHEMBL1909201 0: 97.72%, 1: 2.28%
CHEMBL1909203 0: 98.05%, 1: 1.95%
CHEMBL1909204 0: 97.17%, 1: 2.83%
CHEMBL1909205 0: 98.67%, 1: 1.33%
CHEMBL1909206 0: 99.16%, 1: 0.84%
CHEMBL1909209 0: 92.34%, 1: 7.66%
CHEMBL1909210 0: 96.05%, 1: 3.95%
CHEMBL1909211 0: 88.80%, 1: 11.20%
CHEMBL1909212 0: 98.30%, 1: 1.70%
CHEMBL1909213 0: 99.28%, 1: 0.72%
CHEMBL1909214 0: 98.79%, 1: 1.21%
CHEMBL1909215 0: 95.61%, 1: 4.39%
CHEMBL1963686 0: 61.95%, 1: 38.05%
CHEMBL1963687 0: 64.42%, 1: 35.58%
CHEMBL1963688 0: 62.52%, 1: 37.48%
CHEMBL1963689 0: 89.69%, 1: 10.31%
CHEMBL1963690 0: 57.36%, 1: 42.64%
CHEMBL1963691 0: 49.33%, 1: 50.67%
CHEMBL1963692 0: 53.73%, 1: 46.27%
CHEMBL1963693 0: 71.00%, 1: 29.00%
CHEMBL1963694 0: 91.05%, 1: 8.95%
CHEMBL1963695 0: 74.90%, 1: 25.10%
CHEMBL1963696 0: 77.85%, 1: 22.15%
CHEMBL1963697 0: 74.06%, 1: 25.94%
CHEMBL1963698 0: 74.60%, 1: 25.40%
CHEMBL1963699 0: 83.18%, 1: 16.82%
CHEMBL1963701 0: 67.32%, 1: 32.68%
CHEMBL1963702 0: 83.82%, 1: 16.18%
CHEMBL1963703 0: 68.65%, 1: 31.35%
CHEMBL1963704 0: 65.28%, 1: 34.72%
CHEMBL1963705 0: 55.92%, 1: 44.08%
CHEMBL1963706 0: 53.38%, 1: 46.62%
CHEMBL1963707 0: 53.25%, 1: 46.75%
CHEMBL1963708 0: 48.77%, 1: 51.23%
CHEMBL1963710 0: 63.21%, 1: 36.79%
CHEMBL1963711 0: 94.72%, 1: 5.28%
CHEMBL1963712 0: 75.76%, 1: 24.24%
CHEMBL1963714 0: 74.21%, 1: 25.79%
CHEMBL1963715 0: 50.28%, 1: 49.72%
CHEMBL1963716 0: 91.91%, 1: 8.09%
CHEMBL1963717 0: 55.50%, 1: 44.50%
CHEMBL1963718 0: 65.45%, 1: 34.55%
CHEMBL1963719 0: 63.39%, 1: 36.61%
CHEMBL1963720 0: 69.73%, 1: 30.27%
CHEMBL1963721 0: 56.94%, 1: 43.06%
CHEMBL1963722 0: 49.40%, 1: 50.60%
CHEMBL1963723 0: 53.82%, 1: 46.18%
CHEMBL1963724 0: 67.32%, 1: 32.68%
CHEMBL1963725 0: 69.97%, 1: 30.03%
CHEMBL1963727 0: 56.59%, 1: 43.41%
CHEMBL1963728 0: 92.02%, 1: 7.98%
CHEMBL1963729 0: 84.64%, 1: 15.36%
CHEMBL1963731 0: 61.35%, 1: 38.65%
CHEMBL1963733 0: 79.77%, 1: 20.23%
CHEMBL1963734 0: 79.51%, 1: 20.49%
CHEMBL1963735 0: 71.66%, 1: 28.34%
CHEMBL1963736 0: 78.32%, 1: 21.68%
CHEMBL1963737 0: 86.79%, 1: 13.21%
CHEMBL1963738 0: 61.18%, 1: 38.82%
CHEMBL1963739 0: 84.32%, 1: 15.68%
CHEMBL1963740 0: 64.30%, 1: 35.70%
CHEMBL1963741 0: 67.03%, 1: 32.97%
CHEMBL1963742 0: 73.47%, 1: 26.53%
CHEMBL1963743 0: 66.24%, 1: 33.76%
CHEMBL1963744 0: 71.97%, 1: 28.03%
CHEMBL1963745 0: 67.16%, 1: 32.84%
CHEMBL1963746 0: 66.34%, 1: 33.66%
CHEMBL1963747 0: 81.33%, 1: 18.67%
CHEMBL1963748 0: 58.75%, 1: 41.25%
CHEMBL1963749 0: 55.02%, 1: 44.98%
CHEMBL1963750 0: 58.61%, 1: 41.39%
CHEMBL1963751 0: 70.90%, 1: 29.10%
CHEMBL1963752 0: 64.78%, 1: 35.22%
CHEMBL1963753 0: 88.02%, 1: 11.98%
CHEMBL1963754 0: 59.12%, 1: 40.88%
CHEMBL1963756 0: 61.66%, 1: 38.34%
CHEMBL1963757 0: 73.28%, 1: 26.72%
CHEMBL1963758 0: 82.56%, 1: 17.44%
CHEMBL1963759 0: 81.38%, 1: 18.62%
CHEMBL1963760 0: 86.36%, 1: 13.64%
CHEMBL1963761 0: 83.63%, 1: 16.37%
CHEMBL1963763 0: 72.14%, 1: 27.86%
CHEMBL1963764 0: 62.50%, 1: 37.50%
CHEMBL1963765 0: 86.41%, 1: 13.59%
CHEMBL1963766 0: 81.38%, 1: 18.62%
CHEMBL1963767 0: 77.76%, 1: 22.24%
CHEMBL1963768 0: 77.63%, 1: 22.37%
CHEMBL1963770 0: 89.44%, 1: 10.56%
CHEMBL1963771 0: 53.53%, 1: 46.47%
CHEMBL1963772 0: 48.99%, 1: 51.01%
CHEMBL1963773 0: 59.00%, 1: 41.00%
CHEMBL1963775 0: 76.92%, 1: 23.08%
CHEMBL1963776 0: 87.71%, 1: 12.29%
CHEMBL1963777 0: 63.92%, 1: 36.08%
CHEMBL1963778 0: 51.70%, 1: 48.30%
CHEMBL1963779 0: 54.69%, 1: 45.31%
CHEMBL1963780 0: 89.06%, 1: 10.94%
CHEMBL1963781 0: 94.31%, 1: 5.69%
CHEMBL1963782 0: 75.86%, 1: 24.14%
CHEMBL1963783 0: 57.36%, 1: 42.64%
CHEMBL1963785 0: 70.10%, 1: 29.90%
CHEMBL1963786 0: 50.08%, 1: 49.92%
CHEMBL1963787 0: 66.39%, 1: 33.61%
CHEMBL1963789 0: 67.34%, 1: 32.66%
CHEMBL1963790 0: 57.93%, 1: 42.07%
CHEMBL1963791 0: 82.08%, 1: 17.92%
CHEMBL1963792 0: 85.73%, 1: 14.27%
CHEMBL1963793 0: 65.48%, 1: 34.52%
CHEMBL1963794 0: 77.53%, 1: 22.47%
CHEMBL1963795 0: 46.98%, 1: 53.02%
CHEMBL1963796 0: 69.76%, 1: 30.24%
CHEMBL1963797 0: 83.64%, 1: 16.36%
CHEMBL1963798 0: 89.45%, 1: 10.55%
CHEMBL1963799 0: 43.54%, 1: 56.46%
CHEMBL1963800 0: 64.88%, 1: 35.12%
CHEMBL1963801 0: 75.90%, 1: 24.10%
CHEMBL1963802 0: 61.41%, 1: 38.59%
CHEMBL1963803 0: 87.44%, 1: 12.56%
CHEMBL1963804 0: 65.90%, 1: 34.10%
CHEMBL1963805 0: 53.77%, 1: 46.23%
CHEMBL1963806 0: 41.19%, 1: 58.81%
CHEMBL1963807 0: 36.83%, 1: 63.17%
CHEMBL1963808 0: 68.82%, 1: 31.18%
CHEMBL1963809 0: 79.97%, 1: 20.03%
CHEMBL1963810 0: 43.69%, 1: 56.31%
CHEMBL1963811 0: 73.98%, 1: 26.02%
CHEMBL1963812 0: 53.08%, 1: 46.92%
CHEMBL1963813 0: 79.19%, 1: 20.81%
CHEMBL1963814 0: 51.10%, 1: 48.90%
CHEMBL1963815 0: 71.80%, 1: 28.20%
CHEMBL1963816 0: 89.64%, 1: 10.36%
CHEMBL1963817 0: 71.90%, 1: 28.10%
CHEMBL1963818 0: 55.13%, 1: 44.87%
CHEMBL1963819 0: 62.64%, 1: 37.36%
CHEMBL1963820 0: 84.46%, 1: 15.54%
CHEMBL1963821 0: 79.31%, 1: 20.69%
CHEMBL1963822 0: 90.91%, 1: 9.09%
CHEMBL1963823 0: 76.61%, 1: 23.39%
CHEMBL1963824 0: 39.57%, 1: 60.43%
CHEMBL1963825 0: 51.12%, 1: 48.88%
CHEMBL1963826 0: 59.25%, 1: 40.75%
CHEMBL1963827 0: 50.74%, 1: 49.26%
CHEMBL1963828 0: 80.86%, 1: 19.14%
CHEMBL1963829 0: 86.41%, 1: 13.59%
CHEMBL1963831 0: 52.56%, 1: 47.44%
CHEMBL1963832 0: 72.98%, 1: 27.02%
CHEMBL1963833 0: 87.13%, 1: 12.87%
CHEMBL1963834 0: 59.69%, 1: 40.31%
CHEMBL1963836 0: 85.18%, 1: 14.82%
CHEMBL1963837 0: 26.01%, 1: 73.99%
CHEMBL1963838 0: 75.83%, 1: 24.17%
CHEMBL1963846 0: 85.07%, 1: 14.93%
CHEMBL1963867 0: 73.13%, 1: 26.87%
CHEMBL1963893 0: 85.61%, 1: 14.39%
CHEMBL1963898 0: 75.62%, 1: 24.38%
CHEMBL1963907 0: 64.35%, 1: 35.65%
CHEMBL1963910 0: 10.50%, 1: 89.50%
CHEMBL1963915 0: 15.00%, 1: 85.00%
CHEMBL1963916 0: 40.48%, 1: 59.52%
CHEMBL1963918 0: 73.35%, 1: 26.65%
CHEMBL1963930 0: 3.64%, 1: 96.36%
CHEMBL1963933 0: 93.91%, 1: 6.09%
CHEMBL1963934 0: 3.64%, 1: 96.36%
CHEMBL1963937 0: 39.27%, 1: 60.73%
CHEMBL1963938 0: 65.85%, 1: 34.15%
CHEMBL1963940 0: 33.79%, 1: 66.21%
CHEMBL1963947 0: 6.25%, 1: 93.75%
CHEMBL1963966 0: 3.29%, 1: 96.71%
CHEMBL1963968 0: 51.52%, 1: 48.48%
CHEMBL1963969 0: 25.83%, 1: 74.17%
CHEMBL1963971 0: 10.20%, 1: 89.80%
CHEMBL1963974 0: 88.26%, 1: 11.74%
CHEMBL1963983 0: 76.39%, 1: 23.61%
CHEMBL1964000 0: 76.77%, 1: 23.23%
CHEMBL1964005 0: 6.67%, 1: 93.33%
CHEMBL1964010 0: 76.06%, 1: 23.94%
CHEMBL1964015 0: 73.77%, 1: 26.23%
CHEMBL1964022 0: 76.61%, 1: 23.39%
CHEMBL1964023 0: 73.75%, 1: 26.25%
CHEMBL1964081 0: 47.98%, 1: 52.02%
CHEMBL1964095 0: 65.41%, 1: 34.59%
CHEMBL1964096 0: 11.36%, 1: 88.64%
CHEMBL1964100 0: 79.62%, 1: 20.38%
CHEMBL1964101 0: 34.62%, 1: 65.38%
CHEMBL1964102 0: 71.54%, 1: 28.46%
CHEMBL1964103 0: 51.58%, 1: 48.42%
CHEMBL1964104 0: 63.88%, 1: 36.12%
CHEMBL1964105 0: 64.53%, 1: 35.47%
CHEMBL1964106 0: 57.96%, 1: 42.04%
CHEMBL1964108 0: 69.62%, 1: 30.38%
CHEMBL1964111 0: 73.41%, 1: 26.59%
CHEMBL1964112 0: 87.59%, 1: 12.41%
CHEMBL1964114 0: 61.11%, 1: 38.89%
CHEMBL1964115 0: 43.36%, 1: 56.64%
CHEMBL1964116 0: 70.55%, 1: 29.45%
CHEMBL1964117 0: 63.66%, 1: 36.34%
CHEMBL1964118 0: 60.33%, 1: 39.67%
CHEMBL1964119 0: 57.23%, 1: 42.77%
CHEMBL2028073 0: 76.99%, 1: 23.01%
CHEMBL2028074 0: 74.88%, 1: 25.12%
CHEMBL2028075 0: 56.46%, 1: 43.54%
CHEMBL2028076 0: 85.04%, 1: 14.96%
CHEMBL2028077 0: 96.19%, 1: 3.81%
CHEMBL2095143 0: 93.77%, 1: 6.23%
CHEMBL2098499 0: 74.45%, 1: 25.55%
CHEMBL2114715 0: 60.31%, 1: 39.69%
CHEMBL2114716 0: 12.27%, 1: 87.73%
CHEMBL2114719 0: 77.41%, 1: 22.59%
CHEMBL2114725 0: 51.74%, 1: 48.26%
CHEMBL2114727 0: 16.96%, 1: 83.04%
CHEMBL2114728 0: 9.82%, 1: 90.18%
CHEMBL2114737 0: 87.93%, 1: 12.07%
CHEMBL2114742 0: 33.90%, 1: 66.10%
CHEMBL2114748 0: 59.44%, 1: 40.56%
CHEMBL2114752 0: 15.02%, 1: 84.98%
CHEMBL2114753 0: 8.36%, 1: 91.64%
CHEMBL2114761 0: 78.40%, 1: 21.60%
CHEMBL2114764 0: 42.92%, 1: 57.08%
CHEMBL2114771 0: 64.02%, 1: 35.98%
CHEMBL2114791 0: 1.99%, 1: 98.01%
CHEMBL2114797 0: 42.41%, 1: 57.59%
CHEMBL2114811 0: 10.16%, 1: 89.84%
CHEMBL2114814 0: 44.98%, 1: 55.02%
CHEMBL2114816 0: 37.94%, 1: 62.06%
CHEMBL2114818 0: 4.15%, 1: 95.85%
CHEMBL2114820 0: 16.31%, 1: 83.69%
CHEMBL2114821 0: 18.77%, 1: 81.23%
CHEMBL2114823 0: 20.90%, 1: 79.10%
CHEMBL2114825 0: 18.62%, 1: 81.38%
CHEMBL2114827 0: 93.80%, 1: 6.20%
CHEMBL2114829 0: 25.17%, 1: 74.83%
CHEMBL2114830 0: 20.17%, 1: 79.83%
CHEMBL2114839 0: 28.69%, 1: 71.31%
CHEMBL2114842 0: 62.00%, 1: 38.00%
CHEMBL2114844 0: 32.59%, 1: 67.41%
CHEMBL2114847 0: 13.84%, 1: 86.16%
CHEMBL2114850 0: 8.62%, 1: 91.38%
CHEMBL2114852 0: 3.84%, 1: 96.16%
CHEMBL2114857 0: 50.00%, 1: 50.00%
CHEMBL2114858 0: 32.36%, 1: 67.64%
CHEMBL2114863 0: 33.55%, 1: 66.45%
CHEMBL2114865 0: 3.43%, 1: 96.57%
CHEMBL2114872 0: 9.51%, 1: 90.49%
CHEMBL2114874 0: 28.03%, 1: 71.97%
CHEMBL2114882 0: 92.25%, 1: 7.75%
CHEMBL2114896 0: 51.22%, 1: 48.78%
CHEMBL2114899 0: 87.76%, 1: 12.24%
CHEMBL2114909 0: 1.94%, 1: 98.06%
CHEMBL2114916 0: 42.50%, 1: 57.50%
CHEMBL2114926 0: 41.57%, 1: 58.43%
CHEMBL2114928 0: 86.06%, 1: 13.94%
CHEMBL2114930 0: 4.68%, 1: 95.32%
CHEMBL2114931 0: 97.44%, 1: 2.56%
CHEMBL2114932 0: 4.35%, 1: 95.65%
CHEMBL2354206 0: 93.86%, 1: 6.14%
CHEMBL2354207 0: 15.46%, 1: 84.54%
CHEMBL2354217 0: 20.69%, 1: 79.31%
CHEMBL2354227 0: 21.70%, 1: 78.30%
CHEMBL2354228 0: 59.88%, 1: 40.12%
CHEMBL2354248 0: 96.55%, 1: 3.45%
CHEMBL2354256 0: 7.85%, 1: 92.15%
CHEMBL2354269 0: 9.42%, 1: 90.58%
CHEMBL2354274 0: 5.26%, 1: 94.74%
CHEMBL2354276 0: 75.30%, 1: 24.70%
CHEMBL2354289 0: 34.38%, 1: 65.62%
CHEMBL2354292 0: 77.91%, 1: 22.09%
CHEMBL2354303 0: 86.76%, 1: 13.24%
CHEMBL2354305 0: 14.37%, 1: 85.63%
CHEMBL2354308 0: 9.09%, 1: 90.91%
CHEMBL2378059 0: 2.84%, 1: 97.16%
CHEMBL2449559 0: 17.37%, 1: 82.63%
CHEMBL3214794 0: 64.60%, 1: 35.40%
CHEMBL3214801 0: 28.46%, 1: 71.54%
CHEMBL3214812 0: 33.19%, 1: 66.81%
CHEMBL3214816 0: 25.43%, 1: 74.57%
CHEMBL3214851 0: 77.02%, 1: 22.98%
CHEMBL3214906 0: 61.72%, 1: 38.28%
CHEMBL3214907 0: 48.12%, 1: 51.88%
CHEMBL3214929 0: 44.25%, 1: 55.75%
CHEMBL3214930 0: 79.15%, 1: 20.85%
CHEMBL3214944 0: 32.70%, 1: 67.30%
CHEMBL3214958 0: 69.31%, 1: 30.69%
CHEMBL3214959 0: 31.09%, 1: 68.91%
CHEMBL3214970 0: 20.09%, 1: 79.91%
CHEMBL3214992 0: 27.76%, 1: 72.24%
CHEMBL3214993 0: 32.48%, 1: 67.52%
CHEMBL3214997 0: 27.27%, 1: 72.73%
CHEMBL3215006 0: 73.18%, 1: 26.82%
CHEMBL3215013 0: 10.05%, 1: 89.95%
CHEMBL3215025 0: 7.49%, 1: 92.51%
CHEMBL3215034 0: 39.87%, 1: 60.13%
CHEMBL3215078 0: 41.84%, 1: 58.16%
CHEMBL3215092 0: 57.98%, 1: 42.02%
CHEMBL3215096 0: 30.30%, 1: 69.70%
CHEMBL3215112 0: 7.48%, 1: 92.52%
CHEMBL3215116 0: 83.87%, 1: 16.13%
CHEMBL3215128 0: 14.05%, 1: 85.95%
CHEMBL3215154 0: 44.44%, 1: 55.56%
CHEMBL3215157 0: 5.96%, 1: 94.04%
CHEMBL3215158 0: 12.59%, 1: 87.41%
CHEMBL3215171 0: 97.98%, 1: 2.02%
CHEMBL3215176 0: 91.72%, 1: 8.28%
CHEMBL3215185 0: 91.95%, 1: 8.05%
CHEMBL3215187 0: 7.98%, 1: 92.02%
CHEMBL3215216 0: 15.79%, 1: 84.21%
CHEMBL3215220 0: 91.18%, 1: 8.82%
CHEMBL3215227 0: 32.66%, 1: 67.34%
CHEMBL3215228 0: 88.71%, 1: 11.29%
CHEMBL3215276 0: 7.46%, 1: 92.54%
CHEMBL3215277 0: 90.53%, 1: 9.47%
CHEMBL3215288 0: 27.05%, 1: 72.95%
CHEMBL829401 0: 10.53%, 1: 89.47%
CHEMBL830839 0: 3.07%, 1: 96.93%
CHEMBL830842 0: 10.53%, 1: 89.47%
CHEMBL914418 0: 14.93%, 1: 85.07%
CHEMBL918058 0: 93.33%, 1: 6.67%
  0% 0/212 [00:00<?, ?it/s]100% 212/212 [00:00<00:00, 5351.78it/s]
Total scaffolds = 77 | train scaffolds = 66 | val scaffolds = 11 | test scaffolds = 0
/home/ec2-user/molecule-metalearning/chemprop/chemprop/data/scaffold.py:151: RuntimeWarning: Mean of empty slice
  target_avgs.append(np.nanmean(targets, axis=0))
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([0.75,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan]), array([12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/365 [00:00<?, ?it/s]100% 365/365 [00:00<00:00, 4017.45it/s]
Total scaffolds = 194 | train scaffolds = 133 | val scaffolds = 61 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([ 0, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/311 [00:00<?, ?it/s]100% 311/311 [00:00<00:00, 5451.75it/s]
Total scaffolds = 99 | train scaffolds = 64 | val scaffolds = 35 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan, 0.32608696,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0, 46,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([       nan,        nan, 0.33333333,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, 0.4, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([ 0,  0, 13,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([ 0,  0, 31,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/152 [00:00<?, ?it/s]100% 152/152 [00:00<00:00, 5764.90it/s]
Total scaffolds = 60 | train scaffolds = 48 | val scaffolds = 12 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan, 0.90566038, 0.78787879,
       0.94117647, 0.91304348, 0.95918367, 0.8974359 , 0.65853659,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0, 53, 33, 51, 46, 49, 39, 41,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 3, 3, 3, 3, 2, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  0.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1., nan,  1.,  1.,  1., nan,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([       nan,        nan,        nan, 0.66666667, 0.5       ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([0, 0, 0, 3, 2, 3, 2, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  0.,  1.,  1.,  1., nan,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, 1. , 1. , 1. , 1. , 1. , 0.6, 0.6, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 7, 4, 7, 6, 7, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/143 [00:00<?, ?it/s]100% 143/143 [00:00<00:00, 5752.73it/s]
Total scaffolds = 61 | train scaffolds = 42 | val scaffolds = 19 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan, 0.84848485, 0.5625    ,
       0.87179487, 0.83333333, 0.97297297, 0.96153846, 0.71428571,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0, 33, 48, 39, 36, 37, 26, 28,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan, nan,  0.,  1.,  0.,  1.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 2, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan,  1.,  1.,  1.,  0., nan,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  0.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan,  0.,  1., nan,  1., nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  1.,  0., nan,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([       nan,        nan,        nan, 1.        , 1.        ,
       1.        , 1.        , 1.        , 0.66666667, 1.        ,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([0, 0, 0, 4, 4, 4, 4, 4, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/177 [00:00<?, ?it/s]100% 177/177 [00:00<00:00, 5814.75it/s]
Total scaffolds = 64 | train scaffolds = 42 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan, 0.92156863, 0.66666667,
       0.92307692, 0.89285714, 0.98305085, 0.9047619 , 0.625     ,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0, 51, 39, 65, 56, 59, 42, 48,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan,  1.,  0.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  0., nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  1.,  0., nan,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  1., nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 2, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  1., nan,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, 1. , 0. , 1. , 0.5, 1. , 1. , 0. , nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 2, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/166 [00:00<?, ?it/s]100% 166/166 [00:00<00:00, 5784.86it/s]
Total scaffolds = 65 | train scaffolds = 48 | val scaffolds = 17 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan, 0.91304348, 0.72222222,
       0.92857143, 0.9       , 0.98113208, 0.95      , 0.67391304,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0, 46, 36, 56, 60, 53, 40, 46,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0., nan,  0.,  0., nan,  0.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  0.,  0.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1., nan,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  1., nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1., nan,  1.,  1.,  1., nan,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  1., nan,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/188 [00:00<?, ?it/s]100% 188/188 [00:00<00:00, 5761.87it/s]
Total scaffolds = 71 | train scaffolds = 49 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan, 0.95918367, 0.72972973,
       0.94915254, 0.96226415, 0.96969697, 0.93023256, 0.65306122,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0, 49, 37, 59, 53, 66, 43, 49,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1., nan,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  1., nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 2, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([       nan,        nan,        nan, 0.66666667, 0.5       ,
       1.        , 1.        , 1.        , 1.        , 1.        ,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([0, 0, 0, 3, 2, 3, 2, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  0.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/131 [00:00<?, ?it/s]100% 131/131 [00:00<00:00, 5713.48it/s]
Total scaffolds = 48 | train scaffolds = 26 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan, 0.97435897, 0.80769231,
       1.        , 0.975     , 0.97674419, 0.91304348, 0.80487805,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0, 39, 26, 42, 40, 43, 46, 41,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan,  0., nan, nan,  0.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 4, 4, 4, 4, 4, 4, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 2, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1., nan,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 3, 3, 3, 3, 2, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([       nan,        nan,        nan, 1.        , 1.        ,
       1.        , 1.        , 1.        , 0.6       , 0.33333333,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([0, 0, 0, 5, 3, 5, 4, 5, 5, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/138 [00:00<?, ?it/s]100% 138/138 [00:00<00:00, 5678.38it/s]
Total scaffolds = 51 | train scaffolds = 30 | val scaffolds = 21 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan, 0.97560976, 0.78571429,
       1.        , 0.95652174, 0.97959184, 0.95121951, 0.65384615,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0, 41, 28, 48, 46, 49, 41, 52,  0,  0,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan,  1., nan,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 2, 0, 2, 2, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan,  1.,  1.,  1.,  0., nan,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1., nan,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1., nan,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan,  0., nan, nan,  0.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1., nan,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/307 [00:00<?, ?it/s]100% 307/307 [00:00<00:00, 4364.64it/s]
Total scaffolds = 32 | train scaffolds = 21 | val scaffolds = 11 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
       0.63492063, 0.44444444,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 63, 45,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([     nan,      nan,      nan,      nan,      nan,      nan,
            nan,      nan,      nan,      nan, 0.8     , 0.640625,
            nan,      nan,      nan,      nan,      nan,      nan,
            nan,      nan]), array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 105,  64,   0,
         0,   0,   0,   0,   0,   0,   0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
       0.53846154, 0.47058824,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 39, 34,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan, 0.8 ,
       0.75,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 4, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  0.,  0., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,  1., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1., nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,  1., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 4, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  0., nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,  1., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/246 [00:00<?, ?it/s]100% 246/246 [00:00<00:00, 4389.19it/s]
Total scaffolds = 31 | train scaffolds = 15 | val scaffolds = 16 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
       0.53333333, 0.42      ,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 45, 50,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([     nan,      nan,      nan,      nan,      nan,      nan,
            nan,      nan,      nan,      nan, 0.703125, 0.625   ,
            nan,      nan,      nan,      nan,      nan,      nan,
            nan,      nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 64, 72,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
       0.47058824, 0.44736842,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 34, 38,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan, 0.75,
       0.6 ,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 5, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  0.,  0., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10, 10,  0,  0,  0,  0,  0,
        0,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,  1., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  0., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, 0. , 0.5, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  0.,  0., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,  1., nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/368 [00:00<?, ?it/s]100% 368/368 [00:00<00:00, 3802.47it/s]
Total scaffolds = 245 | train scaffolds = 196 | val scaffolds = 49 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/614 [00:00<?, ?it/s] 73% 448/614 [00:00<00:00, 4473.83it/s]100% 614/614 [00:00<00:00, 4468.05it/s]
Total scaffolds = 409 | train scaffolds = 339 | val scaffolds = 70 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]))]
  0% 0/199 [00:00<?, ?it/s]100% 199/199 [00:00<00:00, 6428.97it/s]
Total scaffolds = 98 | train scaffolds = 67 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.52173913,
              nan,        nan, 0.61764706,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 46,  0,  0,
       34,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.66666667,
       1.        ,        nan, 0.76      ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 33,  1,  0,
       25,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.33333333,
              nan,        nan, 1.        ,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
  0% 0/249 [00:00<?, ?it/s]100% 249/249 [00:00<00:00, 5822.37it/s]
Total scaffolds = 213 | train scaffolds = 168 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]))]
  0% 0/349 [00:00<?, ?it/s]100% 349/349 [00:00<00:00, 5155.90it/s]
Total scaffolds = 328 | train scaffolds = 261 | val scaffolds = 67 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]))]
  0% 0/181 [00:00<?, ?it/s]100% 181/181 [00:00<00:00, 6337.23it/s]
Total scaffolds = 94 | train scaffolds = 64 | val scaffolds = 30 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.64705882,
              nan,        nan, 0.56410256,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 34,  0,  0,
       39,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.84      ,
       1.        ,        nan, 0.65517241,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 25,  1,  0,
       29,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
  0% 0/156 [00:00<?, ?it/s]100% 156/156 [00:00<00:00, 5358.73it/s]
Total scaffolds = 139 | train scaffolds = 111 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  0., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]))]
  0% 0/218 [00:00<?, ?it/s]100% 218/218 [00:00<00:00, 4521.47it/s]
Total scaffolds = 183 | train scaffolds = 146 | val scaffolds = 37 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]))]
  0% 0/129 [00:00<?, ?it/s]100% 129/129 [00:00<00:00, 5525.25it/s]
Total scaffolds = 71 | train scaffolds = 46 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.86363636,
              nan,        nan, 0.81818182,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 22,  0,  0,
       22,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.89473684,
       1.        ,        nan, 0.85      ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 19,  1,  0,
       20,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
  0% 0/235 [00:00<?, ?it/s]100% 235/235 [00:00<00:00, 5858.42it/s]
Total scaffolds = 235 | train scaffolds = 188 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/270 [00:00<?, ?it/s]100% 270/270 [00:00<00:00, 6070.78it/s]
Total scaffolds = 215 | train scaffolds = 170 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/349 [00:00<?, ?it/s]100% 349/349 [00:00<00:00, 5069.37it/s]
Total scaffolds = 328 | train scaffolds = 261 | val scaffolds = 67 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]))]
  0% 0/1000 [00:00<?, ?it/s] 64% 637/1000 [00:00<00:00, 6369.81it/s]100% 1000/1000 [00:00<00:00, 6408.07it/s]
Total scaffolds = 626 | train scaffolds = 501 | val scaffolds = 125 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/179 [00:00<?, ?it/s]100% 179/179 [00:00<00:00, 4448.83it/s]
Total scaffolds = 153 | train scaffolds = 122 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/263 [00:00<?, ?it/s]100% 263/263 [00:00<00:00, 5670.08it/s]
Total scaffolds = 179 | train scaffolds = 142 | val scaffolds = 37 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/184 [00:00<?, ?it/s]100% 184/184 [00:00<00:00, 6293.18it/s]
Total scaffolds = 84 | train scaffolds = 56 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.85714286,
              nan,        nan, 0.77272727,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 21,  0,  0,
       22,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.90909091,
       1.        ,        nan, 0.9047619 ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 22,  1,  0,
       21,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
  0% 0/544 [00:00<?, ?it/s] 87% 473/544 [00:00<00:00, 4724.85it/s]100% 544/544 [00:00<00:00, 4647.41it/s]
Total scaffolds = 446 | train scaffolds = 358 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/132 [00:00<?, ?it/s]100% 132/132 [00:00<00:00, 6319.53it/s]
Total scaffolds = 67 | train scaffolds = 44 | val scaffolds = 23 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.84      ,
              nan,        nan, 0.83333333,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 25,  0,  0,
       24,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.95      ,
       1.        ,        nan, 0.85714286,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 20,  1,  0,
       21,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 1. , nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1]))]
  0% 0/268 [00:00<?, ?it/s]100% 268/268 [00:00<00:00, 4418.15it/s]
Total scaffolds = 228 | train scaffolds = 182 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/167 [00:00<?, ?it/s]100% 167/167 [00:00<00:00, 5461.67it/s]
Total scaffolds = 148 | train scaffolds = 115 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/130 [00:00<?, ?it/s]100% 130/130 [00:00<00:00, 5450.36it/s]
Total scaffolds = 74 | train scaffolds = 52 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.9 ,  nan,  nan, 0.75,  nan,  nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 20,  0,  0,
       20,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.94736842,
       1.        ,        nan, 0.85714286,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 19,  1,  0,
       21,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 1.        , 1.        , 1.        ,
              nan,        nan, 0.33333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 0, 0, 3, 0, 0]))]
  0% 0/131 [00:00<?, ?it/s]100% 131/131 [00:00<00:00, 4217.32it/s]
Total scaffolds = 31 | train scaffolds = 16 | val scaffolds = 15 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/136 [00:00<?, ?it/s]100% 136/136 [00:00<00:00, 3876.96it/s]
Total scaffolds = 53 | train scaffolds = 47 | val scaffolds = 6 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/305 [00:00<?, ?it/s]100% 305/305 [00:00<00:00, 4188.66it/s]
Total scaffolds = 262 | train scaffolds = 209 | val scaffolds = 53 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/400 [00:00<?, ?it/s]100% 400/400 [00:00<00:00, 5258.08it/s]
Total scaffolds = 280 | train scaffolds = 220 | val scaffolds = 60 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/132 [00:00<?, ?it/s]100% 132/132 [00:00<00:00, 4153.27it/s]
Total scaffolds = 119 | train scaffolds = 92 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/140 [00:00<?, ?it/s]100% 140/140 [00:00<00:00, 5622.55it/s]
Total scaffolds = 76 | train scaffolds = 50 | val scaffolds = 26 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan, 0.  , 0.8 ,  nan,  nan, 0.72,  nan,  nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 25,  0,  0,
       25,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 1.        ,
       1.        ,        nan, 0.86363636,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 21,  1,  0,
       22,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 1.        , 1.        , 1.        ,
              nan,        nan, 0.33333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
  0% 0/163 [00:00<?, ?it/s]100% 163/163 [00:00<00:00, 6357.13it/s]
Total scaffolds = 80 | train scaffolds = 52 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.73333333,
              nan,        nan, 0.67741935,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 30,  0,  0,
       31,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.80769231,
       1.        ,        nan, 0.73076923,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 26,  1,  0,
       26,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
  0% 0/158 [00:00<?, ?it/s]100% 158/158 [00:00<00:00, 5201.16it/s]
Total scaffolds = 147 | train scaffolds = 116 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/151 [00:00<?, ?it/s]100% 151/151 [00:00<00:00, 6082.96it/s]
Total scaffolds = 77 | train scaffolds = 50 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.71428571,
              nan,        nan, 0.65517241,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 28,  0,  0,
       29,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.95652174,
       1.        ,        nan, 0.86363636,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 23,  1,  0,
       22,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 1.        , 1.        , 1.        ,
              nan,        nan, 0.33333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
  0% 0/198 [00:00<?, ?it/s]100% 198/198 [00:00<00:00, 5996.93it/s]
Total scaffolds = 161 | train scaffolds = 128 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/136 [00:00<?, ?it/s]100% 136/136 [00:00<00:00, 3944.52it/s]
Total scaffolds = 53 | train scaffolds = 47 | val scaffolds = 6 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/157 [00:00<?, ?it/s]100% 157/157 [00:00<00:00, 5813.18it/s]
Total scaffolds = 82 | train scaffolds = 54 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.75      ,
              nan,        nan, 0.67857143,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 28,  0,  0,
       28,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.80769231,
       1.        ,        nan, 0.73076923,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 26,  1,  0,
       26,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
  0% 0/226 [00:00<?, ?it/s]100% 226/226 [00:00<00:00, 5166.10it/s]
Total scaffolds = 208 | train scaffolds = 164 | val scaffolds = 44 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/207 [00:00<?, ?it/s]100% 207/207 [00:00<00:00, 5039.12it/s]
Total scaffolds = 160 | train scaffolds = 123 | val scaffolds = 37 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/401 [00:00<?, ?it/s]100% 401/401 [00:00<00:00, 5768.50it/s]
Total scaffolds = 257 | train scaffolds = 189 | val scaffolds = 68 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/349 [00:00<?, ?it/s]100% 349/349 [00:00<00:00, 5026.02it/s]
Total scaffolds = 328 | train scaffolds = 261 | val scaffolds = 67 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]))]
  0% 0/982 [00:00<?, ?it/s] 51% 497/982 [00:00<00:00, 4962.85it/s]100% 982/982 [00:00<00:00, 4958.67it/s]
Total scaffolds = 744 | train scaffolds = 602 | val scaffolds = 142 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/270 [00:00<?, ?it/s]100% 270/270 [00:00<00:00, 6015.67it/s]
Total scaffolds = 215 | train scaffolds = 170 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/546 [00:00<?, ?it/s] 86% 470/546 [00:00<00:00, 4693.16it/s]100% 546/546 [00:00<00:00, 4730.63it/s]
Total scaffolds = 448 | train scaffolds = 357 | val scaffolds = 91 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/190 [00:00<?, ?it/s]100% 190/190 [00:00<00:00, 4918.79it/s]
Total scaffolds = 145 | train scaffolds = 110 | val scaffolds = 35 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/140 [00:00<?, ?it/s]100% 140/140 [00:00<00:00, 5061.79it/s]
Total scaffolds = 127 | train scaffolds = 99 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/164 [00:00<?, ?it/s]100% 164/164 [00:00<00:00, 6101.67it/s]
Total scaffolds = 85 | train scaffolds = 59 | val scaffolds = 26 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.63636364,
              nan,        nan, 0.625     ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 33,  0,  0,
       32,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.95652174,
       1.        ,        nan, 0.86363636,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 23,  1,  0,
       22,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0]))]
  0% 0/153 [00:00<?, ?it/s]100% 153/153 [00:00<00:00, 5911.06it/s]
Total scaffolds = 77 | train scaffolds = 53 | val scaffolds = 24 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.74193548,
              nan,        nan, 0.68965517,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 31,  0,  0,
       29,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.91304348,
       1.        ,        nan, 0.86363636,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 23,  1,  0,
       22,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
  0% 0/685 [00:00<?, ?it/s] 87% 599/685 [00:00<00:00, 5983.10it/s]100% 685/685 [00:00<00:00, 5971.12it/s]
Total scaffolds = 287 | train scaffolds = 232 | val scaffolds = 55 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/175 [00:00<?, ?it/s]100% 175/175 [00:00<00:00, 4332.45it/s]
Total scaffolds = 148 | train scaffolds = 120 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/152 [00:00<?, ?it/s]100% 152/152 [00:00<00:00, 4626.05it/s]
Total scaffolds = 142 | train scaffolds = 115 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/308 [00:00<?, ?it/s]100% 308/308 [00:00<00:00, 4847.92it/s]
Total scaffolds = 224 | train scaffolds = 175 | val scaffolds = 49 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/139 [00:00<?, ?it/s]100% 139/139 [00:00<00:00, 5856.20it/s]
Total scaffolds = 75 | train scaffolds = 50 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.72,  nan,  nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 25,  0,  0,
       25,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.86956522,
       1.        ,        nan, 0.7826087 ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 23,  1,  0,
       23,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 1.        , 1.        , 1.        ,
              nan,        nan, 0.33333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
  0% 0/347 [00:00<?, ?it/s]100% 347/347 [00:00<00:00, 5360.68it/s]
Total scaffolds = 305 | train scaffolds = 240 | val scaffolds = 65 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/203 [00:00<?, ?it/s]100% 203/203 [00:00<00:00, 5263.20it/s]
Total scaffolds = 113 | train scaffolds = 88 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       0.5, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/160 [00:00<?, ?it/s]100% 160/160 [00:00<00:00, 6187.89it/s]
Total scaffolds = 80 | train scaffolds = 57 | val scaffolds = 23 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.71428571,
              nan,        nan, 0.67857143,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 28,  0,  0,
       28,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.86956522,
       1.        ,        nan, 0.75      ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 23,  1,  0,
       24,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 1.        , 1.        , 1.        ,
              nan,        nan, 0.33333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]))]
  0% 0/183 [00:00<?, ?it/s]100% 183/183 [00:00<00:00, 6440.05it/s]
Total scaffolds = 91 | train scaffolds = 59 | val scaffolds = 32 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.63888889,
              nan,        nan, 0.61764706,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 36,  0,  0,
       34,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.78571429,
       1.        ,        nan, 0.73076923,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 28,  1,  0,
       26,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 1. , nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
  0% 0/400 [00:00<?, ?it/s]100% 400/400 [00:00<00:00, 5257.27it/s]
Total scaffolds = 280 | train scaffolds = 220 | val scaffolds = 60 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/297 [00:00<?, ?it/s]100% 297/297 [00:00<00:00, 4766.18it/s]
Total scaffolds = 273 | train scaffolds = 222 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  0., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]))]
  0% 0/327 [00:00<?, ?it/s]100% 327/327 [00:00<00:00, 4279.98it/s]
Total scaffolds = 283 | train scaffolds = 223 | val scaffolds = 60 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/152 [00:00<?, ?it/s]100% 152/152 [00:00<00:00, 4548.94it/s]
Total scaffolds = 142 | train scaffolds = 115 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/367 [00:00<?, ?it/s]100% 367/367 [00:00<00:00, 4872.84it/s]
Total scaffolds = 298 | train scaffolds = 247 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/385 [00:00<?, ?it/s]100% 385/385 [00:00<00:00, 5632.14it/s]
Total scaffolds = 251 | train scaffolds = 204 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/164 [00:00<?, ?it/s]100% 164/164 [00:00<00:00, 6284.58it/s]
Total scaffolds = 81 | train scaffolds = 53 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.67647059,
              nan,        nan, 0.61764706,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 34,  0,  0,
       34,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.80769231,
       1.        ,        nan, 0.73076923,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 26,  1,  0,
       26,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0]))]
  0% 0/143 [00:00<?, ?it/s]100% 143/143 [00:00<00:00, 4959.45it/s]
Total scaffolds = 122 | train scaffolds = 97 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/988 [00:00<?, ?it/s] 49% 485/988 [00:00<00:00, 4848.74it/s]100% 988/988 [00:00<00:00, 4961.97it/s]
Total scaffolds = 752 | train scaffolds = 591 | val scaffolds = 161 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/883 [00:00<?, ?it/s] 52% 456/883 [00:00<00:00, 4553.43it/s]100% 883/883 [00:00<00:00, 4581.66it/s]
Total scaffolds = 656 | train scaffolds = 506 | val scaffolds = 150 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/235 [00:00<?, ?it/s]100% 235/235 [00:00<00:00, 5760.94it/s]
Total scaffolds = 235 | train scaffolds = 188 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/546 [00:00<?, ?it/s] 99% 542/546 [00:00<00:00, 5405.62it/s]100% 546/546 [00:00<00:00, 5368.46it/s]
Total scaffolds = 365 | train scaffolds = 297 | val scaffolds = 68 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 2])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/693 [00:00<?, ?it/s] 77% 537/693 [00:00<00:00, 5369.33it/s]100% 693/693 [00:00<00:00, 5259.05it/s]
Total scaffolds = 506 | train scaffolds = 408 | val scaffolds = 98 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/386 [00:00<?, ?it/s]100% 386/386 [00:00<00:00, 4849.57it/s]
Total scaffolds = 342 | train scaffolds = 270 | val scaffolds = 72 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/133 [00:00<?, ?it/s]100% 133/133 [00:00<00:00, 5055.85it/s]
Total scaffolds = 120 | train scaffolds = 97 | val scaffolds = 23 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/439 [00:00<?, ?it/s]100% 439/439 [00:00<00:00, 4838.99it/s]
Total scaffolds = 404 | train scaffolds = 318 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/163 [00:00<?, ?it/s]100% 163/163 [00:00<00:00, 6651.08it/s]
Total scaffolds = 78 | train scaffolds = 52 | val scaffolds = 26 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.79166667,
              nan,        nan, 0.73913043,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 24,  0,  0,
       23,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , 1. , nan, 0.9, nan, nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 20,  1,  0,
       20,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 1.        , 1.        , 1.        ,
              nan,        nan, 0.33333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0]))]
  0% 0/169 [00:00<?, ?it/s]100% 169/169 [00:00<00:00, 6044.64it/s]
Total scaffolds = 87 | train scaffolds = 58 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.75      ,
              nan,        nan, 0.67857143,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 28,  0,  0,
       28,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.79166667,
       1.        ,        nan, 0.75      ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 24,  1,  0,
       24,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
  0% 0/161 [00:00<?, ?it/s]100% 161/161 [00:00<00:00, 6942.07it/s]
Total scaffolds = 81 | train scaffolds = 50 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.9       ,
              nan,        nan, 0.77777778,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10,  0,  0,
        9,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 11,  0,  0,
       11,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/128 [00:00<?, ?it/s]100% 128/128 [00:00<00:00, 4822.08it/s]
Total scaffolds = 115 | train scaffolds = 90 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]))]
  0% 0/139 [00:00<?, ?it/s]100% 139/139 [00:00<00:00, 4910.04it/s]
Total scaffolds = 109 | train scaffolds = 87 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/327 [00:00<?, ?it/s]100% 327/327 [00:00<00:00, 4700.56it/s]
Total scaffolds = 276 | train scaffolds = 220 | val scaffolds = 56 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/890 [00:00<?, ?it/s] 60% 532/890 [00:00<00:00, 5314.95it/s]100% 890/890 [00:00<00:00, 5273.10it/s]
Total scaffolds = 685 | train scaffolds = 555 | val scaffolds = 130 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/191 [00:00<?, ?it/s]100% 191/191 [00:00<00:00, 5991.91it/s]
Total scaffolds = 159 | train scaffolds = 127 | val scaffolds = 32 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/192 [00:00<?, ?it/s]100% 192/192 [00:00<00:00, 6127.82it/s]
Total scaffolds = 147 | train scaffolds = 126 | val scaffolds = 21 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/545 [00:00<?, ?it/s] 86% 466/545 [00:00<00:00, 4656.67it/s]100% 545/545 [00:00<00:00, 4700.41it/s]
Total scaffolds = 447 | train scaffolds = 355 | val scaffolds = 92 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/152 [00:00<?, ?it/s]100% 152/152 [00:00<00:00, 4610.03it/s]
Total scaffolds = 142 | train scaffolds = 115 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/342 [00:00<?, ?it/s]100% 342/342 [00:00<00:00, 5057.40it/s]
Total scaffolds = 270 | train scaffolds = 213 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/244 [00:00<?, ?it/s]100% 244/244 [00:00<00:00, 4230.28it/s]
Total scaffolds = 208 | train scaffolds = 165 | val scaffolds = 43 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/131 [00:00<?, ?it/s]100% 131/131 [00:00<00:00, 5934.72it/s]
Total scaffolds = 80 | train scaffolds = 57 | val scaffolds = 23 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/233 [00:00<?, ?it/s]100% 233/233 [00:00<00:00, 5227.06it/s]
Total scaffolds = 228 | train scaffolds = 182 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/367 [00:00<?, ?it/s]100% 367/367 [00:00<00:00, 4809.02it/s]
Total scaffolds = 298 | train scaffolds = 247 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/188 [00:00<?, ?it/s]100% 188/188 [00:00<00:00, 5221.94it/s]
Total scaffolds = 146 | train scaffolds = 113 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/173 [00:00<?, ?it/s]100% 173/173 [00:00<00:00, 5337.52it/s]
Total scaffolds = 63 | train scaffolds = 47 | val scaffolds = 16 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/321 [00:00<?, ?it/s]100% 321/321 [00:00<00:00, 4664.36it/s]
Total scaffolds = 272 | train scaffolds = 218 | val scaffolds = 54 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/176 [00:00<?, ?it/s]100% 176/176 [00:00<00:00, 4417.70it/s]
Total scaffolds = 163 | train scaffolds = 129 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/386 [00:00<?, ?it/s]100% 386/386 [00:00<00:00, 4765.81it/s]
Total scaffolds = 342 | train scaffolds = 270 | val scaffolds = 72 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/128 [00:00<?, ?it/s]100% 128/128 [00:00<00:00, 4920.59it/s]
Total scaffolds = 120 | train scaffolds = 95 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/268 [00:00<?, ?it/s]100% 268/268 [00:00<00:00, 4325.82it/s]
Total scaffolds = 228 | train scaffolds = 182 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/145 [00:00<?, ?it/s]100% 145/145 [00:00<00:00, 5357.65it/s]
Total scaffolds = 86 | train scaffolds = 65 | val scaffolds = 21 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.73913043,
              nan,        nan, 0.75      ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 23,  0,  0,
       20,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 1.        ,
       1.        ,        nan, 0.92857143,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 14,  1,  0,
       14,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
  0% 0/254 [00:00<?, ?it/s]100% 254/254 [00:00<00:00, 5184.50it/s]
Total scaffolds = 224 | train scaffolds = 179 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/164 [00:00<?, ?it/s]100% 164/164 [00:00<00:00, 6247.42it/s]
Total scaffolds = 83 | train scaffolds = 59 | val scaffolds = 24 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([    nan,     nan,     nan,     nan,     nan,     nan,     nan,
           nan,     nan,     nan,     nan,     nan,     nan,     nan,
       0.6875 ,     nan,     nan, 0.65625,     nan,     nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 32,  0,  0,
       32,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.91666667,
       1.        ,        nan, 0.76      ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 24,  1,  0,
       25,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
  0% 0/141 [00:00<?, ?it/s]100% 141/141 [00:00<00:00, 5319.61it/s]
Total scaffolds = 37 | train scaffolds = 24 | val scaffolds = 13 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/342 [00:00<?, ?it/s]100% 342/342 [00:00<00:00, 4989.16it/s]
Total scaffolds = 270 | train scaffolds = 213 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/210 [00:00<?, ?it/s]100% 210/210 [00:00<00:00, 4964.93it/s]
Total scaffolds = 182 | train scaffolds = 146 | val scaffolds = 36 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/160 [00:00<?, ?it/s]100% 160/160 [00:00<00:00, 6152.88it/s]
Total scaffolds = 84 | train scaffolds = 58 | val scaffolds = 26 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.76923077,
              nan,        nan, 0.7037037 ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 26,  0,  0,
       27,  0,  0])), (array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.84, 1.  ,  nan, 0.76,  nan,  nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 25,  1,  0,
       25,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1]))]
  0% 0/212 [00:00<?, ?it/s]100% 212/212 [00:00<00:00, 5150.05it/s]
Total scaffolds = 156 | train scaffolds = 119 | val scaffolds = 37 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/270 [00:00<?, ?it/s]100% 270/270 [00:00<00:00, 5975.52it/s]
Total scaffolds = 215 | train scaffolds = 170 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/529 [00:00<?, ?it/s]100% 529/529 [00:00<00:00, 5341.34it/s]
Total scaffolds = 426 | train scaffolds = 335 | val scaffolds = 91 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/170 [00:00<?, ?it/s]100% 170/170 [00:00<00:00, 7384.49it/s]
Total scaffolds = 73 | train scaffolds = 44 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.85714286,
       1.        ,        nan, 0.92307692,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 14,  1,  0,
       13,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.76470588,
              nan,        nan, 0.82352941,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 17,  0,  0,
       17,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
  0% 0/209 [00:00<?, ?it/s]100% 209/209 [00:00<00:00, 4691.19it/s]
Total scaffolds = 148 | train scaffolds = 111 | val scaffolds = 37 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , nan, nan, nan, nan, 0.5]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/400 [00:00<?, ?it/s]100% 400/400 [00:00<00:00, 5203.59it/s]
Total scaffolds = 280 | train scaffolds = 220 | val scaffolds = 60 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/1012 [00:00<?, ?it/s] 46% 465/1012 [00:00<00:00, 4640.73it/s] 90% 914/1012 [00:00<00:00, 4593.55it/s]100% 1012/1012 [00:00<00:00, 4486.01it/s]
Total scaffolds = 644 | train scaffolds = 506 | val scaffolds = 138 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/436 [00:00<?, ?it/s]100% 436/436 [00:00<00:00, 4359.29it/s]
Total scaffolds = 346 | train scaffolds = 268 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/478 [00:00<?, ?it/s]100% 478/478 [00:00<00:00, 5186.34it/s]
Total scaffolds = 206 | train scaffolds = 159 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/144 [00:00<?, ?it/s]100% 144/144 [00:00<00:00, 5654.55it/s]
Total scaffolds = 75 | train scaffolds = 50 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.77419355,
              nan,        nan, 0.72413793,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 31,  0,  0,
       29,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 1.        ,
       1.        ,        nan, 0.86363636,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 21,  1,  0,
       22,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
  0% 0/128 [00:00<?, ?it/s]100% 128/128 [00:00<00:00, 5223.34it/s]
Total scaffolds = 98 | train scaffolds = 76 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/210 [00:00<?, ?it/s]100% 210/210 [00:00<00:00, 5061.92it/s]
Total scaffolds = 154 | train scaffolds = 123 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/233 [00:00<?, ?it/s]100% 233/233 [00:00<00:00, 5738.87it/s]
Total scaffolds = 233 | train scaffolds = 186 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/148 [00:00<?, ?it/s]100% 148/148 [00:00<00:00, 5895.74it/s]
Total scaffolds = 77 | train scaffolds = 51 | val scaffolds = 26 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.76666667,
              nan,        nan, 0.7       ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 30,  0,  0,
       30,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.91304348,
       1.        ,        nan, 0.82608696,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 23,  1,  0,
       23,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
  0% 0/895 [00:00<?, ?it/s] 55% 490/895 [00:00<00:00, 4894.17it/s]100% 895/895 [00:00<00:00, 4876.15it/s]
Total scaffolds = 693 | train scaffolds = 558 | val scaffolds = 135 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/134 [00:00<?, ?it/s]100% 134/134 [00:00<00:00, 4582.00it/s]
Total scaffolds = 125 | train scaffolds = 98 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/410 [00:00<?, ?it/s]100% 410/410 [00:00<00:00, 5132.32it/s]
Total scaffolds = 177 | train scaffolds = 138 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/209 [00:00<?, ?it/s]100% 209/209 [00:00<00:00, 5036.80it/s]
Total scaffolds = 154 | train scaffolds = 123 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/129 [00:00<?, ?it/s]100% 129/129 [00:00<00:00, 4722.16it/s]
Total scaffolds = 112 | train scaffolds = 90 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/254 [00:00<?, ?it/s]100% 254/254 [00:00<00:00, 5136.04it/s]
Total scaffolds = 224 | train scaffolds = 179 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/204 [00:00<?, ?it/s]100% 204/204 [00:00<00:00, 3673.46it/s]
Total scaffolds = 109 | train scaffolds = 76 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/188 [00:00<?, ?it/s]100% 188/188 [00:00<00:00, 5077.82it/s]
Total scaffolds = 146 | train scaffolds = 113 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/585 [00:00<?, ?it/s]100% 585/585 [00:00<00:00, 5909.75it/s]
Total scaffolds = 354 | train scaffolds = 262 | val scaffolds = 92 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 1.        ,
              nan,        nan, 1.        ,        nan, 0.33333333]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 2, 0, 3])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/1009 [00:00<?, ?it/s] 49% 493/1009 [00:00<00:00, 4921.31it/s] 98% 989/1009 [00:00<00:00, 4932.74it/s]100% 1009/1009 [00:00<00:00, 4929.17it/s]
Total scaffolds = 761 | train scaffolds = 597 | val scaffolds = 164 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/154 [00:00<?, ?it/s]100% 154/154 [00:00<00:00, 5664.25it/s]
Total scaffolds = 82 | train scaffolds = 57 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.73333333,
              nan,        nan, 0.67741935,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 30,  0,  0,
       31,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.90909091,
       1.        ,        nan, 0.7826087 ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 22,  1,  0,
       23,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan, 1.        , 1.        , 1.        ,
              nan,        nan, 0.33333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
  0% 0/462 [00:00<?, ?it/s]100% 462/462 [00:00<00:00, 5590.32it/s]
Total scaffolds = 344 | train scaffolds = 272 | val scaffolds = 72 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/156 [00:00<?, ?it/s]100% 156/156 [00:00<00:00, 5292.33it/s]
Total scaffolds = 139 | train scaffolds = 111 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  0., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]))]
  0% 0/362 [00:00<?, ?it/s]100% 362/362 [00:00<00:00, 5693.14it/s]
Total scaffolds = 297 | train scaffolds = 232 | val scaffolds = 65 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/172 [00:00<?, ?it/s]100% 172/172 [00:00<00:00, 4318.36it/s]
Total scaffolds = 149 | train scaffolds = 124 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/892 [00:00<?, ?it/s] 54% 483/892 [00:00<00:00, 4827.13it/s]100% 892/892 [00:00<00:00, 4763.23it/s]
Total scaffolds = 726 | train scaffolds = 579 | val scaffolds = 147 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/133 [00:00<?, ?it/s]100% 133/133 [00:00<00:00, 4466.38it/s]
Total scaffolds = 103 | train scaffolds = 79 | val scaffolds = 24 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/694 [00:00<?, ?it/s] 75% 521/694 [00:00<00:00, 5204.58it/s]100% 694/694 [00:00<00:00, 5173.16it/s]
Total scaffolds = 577 | train scaffolds = 469 | val scaffolds = 108 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/193 [00:00<?, ?it/s]100% 193/193 [00:00<00:00, 6913.61it/s]
Total scaffolds = 70 | train scaffolds = 48 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/631 [00:00<?, ?it/s] 65% 408/631 [00:00<00:00, 4078.75it/s]100% 631/631 [00:00<00:00, 3990.34it/s]
Total scaffolds = 526 | train scaffolds = 417 | val scaffolds = 109 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/170 [00:00<?, ?it/s]100% 170/170 [00:00<00:00, 4449.30it/s]
Total scaffolds = 151 | train scaffolds = 120 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/210 [00:00<?, ?it/s]100% 210/210 [00:00<00:00, 4209.10it/s]
Total scaffolds = 176 | train scaffolds = 136 | val scaffolds = 40 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/694 [00:00<?, ?it/s] 65% 449/694 [00:00<00:00, 4481.24it/s]100% 694/694 [00:00<00:00, 4468.20it/s]
Total scaffolds = 522 | train scaffolds = 416 | val scaffolds = 106 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/283 [00:00<?, ?it/s]100% 283/283 [00:00<00:00, 4621.04it/s]
Total scaffolds = 148 | train scaffolds = 120 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/191 [00:00<?, ?it/s]100% 191/191 [00:00<00:00, 6909.30it/s]
Total scaffolds = 57 | train scaffolds = 29 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/586 [00:00<?, ?it/s] 93% 545/586 [00:00<00:00, 5438.13it/s]100% 586/586 [00:00<00:00, 5419.86it/s]
Total scaffolds = 418 | train scaffolds = 332 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
  0% 0/894 [00:00<?, ?it/s] 49% 442/894 [00:00<00:00, 4416.79it/s] 98% 874/894 [00:00<00:00, 4384.22it/s]100% 894/894 [00:00<00:00, 4367.30it/s]
Total scaffolds = 717 | train scaffolds = 571 | val scaffolds = 146 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/674 [00:00<?, ?it/s] 75% 503/674 [00:00<00:00, 5024.42it/s]100% 674/674 [00:00<00:00, 4889.89it/s]
Total scaffolds = 502 | train scaffolds = 404 | val scaffolds = 98 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/137 [00:00<?, ?it/s]100% 137/137 [00:00<00:00, 4836.87it/s]
Total scaffolds = 113 | train scaffolds = 89 | val scaffolds = 24 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/165 [00:00<?, ?it/s]100% 165/165 [00:00<00:00, 5071.00it/s]
Total scaffolds = 142 | train scaffolds = 112 | val scaffolds = 30 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/179 [00:00<?, ?it/s]100% 179/179 [00:00<00:00, 7593.84it/s]
Total scaffolds = 130 | train scaffolds = 103 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/284 [00:00<?, ?it/s]100% 284/284 [00:00<00:00, 4664.99it/s]
Total scaffolds = 148 | train scaffolds = 120 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/132 [00:00<?, ?it/s]100% 132/132 [00:00<00:00, 3946.43it/s]
Total scaffolds = 103 | train scaffolds = 84 | val scaffolds = 19 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/168 [00:00<?, ?it/s]100% 168/168 [00:00<00:00, 4901.90it/s]
Total scaffolds = 134 | train scaffolds = 107 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/232 [00:00<?, ?it/s]100% 232/232 [00:00<00:00, 4679.45it/s]
Total scaffolds = 213 | train scaffolds = 168 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/140 [00:00<?, ?it/s]100% 140/140 [00:00<00:00, 5015.14it/s]
Total scaffolds = 127 | train scaffolds = 99 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/215 [00:00<?, ?it/s]100% 215/215 [00:00<00:00, 5208.78it/s]
Total scaffolds = 151 | train scaffolds = 118 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/174 [00:00<?, ?it/s]100% 174/174 [00:00<00:00, 6867.24it/s]
Total scaffolds = 63 | train scaffolds = 41 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/282 [00:00<?, ?it/s]100% 282/282 [00:00<00:00, 5104.43it/s]
Total scaffolds = 223 | train scaffolds = 177 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/323 [00:00<?, ?it/s] 93% 300/323 [00:00<00:00, 2964.92it/s]100% 323/323 [00:00<00:00, 3052.78it/s]
Total scaffolds = 227 | train scaffolds = 177 | val scaffolds = 50 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/468 [00:00<?, ?it/s]100% 466/468 [00:00<00:00, 4657.22it/s]100% 468/468 [00:00<00:00, 4635.71it/s]
Total scaffolds = 354 | train scaffolds = 282 | val scaffolds = 72 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/168 [00:00<?, ?it/s]100% 168/168 [00:00<00:00, 4826.85it/s]
Total scaffolds = 134 | train scaffolds = 107 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/586 [00:00<?, ?it/s] 91% 534/586 [00:00<00:00, 5339.20it/s]100% 586/586 [00:00<00:00, 5317.24it/s]
Total scaffolds = 418 | train scaffolds = 332 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
  0% 0/631 [00:00<?, ?it/s] 64% 402/631 [00:00<00:00, 4007.16it/s]100% 631/631 [00:00<00:00, 3904.73it/s]
Total scaffolds = 526 | train scaffolds = 417 | val scaffolds = 109 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/300 [00:00<?, ?it/s]100% 300/300 [00:00<00:00, 4440.33it/s]
Total scaffolds = 265 | train scaffolds = 214 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/218 [00:00<?, ?it/s]100% 218/218 [00:00<00:00, 4461.31it/s]
Total scaffolds = 155 | train scaffolds = 125 | val scaffolds = 30 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/223 [00:00<?, ?it/s]100% 223/223 [00:00<00:00, 5233.14it/s]
Total scaffolds = 169 | train scaffolds = 131 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/140 [00:00<?, ?it/s]100% 140/140 [00:00<00:00, 4808.84it/s]
Total scaffolds = 120 | train scaffolds = 98 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/694 [00:00<?, ?it/s] 65% 452/694 [00:00<00:00, 4515.75it/s]100% 694/694 [00:00<00:00, 4475.92it/s]
Total scaffolds = 522 | train scaffolds = 416 | val scaffolds = 106 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/244 [00:00<?, ?it/s]100% 244/244 [00:00<00:00, 5134.79it/s]
Total scaffolds = 235 | train scaffolds = 187 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/468 [00:00<?, ?it/s]100% 466/468 [00:00<00:00, 4653.94it/s]100% 468/468 [00:00<00:00, 4628.50it/s]
Total scaffolds = 354 | train scaffolds = 282 | val scaffolds = 72 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/140 [00:00<?, ?it/s]100% 140/140 [00:00<00:00, 4935.06it/s]
Total scaffolds = 127 | train scaffolds = 99 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/358 [00:00<?, ?it/s]100% 358/358 [00:00<00:00, 5083.59it/s]
Total scaffolds = 236 | train scaffolds = 181 | val scaffolds = 55 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 1.        ,
              nan,        nan,        nan,        nan, 0.33333333]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/534 [00:00<?, ?it/s] 94% 502/534 [00:00<00:00, 5019.07it/s]100% 534/534 [00:00<00:00, 5043.83it/s]
Total scaffolds = 464 | train scaffolds = 373 | val scaffolds = 91 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/459 [00:00<?, ?it/s]100% 459/459 [00:00<00:00, 6246.38it/s]
Total scaffolds = 293 | train scaffolds = 238 | val scaffolds = 55 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/704 [00:00<?, ?it/s] 85% 601/704 [00:00<00:00, 6009.23it/s]100% 704/704 [00:00<00:00, 5952.53it/s]
Total scaffolds = 526 | train scaffolds = 418 | val scaffolds = 108 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/244 [00:00<?, ?it/s]100% 244/244 [00:00<00:00, 5224.63it/s]
Total scaffolds = 235 | train scaffolds = 187 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/210 [00:00<?, ?it/s]100% 210/210 [00:00<00:00, 4658.51it/s]
Total scaffolds = 163 | train scaffolds = 127 | val scaffolds = 36 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/649 [00:00<?, ?it/s] 85% 553/649 [00:00<00:00, 5519.20it/s]100% 649/649 [00:00<00:00, 5551.06it/s]
Total scaffolds = 545 | train scaffolds = 441 | val scaffolds = 104 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/184 [00:00<?, ?it/s]100% 184/184 [00:00<00:00, 4279.97it/s]
Total scaffolds = 156 | train scaffolds = 126 | val scaffolds = 30 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/145 [00:00<?, ?it/s]100% 145/145 [00:00<00:00, 5291.74it/s]
Total scaffolds = 118 | train scaffolds = 98 | val scaffolds = 20 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/247 [00:00<?, ?it/s]100% 247/247 [00:00<00:00, 4572.87it/s]
Total scaffolds = 218 | train scaffolds = 175 | val scaffolds = 43 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/323 [00:00<?, ?it/s] 93% 299/323 [00:00<00:00, 2974.29it/s]100% 323/323 [00:00<00:00, 3031.81it/s]
Total scaffolds = 227 | train scaffolds = 177 | val scaffolds = 50 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/694 [00:00<?, ?it/s] 65% 451/694 [00:00<00:00, 4500.83it/s]100% 694/694 [00:00<00:00, 4473.16it/s]
Total scaffolds = 522 | train scaffolds = 416 | val scaffolds = 106 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/704 [00:00<?, ?it/s] 84% 590/704 [00:00<00:00, 5895.28it/s]100% 704/704 [00:00<00:00, 5863.48it/s]
Total scaffolds = 526 | train scaffolds = 418 | val scaffolds = 108 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/262 [00:00<?, ?it/s]100% 262/262 [00:00<00:00, 5980.65it/s]
Total scaffolds = 150 | train scaffolds = 121 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/704 [00:00<?, ?it/s] 84% 591/704 [00:00<00:00, 5900.34it/s]100% 704/704 [00:00<00:00, 5802.34it/s]
Total scaffolds = 526 | train scaffolds = 418 | val scaffolds = 108 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/465 [00:00<?, ?it/s]100% 465/465 [00:00<00:00, 6315.66it/s]
Total scaffolds = 196 | train scaffolds = 170 | val scaffolds = 26 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/295 [00:00<?, ?it/s]100% 295/295 [00:00<00:00, 5806.20it/s]
Total scaffolds = 260 | train scaffolds = 209 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/674 [00:00<?, ?it/s] 74% 497/674 [00:00<00:00, 4967.00it/s]100% 674/674 [00:00<00:00, 4790.78it/s]
Total scaffolds = 502 | train scaffolds = 404 | val scaffolds = 98 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/694 [00:00<?, ?it/s] 65% 449/694 [00:00<00:00, 4485.75it/s]100% 694/694 [00:00<00:00, 4439.80it/s]
Total scaffolds = 522 | train scaffolds = 416 | val scaffolds = 106 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/131 [00:00<?, ?it/s]100% 131/131 [00:00<00:00, 4647.96it/s]
Total scaffolds = 80 | train scaffolds = 65 | val scaffolds = 15 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/704 [00:00<?, ?it/s] 82% 580/704 [00:00<00:00, 5795.79it/s]100% 704/704 [00:00<00:00, 5686.11it/s]
Total scaffolds = 526 | train scaffolds = 418 | val scaffolds = 108 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/215 [00:00<?, ?it/s]100% 215/215 [00:00<00:00, 4402.94it/s]
Total scaffolds = 152 | train scaffolds = 124 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/595 [00:00<?, ?it/s] 91% 544/595 [00:00<00:00, 5439.81it/s]100% 595/595 [00:00<00:00, 5419.64it/s]
Total scaffolds = 452 | train scaffolds = 366 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/168 [00:00<?, ?it/s]100% 168/168 [00:00<00:00, 4833.04it/s]
Total scaffolds = 134 | train scaffolds = 107 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/178 [00:00<?, ?it/s]100% 178/178 [00:00<00:00, 4180.24it/s]
Total scaffolds = 131 | train scaffolds = 102 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/309 [00:00<?, ?it/s]100% 309/309 [00:00<00:00, 5004.19it/s]
Total scaffolds = 219 | train scaffolds = 164 | val scaffolds = 55 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 1.        ,
              nan,        nan, 1.        ,        nan, 0.33333333]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 3])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/448 [00:00<?, ?it/s]100% 448/448 [00:00<00:00, 5031.15it/s]
Total scaffolds = 367 | train scaffolds = 299 | val scaffolds = 68 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  0., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  0., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  0., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/285 [00:00<?, ?it/s]100% 285/285 [00:00<00:00, 4621.17it/s]
Total scaffolds = 148 | train scaffolds = 120 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/148 [00:00<?, ?it/s]100% 148/148 [00:00<00:00, 5070.76it/s]
Total scaffolds = 132 | train scaffolds = 105 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/940 [00:00<?, ?it/s] 50% 473/940 [00:00<00:00, 4728.07it/s]100% 940/940 [00:00<00:00, 4722.16it/s]
Total scaffolds = 772 | train scaffolds = 625 | val scaffolds = 147 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/247 [00:00<?, ?it/s]100% 247/247 [00:00<00:00, 4555.76it/s]
Total scaffolds = 218 | train scaffolds = 175 | val scaffolds = 43 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/316 [00:00<?, ?it/s]100% 316/316 [00:00<00:00, 4714.21it/s]
Total scaffolds = 287 | train scaffolds = 228 | val scaffolds = 59 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/416 [00:00<?, ?it/s]100% 416/416 [00:00<00:00, 5003.17it/s]
Total scaffolds = 399 | train scaffolds = 320 | val scaffolds = 79 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/147 [00:00<?, ?it/s]100% 147/147 [00:00<00:00, 4021.04it/s]
Total scaffolds = 111 | train scaffolds = 90 | val scaffolds = 21 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/940 [00:00<?, ?it/s] 49% 464/940 [00:00<00:00, 4638.49it/s]100% 940/940 [00:00<00:00, 4726.25it/s]
Total scaffolds = 772 | train scaffolds = 625 | val scaffolds = 147 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/234 [00:00<?, ?it/s]100% 234/234 [00:00<00:00, 4801.37it/s]
Total scaffolds = 175 | train scaffolds = 136 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/663 [00:00<?, ?it/s] 75% 496/663 [00:00<00:00, 4952.16it/s]100% 663/663 [00:00<00:00, 4934.38it/s]
Total scaffolds = 550 | train scaffolds = 434 | val scaffolds = 116 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/241 [00:00<?, ?it/s]100% 241/241 [00:00<00:00, 4503.19it/s]
Total scaffolds = 216 | train scaffolds = 172 | val scaffolds = 44 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/464 [00:00<?, ?it/s]100% 464/464 [00:00<00:00, 4948.98it/s]
Total scaffolds = 372 | train scaffolds = 301 | val scaffolds = 71 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/586 [00:00<?, ?it/s] 92% 539/586 [00:00<00:00, 5387.49it/s]100% 586/586 [00:00<00:00, 5374.48it/s]
Total scaffolds = 418 | train scaffolds = 332 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
  0% 0/316 [00:00<?, ?it/s]100% 316/316 [00:00<00:00, 4668.04it/s]
Total scaffolds = 287 | train scaffolds = 228 | val scaffolds = 59 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/309 [00:00<?, ?it/s]100% 309/309 [00:00<00:00, 4912.01it/s]
Total scaffolds = 257 | train scaffolds = 200 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/464 [00:00<?, ?it/s] 92% 428/464 [00:00<00:00, 4279.93it/s]100% 464/464 [00:00<00:00, 4175.28it/s]
Total scaffolds = 318 | train scaffolds = 263 | val scaffolds = 55 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/266 [00:00<?, ?it/s]100% 266/266 [00:00<00:00, 5194.81it/s]
Total scaffolds = 125 | train scaffolds = 96 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/188 [00:00<?, ?it/s]100% 188/188 [00:00<00:00, 5661.67it/s]
Total scaffolds = 84 | train scaffolds = 61 | val scaffolds = 23 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/505 [00:00<?, ?it/s]100% 505/505 [00:00<00:00, 5199.14it/s]
Total scaffolds = 407 | train scaffolds = 320 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/417 [00:00<?, ?it/s]100% 417/417 [00:00<00:00, 4756.13it/s]
Total scaffolds = 338 | train scaffolds = 268 | val scaffolds = 70 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/415 [00:00<?, ?it/s]100% 415/415 [00:00<00:00, 4254.14it/s]
Total scaffolds = 279 | train scaffolds = 218 | val scaffolds = 61 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]))]
  0% 0/222 [00:00<?, ?it/s]100% 222/222 [00:00<00:00, 3527.60it/s]
Total scaffolds = 182 | train scaffolds = 142 | val scaffolds = 40 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/228 [00:00<?, ?it/s]100% 228/228 [00:00<00:00, 3989.11it/s]
Total scaffolds = 193 | train scaffolds = 159 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/228 [00:00<?, ?it/s]100% 228/228 [00:00<00:00, 4078.81it/s]
Total scaffolds = 193 | train scaffolds = 159 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/234 [00:00<?, ?it/s]100% 234/234 [00:00<00:00, 4834.46it/s]
Total scaffolds = 175 | train scaffolds = 136 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/397 [00:00<?, ?it/s]100% 397/397 [00:00<00:00, 4222.36it/s]
Total scaffolds = 258 | train scaffolds = 196 | val scaffolds = 62 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1]))]
  0% 0/266 [00:00<?, ?it/s]100% 266/266 [00:00<00:00, 5220.78it/s]
Total scaffolds = 125 | train scaffolds = 96 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/227 [00:00<?, ?it/s]100% 227/227 [00:00<00:00, 4123.75it/s]
Total scaffolds = 192 | train scaffolds = 154 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/375 [00:00<?, ?it/s]100% 375/375 [00:00<00:00, 4711.33it/s]
Total scaffolds = 346 | train scaffolds = 279 | val scaffolds = 67 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/189 [00:00<?, ?it/s]100% 189/189 [00:00<00:00, 4286.01it/s]
Total scaffolds = 174 | train scaffolds = 141 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/544 [00:00<?, ?it/s]100% 544/544 [00:00<00:00, 5519.62it/s]
Total scaffolds = 425 | train scaffolds = 327 | val scaffolds = 98 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/242 [00:00<?, ?it/s]100% 242/242 [00:00<00:00, 5924.82it/s]
Total scaffolds = 229 | train scaffolds = 181 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/374 [00:00<?, ?it/s]100% 374/374 [00:00<00:00, 6299.52it/s]
Total scaffolds = 287 | train scaffolds = 224 | val scaffolds = 63 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/266 [00:00<?, ?it/s]100% 266/266 [00:00<00:00, 4926.81it/s]
Total scaffolds = 125 | train scaffolds = 96 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/586 [00:00<?, ?it/s] 91% 532/586 [00:00<00:00, 5310.77it/s]100% 586/586 [00:00<00:00, 5291.13it/s]
Total scaffolds = 418 | train scaffolds = 332 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
  0% 0/464 [00:00<?, ?it/s]100% 464/464 [00:00<00:00, 4879.08it/s]
Total scaffolds = 372 | train scaffolds = 301 | val scaffolds = 71 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/222 [00:00<?, ?it/s]100% 222/222 [00:00<00:00, 3519.14it/s]
Total scaffolds = 182 | train scaffolds = 142 | val scaffolds = 40 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/215 [00:00<?, ?it/s]100% 215/215 [00:00<00:00, 4753.09it/s]
Total scaffolds = 196 | train scaffolds = 156 | val scaffolds = 40 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/222 [00:00<?, ?it/s]100% 222/222 [00:00<00:00, 3483.60it/s]
Total scaffolds = 182 | train scaffolds = 142 | val scaffolds = 40 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/338 [00:00<?, ?it/s]100% 338/338 [00:00<00:00, 4971.33it/s]
Total scaffolds = 269 | train scaffolds = 214 | val scaffolds = 55 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/294 [00:00<?, ?it/s]100% 294/294 [00:00<00:00, 4766.72it/s]
Total scaffolds = 210 | train scaffolds = 159 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/1010 [00:00<?, ?it/s] 46% 461/1010 [00:00<00:00, 4603.35it/s] 91% 922/1010 [00:00<00:00, 4602.63it/s]100% 1010/1010 [00:00<00:00, 4615.82it/s]
Total scaffolds = 780 | train scaffolds = 619 | val scaffolds = 161 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/266 [00:00<?, ?it/s]100% 266/266 [00:00<00:00, 5176.40it/s]
Total scaffolds = 125 | train scaffolds = 96 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/229 [00:00<?, ?it/s]100% 229/229 [00:00<00:00, 4286.09it/s]
Total scaffolds = 195 | train scaffolds = 158 | val scaffolds = 37 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/655 [00:00<?, ?it/s] 72% 472/655 [00:00<00:00, 4716.69it/s]100% 655/655 [00:00<00:00, 4696.52it/s]
Total scaffolds = 191 | train scaffolds = 148 | val scaffolds = 43 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/573 [00:00<?, ?it/s] 83% 473/573 [00:00<00:00, 4723.26it/s]100% 573/573 [00:00<00:00, 4713.88it/s]
Total scaffolds = 179 | train scaffolds = 137 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/833 [00:00<?, ?it/s] 68% 570/833 [00:00<00:00, 5628.26it/s]100% 833/833 [00:00<00:00, 4411.92it/s]
Total scaffolds = 489 | train scaffolds = 413 | val scaffolds = 76 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/837 [00:00<?, ?it/s] 68% 566/837 [00:00<00:00, 5655.68it/s]100% 837/837 [00:00<00:00, 4393.46it/s]
Total scaffolds = 489 | train scaffolds = 405 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/835 [00:00<?, ?it/s] 68% 565/835 [00:00<00:00, 5643.18it/s]100% 835/835 [00:00<00:00, 4384.68it/s]
Total scaffolds = 489 | train scaffolds = 400 | val scaffolds = 89 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/831 [00:00<?, ?it/s] 68% 563/831 [00:00<00:00, 5625.58it/s]100% 831/831 [00:00<00:00, 4340.97it/s]
Total scaffolds = 485 | train scaffolds = 401 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/831 [00:00<?, ?it/s] 68% 563/831 [00:00<00:00, 5627.19it/s]100% 831/831 [00:00<00:00, 4372.53it/s]
Total scaffolds = 486 | train scaffolds = 404 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/838 [00:00<?, ?it/s] 67% 565/838 [00:00<00:00, 5646.83it/s]100% 838/838 [00:00<00:00, 4396.36it/s]
Total scaffolds = 490 | train scaffolds = 406 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/834 [00:00<?, ?it/s] 68% 567/834 [00:00<00:00, 5624.25it/s]100% 834/834 [00:00<00:00, 4374.57it/s]
Total scaffolds = 489 | train scaffolds = 403 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/825 [00:00<?, ?it/s] 68% 562/825 [00:00<00:00, 5619.98it/s]100% 825/825 [00:00<00:00, 4401.44it/s]
Total scaffolds = 481 | train scaffolds = 397 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/809 [00:00<?, ?it/s] 69% 558/809 [00:00<00:00, 5577.86it/s]100% 809/809 [00:00<00:00, 4437.24it/s]
Total scaffolds = 473 | train scaffolds = 392 | val scaffolds = 81 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/833 [00:00<?, ?it/s] 68% 565/833 [00:00<00:00, 5648.07it/s]100% 833/833 [00:00<00:00, 4309.13it/s]
Total scaffolds = 487 | train scaffolds = 402 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.75      ,
              nan,        nan, 0.66666667,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/834 [00:00<?, ?it/s] 68% 566/834 [00:00<00:00, 5659.98it/s]100% 834/834 [00:00<00:00, 4380.45it/s]
Total scaffolds = 490 | train scaffolds = 404 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/822 [00:00<?, ?it/s] 69% 564/822 [00:00<00:00, 5629.91it/s]100% 822/822 [00:00<00:00, 4409.95it/s]
Total scaffolds = 481 | train scaffolds = 395 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/831 [00:00<?, ?it/s] 68% 564/831 [00:00<00:00, 5616.09it/s]100% 831/831 [00:00<00:00, 4369.32it/s]
Total scaffolds = 487 | train scaffolds = 405 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/827 [00:00<?, ?it/s] 68% 564/827 [00:00<00:00, 5590.56it/s]100% 827/827 [00:00<00:00, 4387.64it/s]
Total scaffolds = 484 | train scaffolds = 402 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/832 [00:00<?, ?it/s] 68% 563/832 [00:00<00:00, 5626.81it/s]100% 832/832 [00:00<00:00, 4389.49it/s]
Total scaffolds = 488 | train scaffolds = 400 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/840 [00:00<?, ?it/s] 68% 572/840 [00:00<00:00, 5660.41it/s]100% 840/840 [00:00<00:00, 4349.15it/s]
Total scaffolds = 492 | train scaffolds = 407 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/840 [00:00<?, ?it/s] 68% 567/840 [00:00<00:00, 5663.78it/s]100% 840/840 [00:00<00:00, 4390.57it/s]
Total scaffolds = 492 | train scaffolds = 407 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/833 [00:00<?, ?it/s] 68% 563/833 [00:00<00:00, 5624.08it/s]100% 833/833 [00:00<00:00, 4359.67it/s]
Total scaffolds = 490 | train scaffolds = 414 | val scaffolds = 76 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/837 [00:00<?, ?it/s] 67% 558/837 [00:00<00:00, 5570.87it/s]100% 837/837 [00:00<00:00, 4303.52it/s]
Total scaffolds = 490 | train scaffolds = 404 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/837 [00:00<?, ?it/s] 67% 561/837 [00:00<00:00, 5606.74it/s]100% 837/837 [00:00<00:00, 4356.00it/s]
Total scaffolds = 490 | train scaffolds = 404 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/828 [00:00<?, ?it/s] 68% 561/828 [00:00<00:00, 5548.05it/s]100% 828/828 [00:00<00:00, 4346.90it/s]
Total scaffolds = 486 | train scaffolds = 398 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
  0% 0/825 [00:00<?, ?it/s] 68% 562/825 [00:00<00:00, 5617.59it/s]100% 825/825 [00:00<00:00, 4316.53it/s]
Total scaffolds = 487 | train scaffolds = 401 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/828 [00:00<?, ?it/s] 68% 565/828 [00:00<00:00, 5627.33it/s]100% 828/828 [00:00<00:00, 4402.76it/s]
Total scaffolds = 485 | train scaffolds = 401 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/819 [00:00<?, ?it/s] 69% 566/819 [00:00<00:00, 5653.79it/s]100% 819/819 [00:00<00:00, 4403.42it/s]
Total scaffolds = 477 | train scaffolds = 394 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/837 [00:00<?, ?it/s] 68% 565/837 [00:00<00:00, 5643.30it/s]100% 837/837 [00:00<00:00, 4358.24it/s]
Total scaffolds = 489 | train scaffolds = 404 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/839 [00:00<?, ?it/s] 67% 565/839 [00:00<00:00, 5643.26it/s]100% 839/839 [00:00<00:00, 4395.20it/s]
Total scaffolds = 492 | train scaffolds = 405 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/833 [00:00<?, ?it/s] 68% 563/833 [00:00<00:00, 5625.23it/s]100% 833/833 [00:00<00:00, 4385.06it/s]
Total scaffolds = 491 | train scaffolds = 407 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/836 [00:00<?, ?it/s] 67% 564/836 [00:00<00:00, 5628.88it/s]100% 836/836 [00:00<00:00, 4359.38it/s]
Total scaffolds = 492 | train scaffolds = 406 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/832 [00:00<?, ?it/s] 68% 564/832 [00:00<00:00, 5637.98it/s]100% 832/832 [00:00<00:00, 4379.39it/s]
Total scaffolds = 488 | train scaffolds = 365 | val scaffolds = 123 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, 0.5, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/832 [00:00<?, ?it/s] 68% 567/832 [00:00<00:00, 5605.77it/s]100% 832/832 [00:00<00:00, 4390.47it/s]
Total scaffolds = 487 | train scaffolds = 409 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/825 [00:00<?, ?it/s] 67% 552/825 [00:00<00:00, 5519.29it/s]100% 825/825 [00:00<00:00, 4349.59it/s]
Total scaffolds = 483 | train scaffolds = 360 | val scaffolds = 123 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/824 [00:00<?, ?it/s] 68% 563/824 [00:00<00:00, 5628.34it/s]100% 824/824 [00:00<00:00, 4405.21it/s]
Total scaffolds = 482 | train scaffolds = 404 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/834 [00:00<?, ?it/s] 68% 570/834 [00:00<00:00, 5698.20it/s]100% 834/834 [00:00<00:00, 4442.63it/s]
Total scaffolds = 487 | train scaffolds = 400 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/832 [00:00<?, ?it/s] 68% 565/832 [00:00<00:00, 5598.14it/s]100% 832/832 [00:00<00:00, 4337.54it/s]
Total scaffolds = 489 | train scaffolds = 401 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/835 [00:00<?, ?it/s] 67% 562/835 [00:00<00:00, 5612.46it/s]100% 835/835 [00:00<00:00, 4360.69it/s]
Total scaffolds = 490 | train scaffolds = 407 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/837 [00:00<?, ?it/s] 67% 561/837 [00:00<00:00, 5600.50it/s]100% 837/837 [00:00<00:00, 4347.62it/s]
Total scaffolds = 490 | train scaffolds = 402 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/837 [00:00<?, ?it/s] 67% 560/837 [00:00<00:00, 5595.84it/s]100% 837/837 [00:00<00:00, 4305.77it/s]
Total scaffolds = 490 | train scaffolds = 404 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/837 [00:00<?, ?it/s] 67% 560/837 [00:00<00:00, 5594.15it/s]100% 837/837 [00:00<00:00, 4350.63it/s]
Total scaffolds = 491 | train scaffolds = 407 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/839 [00:00<?, ?it/s] 67% 563/839 [00:00<00:00, 5621.05it/s]100% 839/839 [00:00<00:00, 4381.94it/s]
Total scaffolds = 491 | train scaffolds = 403 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/828 [00:00<?, ?it/s] 68% 563/828 [00:00<00:00, 5596.00it/s]100% 828/828 [00:00<00:00, 4356.49it/s]
Total scaffolds = 485 | train scaffolds = 373 | val scaffolds = 112 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/839 [00:00<?, ?it/s] 67% 564/839 [00:00<00:00, 5627.11it/s]100% 839/839 [00:00<00:00, 4389.38it/s]
Total scaffolds = 491 | train scaffolds = 405 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/836 [00:00<?, ?it/s] 68% 566/836 [00:00<00:00, 5654.59it/s]100% 836/836 [00:00<00:00, 4387.41it/s]
Total scaffolds = 489 | train scaffolds = 402 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/815 [00:00<?, ?it/s] 69% 561/815 [00:00<00:00, 5598.27it/s]100% 815/815 [00:00<00:00, 4350.62it/s]
Total scaffolds = 483 | train scaffolds = 400 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/832 [00:00<?, ?it/s] 68% 563/832 [00:00<00:00, 5623.78it/s]100% 832/832 [00:00<00:00, 4362.51it/s]
Total scaffolds = 488 | train scaffolds = 400 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/837 [00:00<?, ?it/s] 67% 564/837 [00:00<00:00, 5637.84it/s]100% 837/837 [00:00<00:00, 4389.01it/s]
Total scaffolds = 492 | train scaffolds = 408 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/835 [00:00<?, ?it/s] 66% 550/835 [00:00<00:00, 5487.37it/s]100% 835/835 [00:00<00:00, 4270.90it/s]
Total scaffolds = 488 | train scaffolds = 411 | val scaffolds = 77 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/836 [00:00<?, ?it/s] 67% 564/836 [00:00<00:00, 5634.31it/s]100% 836/836 [00:00<00:00, 4374.92it/s]
Total scaffolds = 490 | train scaffolds = 409 | val scaffolds = 81 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/840 [00:00<?, ?it/s] 67% 562/840 [00:00<00:00, 5613.80it/s]100% 840/840 [00:00<00:00, 4367.27it/s]
Total scaffolds = 492 | train scaffolds = 407 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/837 [00:00<?, ?it/s] 68% 570/837 [00:00<00:00, 5691.43it/s]100% 837/837 [00:00<00:00, 4403.45it/s]
Total scaffolds = 492 | train scaffolds = 407 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/833 [00:00<?, ?it/s] 68% 565/833 [00:00<00:00, 5637.99it/s]100% 833/833 [00:00<00:00, 4386.87it/s]
Total scaffolds = 488 | train scaffolds = 405 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, 0.5, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/831 [00:00<?, ?it/s] 68% 561/831 [00:00<00:00, 5602.39it/s]100% 831/831 [00:00<00:00, 4341.53it/s]
Total scaffolds = 489 | train scaffolds = 409 | val scaffolds = 80 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/831 [00:00<?, ?it/s] 67% 560/831 [00:00<00:00, 5590.53it/s]100% 831/831 [00:00<00:00, 4305.62it/s]
Total scaffolds = 488 | train scaffolds = 403 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/837 [00:00<?, ?it/s] 67% 560/837 [00:00<00:00, 5597.56it/s]100% 837/837 [00:00<00:00, 4352.49it/s]
Total scaffolds = 489 | train scaffolds = 404 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/819 [00:00<?, ?it/s] 68% 553/819 [00:00<00:00, 5523.84it/s]100% 819/819 [00:00<00:00, 4315.81it/s]
Total scaffolds = 479 | train scaffolds = 362 | val scaffolds = 117 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/838 [00:00<?, ?it/s] 67% 560/838 [00:00<00:00, 5597.02it/s]100% 838/838 [00:00<00:00, 4321.53it/s]
Total scaffolds = 490 | train scaffolds = 408 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/836 [00:00<?, ?it/s] 67% 564/836 [00:00<00:00, 5626.87it/s]100% 836/836 [00:00<00:00, 4378.22it/s]
Total scaffolds = 490 | train scaffolds = 405 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/834 [00:00<?, ?it/s] 68% 565/834 [00:00<00:00, 5644.56it/s]100% 834/834 [00:00<00:00, 4386.92it/s]
Total scaffolds = 488 | train scaffolds = 375 | val scaffolds = 113 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/820 [00:00<?, ?it/s] 68% 558/820 [00:00<00:00, 5571.44it/s]100% 820/820 [00:00<00:00, 4391.73it/s]
Total scaffolds = 483 | train scaffolds = 396 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.75,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/812 [00:00<?, ?it/s] 69% 561/812 [00:00<00:00, 5601.48it/s]100% 812/812 [00:00<00:00, 4381.48it/s]
Total scaffolds = 477 | train scaffolds = 391 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.75,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/828 [00:00<?, ?it/s] 68% 561/828 [00:00<00:00, 5605.23it/s]100% 828/828 [00:00<00:00, 4372.46it/s]
Total scaffolds = 487 | train scaffolds = 399 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/829 [00:00<?, ?it/s] 68% 564/829 [00:00<00:00, 5576.39it/s]100% 829/829 [00:00<00:00, 4343.29it/s]
Total scaffolds = 488 | train scaffolds = 401 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, 0.5, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ 1., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/831 [00:00<?, ?it/s] 68% 564/831 [00:00<00:00, 5630.61it/s]100% 831/831 [00:00<00:00, 4367.73it/s]
Total scaffolds = 489 | train scaffolds = 403 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/820 [00:00<?, ?it/s] 68% 557/820 [00:00<00:00, 5569.59it/s]100% 820/820 [00:00<00:00, 4356.52it/s]
Total scaffolds = 482 | train scaffolds = 396 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/594 [00:00<?, ?it/s] 75% 446/594 [00:00<00:00, 4457.06it/s]100% 594/594 [00:00<00:00, 4510.98it/s]
Total scaffolds = 359 | train scaffolds = 282 | val scaffolds = 77 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/326 [00:00<?, ?it/s]100% 326/326 [00:00<00:00, 4737.70it/s]
Total scaffolds = 253 | train scaffolds = 192 | val scaffolds = 61 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/683 [00:00<?, ?it/s] 65% 441/683 [00:00<00:00, 4408.88it/s]100% 683/683 [00:00<00:00, 4483.22it/s]
Total scaffolds = 440 | train scaffolds = 371 | val scaffolds = 69 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/456 [00:00<?, ?it/s]100% 456/456 [00:00<00:00, 4554.37it/s]100% 456/456 [00:00<00:00, 4544.39it/s]
Total scaffolds = 294 | train scaffolds = 237 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/999 [00:00<?, ?it/s] 44% 443/999 [00:00<00:00, 4428.90it/s] 91% 908/999 [00:00<00:00, 4489.77it/s]100% 999/999 [00:00<00:00, 4503.95it/s]
Total scaffolds = 588 | train scaffolds = 455 | val scaffolds = 133 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/892 [00:00<?, ?it/s] 49% 439/892 [00:00<00:00, 4386.94it/s]100% 891/892 [00:00<00:00, 4424.16it/s]100% 892/892 [00:00<00:00, 4442.93it/s]
Total scaffolds = 526 | train scaffolds = 421 | val scaffolds = 105 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/335 [00:00<?, ?it/s]100% 335/335 [00:00<00:00, 4538.95it/s]
Total scaffolds = 252 | train scaffolds = 202 | val scaffolds = 50 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/838 [00:00<?, ?it/s] 52% 434/838 [00:00<00:00, 4332.10it/s]100% 838/838 [00:00<00:00, 4396.02it/s]
Total scaffolds = 502 | train scaffolds = 385 | val scaffolds = 117 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/726 [00:00<?, ?it/s] 60% 435/726 [00:00<00:00, 4345.67it/s]100% 726/726 [00:00<00:00, 4423.36it/s]
Total scaffolds = 424 | train scaffolds = 332 | val scaffolds = 92 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/518 [00:00<?, ?it/s] 83% 431/518 [00:00<00:00, 4309.09it/s]100% 518/518 [00:00<00:00, 4358.43it/s]
Total scaffolds = 311 | train scaffolds = 249 | val scaffolds = 62 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/578 [00:00<?, ?it/s] 76% 441/578 [00:00<00:00, 4399.82it/s]100% 578/578 [00:00<00:00, 4438.73it/s]
Total scaffolds = 349 | train scaffolds = 273 | val scaffolds = 76 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/721 [00:00<?, ?it/s] 59% 425/721 [00:00<00:00, 4246.30it/s]100% 721/721 [00:00<00:00, 4380.65it/s]
Total scaffolds = 425 | train scaffolds = 329 | val scaffolds = 96 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/315 [00:00<?, ?it/s]100% 315/315 [00:00<00:00, 4360.49it/s]
Total scaffolds = 235 | train scaffolds = 187 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/767 [00:00<?, ?it/s] 58% 448/767 [00:00<00:00, 4473.75it/s]100% 767/767 [00:00<00:00, 4536.10it/s]
Total scaffolds = 451 | train scaffolds = 369 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/912 [00:00<?, ?it/s] 47% 430/912 [00:00<00:00, 4291.73it/s] 97% 881/912 [00:00<00:00, 4352.57it/s]100% 912/912 [00:00<00:00, 4392.09it/s]
Total scaffolds = 542 | train scaffolds = 445 | val scaffolds = 97 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/513 [00:00<?, ?it/s] 85% 435/513 [00:00<00:00, 4345.87it/s]100% 513/513 [00:00<00:00, 4394.11it/s]
Total scaffolds = 309 | train scaffolds = 246 | val scaffolds = 63 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/740 [00:00<?, ?it/s] 59% 440/740 [00:00<00:00, 4391.53it/s]100% 740/740 [00:00<00:00, 4470.70it/s]
Total scaffolds = 437 | train scaffolds = 345 | val scaffolds = 92 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/746 [00:00<?, ?it/s] 59% 438/746 [00:00<00:00, 4369.34it/s]100% 746/746 [00:00<00:00, 4442.50it/s]
Total scaffolds = 467 | train scaffolds = 374 | val scaffolds = 93 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/725 [00:00<?, ?it/s] 61% 441/725 [00:00<00:00, 4401.80it/s]100% 725/725 [00:00<00:00, 4496.87it/s]
Total scaffolds = 465 | train scaffolds = 374 | val scaffolds = 91 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/738 [00:00<?, ?it/s] 59% 439/738 [00:00<00:00, 4384.41it/s]100% 738/738 [00:00<00:00, 4455.97it/s]
Total scaffolds = 433 | train scaffolds = 363 | val scaffolds = 70 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/689 [00:00<?, ?it/s] 64% 441/689 [00:00<00:00, 4408.64it/s]100% 689/689 [00:00<00:00, 4469.29it/s]
Total scaffolds = 415 | train scaffolds = 328 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/579 [00:00<?, ?it/s] 76% 440/579 [00:00<00:00, 4379.66it/s]100% 579/579 [00:00<00:00, 4428.81it/s]
Total scaffolds = 353 | train scaffolds = 277 | val scaffolds = 76 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/398 [00:00<?, ?it/s]100% 398/398 [00:00<00:00, 4539.51it/s]
Total scaffolds = 256 | train scaffolds = 199 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/524 [00:00<?, ?it/s] 82% 431/524 [00:00<00:00, 4306.04it/s]100% 524/524 [00:00<00:00, 4282.95it/s]
Total scaffolds = 301 | train scaffolds = 234 | val scaffolds = 67 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/760 [00:00<?, ?it/s] 59% 445/760 [00:00<00:00, 4438.24it/s]100% 760/760 [00:00<00:00, 4526.77it/s]
Total scaffolds = 477 | train scaffolds = 388 | val scaffolds = 89 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/881 [00:00<?, ?it/s] 50% 438/881 [00:00<00:00, 4375.59it/s]100% 881/881 [00:00<00:00, 4474.05it/s]
Total scaffolds = 522 | train scaffolds = 436 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/507 [00:00<?, ?it/s] 86% 434/507 [00:00<00:00, 4334.12it/s]100% 507/507 [00:00<00:00, 4371.60it/s]
Total scaffolds = 303 | train scaffolds = 228 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/582 [00:00<?, ?it/s] 76% 443/582 [00:00<00:00, 4427.70it/s]100% 582/582 [00:00<00:00, 4455.88it/s]
Total scaffolds = 337 | train scaffolds = 272 | val scaffolds = 65 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/932 [00:00<?, ?it/s] 46% 429/932 [00:00<00:00, 4288.50it/s] 93% 868/932 [00:00<00:00, 4315.52it/s]100% 932/932 [00:00<00:00, 4334.65it/s]
Total scaffolds = 532 | train scaffolds = 428 | val scaffolds = 104 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/926 [00:00<?, ?it/s] 47% 433/926 [00:00<00:00, 4327.89it/s] 97% 900/926 [00:00<00:00, 4421.50it/s]100% 926/926 [00:00<00:00, 4486.37it/s]
Total scaffolds = 536 | train scaffolds = 434 | val scaffolds = 102 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/674 [00:00<?, ?it/s] 65% 436/674 [00:00<00:00, 4359.84it/s]100% 674/674 [00:00<00:00, 4420.30it/s]
Total scaffolds = 400 | train scaffolds = 327 | val scaffolds = 73 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/346 [00:00<?, ?it/s]100% 346/346 [00:00<00:00, 4455.03it/s]
Total scaffolds = 254 | train scaffolds = 202 | val scaffolds = 52 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/836 [00:00<?, ?it/s] 53% 443/836 [00:00<00:00, 4422.85it/s]100% 836/836 [00:00<00:00, 4478.45it/s]
Total scaffolds = 461 | train scaffolds = 384 | val scaffolds = 77 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/851 [00:00<?, ?it/s] 52% 444/851 [00:00<00:00, 4432.61it/s]100% 851/851 [00:00<00:00, 4517.48it/s]
Total scaffolds = 465 | train scaffolds = 368 | val scaffolds = 97 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  0.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/609 [00:00<?, ?it/s] 73% 445/609 [00:00<00:00, 4449.07it/s]100% 609/609 [00:00<00:00, 4475.34it/s]
Total scaffolds = 365 | train scaffolds = 286 | val scaffolds = 79 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/909 [00:00<?, ?it/s] 48% 435/909 [00:00<00:00, 4346.61it/s] 98% 895/909 [00:00<00:00, 4418.20it/s]100% 909/909 [00:00<00:00, 4460.80it/s]
Total scaffolds = 551 | train scaffolds = 448 | val scaffolds = 103 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/933 [00:00<?, ?it/s] 48% 448/933 [00:00<00:00, 4477.24it/s] 98% 916/933 [00:00<00:00, 4535.72it/s]100% 933/933 [00:00<00:00, 4563.17it/s]
Total scaffolds = 564 | train scaffolds = 454 | val scaffolds = 110 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/514 [00:00<?, ?it/s] 84% 433/514 [00:00<00:00, 4329.72it/s]100% 514/514 [00:00<00:00, 4369.76it/s]
Total scaffolds = 308 | train scaffolds = 230 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/639 [00:00<?, ?it/s] 69% 442/639 [00:00<00:00, 4416.53it/s]100% 639/639 [00:00<00:00, 4465.40it/s]
Total scaffolds = 349 | train scaffolds = 276 | val scaffolds = 73 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/707 [00:00<?, ?it/s] 61% 431/707 [00:00<00:00, 4307.32it/s]100% 707/707 [00:00<00:00, 4332.36it/s]
Total scaffolds = 432 | train scaffolds = 340 | val scaffolds = 92 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/654 [00:00<?, ?it/s] 65% 426/654 [00:00<00:00, 4254.02it/s]100% 654/654 [00:00<00:00, 4346.10it/s]
Total scaffolds = 366 | train scaffolds = 301 | val scaffolds = 65 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/734 [00:00<?, ?it/s] 60% 442/734 [00:00<00:00, 4415.44it/s]100% 734/734 [00:00<00:00, 4495.11it/s]
Total scaffolds = 432 | train scaffolds = 348 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/1001 [00:00<?, ?it/s] 44% 440/1001 [00:00<00:00, 4397.46it/s] 89% 891/1001 [00:00<00:00, 4430.55it/s]100% 1001/1001 [00:00<00:00, 4457.40it/s]
Total scaffolds = 569 | train scaffolds = 445 | val scaffolds = 124 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/742 [00:00<?, ?it/s] 59% 437/742 [00:00<00:00, 4368.86it/s]100% 742/742 [00:00<00:00, 4442.04it/s]
Total scaffolds = 434 | train scaffolds = 357 | val scaffolds = 77 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/626 [00:00<?, ?it/s] 70% 439/626 [00:00<00:00, 4389.85it/s]100% 626/626 [00:00<00:00, 4441.58it/s]
Total scaffolds = 364 | train scaffolds = 298 | val scaffolds = 66 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/759 [00:00<?, ?it/s] 59% 445/759 [00:00<00:00, 4447.08it/s]100% 759/759 [00:00<00:00, 4516.42it/s]
Total scaffolds = 445 | train scaffolds = 370 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/986 [00:00<?, ?it/s] 45% 440/986 [00:00<00:00, 4386.82it/s] 90% 891/986 [00:00<00:00, 4421.22it/s]100% 986/986 [00:00<00:00, 4439.96it/s]
Total scaffolds = 582 | train scaffolds = 460 | val scaffolds = 122 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/882 [00:00<?, ?it/s] 50% 439/882 [00:00<00:00, 4389.82it/s]100% 882/882 [00:00<00:00, 4501.07it/s]
Total scaffolds = 508 | train scaffolds = 382 | val scaffolds = 126 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/551 [00:00<?, ?it/s] 80% 443/551 [00:00<00:00, 4426.63it/s]100% 551/551 [00:00<00:00, 4467.57it/s]
Total scaffolds = 340 | train scaffolds = 251 | val scaffolds = 89 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/446 [00:00<?, ?it/s] 99% 440/446 [00:00<00:00, 4394.19it/s]100% 446/446 [00:00<00:00, 4377.44it/s]
Total scaffolds = 280 | train scaffolds = 227 | val scaffolds = 53 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/606 [00:00<?, ?it/s] 70% 427/606 [00:00<00:00, 4269.34it/s]100% 606/606 [00:00<00:00, 4321.75it/s]
Total scaffolds = 343 | train scaffolds = 278 | val scaffolds = 65 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/915 [00:00<?, ?it/s] 47% 431/915 [00:00<00:00, 4306.41it/s] 97% 885/915 [00:00<00:00, 4372.73it/s]100% 915/915 [00:00<00:00, 4420.47it/s]
Total scaffolds = 539 | train scaffolds = 439 | val scaffolds = 100 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/600 [00:00<?, ?it/s] 74% 443/600 [00:00<00:00, 4425.48it/s]100% 600/600 [00:00<00:00, 4457.76it/s]
Total scaffolds = 359 | train scaffolds = 293 | val scaffolds = 66 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/686 [00:00<?, ?it/s] 63% 432/686 [00:00<00:00, 4316.52it/s]100% 686/686 [00:00<00:00, 4381.30it/s]
Total scaffolds = 388 | train scaffolds = 316 | val scaffolds = 72 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/767 [00:00<?, ?it/s] 59% 453/767 [00:00<00:00, 4522.58it/s]100% 767/767 [00:00<00:00, 4602.40it/s]
Total scaffolds = 495 | train scaffolds = 392 | val scaffolds = 103 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/877 [00:00<?, ?it/s] 50% 435/877 [00:00<00:00, 4347.43it/s]100% 877/877 [00:00<00:00, 4417.72it/s]
Total scaffolds = 524 | train scaffolds = 429 | val scaffolds = 95 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/488 [00:00<?, ?it/s] 89% 432/488 [00:00<00:00, 4312.73it/s]100% 488/488 [00:00<00:00, 4336.74it/s]
Total scaffolds = 302 | train scaffolds = 224 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/636 [00:00<?, ?it/s] 67% 428/636 [00:00<00:00, 4275.02it/s]100% 636/636 [00:00<00:00, 4350.49it/s]
Total scaffolds = 356 | train scaffolds = 278 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/509 [00:00<?, ?it/s] 85% 435/509 [00:00<00:00, 4343.46it/s]100% 509/509 [00:00<00:00, 4383.37it/s]
Total scaffolds = 306 | train scaffolds = 237 | val scaffolds = 69 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/680 [00:00<?, ?it/s] 66% 446/680 [00:00<00:00, 4453.59it/s]100% 680/680 [00:00<00:00, 4512.23it/s]
Total scaffolds = 376 | train scaffolds = 296 | val scaffolds = 80 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/759 [00:00<?, ?it/s] 58% 440/759 [00:00<00:00, 4392.86it/s]100% 759/759 [00:00<00:00, 4452.12it/s]
Total scaffolds = 441 | train scaffolds = 352 | val scaffolds = 89 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/887 [00:00<?, ?it/s] 48% 430/887 [00:00<00:00, 4298.96it/s]100% 887/887 [00:00<00:00, 4440.66it/s]
Total scaffolds = 529 | train scaffolds = 427 | val scaffolds = 102 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/390 [00:00<?, ?it/s]100% 390/390 [00:00<00:00, 4495.11it/s]
Total scaffolds = 256 | train scaffolds = 214 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/725 [00:00<?, ?it/s] 61% 443/725 [00:00<00:00, 4429.99it/s]100% 725/725 [00:00<00:00, 4467.20it/s]
Total scaffolds = 428 | train scaffolds = 355 | val scaffolds = 73 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/418 [00:00<?, ?it/s]100% 418/418 [00:00<00:00, 4452.94it/s]
Total scaffolds = 249 | train scaffolds = 196 | val scaffolds = 53 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/727 [00:00<?, ?it/s] 60% 434/727 [00:00<00:00, 4339.20it/s]100% 727/727 [00:00<00:00, 4372.02it/s]
Total scaffolds = 422 | train scaffolds = 326 | val scaffolds = 96 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/603 [00:00<?, ?it/s] 73% 443/603 [00:00<00:00, 4423.68it/s]100% 603/603 [00:00<00:00, 4466.62it/s]
Total scaffolds = 365 | train scaffolds = 286 | val scaffolds = 79 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/584 [00:00<?, ?it/s] 78% 457/584 [00:00<00:00, 4563.72it/s]100% 584/584 [00:00<00:00, 4611.02it/s]
Total scaffolds = 362 | train scaffolds = 287 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/633 [00:00<?, ?it/s] 70% 443/633 [00:00<00:00, 4428.60it/s]100% 633/633 [00:00<00:00, 4462.42it/s]
Total scaffolds = 384 | train scaffolds = 302 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/596 [00:00<?, ?it/s] 75% 446/596 [00:00<00:00, 4453.63it/s]100% 596/596 [00:00<00:00, 4463.32it/s]
Total scaffolds = 366 | train scaffolds = 278 | val scaffolds = 88 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/742 [00:00<?, ?it/s] 59% 437/742 [00:00<00:00, 4363.59it/s]100% 742/742 [00:00<00:00, 4392.80it/s]
Total scaffolds = 439 | train scaffolds = 343 | val scaffolds = 96 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/666 [00:00<?, ?it/s] 65% 432/666 [00:00<00:00, 4312.17it/s]100% 666/666 [00:00<00:00, 4379.36it/s]
Total scaffolds = 379 | train scaffolds = 293 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/464 [00:00<?, ?it/s] 95% 443/464 [00:00<00:00, 4424.35it/s]100% 464/464 [00:00<00:00, 4427.14it/s]
Total scaffolds = 287 | train scaffolds = 233 | val scaffolds = 54 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/949 [00:00<?, ?it/s] 46% 440/949 [00:00<00:00, 4398.49it/s] 95% 901/949 [00:00<00:00, 4458.86it/s]100% 949/949 [00:00<00:00, 4498.96it/s]
Total scaffolds = 568 | train scaffolds = 453 | val scaffolds = 115 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/888 [00:00<?, ?it/s] 50% 440/888 [00:00<00:00, 4388.39it/s]100% 888/888 [00:00<00:00, 4482.98it/s]
Total scaffolds = 523 | train scaffolds = 401 | val scaffolds = 122 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/522 [00:00<?, ?it/s] 88% 458/522 [00:00<00:00, 4571.13it/s]100% 522/522 [00:00<00:00, 4582.35it/s]
Total scaffolds = 314 | train scaffolds = 232 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/910 [00:00<?, ?it/s] 46% 421/910 [00:00<00:00, 4205.39it/s] 96% 874/910 [00:00<00:00, 4295.64it/s]100% 910/910 [00:00<00:00, 4361.28it/s]
Total scaffolds = 543 | train scaffolds = 444 | val scaffolds = 99 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/293 [00:00<?, ?it/s]100% 293/293 [00:00<00:00, 4275.44it/s]
Total scaffolds = 209 | train scaffolds = 163 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/316 [00:00<?, ?it/s]100% 316/316 [00:00<00:00, 4527.43it/s]
Total scaffolds = 234 | train scaffolds = 201 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/294 [00:00<?, ?it/s]100% 294/294 [00:00<00:00, 4798.04it/s]
Total scaffolds = 230 | train scaffolds = 182 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/938 [00:00<?, ?it/s] 47% 443/938 [00:00<00:00, 4428.35it/s] 96% 904/938 [00:00<00:00, 4479.98it/s]100% 938/938 [00:00<00:00, 4511.20it/s]
Total scaffolds = 538 | train scaffolds = 433 | val scaffolds = 105 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/576 [00:00<?, ?it/s] 76% 440/576 [00:00<00:00, 4396.40it/s]100% 576/576 [00:00<00:00, 4402.16it/s]
Total scaffolds = 347 | train scaffolds = 277 | val scaffolds = 70 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan,  0.,  0.,  0.,  0.,  0.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/957 [00:00<?, ?it/s] 46% 444/957 [00:00<00:00, 4426.75it/s] 96% 914/957 [00:00<00:00, 4505.18it/s]100% 957/957 [00:00<00:00, 4563.95it/s]
Total scaffolds = 594 | train scaffolds = 466 | val scaffolds = 128 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/734 [00:00<?, ?it/s] 59% 436/734 [00:00<00:00, 4352.81it/s]100% 734/734 [00:00<00:00, 4408.80it/s]
Total scaffolds = 429 | train scaffolds = 353 | val scaffolds = 76 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/766 [00:00<?, ?it/s] 55% 424/766 [00:00<00:00, 4235.53it/s]100% 766/766 [00:00<00:00, 4310.56it/s]
Total scaffolds = 439 | train scaffolds = 352 | val scaffolds = 87 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/615 [00:00<?, ?it/s] 72% 442/615 [00:00<00:00, 4419.88it/s]100% 615/615 [00:00<00:00, 4485.09it/s]
Total scaffolds = 367 | train scaffolds = 299 | val scaffolds = 68 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/592 [00:00<?, ?it/s] 71% 422/592 [00:00<00:00, 4211.06it/s]100% 592/592 [00:00<00:00, 4313.70it/s]
Total scaffolds = 362 | train scaffolds = 282 | val scaffolds = 80 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/346 [00:00<?, ?it/s]100% 346/346 [00:00<00:00, 4595.90it/s]
Total scaffolds = 256 | train scaffolds = 205 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/1015 [00:00<?, ?it/s] 43% 435/1015 [00:00<00:00, 4344.31it/s] 90% 910/1015 [00:00<00:00, 4457.16it/s]100% 1015/1015 [00:00<00:00, 4543.70it/s]
Total scaffolds = 589 | train scaffolds = 471 | val scaffolds = 118 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/692 [00:00<?, ?it/s] 63% 435/692 [00:00<00:00, 4337.18it/s]100% 692/692 [00:00<00:00, 4409.48it/s]
Total scaffolds = 404 | train scaffolds = 333 | val scaffolds = 71 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/799 [00:00<?, ?it/s] 54% 434/799 [00:00<00:00, 4337.37it/s]100% 799/799 [00:00<00:00, 4410.76it/s]
Total scaffolds = 480 | train scaffolds = 379 | val scaffolds = 101 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/730 [00:00<?, ?it/s] 60% 441/730 [00:00<00:00, 4404.12it/s]100% 730/730 [00:00<00:00, 4469.16it/s]
Total scaffolds = 431 | train scaffolds = 356 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/721 [00:00<?, ?it/s] 61% 443/721 [00:00<00:00, 4419.22it/s]100% 721/721 [00:00<00:00, 4425.94it/s]
Total scaffolds = 425 | train scaffolds = 339 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/645 [00:00<?, ?it/s] 68% 438/645 [00:00<00:00, 4373.61it/s]100% 645/645 [00:00<00:00, 4393.75it/s]
Total scaffolds = 370 | train scaffolds = 287 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/592 [00:00<?, ?it/s] 75% 445/592 [00:00<00:00, 4442.24it/s]100% 592/592 [00:00<00:00, 4475.59it/s]
Total scaffolds = 358 | train scaffolds = 281 | val scaffolds = 77 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/538 [00:00<?, ?it/s] 82% 443/538 [00:00<00:00, 4429.49it/s]100% 538/538 [00:00<00:00, 4468.86it/s]
Total scaffolds = 331 | train scaffolds = 261 | val scaffolds = 70 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/474 [00:00<?, ?it/s] 93% 442/474 [00:00<00:00, 4408.55it/s]100% 474/474 [00:00<00:00, 4418.56it/s]
Total scaffolds = 293 | train scaffolds = 239 | val scaffolds = 54 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/356 [00:00<?, ?it/s]100% 356/356 [00:00<00:00, 4831.99it/s]
Total scaffolds = 273 | train scaffolds = 216 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/763 [00:00<?, ?it/s] 58% 439/763 [00:00<00:00, 4386.99it/s]100% 763/763 [00:00<00:00, 4493.17it/s]
Total scaffolds = 454 | train scaffolds = 372 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/722 [00:00<?, ?it/s] 60% 434/722 [00:00<00:00, 4336.53it/s]100% 722/722 [00:00<00:00, 4386.78it/s]
Total scaffolds = 436 | train scaffolds = 361 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/583 [00:00<?, ?it/s] 76% 445/583 [00:00<00:00, 4437.94it/s]100% 583/583 [00:00<00:00, 4465.78it/s]
Total scaffolds = 351 | train scaffolds = 261 | val scaffolds = 90 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/398 [00:00<?, ?it/s]100% 398/398 [00:00<00:00, 4642.43it/s]
Total scaffolds = 265 | train scaffolds = 217 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/566 [00:00<?, ?it/s] 76% 432/566 [00:00<00:00, 4312.53it/s]100% 566/566 [00:00<00:00, 4350.19it/s]
Total scaffolds = 330 | train scaffolds = 264 | val scaffolds = 66 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/318 [00:00<?, ?it/s]100% 318/318 [00:00<00:00, 4445.18it/s]
Total scaffolds = 237 | train scaffolds = 186 | val scaffolds = 51 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/335 [00:00<?, ?it/s]100% 335/335 [00:00<00:00, 4476.21it/s]
Total scaffolds = 250 | train scaffolds = 196 | val scaffolds = 54 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  1.,  1.,  1.,  1.,  1.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/983 [00:00<?, ?it/s] 45% 445/983 [00:00<00:00, 4445.66it/s] 92% 908/983 [00:00<00:00, 4498.30it/s]100% 983/983 [00:00<00:00, 4517.65it/s]
Total scaffolds = 594 | train scaffolds = 474 | val scaffolds = 120 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/739 [00:00<?, ?it/s] 61% 451/739 [00:00<00:00, 4504.66it/s]100% 739/739 [00:00<00:00, 4574.22it/s]
Total scaffolds = 431 | train scaffolds = 353 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/666 [00:00<?, ?it/s] 66% 437/666 [00:00<00:00, 4360.18it/s]100% 666/666 [00:00<00:00, 4424.62it/s]
Total scaffolds = 382 | train scaffolds = 310 | val scaffolds = 72 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/342 [00:00<?, ?it/s]100% 342/342 [00:00<00:00, 4757.75it/s]
Total scaffolds = 259 | train scaffolds = 207 | val scaffolds = 52 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/682 [00:00<?, ?it/s] 64% 437/682 [00:00<00:00, 4364.87it/s]100% 682/682 [00:00<00:00, 4404.17it/s]
Total scaffolds = 398 | train scaffolds = 315 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/197 [00:00<?, ?it/s]100% 197/197 [00:00<00:00, 4553.45it/s]
Total scaffolds = 153 | train scaffolds = 132 | val scaffolds = 21 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/1002 [00:00<?, ?it/s] 44% 442/1002 [00:00<00:00, 4418.53it/s] 91% 910/1002 [00:00<00:00, 4491.06it/s]100% 1002/1002 [00:00<00:00, 4512.53it/s]
Total scaffolds = 588 | train scaffolds = 457 | val scaffolds = 131 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/461 [00:00<?, ?it/s] 95% 440/461 [00:00<00:00, 4399.63it/s]100% 461/461 [00:00<00:00, 4385.90it/s]
Total scaffolds = 285 | train scaffolds = 237 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/695 [00:00<?, ?it/s] 63% 436/695 [00:00<00:00, 4357.32it/s]100% 695/695 [00:00<00:00, 4428.38it/s]
Total scaffolds = 410 | train scaffolds = 325 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/815 [00:00<?, ?it/s] 52% 424/815 [00:00<00:00, 4234.58it/s]100% 815/815 [00:00<00:00, 4294.30it/s]
Total scaffolds = 460 | train scaffolds = 350 | val scaffolds = 110 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/604 [00:00<?, ?it/s] 73% 441/604 [00:00<00:00, 4403.45it/s]100% 604/604 [00:00<00:00, 4448.60it/s]
Total scaffolds = 367 | train scaffolds = 289 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/728 [00:00<?, ?it/s] 60% 438/728 [00:00<00:00, 4374.32it/s]100% 728/728 [00:00<00:00, 4457.30it/s]
Total scaffolds = 426 | train scaffolds = 355 | val scaffolds = 71 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/624 [00:00<?, ?it/s] 70% 439/624 [00:00<00:00, 4386.18it/s]100% 624/624 [00:00<00:00, 4414.51it/s]
Total scaffolds = 369 | train scaffolds = 288 | val scaffolds = 81 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/696 [00:00<?, ?it/s] 63% 441/696 [00:00<00:00, 4396.76it/s]100% 696/696 [00:00<00:00, 4402.80it/s]
Total scaffolds = 408 | train scaffolds = 343 | val scaffolds = 65 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/898 [00:00<?, ?it/s] 48% 433/898 [00:00<00:00, 4324.15it/s]100% 896/898 [00:00<00:00, 4411.31it/s]100% 898/898 [00:00<00:00, 4470.58it/s]
Total scaffolds = 550 | train scaffolds = 439 | val scaffolds = 111 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/422 [00:00<?, ?it/s]100% 422/422 [00:00<00:00, 4573.76it/s]
Total scaffolds = 288 | train scaffolds = 242 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  0.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/403 [00:00<?, ?it/s]100% 403/403 [00:00<00:00, 4536.68it/s]
Total scaffolds = 273 | train scaffolds = 220 | val scaffolds = 53 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/773 [00:00<?, ?it/s] 55% 424/773 [00:00<00:00, 4234.09it/s]100% 773/773 [00:00<00:00, 4286.31it/s]
Total scaffolds = 448 | train scaffolds = 356 | val scaffolds = 92 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/609 [00:00<?, ?it/s] 72% 439/609 [00:00<00:00, 4382.10it/s]100% 609/609 [00:00<00:00, 4447.16it/s]
Total scaffolds = 369 | train scaffolds = 283 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/653 [00:00<?, ?it/s] 63% 414/653 [00:00<00:00, 4139.57it/s]100% 653/653 [00:00<00:00, 4249.37it/s]
Total scaffolds = 384 | train scaffolds = 317 | val scaffolds = 67 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/743 [00:00<?, ?it/s] 61% 451/743 [00:00<00:00, 4502.50it/s]100% 743/743 [00:00<00:00, 4571.29it/s]
Total scaffolds = 442 | train scaffolds = 365 | val scaffolds = 77 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/390 [00:00<?, ?it/s]100% 390/390 [00:00<00:00, 4746.68it/s]
Total scaffolds = 267 | train scaffolds = 209 | val scaffolds = 58 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0.,  0.,  0.,  0.,  1.,  1., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/918 [00:00<?, ?it/s] 47% 433/918 [00:00<00:00, 4326.22it/s] 97% 889/918 [00:00<00:00, 4390.55it/s]100% 918/918 [00:00<00:00, 4436.67it/s]
Total scaffolds = 544 | train scaffolds = 442 | val scaffolds = 102 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/668 [00:00<?, ?it/s] 65% 435/668 [00:00<00:00, 4345.51it/s]100% 668/668 [00:00<00:00, 4418.86it/s]
Total scaffolds = 395 | train scaffolds = 322 | val scaffolds = 73 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/645 [00:00<?, ?it/s] 70% 449/645 [00:00<00:00, 4489.69it/s]100% 645/645 [00:00<00:00, 4505.24it/s]
Total scaffolds = 376 | train scaffolds = 301 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/722 [00:00<?, ?it/s] 60% 436/722 [00:00<00:00, 4354.56it/s]100% 722/722 [00:00<00:00, 4430.16it/s]
Total scaffolds = 422 | train scaffolds = 328 | val scaffolds = 94 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/988 [00:00<?, ?it/s] 45% 447/988 [00:00<00:00, 4460.35it/s] 94% 933/988 [00:00<00:00, 4572.32it/s]100% 988/988 [00:00<00:00, 4655.46it/s]
Total scaffolds = 603 | train scaffolds = 491 | val scaffolds = 112 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/240 [00:00<?, ?it/s]100% 240/240 [00:00<00:00, 5021.14it/s]
Total scaffolds = 210 | train scaffolds = 168 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/221 [00:00<?, ?it/s]100% 221/221 [00:00<00:00, 4416.82it/s]
Total scaffolds = 200 | train scaffolds = 156 | val scaffolds = 44 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/201 [00:00<?, ?it/s]100% 201/201 [00:00<00:00, 3683.23it/s]
Total scaffolds = 158 | train scaffolds = 120 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/535 [00:00<?, ?it/s] 86% 458/535 [00:00<00:00, 4579.68it/s]100% 535/535 [00:00<00:00, 4496.93it/s]
Total scaffolds = 442 | train scaffolds = 360 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/201 [00:00<?, ?it/s]100% 201/201 [00:00<00:00, 3751.58it/s]
Total scaffolds = 158 | train scaffolds = 120 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/230 [00:00<?, ?it/s]100% 230/230 [00:00<00:00, 4098.18it/s]
Total scaffolds = 193 | train scaffolds = 155 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/200 [00:00<?, ?it/s]100% 200/200 [00:00<00:00, 4376.77it/s]
Total scaffolds = 171 | train scaffolds = 136 | val scaffolds = 35 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/220 [00:00<?, ?it/s]100% 220/220 [00:00<00:00, 5012.61it/s]
Total scaffolds = 154 | train scaffolds = 127 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/620 [00:00<?, ?it/s] 63% 393/620 [00:00<00:00, 3924.61it/s]100% 620/620 [00:00<00:00, 3826.78it/s]
Total scaffolds = 483 | train scaffolds = 383 | val scaffolds = 100 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]))]
  0% 0/957 [00:00<?, ?it/s] 46% 442/957 [00:00<00:00, 4418.57it/s] 87% 829/957 [00:00<00:00, 4237.33it/s]100% 957/957 [00:00<00:00, 3870.79it/s]
Total scaffolds = 571 | train scaffolds = 423 | val scaffolds = 148 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
  0% 0/165 [00:00<?, ?it/s]100% 165/165 [00:00<00:00, 5030.49it/s]
Total scaffolds = 146 | train scaffolds = 119 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/230 [00:00<?, ?it/s]100% 230/230 [00:00<00:00, 4079.56it/s]
Total scaffolds = 162 | train scaffolds = 124 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/275 [00:00<?, ?it/s]100% 275/275 [00:00<00:00, 4684.66it/s]
Total scaffolds = 242 | train scaffolds = 188 | val scaffolds = 54 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/568 [00:00<?, ?it/s] 96% 545/568 [00:00<00:00, 5445.15it/s]100% 568/568 [00:00<00:00, 5419.90it/s]
Total scaffolds = 535 | train scaffolds = 428 | val scaffolds = 107 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/145 [00:00<?, ?it/s]100% 145/145 [00:00<00:00, 2823.59it/s]
Total scaffolds = 110 | train scaffolds = 85 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/416 [00:00<?, ?it/s]100% 416/416 [00:00<00:00, 5176.68it/s]
Total scaffolds = 289 | train scaffolds = 228 | val scaffolds = 61 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/791 [00:00<?, ?it/s] 60% 475/791 [00:00<00:00, 4744.09it/s]100% 791/791 [00:00<00:00, 4621.44it/s]
Total scaffolds = 630 | train scaffolds = 498 | val scaffolds = 132 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/165 [00:00<?, ?it/s]100% 165/165 [00:00<00:00, 5096.13it/s]
Total scaffolds = 146 | train scaffolds = 119 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/240 [00:00<?, ?it/s]100% 240/240 [00:00<00:00, 5081.46it/s]
Total scaffolds = 210 | train scaffolds = 168 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/343 [00:00<?, ?it/s]100% 343/343 [00:00<00:00, 4298.98it/s]
Total scaffolds = 305 | train scaffolds = 245 | val scaffolds = 60 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/213 [00:00<?, ?it/s]100% 213/213 [00:00<00:00, 4565.97it/s]
Total scaffolds = 189 | train scaffolds = 147 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/144 [00:00<?, ?it/s]100% 144/144 [00:00<00:00, 4766.52it/s]
Total scaffolds = 131 | train scaffolds = 104 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/396 [00:00<?, ?it/s]100% 396/396 [00:00<00:00, 4391.20it/s]
Total scaffolds = 343 | train scaffolds = 268 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/165 [00:00<?, ?it/s]100% 165/165 [00:00<00:00, 5065.36it/s]
Total scaffolds = 146 | train scaffolds = 119 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/213 [00:00<?, ?it/s]100% 213/213 [00:00<00:00, 4552.24it/s]
Total scaffolds = 189 | train scaffolds = 147 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/995 [00:00<?, ?it/s] 43% 432/995 [00:00<00:00, 4309.63it/s] 79% 788/995 [00:00<00:00, 4049.47it/s]100% 995/995 [00:00<00:00, 3712.87it/s]
Total scaffolds = 596 | train scaffolds = 466 | val scaffolds = 130 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/240 [00:00<?, ?it/s]100% 240/240 [00:00<00:00, 4902.35it/s]
Total scaffolds = 210 | train scaffolds = 168 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/519 [00:00<?, ?it/s] 97% 504/519 [00:00<00:00, 5030.06it/s]100% 519/519 [00:00<00:00, 4889.82it/s]
Total scaffolds = 358 | train scaffolds = 276 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/980 [00:00<?, ?it/s] 44% 432/980 [00:00<00:00, 4312.72it/s] 81% 793/980 [00:00<00:00, 4073.41it/s]100% 980/980 [00:00<00:00, 3698.07it/s]
Total scaffolds = 586 | train scaffolds = 464 | val scaffolds = 122 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/396 [00:00<?, ?it/s]100% 396/396 [00:00<00:00, 4395.79it/s]
Total scaffolds = 343 | train scaffolds = 268 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/579 [00:00<?, ?it/s] 76% 442/579 [00:00<00:00, 4403.59it/s]100% 579/579 [00:00<00:00, 4445.27it/s]
Total scaffolds = 354 | train scaffolds = 279 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/982 [00:00<?, ?it/s] 45% 440/982 [00:00<00:00, 4395.45it/s] 92% 901/982 [00:00<00:00, 4455.57it/s]100% 982/982 [00:00<00:00, 4498.52it/s]
Total scaffolds = 560 | train scaffolds = 451 | val scaffolds = 109 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/745 [00:00<?, ?it/s] 59% 439/745 [00:00<00:00, 4386.76it/s]100% 745/745 [00:00<00:00, 4464.22it/s]
Total scaffolds = 436 | train scaffolds = 369 | val scaffolds = 67 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/316 [00:00<?, ?it/s]100% 316/316 [00:00<00:00, 4419.89it/s]
Total scaffolds = 237 | train scaffolds = 189 | val scaffolds = 48 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/958 [00:00<?, ?it/s] 46% 437/958 [00:00<00:00, 4367.51it/s] 92% 882/958 [00:00<00:00, 4391.56it/s]100% 958/958 [00:00<00:00, 4402.52it/s]
Total scaffolds = 562 | train scaffolds = 426 | val scaffolds = 136 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/609 [00:00<?, ?it/s] 72% 437/609 [00:00<00:00, 4368.24it/s]100% 609/609 [00:00<00:00, 4416.36it/s]
Total scaffolds = 368 | train scaffolds = 283 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/314 [00:00<?, ?it/s]100% 314/314 [00:00<00:00, 4409.09it/s]
Total scaffolds = 241 | train scaffolds = 203 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/316 [00:00<?, ?it/s]100% 316/316 [00:00<00:00, 4380.14it/s]
Total scaffolds = 233 | train scaffolds = 191 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan,  0.,  0., nan,  0.,  0.,  1.,  0., nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/331 [00:00<?, ?it/s]100% 331/331 [00:00<00:00, 4392.18it/s]
Total scaffolds = 242 | train scaffolds = 189 | val scaffolds = 53 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/274 [00:00<?, ?it/s]100% 274/274 [00:00<00:00, 4241.93it/s]
Total scaffolds = 198 | train scaffolds = 152 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/432 [00:00<?, ?it/s]100% 432/432 [00:00<00:00, 4382.82it/s]
Total scaffolds = 265 | train scaffolds = 218 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/339 [00:00<?, ?it/s]100% 339/339 [00:00<00:00, 4544.53it/s]
Total scaffolds = 251 | train scaffolds = 194 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/523 [00:00<?, ?it/s] 84% 441/523 [00:00<00:00, 4405.02it/s]100% 523/523 [00:00<00:00, 4440.55it/s]
Total scaffolds = 324 | train scaffolds = 247 | val scaffolds = 77 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/322 [00:00<?, ?it/s]100% 322/322 [00:00<00:00, 4379.62it/s]
Total scaffolds = 231 | train scaffolds = 174 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/484 [00:00<?, ?it/s] 91% 439/484 [00:00<00:00, 4377.04it/s]100% 484/484 [00:00<00:00, 4397.25it/s]
Total scaffolds = 300 | train scaffolds = 229 | val scaffolds = 71 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/809 [00:00<?, ?it/s] 53% 427/809 [00:00<00:00, 4267.49it/s]100% 809/809 [00:00<00:00, 4292.31it/s]
Total scaffolds = 450 | train scaffolds = 359 | val scaffolds = 91 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/226 [00:00<?, ?it/s]100% 226/226 [00:00<00:00, 4549.18it/s]
Total scaffolds = 176 | train scaffolds = 141 | val scaffolds = 35 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/215 [00:00<?, ?it/s]100% 215/215 [00:00<00:00, 4585.08it/s]
Total scaffolds = 169 | train scaffolds = 135 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/209 [00:00<?, ?it/s]100% 209/209 [00:00<00:00, 4363.24it/s]
Total scaffolds = 172 | train scaffolds = 139 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/234 [00:00<?, ?it/s]100% 234/234 [00:00<00:00, 4663.11it/s]
Total scaffolds = 184 | train scaffolds = 145 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/131 [00:00<?, ?it/s]100% 131/131 [00:00<00:00, 3646.01it/s]
Total scaffolds = 98 | train scaffolds = 79 | val scaffolds = 19 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/937 [00:00<?, ?it/s] 50% 466/937 [00:00<00:00, 4656.65it/s]100% 937/937 [00:00<00:00, 4685.76it/s]
Total scaffolds = 780 | train scaffolds = 623 | val scaffolds = 157 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/363 [00:00<?, ?it/s]100% 363/363 [00:00<00:00, 6074.55it/s]
Total scaffolds = 260 | train scaffolds = 199 | val scaffolds = 61 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/373 [00:00<?, ?it/s]100% 373/373 [00:00<00:00, 5014.47it/s]
Total scaffolds = 318 | train scaffolds = 255 | val scaffolds = 63 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/230 [00:00<?, ?it/s]100% 230/230 [00:00<00:00, 4957.40it/s]
Total scaffolds = 157 | train scaffolds = 124 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/224 [00:00<?, ?it/s]100% 224/224 [00:00<00:00, 4746.34it/s]
Total scaffolds = 194 | train scaffolds = 160 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/177 [00:00<?, ?it/s]100% 177/177 [00:00<00:00, 4928.22it/s]
Total scaffolds = 149 | train scaffolds = 120 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/249 [00:00<?, ?it/s]100% 249/249 [00:00<00:00, 4649.28it/s]
Total scaffolds = 192 | train scaffolds = 153 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/233 [00:00<?, ?it/s]100% 233/233 [00:00<00:00, 4018.94it/s]
Total scaffolds = 165 | train scaffolds = 124 | val scaffolds = 41 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/323 [00:00<?, ?it/s]100% 323/323 [00:00<00:00, 5820.32it/s]
Total scaffolds = 294 | train scaffolds = 236 | val scaffolds = 58 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/338 [00:00<?, ?it/s]100% 338/338 [00:00<00:00, 4436.92it/s]
Total scaffolds = 269 | train scaffolds = 212 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/233 [00:00<?, ?it/s]100% 233/233 [00:00<00:00, 3996.86it/s]
Total scaffolds = 165 | train scaffolds = 124 | val scaffolds = 41 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/239 [00:00<?, ?it/s]100% 239/239 [00:00<00:00, 5037.46it/s]
Total scaffolds = 194 | train scaffolds = 155 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/251 [00:00<?, ?it/s]100% 251/251 [00:00<00:00, 3478.58it/s]
Total scaffolds = 214 | train scaffolds = 175 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/187 [00:00<?, ?it/s]100% 187/187 [00:00<00:00, 4093.78it/s]
Total scaffolds = 143 | train scaffolds = 111 | val scaffolds = 32 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/249 [00:00<?, ?it/s]100% 249/249 [00:00<00:00, 4556.20it/s]
Total scaffolds = 192 | train scaffolds = 153 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/369 [00:00<?, ?it/s]100% 369/369 [00:00<00:00, 4528.17it/s]
Total scaffolds = 302 | train scaffolds = 253 | val scaffolds = 49 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/1011 [00:00<?, ?it/s] 46% 467/1011 [00:00<00:00, 4666.46it/s] 87% 881/1011 [00:00<00:00, 4494.45it/s]100% 1011/1011 [00:00<00:00, 4388.56it/s]
Total scaffolds = 713 | train scaffolds = 578 | val scaffolds = 135 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/233 [00:00<?, ?it/s]100% 233/233 [00:00<00:00, 3949.23it/s]
Total scaffolds = 165 | train scaffolds = 124 | val scaffolds = 41 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/586 [00:00<?, ?it/s] 83% 487/586 [00:00<00:00, 4862.67it/s]100% 586/586 [00:00<00:00, 4889.19it/s]
Total scaffolds = 408 | train scaffolds = 319 | val scaffolds = 89 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/177 [00:00<?, ?it/s]100% 177/177 [00:00<00:00, 4876.42it/s]
Total scaffolds = 149 | train scaffolds = 120 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/290 [00:00<?, ?it/s]100% 290/290 [00:00<00:00, 4144.63it/s]
Total scaffolds = 232 | train scaffolds = 180 | val scaffolds = 52 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/129 [00:00<?, ?it/s]100% 129/129 [00:00<00:00, 4205.22it/s]
Total scaffolds = 108 | train scaffolds = 85 | val scaffolds = 23 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/290 [00:00<?, ?it/s]100% 290/290 [00:00<00:00, 5239.27it/s]
Total scaffolds = 220 | train scaffolds = 171 | val scaffolds = 49 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/233 [00:00<?, ?it/s]100% 233/233 [00:00<00:00, 3973.82it/s]
Total scaffolds = 165 | train scaffolds = 124 | val scaffolds = 41 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/237 [00:00<?, ?it/s]100% 237/237 [00:00<00:00, 4729.61it/s]
Total scaffolds = 152 | train scaffolds = 127 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/350 [00:00<?, ?it/s]100% 350/350 [00:00<00:00, 8312.70it/s]
Total scaffolds = 135 | train scaffolds = 80 | val scaffolds = 55 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.65517241,
              nan,        nan, 0.66666667,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 29,  0,  0,
       27,  0,  0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.82352941,
       1.        ,        nan, 0.8125    ,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 17,  1,  0,
       16,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.75,  nan,  nan, 1.  ,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 3, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]))]
  0% 0/224 [00:00<?, ?it/s]100% 224/224 [00:00<00:00, 4746.29it/s]
Total scaffolds = 194 | train scaffolds = 160 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/159 [00:00<?, ?it/s]100% 159/159 [00:00<00:00, 3792.06it/s]
Total scaffolds = 115 | train scaffolds = 87 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/290 [00:00<?, ?it/s]100% 290/290 [00:00<00:00, 5203.72it/s]
Total scaffolds = 220 | train scaffolds = 171 | val scaffolds = 49 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/365 [00:00<?, ?it/s]100% 365/365 [00:00<00:00, 5216.10it/s]
Total scaffolds = 314 | train scaffolds = 255 | val scaffolds = 59 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/338 [00:00<?, ?it/s]100% 338/338 [00:00<00:00, 4358.43it/s]
Total scaffolds = 269 | train scaffolds = 212 | val scaffolds = 57 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/343 [00:00<?, ?it/s]100% 343/343 [00:00<00:00, 4280.87it/s]
Total scaffolds = 305 | train scaffolds = 245 | val scaffolds = 60 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/152 [00:00<?, ?it/s]100% 152/152 [00:00<00:00, 4101.61it/s]
Total scaffolds = 121 | train scaffolds = 93 | val scaffolds = 28 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/408 [00:00<?, ?it/s]100% 408/408 [00:00<00:00, 4578.16it/s]
Total scaffolds = 260 | train scaffolds = 208 | val scaffolds = 52 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/389 [00:00<?, ?it/s]100% 389/389 [00:00<00:00, 3895.98it/s]
Total scaffolds = 351 | train scaffolds = 278 | val scaffolds = 73 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/239 [00:00<?, ?it/s]100% 239/239 [00:00<00:00, 4959.23it/s]
Total scaffolds = 194 | train scaffolds = 155 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/400 [00:00<?, ?it/s]100% 400/400 [00:00<00:00, 5206.68it/s]
Total scaffolds = 333 | train scaffolds = 270 | val scaffolds = 63 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/777 [00:00<?, ?it/s] 60% 467/777 [00:00<00:00, 4669.85it/s]100% 777/777 [00:00<00:00, 4585.86it/s]
Total scaffolds = 619 | train scaffolds = 497 | val scaffolds = 122 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/343 [00:00<?, ?it/s]100% 343/343 [00:00<00:00, 4288.76it/s]
Total scaffolds = 305 | train scaffolds = 245 | val scaffolds = 60 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/464 [00:00<?, ?it/s] 81% 377/464 [00:00<00:00, 3766.32it/s]100% 464/464 [00:00<00:00, 3687.39it/s]
Total scaffolds = 384 | train scaffolds = 306 | val scaffolds = 78 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/847 [00:00<?, ?it/s] 65% 547/847 [00:00<00:00, 5463.01it/s]100% 847/847 [00:00<00:00, 5395.55it/s]
Total scaffolds = 625 | train scaffolds = 487 | val scaffolds = 138 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/777 [00:00<?, ?it/s] 60% 467/777 [00:00<00:00, 4668.79it/s]100% 777/777 [00:00<00:00, 4638.30it/s]
Total scaffolds = 619 | train scaffolds = 497 | val scaffolds = 122 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/165 [00:00<?, ?it/s]100% 165/165 [00:00<00:00, 5074.76it/s]
Total scaffolds = 146 | train scaffolds = 119 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/171 [00:00<?, ?it/s]100% 171/171 [00:00<00:00, 4658.70it/s]
Total scaffolds = 128 | train scaffolds = 96 | val scaffolds = 32 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/898 [00:00<?, ?it/s] 54% 483/898 [00:00<00:00, 4827.09it/s]100% 898/898 [00:00<00:00, 4688.37it/s]
Total scaffolds = 789 | train scaffolds = 628 | val scaffolds = 161 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/1011 [00:00<?, ?it/s] 46% 465/1011 [00:00<00:00, 4642.27it/s] 88% 885/1011 [00:00<00:00, 4499.32it/s]100% 1011/1011 [00:00<00:00, 4409.71it/s]
Total scaffolds = 713 | train scaffolds = 578 | val scaffolds = 135 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/228 [00:00<?, ?it/s]100% 228/228 [00:00<00:00, 5364.13it/s]
Total scaffolds = 194 | train scaffolds = 154 | val scaffolds = 40 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/207 [00:00<?, ?it/s]100% 207/207 [00:00<00:00, 4154.98it/s]
Total scaffolds = 144 | train scaffolds = 110 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/174 [00:00<?, ?it/s]100% 174/174 [00:00<00:00, 5204.82it/s]
Total scaffolds = 164 | train scaffolds = 130 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/235 [00:00<?, ?it/s]100% 235/235 [00:00<00:00, 4821.44it/s]
Total scaffolds = 200 | train scaffolds = 156 | val scaffolds = 44 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/172 [00:00<?, ?it/s]100% 172/172 [00:00<00:00, 4038.99it/s]
Total scaffolds = 116 | train scaffolds = 86 | val scaffolds = 30 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/232 [00:00<?, ?it/s]100% 232/232 [00:00<00:00, 4846.56it/s]
Total scaffolds = 168 | train scaffolds = 137 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/395 [00:00<?, ?it/s]100% 395/395 [00:00<00:00, 4303.06it/s]
Total scaffolds = 298 | train scaffolds = 228 | val scaffolds = 70 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/414 [00:00<?, ?it/s]100% 414/414 [00:00<00:00, 5351.46it/s]
Total scaffolds = 306 | train scaffolds = 237 | val scaffolds = 69 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/247 [00:00<?, ?it/s]100% 247/247 [00:00<00:00, 4208.22it/s]
Total scaffolds = 221 | train scaffolds = 174 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/247 [00:00<?, ?it/s]100% 247/247 [00:00<00:00, 4197.92it/s]
Total scaffolds = 221 | train scaffolds = 174 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/413 [00:00<?, ?it/s]100% 413/413 [00:00<00:00, 5301.11it/s]
Total scaffolds = 306 | train scaffolds = 237 | val scaffolds = 69 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/249 [00:00<?, ?it/s]100% 249/249 [00:00<00:00, 4798.97it/s]
Total scaffolds = 214 | train scaffolds = 168 | val scaffolds = 46 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/204 [00:00<?, ?it/s]100% 204/204 [00:00<00:00, 4786.22it/s]
Total scaffolds = 179 | train scaffolds = 143 | val scaffolds = 36 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/174 [00:00<?, ?it/s]100% 174/174 [00:00<00:00, 5224.08it/s]
Total scaffolds = 164 | train scaffolds = 130 | val scaffolds = 34 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/704 [00:00<?, ?it/s] 61% 432/704 [00:00<00:00, 4316.68it/s]100% 704/704 [00:00<00:00, 4132.84it/s]
Total scaffolds = 570 | train scaffolds = 452 | val scaffolds = 118 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/141 [00:00<?, ?it/s]100% 141/141 [00:00<00:00, 4759.31it/s]
Total scaffolds = 56 | train scaffolds = 50 | val scaffolds = 6 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/236 [00:00<?, ?it/s]100% 236/236 [00:00<00:00, 4321.85it/s]
Total scaffolds = 109 | train scaffolds = 80 | val scaffolds = 29 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/226 [00:00<?, ?it/s]100% 226/226 [00:00<00:00, 5107.48it/s]
Total scaffolds = 153 | train scaffolds = 122 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]))]
  0% 0/130 [00:00<?, ?it/s]100% 130/130 [00:00<00:00, 4620.37it/s]
Total scaffolds = 104 | train scaffolds = 83 | val scaffolds = 21 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/226 [00:00<?, ?it/s]100% 226/226 [00:00<00:00, 5138.13it/s]
Total scaffolds = 153 | train scaffolds = 122 | val scaffolds = 31 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]))]
  0% 0/232 [00:00<?, ?it/s]100% 232/232 [00:00<00:00, 4240.23it/s]
Total scaffolds = 197 | train scaffolds = 158 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/248 [00:00<?, ?it/s]100% 248/248 [00:00<00:00, 4854.07it/s]
Total scaffolds = 217 | train scaffolds = 182 | val scaffolds = 35 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/755 [00:00<?, ?it/s] 63% 476/755 [00:00<00:00, 4752.79it/s]100% 755/755 [00:00<00:00, 4660.50it/s]
Total scaffolds = 612 | train scaffolds = 492 | val scaffolds = 120 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/133 [00:00<?, ?it/s]100% 133/133 [00:00<00:00, 4533.13it/s]
Total scaffolds = 113 | train scaffolds = 91 | val scaffolds = 22 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/635 [00:00<?, ?it/s] 67% 426/635 [00:00<00:00, 4250.65it/s]100% 635/635 [00:00<00:00, 3708.87it/s]
Total scaffolds = 498 | train scaffolds = 397 | val scaffolds = 101 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/235 [00:00<?, ?it/s]100% 235/235 [00:00<00:00, 3501.70it/s]
Total scaffolds = 203 | train scaffolds = 160 | val scaffolds = 43 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/189 [00:00<?, ?it/s]100% 189/189 [00:00<00:00, 4463.51it/s]
Total scaffolds = 176 | train scaffolds = 143 | val scaffolds = 33 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/238 [00:00<?, ?it/s]100% 238/238 [00:00<00:00, 5049.85it/s]
Total scaffolds = 190 | train scaffolds = 152 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/463 [00:00<?, ?it/s] 81% 374/463 [00:00<00:00, 3734.09it/s]100% 463/463 [00:00<00:00, 3463.50it/s]
Total scaffolds = 353 | train scaffolds = 274 | val scaffolds = 79 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/580 [00:00<?, ?it/s] 85% 494/580 [00:00<00:00, 4925.31it/s]100% 580/580 [00:00<00:00, 4393.57it/s]
Total scaffolds = 443 | train scaffolds = 352 | val scaffolds = 91 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/508 [00:00<?, ?it/s] 93% 473/508 [00:00<00:00, 4727.73it/s]100% 508/508 [00:00<00:00, 4499.43it/s]
Total scaffolds = 396 | train scaffolds = 310 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/198 [00:00<?, ?it/s]100% 198/198 [00:00<00:00, 3808.88it/s]
Total scaffolds = 193 | train scaffolds = 153 | val scaffolds = 40 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/220 [00:00<?, ?it/s]100% 220/220 [00:00<00:00, 5129.62it/s]
Total scaffolds = 149 | train scaffolds = 122 | val scaffolds = 27 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/388 [00:00<?, ?it/s] 95% 369/388 [00:00<00:00, 3683.97it/s]100% 388/388 [00:00<00:00, 3707.81it/s]
Total scaffolds = 280 | train scaffolds = 221 | val scaffolds = 59 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/494 [00:00<?, ?it/s] 93% 460/494 [00:00<00:00, 4596.11it/s]100% 494/494 [00:00<00:00, 4513.26it/s]
Total scaffolds = 417 | train scaffolds = 336 | val scaffolds = 81 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/153 [00:00<?, ?it/s]100% 153/153 [00:00<00:00, 4328.34it/s]
Total scaffolds = 134 | train scaffolds = 104 | val scaffolds = 30 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/238 [00:00<?, ?it/s]100% 238/238 [00:00<00:00, 4949.23it/s]
Total scaffolds = 190 | train scaffolds = 152 | val scaffolds = 38 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/330 [00:00<?, ?it/s]100% 330/330 [00:00<00:00, 5660.31it/s]
Total scaffolds = 237 | train scaffolds = 184 | val scaffolds = 53 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/254 [00:00<?, ?it/s]100% 254/254 [00:00<00:00, 6307.45it/s]
Total scaffolds = 130 | train scaffolds = 94 | val scaffolds = 36 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan, 0.        , 0.75      ,
              nan,        nan, 0.83333333,        nan,        nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 0, 0, 6, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0.,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/242 [00:00<?, ?it/s]100% 242/242 [00:00<00:00, 5385.90it/s]
Total scaffolds = 162 | train scaffolds = 132 | val scaffolds = 30 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]))]
  0% 0/198 [00:00<?, ?it/s]100% 198/198 [00:00<00:00, 4026.57it/s]
Total scaffolds = 193 | train scaffolds = 154 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/453 [00:00<?, ?it/s] 96% 437/453 [00:00<00:00, 4369.67it/s]100% 453/453 [00:00<00:00, 4355.16it/s]
Total scaffolds = 384 | train scaffolds = 309 | val scaffolds = 75 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/143 [00:00<?, ?it/s]100% 143/143 [00:00<00:00, 4781.76it/s]
Total scaffolds = 124 | train scaffolds = 99 | val scaffolds = 25 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/248 [00:00<?, ?it/s]100% 248/248 [00:00<00:00, 4030.73it/s]
Total scaffolds = 239 | train scaffolds = 189 | val scaffolds = 50 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/385 [00:00<?, ?it/s]100% 385/385 [00:00<00:00, 6208.96it/s]
Total scaffolds = 249 | train scaffolds = 184 | val scaffolds = 65 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 1. , nan, nan, 1. , nan, 0.5]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 2])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/451 [00:00<?, ?it/s]100% 451/451 [00:00<00:00, 4639.72it/s]
Total scaffolds = 366 | train scaffolds = 282 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/133 [00:00<?, ?it/s]100% 133/133 [00:00<00:00, 4536.63it/s]
Total scaffolds = 105 | train scaffolds = 81 | val scaffolds = 24 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/272 [00:00<?, ?it/s]100% 272/272 [00:00<00:00, 4779.13it/s]
Total scaffolds = 207 | train scaffolds = 165 | val scaffolds = 42 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/493 [00:00<?, ?it/s] 47% 232/493 [00:00<00:00, 2314.34it/s] 94% 462/493 [00:00<00:00, 2308.67it/s]100% 493/493 [00:00<00:00, 2303.64it/s]
Total scaffolds = 174 | train scaffolds = 129 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/248 [00:00<?, ?it/s]100% 248/248 [00:00<00:00, 4071.04it/s]
Total scaffolds = 239 | train scaffolds = 189 | val scaffolds = 50 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/134 [00:00<?, ?it/s]100% 134/134 [00:00<00:00, 3616.66it/s]
Total scaffolds = 91 | train scaffolds = 73 | val scaffolds = 18 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/581 [00:00<?, ?it/s] 73% 423/581 [00:00<00:00, 4220.03it/s]100% 581/581 [00:00<00:00, 3894.35it/s]
Total scaffolds = 483 | train scaffolds = 377 | val scaffolds = 106 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/451 [00:00<?, ?it/s]100% 449/451 [00:00<00:00, 4473.08it/s]100% 451/451 [00:00<00:00, 4440.19it/s]
Total scaffolds = 350 | train scaffolds = 276 | val scaffolds = 74 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/133 [00:00<?, ?it/s]100% 133/133 [00:00<00:00, 5199.87it/s]
Total scaffolds = 36 | train scaffolds = 20 | val scaffolds = 16 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/163 [00:00<?, ?it/s]100% 163/163 [00:00<00:00, 5221.26it/s]
Total scaffolds = 41 | train scaffolds = 30 | val scaffolds = 11 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/133 [00:00<?, ?it/s]100% 133/133 [00:00<00:00, 5250.13it/s]
Total scaffolds = 36 | train scaffolds = 20 | val scaffolds = 16 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/134 [00:00<?, ?it/s]100% 134/134 [00:00<00:00, 4089.71it/s]
Total scaffolds = 75 | train scaffolds = 57 | val scaffolds = 18 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/131 [00:00<?, ?it/s]100% 131/131 [00:00<00:00, 6092.18it/s]
Total scaffolds = 109 | train scaffolds = 85 | val scaffolds = 24 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  0., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan,  1., nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/385 [00:00<?, ?it/s]100% 385/385 [00:00<00:00, 5261.69it/s]
Total scaffolds = 251 | train scaffolds = 204 | val scaffolds = 47 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/669 [00:00<?, ?it/s] 90% 604/669 [00:00<00:00, 6036.44it/s]100% 669/669 [00:00<00:00, 5973.66it/s]
Total scaffolds = 622 | train scaffolds = 493 | val scaffolds = 129 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/234 [00:00<?, ?it/s]100% 234/234 [00:00<00:00, 4852.17it/s]
Total scaffolds = 175 | train scaffolds = 136 | val scaffolds = 39 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/829 [00:00<?, ?it/s] 68% 567/829 [00:00<00:00, 5658.78it/s]100% 829/829 [00:00<00:00, 4426.28it/s]
Total scaffolds = 487 | train scaffolds = 377 | val scaffolds = 110 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/814 [00:00<?, ?it/s] 70% 571/814 [00:00<00:00, 5707.61it/s]100% 814/814 [00:00<00:00, 4442.00it/s]
Total scaffolds = 480 | train scaffolds = 356 | val scaffolds = 124 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/828 [00:00<?, ?it/s] 68% 561/828 [00:00<00:00, 5566.16it/s]100% 828/828 [00:00<00:00, 4367.91it/s]
Total scaffolds = 486 | train scaffolds = 403 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/837 [00:00<?, ?it/s] 67% 561/837 [00:00<00:00, 5604.67it/s]100% 837/837 [00:00<00:00, 4356.59it/s]
Total scaffolds = 489 | train scaffolds = 406 | val scaffolds = 83 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/816 [00:00<?, ?it/s] 68% 558/816 [00:00<00:00, 5572.63it/s]100% 816/816 [00:00<00:00, 4403.87it/s]
Total scaffolds = 480 | train scaffolds = 398 | val scaffolds = 82 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/838 [00:00<?, ?it/s] 67% 559/838 [00:00<00:00, 5587.39it/s]100% 838/838 [00:00<00:00, 4346.24it/s]
Total scaffolds = 491 | train scaffolds = 406 | val scaffolds = 85 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/836 [00:00<?, ?it/s] 66% 552/836 [00:00<00:00, 5517.11it/s]100% 836/836 [00:00<00:00, 4333.10it/s]
Total scaffolds = 489 | train scaffolds = 403 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/828 [00:00<?, ?it/s] 68% 562/828 [00:00<00:00, 5551.29it/s]100% 828/828 [00:00<00:00, 4333.43it/s]
Total scaffolds = 488 | train scaffolds = 404 | val scaffolds = 84 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/742 [00:00<?, ?it/s] 59% 435/742 [00:00<00:00, 4345.16it/s]100% 742/742 [00:00<00:00, 4421.01it/s]
Total scaffolds = 435 | train scaffolds = 355 | val scaffolds = 80 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/738 [00:00<?, ?it/s] 59% 433/738 [00:00<00:00, 4325.36it/s]100% 738/738 [00:00<00:00, 4395.10it/s]
Total scaffolds = 431 | train scaffolds = 334 | val scaffolds = 97 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/757 [00:00<?, ?it/s] 57% 430/757 [00:00<00:00, 4287.87it/s]100% 757/757 [00:00<00:00, 4390.09it/s]
Total scaffolds = 428 | train scaffolds = 342 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/605 [00:00<?, ?it/s] 72% 438/605 [00:00<00:00, 4379.87it/s]100% 605/605 [00:00<00:00, 4424.22it/s]
Total scaffolds = 365 | train scaffolds = 279 | val scaffolds = 86 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/620 [00:00<?, ?it/s] 63% 389/620 [00:00<00:00, 3882.52it/s]100% 620/620 [00:00<00:00, 3775.68it/s]
Total scaffolds = 483 | train scaffolds = 383 | val scaffolds = 100 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        1., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]))]
  0% 0/290 [00:00<?, ?it/s]100% 290/290 [00:00<00:00, 4057.35it/s]
Total scaffolds = 232 | train scaffolds = 180 | val scaffolds = 52 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan,  1., nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/578 [00:00<?, ?it/s] 89% 514/578 [00:00<00:00, 5136.56it/s]100% 578/578 [00:00<00:00, 5074.74it/s]
Total scaffolds = 425 | train scaffolds = 329 | val scaffolds = 96 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/239 [00:00<?, ?it/s]100% 239/239 [00:00<00:00, 4272.39it/s]
Total scaffolds = 228 | train scaffolds = 183 | val scaffolds = 45 | test scaffolds = 0
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/546 [00:00<?, ?it/s] 84% 460/546 [00:00<00:00, 4598.64it/s]100% 546/546 [00:00<00:00, 4546.74it/s]
Total scaffolds = 448 | train scaffolds = 357 | val scaffolds = 40 | test scaffolds = 51
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/314 [00:00<?, ?it/s]100% 314/314 [00:00<00:00, 4864.11it/s]
Total scaffolds = 147 | train scaffolds = 125 | val scaffolds = 5 | test scaffolds = 17
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/390 [00:00<?, ?it/s]100% 390/390 [00:00<00:00, 5217.83it/s]
Total scaffolds = 364 | train scaffolds = 293 | val scaffolds = 32 | test scaffolds = 39
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/165 [00:00<?, ?it/s]100% 165/165 [00:00<00:00, 5832.64it/s]
Total scaffolds = 117 | train scaffolds = 91 | val scaffolds = 15 | test scaffolds = 11
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/138 [00:00<?, ?it/s]100% 138/138 [00:00<00:00, 5903.13it/s]
Total scaffolds = 104 | train scaffolds = 80 | val scaffolds = 12 | test scaffolds = 12
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/468 [00:00<?, ?it/s] 99% 462/468 [00:00<00:00, 4611.89it/s]100% 468/468 [00:00<00:00, 4593.79it/s]
Total scaffolds = 354 | train scaffolds = 282 | val scaffolds = 32 | test scaffolds = 40
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/144 [00:00<?, ?it/s]100% 144/144 [00:00<00:00, 4617.16it/s]
Total scaffolds = 117 | train scaffolds = 94 | val scaffolds = 9 | test scaffolds = 14
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/304 [00:00<?, ?it/s]100% 304/304 [00:00<00:00, 4917.37it/s]
Total scaffolds = 251 | train scaffolds = 198 | val scaffolds = 28 | test scaffolds = 25
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/222 [00:00<?, ?it/s]100% 222/222 [00:00<00:00, 3471.69it/s]
Total scaffolds = 182 | train scaffolds = 142 | val scaffolds = 20 | test scaffolds = 20
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/385 [00:00<?, ?it/s]100% 385/385 [00:00<00:00, 5608.41it/s]
Total scaffolds = 366 | train scaffolds = 294 | val scaffolds = 34 | test scaffolds = 38
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan,  0., nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/835 [00:00<?, ?it/s] 66% 551/835 [00:00<00:00, 5507.27it/s]100% 835/835 [00:00<00:00, 4291.06it/s]
Total scaffolds = 490 | train scaffolds = 364 | val scaffolds = 59 | test scaffolds = 67
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,  1.,
        1.,  1., nan, nan,  0., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/838 [00:00<?, ?it/s] 66% 553/838 [00:00<00:00, 5526.12it/s]100% 838/838 [00:00<00:00, 4293.83it/s]
Total scaffolds = 491 | train scaffolds = 376 | val scaffolds = 53 | test scaffolds = 62
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]))]
  0% 0/838 [00:00<?, ?it/s] 66% 552/838 [00:00<00:00, 5517.58it/s]100% 838/838 [00:00<00:00, 4300.49it/s]
Total scaffolds = 491 | train scaffolds = 375 | val scaffolds = 57 | test scaffolds = 59
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]))]
  0% 0/835 [00:00<?, ?it/s] 66% 554/835 [00:00<00:00, 5535.90it/s]100% 835/835 [00:00<00:00, 4302.91it/s]
Total scaffolds = 489 | train scaffolds = 362 | val scaffolds = 63 | test scaffolds = 64
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/839 [00:00<?, ?it/s] 67% 558/839 [00:00<00:00, 5572.43it/s]100% 839/839 [00:00<00:00, 4334.36it/s]
Total scaffolds = 491 | train scaffolds = 366 | val scaffolds = 68 | test scaffolds = 57
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan,  1.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/824 [00:00<?, ?it/s] 67% 556/824 [00:00<00:00, 5554.91it/s]100% 824/824 [00:00<00:00, 4309.35it/s]
Total scaffolds = 482 | train scaffolds = 354 | val scaffolds = 59 | test scaffolds = 69
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([ nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan,
        nan,  nan,  nan, 0.8 ,  nan,  nan, 0.75,  nan,  nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 4, 0, 0])), (array([nan, nan,  0., nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  0., nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, 0.5, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/692 [00:00<?, ?it/s] 63% 435/692 [00:00<00:00, 4344.15it/s]100% 692/692 [00:00<00:00, 4389.01it/s]
Total scaffolds = 410 | train scaffolds = 346 | val scaffolds = 31 | test scaffolds = 33
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/919 [00:00<?, ?it/s] 47% 432/919 [00:00<00:00, 4313.64it/s] 96% 886/919 [00:00<00:00, 4376.28it/s]100% 919/919 [00:00<00:00, 4425.12it/s]
Total scaffolds = 546 | train scaffolds = 443 | val scaffolds = 45 | test scaffolds = 58
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/165 [00:00<?, ?it/s]100% 165/165 [00:00<00:00, 5111.57it/s]
Total scaffolds = 134 | train scaffolds = 111 | val scaffolds = 9 | test scaffolds = 14
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
        0., nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/289 [00:00<?, ?it/s]100% 289/289 [00:00<00:00, 4442.99it/s]
Total scaffolds = 220 | train scaffolds = 171 | val scaffolds = 25 | test scaffolds = 24
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/273 [00:00<?, ?it/s]100% 273/273 [00:00<00:00, 5612.88it/s]
Total scaffolds = 240 | train scaffolds = 197 | val scaffolds = 18 | test scaffolds = 25
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/137 [00:00<?, ?it/s]100% 137/137 [00:00<00:00, 4563.23it/s]
Total scaffolds = 122 | train scaffolds = 95 | val scaffolds = 13 | test scaffolds = 14
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/224 [00:00<?, ?it/s]100% 224/224 [00:00<00:00, 5183.04it/s]
Total scaffolds = 139 | train scaffolds = 105 | val scaffolds = 19 | test scaffolds = 15
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1.,  1., nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 1, 0, 6, 0, 0])), (array([       nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan,        nan,
              nan,        nan,        nan,        nan, 0.6       ,
              nan,        nan, 0.66666667,        nan,        nan]), array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 10,  0,  0,
        9,  0,  0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan,  1., nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan,  1., nan, nan,  1., nan,  0.]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, 0.5, nan, nan, 0.5, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/248 [00:00<?, ?it/s]100% 248/248 [00:00<00:00, 4658.61it/s]
Total scaffolds = 217 | train scaffolds = 182 | val scaffolds = 21 | test scaffolds = 14
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/157 [00:00<?, ?it/s]100% 157/157 [00:00<00:00, 4801.25it/s]
Total scaffolds = 131 | train scaffolds = 104 | val scaffolds = 13 | test scaffolds = 14
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
  0% 0/225 [00:00<?, ?it/s]100% 225/225 [00:00<00:00, 6555.88it/s]
Total scaffolds = 69 | train scaffolds = 53 | val scaffolds = 7 | test scaffolds = 9
Label averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), (array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,
       nan, nan, nan, nan, nan, nan, nan]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))]
Building model 
Number of parameters = 354,901
MAML(
  (module): MoleculeModel(
    (sigmoid): Sigmoid()
    (encoder): MPN(
      (encoder): MPNEncoder(
        (dropout_layer): Dropout(p=0.0, inplace=False)
        (act_func): ReLU()
        (W_i): Linear(in_features=147, out_features=300, bias=False)
        (W_h): Linear(in_features=300, out_features=300, bias=False)
        (W_o): Linear(in_features=433, out_features=300, bias=True)
      )
    )
    (ffn): Sequential(
      (0): Dropout(p=0.0, inplace=False)
      (1): Linear(in_features=300, out_features=300, bias=True)
      (2): ReLU()
      (3): Dropout(p=0.0, inplace=False)
      (4): Linear(in_features=300, out_features=1, bias=True)
    )
  )
)
Moving maml model to cuda
  0% 0/30 [00:00<?, ?it/s]Epoch 0

  0% 0/19 [00:00<?, ?it/s][A

  0% 0/32 [00:00<?, ?it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  3% 1/32 [00:00<00:14,  2.15it/s][A[A

  6% 2/32 [00:00<00:12,  2.49it/s][A[A

  9% 3/32 [00:01<00:18,  1.59it/s][A[A

 12% 4/32 [00:02<00:13,  2.03it/s][A[A

 16% 5/32 [00:02<00:10,  2.52it/s][A[A

 19% 6/32 [00:02<00:08,  3.01it/s][A[A

 22% 7/32 [00:02<00:07,  3.48it/s][A[A

 25% 8/32 [00:02<00:06,  3.91it/s][A[A

 28% 9/32 [00:02<00:05,  4.35it/s][A[A

 31% 10/32 [00:03<00:04,  4.68it/s][A[A

 34% 11/32 [00:03<00:04,  4.39it/s][A[A

 38% 12/32 [00:03<00:04,  4.22it/s][A[A

 41% 13/32 [00:03<00:04,  4.08it/s][A[A

 44% 14/32 [00:04<00:04,  4.10it/s][A[A

 47% 15/32 [00:04<00:03,  4.56it/s][A[A

 50% 16/32 [00:05<00:08,  1.96it/s][A[A

 53% 17/32 [00:05<00:06,  2.37it/s][A[A

 56% 18/32 [00:05<00:04,  2.92it/s][A[A

 59% 19/32 [00:06<00:03,  3.36it/s][A[A

 62% 20/32 [00:06<00:03,  3.65it/s][A[A

 66% 21/32 [00:06<00:02,  4.15it/s][A[A

 69% 22/32 [00:06<00:02,  4.44it/s][A[A

 72% 23/32 [00:06<00:01,  4.65it/s][A[A

 75% 24/32 [00:07<00:01,  4.70it/s][A[A

 78% 25/32 [00:07<00:01,  4.96it/s][A[A

 81% 26/32 [00:07<00:01,  4.86it/s][A[A

 84% 27/32 [00:07<00:01,  4.86it/s][A[A

 88% 28/32 [00:07<00:00,  5.15it/s][A[A

 91% 29/32 [00:08<00:00,  4.91it/s][A[A

 94% 30/32 [00:08<00:00,  5.31it/s][A[A

 97% 31/32 [00:09<00:00,  1.99it/s][A[A

100% 32/32 [00:09<00:00,  2.44it/s][A[A100% 32/32 [00:09<00:00,  3.33it/s]
Meta loss on this task batch = 5.3222e-01, PNorm = 33.9939, GNorm = 0.1841

  5% 1/19 [00:10<03:06, 10.34s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.87it/s][A[A

  6% 2/32 [00:00<00:05,  5.52it/s][A[A

  9% 3/32 [00:00<00:05,  4.91it/s][A[A

 12% 4/32 [00:00<00:05,  4.75it/s][A[A

 16% 5/32 [00:01<00:05,  4.83it/s][A[A

 19% 6/32 [00:01<00:05,  4.85it/s][A[A

 22% 7/32 [00:01<00:04,  5.14it/s][A[A

 25% 8/32 [00:01<00:04,  5.45it/s][A[A

 28% 9/32 [00:01<00:04,  5.45it/s][A[A

 31% 10/32 [00:01<00:03,  5.62it/s][A[A

 34% 11/32 [00:03<00:10,  2.10it/s][A[A

 38% 12/32 [00:03<00:08,  2.45it/s][A[A

 41% 13/32 [00:03<00:06,  2.98it/s][A[A

 44% 14/32 [00:03<00:05,  3.38it/s][A[A

 47% 15/32 [00:03<00:04,  3.77it/s][A[A

 50% 16/32 [00:04<00:03,  4.08it/s][A[A

 53% 17/32 [00:04<00:03,  4.29it/s][A[A

 56% 18/32 [00:04<00:03,  4.37it/s][A[A

 59% 19/32 [00:04<00:02,  4.62it/s][A[A

 62% 20/32 [00:04<00:02,  4.55it/s][A[A

 66% 21/32 [00:05<00:02,  4.75it/s][A[A

 69% 22/32 [00:05<00:02,  4.92it/s][A[A

 72% 23/32 [00:05<00:01,  5.25it/s][A[A

 75% 24/32 [00:05<00:01,  5.52it/s][A[A

 78% 25/32 [00:05<00:01,  5.45it/s][A[A

 81% 26/32 [00:07<00:02,  2.04it/s][A[A

 84% 27/32 [00:07<00:02,  2.48it/s][A[A

 88% 28/32 [00:07<00:01,  2.88it/s][A[A

 91% 29/32 [00:07<00:00,  3.43it/s][A[A

 94% 30/32 [00:07<00:00,  3.80it/s][A[A

 97% 31/32 [00:08<00:00,  4.15it/s][A[A

100% 32/32 [00:08<00:00,  4.61it/s][A[A100% 32/32 [00:08<00:00,  3.91it/s]
Meta loss on this task batch = 5.2953e-01, PNorm = 34.0432, GNorm = 0.2330

 11% 2/19 [00:19<02:48,  9.91s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  6.08it/s][A[A

  6% 2/32 [00:00<00:05,  5.75it/s][A[A

  9% 3/32 [00:00<00:05,  5.25it/s][A[A

 12% 4/32 [00:00<00:05,  4.83it/s][A[A

 16% 5/32 [00:01<00:05,  4.86it/s][A[A

 19% 6/32 [00:01<00:05,  4.74it/s][A[A

 22% 7/32 [00:01<00:05,  4.76it/s][A[A

 25% 8/32 [00:01<00:04,  5.10it/s][A[A

 28% 9/32 [00:02<00:11,  2.02it/s][A[A

 31% 10/32 [00:03<00:09,  2.42it/s][A[A

 34% 11/32 [00:03<00:07,  2.81it/s][A[A

 38% 12/32 [00:03<00:06,  3.27it/s][A[A

 41% 13/32 [00:03<00:05,  3.68it/s][A[A

 44% 14/32 [00:03<00:04,  3.94it/s][A[A

 47% 15/32 [00:04<00:04,  4.07it/s][A[A

 50% 16/32 [00:04<00:03,  4.45it/s][A[A

 53% 17/32 [00:04<00:03,  4.51it/s][A[A

 56% 18/32 [00:04<00:02,  5.00it/s][A[A

 59% 19/32 [00:04<00:02,  5.29it/s][A[A

 62% 20/32 [00:04<00:02,  5.57it/s][A[A

 66% 21/32 [00:05<00:02,  5.47it/s][A[A

 69% 22/32 [00:06<00:04,  2.07it/s][A[A

 72% 23/32 [00:06<00:03,  2.46it/s][A[A

 75% 24/32 [00:06<00:02,  2.90it/s][A[A

 78% 25/32 [00:06<00:02,  3.39it/s][A[A

 81% 26/32 [00:07<00:01,  3.86it/s][A[A

 84% 27/32 [00:07<00:01,  4.03it/s][A[A

 88% 28/32 [00:07<00:00,  4.31it/s][A[A

 91% 29/32 [00:07<00:00,  4.38it/s][A[A

 94% 30/32 [00:07<00:00,  4.32it/s][A[A

 97% 31/32 [00:08<00:00,  4.79it/s][A[A

100% 32/32 [00:08<00:00,  4.89it/s][A[A100% 32/32 [00:08<00:00,  3.84it/s]
Meta loss on this task batch = 5.5689e-01, PNorm = 34.0714, GNorm = 1.0320

 16% 3/19 [00:28<02:34,  9.67s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.37it/s][A[A

  6% 2/32 [00:00<00:06,  4.51it/s][A[A

  9% 3/32 [00:00<00:06,  4.73it/s][A[A

 12% 4/32 [00:01<00:14,  1.93it/s][A[A

 16% 5/32 [00:02<00:11,  2.33it/s][A[A

 19% 6/32 [00:02<00:09,  2.71it/s][A[A

 22% 7/32 [00:02<00:07,  3.19it/s][A[A

 25% 8/32 [00:02<00:06,  3.43it/s][A[A

 28% 9/32 [00:02<00:05,  3.92it/s][A[A

 31% 10/32 [00:03<00:05,  4.08it/s][A[A

 34% 11/32 [00:03<00:04,  4.56it/s][A[A

 38% 12/32 [00:03<00:04,  4.84it/s][A[A

 41% 13/32 [00:03<00:03,  4.79it/s][A[A

 44% 14/32 [00:03<00:03,  4.85it/s][A[A

 47% 15/32 [00:04<00:03,  5.22it/s][A[A

 50% 16/32 [00:04<00:03,  5.17it/s][A[A

 53% 17/32 [00:05<00:07,  2.04it/s][A[A

 56% 18/32 [00:05<00:05,  2.48it/s][A[A

 59% 19/32 [00:05<00:04,  3.09it/s][A[A

 62% 20/32 [00:05<00:03,  3.49it/s][A[A

 66% 21/32 [00:06<00:02,  3.80it/s][A[A

 69% 22/32 [00:06<00:02,  3.93it/s][A[A

 72% 23/32 [00:06<00:02,  4.02it/s][A[A

 75% 24/32 [00:06<00:01,  4.22it/s][A[A

 78% 25/32 [00:06<00:01,  4.66it/s][A[A

 81% 26/32 [00:07<00:01,  4.90it/s][A[A

 84% 27/32 [00:07<00:01,  4.94it/s][A[A

 88% 28/32 [00:07<00:00,  5.08it/s][A[A

 91% 29/32 [00:07<00:00,  5.43it/s][A[A

 94% 30/32 [00:07<00:00,  5.14it/s][A[A

 97% 31/32 [00:09<00:00,  2.03it/s][A[A

100% 32/32 [00:09<00:00,  2.44it/s][A[A100% 32/32 [00:09<00:00,  3.43it/s]
Meta loss on this task batch = 5.7603e-01, PNorm = 34.1089, GNorm = 0.0842

 21% 4/19 [00:38<02:27,  9.81s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.85it/s][A[A

  6% 2/32 [00:00<00:06,  4.99it/s][A[A

  9% 3/32 [00:00<00:05,  4.92it/s][A[A

 12% 4/32 [00:00<00:06,  4.66it/s][A[A

 16% 5/32 [00:01<00:05,  4.77it/s][A[A

 19% 6/32 [00:01<00:05,  4.92it/s][A[A

 22% 7/32 [00:01<00:05,  4.83it/s][A[A

 25% 8/32 [00:01<00:04,  5.13it/s][A[A

 28% 9/32 [00:01<00:04,  5.11it/s][A[A

 31% 10/32 [00:01<00:04,  5.21it/s][A[A

 34% 11/32 [00:02<00:04,  5.23it/s][A[A

 38% 12/32 [00:02<00:03,  5.08it/s][A[A

 41% 13/32 [00:03<00:09,  1.98it/s][A[A

 44% 14/32 [00:03<00:07,  2.41it/s][A[A

 47% 15/32 [00:04<00:05,  2.84it/s][A[A

 50% 16/32 [00:04<00:04,  3.42it/s][A[A

 53% 17/32 [00:04<00:04,  3.57it/s][A[A

 56% 18/32 [00:04<00:03,  3.87it/s][A[A

 59% 19/32 [00:04<00:03,  3.99it/s][A[A

 62% 20/32 [00:05<00:02,  4.03it/s][A[A

 66% 21/32 [00:06<00:05,  1.87it/s][A[A

 69% 22/32 [00:06<00:04,  2.35it/s][A[A

 72% 23/32 [00:06<00:03,  2.79it/s][A[A

 75% 24/32 [00:06<00:02,  3.09it/s][A[A

 78% 25/32 [00:07<00:02,  3.42it/s][A[A

 81% 26/32 [00:07<00:01,  3.82it/s][A[A

 84% 27/32 [00:07<00:01,  4.15it/s][A[A

 88% 28/32 [00:07<00:00,  4.69it/s][A[A

 91% 29/32 [00:07<00:00,  4.64it/s][A[A

 94% 30/32 [00:08<00:00,  4.73it/s][A[A

 97% 31/32 [00:08<00:00,  4.84it/s][A[A

100% 32/32 [00:08<00:00,  4.78it/s][A[A100% 32/32 [00:08<00:00,  3.76it/s]
Meta loss on this task batch = 5.6104e-01, PNorm = 34.1529, GNorm = 0.5814

 26% 5/19 [00:47<02:15,  9.65s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.35it/s][A[A

  6% 2/32 [00:00<00:05,  5.36it/s][A[A

  9% 3/32 [00:00<00:05,  5.70it/s][A[A

 12% 4/32 [00:01<00:13,  2.03it/s][A[A

 16% 5/32 [00:02<00:11,  2.31it/s][A[A

 19% 6/32 [00:02<00:09,  2.69it/s][A[A

 22% 7/32 [00:02<00:07,  3.14it/s][A[A

 25% 8/32 [00:02<00:06,  3.54it/s][A[A

 28% 9/32 [00:02<00:06,  3.68it/s][A[A

 31% 10/32 [00:03<00:05,  3.88it/s][A[A

 34% 11/32 [00:03<00:05,  4.06it/s][A[A

 38% 12/32 [00:03<00:04,  4.27it/s][A[A

 41% 13/32 [00:03<00:04,  4.56it/s][A[A

 44% 14/32 [00:03<00:04,  4.43it/s][A[A

 47% 15/32 [00:05<00:08,  1.93it/s][A[A

 50% 16/32 [00:05<00:06,  2.31it/s][A[A

 53% 17/32 [00:05<00:05,  2.78it/s][A[A

 56% 18/32 [00:05<00:04,  3.12it/s][A[A

 59% 19/32 [00:06<00:03,  3.47it/s][A[A

 62% 20/32 [00:06<00:03,  3.87it/s][A[A

 66% 21/32 [00:06<00:02,  4.18it/s][A[A

 69% 22/32 [00:06<00:02,  4.31it/s][A[A

 72% 23/32 [00:06<00:02,  4.40it/s][A[A

 75% 24/32 [00:08<00:04,  1.94it/s][A[A

 78% 25/32 [00:08<00:02,  2.34it/s][A[A

 81% 26/32 [00:08<00:02,  2.83it/s][A[A

 84% 27/32 [00:08<00:01,  3.17it/s][A[A

 88% 28/32 [00:08<00:01,  3.28it/s][A[A

 91% 29/32 [00:09<00:00,  3.52it/s][A[A

 94% 30/32 [00:09<00:00,  3.92it/s][A[A

 97% 31/32 [00:09<00:00,  4.26it/s][A[A

100% 32/32 [00:09<00:00,  4.49it/s][A[A100% 32/32 [00:09<00:00,  3.27it/s]
Meta loss on this task batch = 4.7498e-01, PNorm = 34.2040, GNorm = 0.3987

 32% 6/19 [00:58<02:09,  9.93s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.59it/s][A[A

  6% 2/32 [00:00<00:05,  5.48it/s][A[A

  9% 3/32 [00:00<00:05,  5.17it/s][A[A

 12% 4/32 [00:01<00:14,  1.97it/s][A[A

 16% 5/32 [00:02<00:11,  2.43it/s][A[A

 19% 6/32 [00:02<00:09,  2.88it/s][A[A

 22% 7/32 [00:02<00:07,  3.25it/s][A[A

 25% 8/32 [00:02<00:06,  3.65it/s][A[A

 28% 9/32 [00:02<00:05,  3.99it/s][A[A

 31% 10/32 [00:03<00:05,  4.17it/s][A[A

 34% 11/32 [00:03<00:04,  4.29it/s][A[A

 38% 12/32 [00:03<00:04,  4.48it/s][A[A

 41% 13/32 [00:03<00:04,  4.51it/s][A[A

 44% 14/32 [00:03<00:03,  4.73it/s][A[A

 47% 15/32 [00:04<00:03,  4.66it/s][A[A

 50% 16/32 [00:04<00:03,  4.56it/s][A[A

 53% 17/32 [00:05<00:07,  1.92it/s][A[A

 56% 18/32 [00:05<00:06,  2.33it/s][A[A

 59% 19/32 [00:05<00:04,  2.75it/s][A[A

 62% 20/32 [00:06<00:03,  3.12it/s][A[A

 66% 21/32 [00:06<00:03,  3.46it/s][A[A

 69% 22/32 [00:06<00:02,  3.75it/s][A[A

 72% 23/32 [00:06<00:02,  3.94it/s][A[A

 75% 24/32 [00:07<00:01,  4.14it/s][A[A

 78% 25/32 [00:07<00:01,  4.37it/s][A[A

 81% 26/32 [00:07<00:01,  4.39it/s][A[A

 84% 27/32 [00:07<00:01,  4.47it/s][A[A

 88% 28/32 [00:07<00:00,  4.48it/s][A[A

 91% 29/32 [00:09<00:01,  1.93it/s][A[A

 94% 30/32 [00:09<00:00,  2.39it/s][A[A

 97% 31/32 [00:09<00:00,  2.79it/s][A[A

100% 32/32 [00:09<00:00,  3.17it/s][A[A100% 32/32 [00:09<00:00,  3.29it/s]
Meta loss on this task batch = 4.7746e-01, PNorm = 34.2555, GNorm = 0.0669

 37% 7/19 [01:08<02:01, 10.11s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.45it/s][A[A

  6% 2/32 [00:00<00:06,  4.34it/s][A[A

  9% 3/32 [00:00<00:06,  4.29it/s][A[A

 12% 4/32 [00:00<00:06,  4.23it/s][A[A

 16% 5/32 [00:01<00:06,  4.38it/s][A[A

 19% 6/32 [00:01<00:05,  4.35it/s][A[A

 22% 7/32 [00:01<00:05,  4.51it/s][A[A

 25% 8/32 [00:01<00:05,  4.34it/s][A[A

 28% 9/32 [00:03<00:12,  1.90it/s][A[A

 31% 10/32 [00:03<00:09,  2.30it/s][A[A

 34% 11/32 [00:03<00:07,  2.73it/s][A[A

 38% 12/32 [00:03<00:06,  3.16it/s][A[A

 41% 13/32 [00:03<00:05,  3.60it/s][A[A

 44% 14/32 [00:04<00:04,  3.90it/s][A[A

 47% 15/32 [00:04<00:04,  4.20it/s][A[A

 50% 16/32 [00:04<00:03,  4.32it/s][A[A

 53% 17/32 [00:04<00:03,  4.25it/s][A[A

 56% 18/32 [00:04<00:03,  4.37it/s][A[A

 59% 19/32 [00:06<00:06,  1.87it/s][A[A

 62% 20/32 [00:06<00:05,  2.30it/s][A[A

 66% 21/32 [00:06<00:04,  2.71it/s][A[A

 69% 22/32 [00:06<00:03,  3.05it/s][A[A

 72% 23/32 [00:07<00:02,  3.43it/s][A[A

 75% 24/32 [00:07<00:02,  3.60it/s][A[A

 78% 25/32 [00:07<00:01,  3.74it/s][A[A

 81% 26/32 [00:07<00:01,  3.85it/s][A[A

 84% 27/32 [00:07<00:01,  4.22it/s][A[A

 88% 28/32 [00:09<00:02,  1.93it/s][A[A

 91% 29/32 [00:09<00:01,  2.40it/s][A[A

 94% 30/32 [00:09<00:00,  2.86it/s][A[A

 97% 31/32 [00:09<00:00,  3.35it/s][A[A

100% 32/32 [00:09<00:00,  3.83it/s][A[A100% 32/32 [00:09<00:00,  3.24it/s]
Meta loss on this task batch = 3.5721e-01, PNorm = 34.2993, GNorm = 0.3050

 42% 8/19 [01:19<01:53, 10.28s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.32it/s][A[A

  6% 2/32 [00:00<00:05,  5.30it/s][A[A

  9% 3/32 [00:00<00:05,  5.39it/s][A[A

 12% 4/32 [00:00<00:05,  5.47it/s][A[A

 16% 5/32 [00:00<00:04,  5.46it/s][A[A

 19% 6/32 [00:01<00:04,  5.35it/s][A[A

 22% 7/32 [00:01<00:04,  5.34it/s][A[A

 25% 8/32 [00:01<00:04,  5.35it/s][A[A

 28% 9/32 [00:01<00:04,  5.50it/s][A[A

 31% 10/32 [00:01<00:03,  5.68it/s][A[A

 34% 11/32 [00:01<00:03,  5.83it/s][A[A

 38% 12/32 [00:03<00:09,  2.13it/s][A[A

 41% 13/32 [00:03<00:07,  2.59it/s][A[A

 44% 14/32 [00:03<00:05,  3.08it/s][A[A

 47% 15/32 [00:03<00:04,  3.54it/s][A[A

 50% 16/32 [00:03<00:04,  3.96it/s][A[A

 53% 17/32 [00:04<00:03,  4.30it/s][A[A

 56% 18/32 [00:04<00:03,  4.41it/s][A[A

 59% 19/32 [00:04<00:02,  4.67it/s][A[A

 62% 20/32 [00:04<00:02,  4.88it/s][A[A

 66% 21/32 [00:04<00:02,  5.03it/s][A[A

 69% 22/32 [00:05<00:01,  5.11it/s][A[A

 72% 23/32 [00:05<00:01,  5.01it/s][A[A

 75% 24/32 [00:05<00:01,  5.08it/s][A[A

 78% 25/32 [00:06<00:03,  1.98it/s][A[A

 81% 26/32 [00:06<00:02,  2.45it/s][A[A

 84% 27/32 [00:07<00:01,  2.96it/s][A[A

 88% 28/32 [00:07<00:01,  3.44it/s][A[A

 91% 29/32 [00:07<00:00,  3.90it/s][A[A

 94% 30/32 [00:07<00:00,  4.28it/s][A[A

 97% 31/32 [00:07<00:00,  4.48it/s][A[A

100% 32/32 [00:07<00:00,  4.79it/s][A[A100% 32/32 [00:07<00:00,  4.04it/s]
Meta loss on this task batch = 9.1014e-02, PNorm = 34.3507, GNorm = 0.4837

 47% 9/19 [01:28<01:38,  9.81s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.75it/s][A[A

  6% 2/32 [00:00<00:05,  5.45it/s][A[A

  9% 3/32 [00:00<00:05,  5.41it/s][A[A

 12% 4/32 [00:00<00:05,  5.50it/s][A[A

 16% 5/32 [00:00<00:05,  5.37it/s][A[A

 19% 6/32 [00:01<00:04,  5.48it/s][A[A

 22% 7/32 [00:01<00:04,  5.50it/s][A[A

 25% 8/32 [00:01<00:04,  5.46it/s][A[A

 28% 9/32 [00:02<00:10,  2.10it/s][A[A

 31% 10/32 [00:02<00:08,  2.62it/s][A[A

 34% 11/32 [00:02<00:06,  3.13it/s][A[A

 38% 12/32 [00:03<00:05,  3.62it/s][A[A

 41% 13/32 [00:03<00:04,  4.12it/s][A[A

 44% 14/32 [00:03<00:04,  4.50it/s][A[A

 47% 15/32 [00:03<00:03,  4.72it/s][A[A

 50% 16/32 [00:03<00:03,  4.60it/s][A[A

 53% 17/32 [00:04<00:03,  4.88it/s][A[A

 56% 18/32 [00:04<00:02,  5.04it/s][A[A

 59% 19/32 [00:04<00:02,  4.80it/s][A[A

 62% 20/32 [00:04<00:02,  4.99it/s][A[A

 66% 21/32 [00:05<00:05,  1.99it/s][A[A

 69% 22/32 [00:06<00:04,  2.44it/s][A[A

 72% 23/32 [00:06<00:03,  2.89it/s][A[A

 75% 24/32 [00:06<00:02,  3.35it/s][A[A

 78% 25/32 [00:06<00:01,  3.81it/s][A[A

 81% 26/32 [00:06<00:01,  3.94it/s][A[A

 84% 27/32 [00:07<00:01,  4.10it/s][A[A

 88% 28/32 [00:07<00:00,  4.21it/s][A[A

 91% 29/32 [00:07<00:00,  4.22it/s][A[A

 94% 30/32 [00:08<00:01,  1.88it/s][A[A

 97% 31/32 [00:08<00:00,  2.29it/s][A[A

100% 32/32 [00:09<00:00,  2.67it/s][A[A100% 32/32 [00:09<00:00,  3.47it/s]
Meta loss on this task batch = 1.8635e-01, PNorm = 34.4052, GNorm = 0.0718

 53% 10/19 [01:38<01:28,  9.87s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.28it/s][A[A

  6% 2/32 [00:00<00:06,  4.32it/s][A[A

  9% 3/32 [00:00<00:06,  4.30it/s][A[A

 12% 4/32 [00:00<00:06,  4.26it/s][A[A

 16% 5/32 [00:01<00:06,  4.31it/s][A[A

 19% 6/32 [00:01<00:06,  4.29it/s][A[A

 22% 7/32 [00:01<00:05,  4.32it/s][A[A

 25% 8/32 [00:02<00:12,  1.88it/s][A[A

 28% 9/32 [00:03<00:10,  2.25it/s][A[A

 31% 10/32 [00:03<00:08,  2.63it/s][A[A

 34% 11/32 [00:03<00:07,  2.98it/s][A[A

 38% 12/32 [00:03<00:06,  3.31it/s][A[A

 41% 13/32 [00:04<00:05,  3.56it/s][A[A

 44% 14/32 [00:04<00:04,  3.76it/s][A[A

 47% 15/32 [00:04<00:04,  3.91it/s][A[A

 50% 16/32 [00:04<00:03,  4.09it/s][A[A

 53% 17/32 [00:04<00:03,  4.16it/s][A[A

 56% 18/32 [00:06<00:07,  1.86it/s][A[A

 59% 19/32 [00:06<00:05,  2.24it/s][A[A

 62% 20/32 [00:06<00:04,  2.62it/s][A[A

 66% 21/32 [00:06<00:03,  2.95it/s][A[A

 69% 22/32 [00:07<00:03,  3.25it/s][A[A

 72% 23/32 [00:07<00:02,  3.51it/s][A[A

 75% 24/32 [00:07<00:02,  3.70it/s][A[A

 78% 25/32 [00:07<00:01,  3.86it/s][A[A

 81% 26/32 [00:09<00:03,  1.81it/s][A[A

 84% 27/32 [00:09<00:02,  2.19it/s][A[A

 88% 28/32 [00:09<00:01,  2.57it/s][A[A

 91% 29/32 [00:09<00:01,  2.92it/s][A[A

 94% 30/32 [00:09<00:00,  3.28it/s][A[A

 97% 31/32 [00:10<00:00,  3.55it/s][A[A

100% 32/32 [00:10<00:00,  3.78it/s][A[A100% 32/32 [00:10<00:00,  3.08it/s]
Meta loss on this task batch = 6.1057e-01, PNorm = 34.4497, GNorm = 0.6961

 58% 11/19 [01:49<01:22, 10.28s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.21it/s][A[A

  6% 2/32 [00:00<00:06,  4.29it/s][A[A

  9% 3/32 [00:00<00:06,  4.34it/s][A[A

 12% 4/32 [00:00<00:06,  4.37it/s][A[A

 16% 5/32 [00:02<00:14,  1.89it/s][A[A

 19% 6/32 [00:02<00:11,  2.27it/s][A[A

 22% 7/32 [00:02<00:09,  2.67it/s][A[A

 25% 8/32 [00:02<00:07,  3.03it/s][A[A

 28% 9/32 [00:03<00:06,  3.37it/s][A[A

 31% 10/32 [00:03<00:06,  3.63it/s][A[A

 34% 11/32 [00:03<00:05,  3.82it/s][A[A

 38% 12/32 [00:03<00:05,  3.95it/s][A[A

 41% 13/32 [00:03<00:04,  4.03it/s][A[A

 44% 14/32 [00:04<00:04,  4.09it/s][A[A

 47% 15/32 [00:04<00:04,  4.18it/s][A[A

 50% 16/32 [00:04<00:03,  4.30it/s][A[A

 53% 17/32 [00:04<00:03,  4.28it/s][A[A

 56% 18/32 [00:05<00:03,  4.33it/s][A[A

 59% 19/32 [00:05<00:03,  4.31it/s][A[A

 62% 20/32 [00:05<00:02,  4.30it/s][A[A

 66% 21/32 [00:05<00:02,  4.33it/s][A[A

 69% 22/32 [00:06<00:02,  4.34it/s][A[A

 72% 23/32 [00:07<00:04,  1.90it/s][A[A

 75% 24/32 [00:07<00:03,  2.29it/s][A[A

 78% 25/32 [00:07<00:02,  2.68it/s][A[A

 81% 26/32 [00:07<00:01,  3.04it/s][A[A

 84% 27/32 [00:08<00:01,  3.34it/s][A[A

 88% 28/32 [00:08<00:01,  3.60it/s][A[A

 91% 29/32 [00:08<00:00,  3.82it/s][A[A

 94% 30/32 [00:08<00:00,  3.96it/s][A[A

 97% 31/32 [00:09<00:00,  4.07it/s][A[A

100% 32/32 [00:10<00:00,  1.85it/s][A[A100% 32/32 [00:10<00:00,  3.11it/s]
Meta loss on this task batch = 6.4845e-01, PNorm = 34.4853, GNorm = 0.9514

 63% 12/19 [02:00<01:13, 10.53s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.39it/s][A[A

  6% 2/32 [00:00<00:06,  4.40it/s][A[A

  9% 3/32 [00:00<00:06,  4.42it/s][A[A

 12% 4/32 [00:00<00:06,  4.43it/s][A[A

 16% 5/32 [00:01<00:06,  4.48it/s][A[A

 19% 6/32 [00:01<00:05,  4.51it/s][A[A

 22% 7/32 [00:01<00:05,  4.49it/s][A[A

 25% 8/32 [00:01<00:05,  4.50it/s][A[A

 28% 9/32 [00:02<00:05,  4.45it/s][A[A

 31% 10/32 [00:02<00:04,  4.47it/s][A[A

 34% 11/32 [00:02<00:04,  4.47it/s][A[A

 38% 12/32 [00:02<00:04,  4.43it/s][A[A

 41% 13/32 [00:03<00:10,  1.87it/s][A[A

 44% 14/32 [00:04<00:08,  2.24it/s][A[A

 47% 15/32 [00:04<00:06,  2.63it/s][A[A

 50% 16/32 [00:04<00:05,  2.96it/s][A[A

 53% 17/32 [00:04<00:04,  3.29it/s][A[A

 56% 18/32 [00:05<00:03,  3.59it/s][A[A

 59% 19/32 [00:05<00:03,  3.76it/s][A[A

 62% 20/32 [00:05<00:03,  3.95it/s][A[A

 66% 21/32 [00:05<00:02,  4.08it/s][A[A

 69% 22/32 [00:06<00:02,  4.18it/s][A[A

 72% 23/32 [00:06<00:02,  4.21it/s][A[A

 75% 24/32 [00:06<00:01,  4.24it/s][A[A

 78% 25/32 [00:07<00:03,  1.88it/s][A[A

 81% 26/32 [00:07<00:02,  2.25it/s][A[A

 84% 27/32 [00:08<00:01,  2.65it/s][A[A

 88% 28/32 [00:08<00:01,  2.96it/s][A[A

 91% 29/32 [00:08<00:00,  3.22it/s][A[A

 94% 30/32 [00:08<00:00,  3.49it/s][A[A

 97% 31/32 [00:09<00:00,  3.72it/s][A[A

100% 32/32 [00:09<00:00,  3.88it/s][A[A100% 32/32 [00:09<00:00,  3.43it/s]
Meta loss on this task batch = 5.9660e-01, PNorm = 34.5237, GNorm = 0.2404

 68% 13/19 [02:10<01:02, 10.42s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:37,  1.22s/it][A[A

  6% 2/32 [00:01<00:27,  1.09it/s][A[A

  9% 3/32 [00:01<00:20,  1.41it/s][A[A

 12% 4/32 [00:01<00:15,  1.77it/s][A[A

 16% 5/32 [00:02<00:12,  2.15it/s][A[A

 19% 6/32 [00:02<00:10,  2.54it/s][A[A

 22% 7/32 [00:02<00:08,  2.88it/s][A[A

 25% 8/32 [00:02<00:07,  3.26it/s][A[A

 28% 9/32 [00:03<00:06,  3.54it/s][A[A

 31% 10/32 [00:03<00:05,  3.77it/s][A[A

 34% 11/32 [00:03<00:05,  3.93it/s][A[A

 38% 12/32 [00:03<00:04,  4.06it/s][A[A

 41% 13/32 [00:03<00:04,  4.17it/s][A[A

 44% 14/32 [00:04<00:04,  4.24it/s][A[A

 47% 15/32 [00:04<00:03,  4.29it/s][A[A

 50% 16/32 [00:04<00:03,  4.27it/s][A[A

 53% 17/32 [00:04<00:03,  4.32it/s][A[A

 56% 18/32 [00:05<00:03,  4.37it/s][A[A

 59% 19/32 [00:05<00:02,  4.49it/s][A[A

 62% 20/32 [00:05<00:02,  4.39it/s][A[A

 66% 21/32 [00:05<00:02,  4.40it/s][A[A

 69% 22/32 [00:05<00:02,  4.41it/s][A[A

 72% 23/32 [00:07<00:04,  1.90it/s][A[A

 75% 24/32 [00:07<00:03,  2.29it/s][A[A

 78% 25/32 [00:07<00:02,  2.66it/s][A[A

 81% 26/32 [00:07<00:02,  2.98it/s][A[A

 84% 27/32 [00:08<00:01,  3.30it/s][A[A

 88% 28/32 [00:08<00:01,  3.52it/s][A[A

 91% 29/32 [00:08<00:00,  3.77it/s][A[A

 94% 30/32 [00:08<00:00,  4.03it/s][A[A

 97% 31/32 [00:09<00:00,  4.10it/s][A[A

100% 32/32 [00:10<00:00,  1.83it/s][A[A100% 32/32 [00:10<00:00,  3.11it/s]
Meta loss on this task batch = 6.2301e-01, PNorm = 34.5660, GNorm = 0.6954

 74% 14/19 [02:21<00:53, 10.63s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.59it/s][A[A

  6% 2/32 [00:00<00:06,  4.36it/s][A[A

  9% 3/32 [00:00<00:06,  4.25it/s][A[A

 12% 4/32 [00:00<00:06,  4.30it/s][A[A

 16% 5/32 [00:01<00:06,  4.45it/s][A[A

 19% 6/32 [00:01<00:06,  4.30it/s][A[A

 22% 7/32 [00:01<00:05,  4.37it/s][A[A

 25% 8/32 [00:01<00:05,  4.59it/s][A[A

 28% 9/32 [00:03<00:12,  1.91it/s][A[A

 31% 10/32 [00:03<00:09,  2.30it/s][A[A

 34% 11/32 [00:03<00:07,  2.74it/s][A[A

 38% 12/32 [00:03<00:06,  3.13it/s][A[A

 41% 13/32 [00:03<00:05,  3.53it/s][A[A

 44% 14/32 [00:04<00:04,  3.79it/s][A[A

 47% 15/32 [00:04<00:04,  4.10it/s][A[A

 50% 16/32 [00:04<00:03,  4.30it/s][A[A

 53% 17/32 [00:04<00:03,  4.28it/s][A[A

 56% 18/32 [00:04<00:03,  4.36it/s][A[A

 59% 19/32 [00:05<00:02,  4.55it/s][A[A

 62% 20/32 [00:05<00:02,  4.48it/s][A[A

 66% 21/32 [00:05<00:02,  4.65it/s][A[A

 69% 22/32 [00:06<00:05,  1.94it/s][A[A

 72% 23/32 [00:07<00:03,  2.29it/s][A[A

 75% 24/32 [00:07<00:02,  2.73it/s][A[A

 78% 25/32 [00:07<00:02,  3.13it/s][A[A

 81% 26/32 [00:07<00:01,  3.33it/s][A[A

 84% 27/32 [00:07<00:01,  3.57it/s][A[A

 88% 28/32 [00:08<00:01,  3.80it/s][A[A

 91% 29/32 [00:08<00:00,  3.97it/s][A[A

 94% 30/32 [00:08<00:00,  4.04it/s][A[A

 97% 31/32 [00:09<00:00,  1.84it/s][A[A

100% 32/32 [00:10<00:00,  2.25it/s][A[A100% 32/32 [00:10<00:00,  3.17it/s]
Meta loss on this task batch = 5.1436e-01, PNorm = 34.6098, GNorm = 0.4377

 79% 15/19 [02:32<00:42, 10.72s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.38it/s][A[A

  6% 2/32 [00:00<00:06,  4.37it/s][A[A

  9% 3/32 [00:00<00:06,  4.41it/s][A[A

 12% 4/32 [00:00<00:06,  4.40it/s][A[A

 16% 5/32 [00:01<00:06,  4.39it/s][A[A

 19% 6/32 [00:01<00:05,  4.36it/s][A[A

 22% 7/32 [00:01<00:05,  4.44it/s][A[A

 25% 8/32 [00:01<00:05,  4.43it/s][A[A

 28% 9/32 [00:02<00:05,  4.39it/s][A[A

 31% 10/32 [00:02<00:05,  4.34it/s][A[A

 34% 11/32 [00:03<00:11,  1.88it/s][A[A

 38% 12/32 [00:03<00:08,  2.28it/s][A[A

 41% 13/32 [00:03<00:07,  2.69it/s][A[A

 44% 14/32 [00:04<00:05,  3.09it/s][A[A

 47% 15/32 [00:04<00:04,  3.43it/s][A[A

 50% 16/32 [00:04<00:04,  3.71it/s][A[A

 53% 17/32 [00:04<00:03,  3.87it/s][A[A

 56% 18/32 [00:05<00:03,  4.26it/s][A[A

 59% 19/32 [00:05<00:03,  4.32it/s][A[A

 62% 20/32 [00:06<00:06,  1.90it/s][A[A

 66% 21/32 [00:06<00:04,  2.31it/s][A[A

 69% 22/32 [00:06<00:03,  2.75it/s][A[A

 72% 23/32 [00:07<00:02,  3.08it/s][A[A

 75% 24/32 [00:07<00:02,  3.27it/s][A[A

 78% 25/32 [00:07<00:01,  3.69it/s][A[A

 81% 26/32 [00:07<00:01,  3.89it/s][A[A

 84% 27/32 [00:08<00:01,  3.90it/s][A[A

 88% 28/32 [00:08<00:00,  4.10it/s][A[A

 91% 29/32 [00:08<00:00,  3.96it/s][A[A

 94% 30/32 [00:09<00:01,  1.84it/s][A[A

 97% 31/32 [00:09<00:00,  2.22it/s][A[A

100% 32/32 [00:10<00:00,  2.57it/s][A[A100% 32/32 [00:10<00:00,  3.13it/s]
Meta loss on this task batch = 5.6442e-01, PNorm = 34.6537, GNorm = 0.2911

 84% 16/19 [02:43<00:32, 10.82s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.05it/s][A[A

  6% 2/32 [00:00<00:07,  4.02it/s][A[A

  9% 3/32 [00:00<00:06,  4.19it/s][A[A

 12% 4/32 [00:00<00:06,  4.40it/s][A[A

 16% 5/32 [00:01<00:06,  4.27it/s][A[A

 19% 6/32 [00:02<00:13,  1.90it/s][A[A

 22% 7/32 [00:02<00:10,  2.32it/s][A[A

 25% 8/32 [00:02<00:09,  2.63it/s][A[A

 28% 9/32 [00:03<00:07,  3.01it/s][A[A

 31% 10/32 [00:03<00:06,  3.61it/s][A[A

 34% 11/32 [00:03<00:05,  3.87it/s][A[A

 38% 12/32 [00:03<00:04,  4.04it/s][A[A

 41% 13/32 [00:03<00:04,  4.23it/s][A[A

 44% 14/32 [00:04<00:04,  4.36it/s][A[A

 47% 15/32 [00:04<00:03,  4.38it/s][A[A

 50% 16/32 [00:04<00:03,  4.37it/s][A[A

 53% 17/32 [00:05<00:07,  1.91it/s][A[A

 56% 18/32 [00:05<00:06,  2.31it/s][A[A

 59% 19/32 [00:06<00:04,  2.62it/s][A[A

 62% 20/32 [00:06<00:03,  3.02it/s][A[A

 66% 21/32 [00:06<00:03,  3.41it/s][A[A

 69% 22/32 [00:06<00:02,  3.70it/s][A[A

 72% 23/32 [00:07<00:02,  3.88it/s][A[A

 75% 24/32 [00:07<00:02,  3.86it/s][A[A

 78% 25/32 [00:07<00:01,  4.14it/s][A[A

 81% 26/32 [00:07<00:01,  4.27it/s][A[A

 84% 27/32 [00:07<00:01,  4.50it/s][A[A

 88% 28/32 [00:08<00:00,  4.62it/s][A[A

 91% 29/32 [00:09<00:01,  1.93it/s][A[A

 94% 30/32 [00:09<00:00,  2.30it/s][A[A

 97% 31/32 [00:09<00:00,  2.77it/s][A[A

100% 32/32 [00:10<00:00,  3.11it/s][A[A100% 32/32 [00:10<00:00,  3.19it/s]
Meta loss on this task batch = 4.5551e-01, PNorm = 34.6962, GNorm = 0.2669

 89% 17/19 [02:54<00:21, 10.83s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.19it/s][A[A

  6% 2/32 [00:00<00:05,  5.05it/s][A[A

  9% 3/32 [00:00<00:05,  4.84it/s][A[A

 12% 4/32 [00:00<00:05,  4.73it/s][A[A

 16% 5/32 [00:01<00:05,  4.58it/s][A[A

 19% 6/32 [00:01<00:05,  4.68it/s][A[A

 22% 7/32 [00:02<00:12,  1.92it/s][A[A

 25% 8/32 [00:02<00:10,  2.29it/s][A[A

 28% 9/32 [00:02<00:08,  2.73it/s][A[A

 31% 10/32 [00:03<00:07,  3.13it/s][A[A

 34% 11/32 [00:03<00:06,  3.48it/s][A[A

 38% 12/32 [00:03<00:05,  3.88it/s][A[A

 41% 13/32 [00:03<00:04,  3.96it/s][A[A

 44% 14/32 [00:04<00:04,  4.26it/s][A[A

 47% 15/32 [00:04<00:04,  4.18it/s][A[A

 50% 16/32 [00:05<00:08,  1.89it/s][A[A

 53% 17/32 [00:05<00:06,  2.34it/s][A[A

 56% 18/32 [00:05<00:05,  2.76it/s][A[A

 59% 19/32 [00:06<00:04,  3.12it/s][A[A

 62% 20/32 [00:06<00:03,  3.45it/s][A[A

 66% 21/32 [00:06<00:02,  3.70it/s][A[A

 69% 22/32 [00:06<00:02,  4.07it/s][A[A

 72% 23/32 [00:06<00:02,  4.01it/s][A[A

 75% 24/32 [00:07<00:02,  3.89it/s][A[A

 78% 25/32 [00:07<00:01,  4.10it/s][A[A

 81% 26/32 [00:07<00:01,  4.27it/s][A[A

 84% 27/32 [00:07<00:01,  4.19it/s][A[A

 88% 28/32 [00:09<00:02,  1.87it/s][A[A

 91% 29/32 [00:09<00:01,  2.27it/s][A[A

 94% 30/32 [00:09<00:00,  2.63it/s][A[A

 97% 31/32 [00:09<00:00,  3.06it/s][A[A

100% 32/32 [00:10<00:00,  3.27it/s][A[A100% 32/32 [00:10<00:00,  3.18it/s]
Meta loss on this task batch = 5.1385e-01, PNorm = 34.7361, GNorm = 0.1507

 95% 18/19 [03:05<00:10, 10.84s/it][A

  0% 0/23 [00:00<?, ?it/s][A[A

  4% 1/23 [00:00<00:05,  4.23it/s][A[A

  9% 2/23 [00:00<00:04,  4.40it/s][A[A

 13% 3/23 [00:00<00:04,  4.49it/s][A[A

 17% 4/23 [00:00<00:04,  4.62it/s][A[A

 22% 5/23 [00:02<00:09,  1.97it/s][A[A

 26% 6/23 [00:02<00:07,  2.39it/s][A[A

 30% 7/23 [00:02<00:05,  2.77it/s][A[A

 35% 8/23 [00:02<00:04,  3.11it/s][A[A

 39% 9/23 [00:02<00:03,  3.64it/s][A[A

 43% 10/23 [00:03<00:03,  3.77it/s][A[A

 48% 11/23 [00:03<00:02,  4.14it/s][A[A

 52% 12/23 [00:03<00:02,  4.23it/s][A[A

 57% 13/23 [00:03<00:02,  4.47it/s][A[A

 61% 14/23 [00:03<00:02,  4.46it/s][A[A

 65% 15/23 [00:05<00:04,  1.77it/s][A[A

 70% 16/23 [00:05<00:03,  2.12it/s][A[A

 74% 17/23 [00:05<00:02,  2.50it/s][A[A

 78% 18/23 [00:06<00:01,  2.82it/s][A[A

 83% 19/23 [00:06<00:01,  3.18it/s][A[A

 87% 20/23 [00:06<00:00,  3.64it/s][A[A

 91% 21/23 [00:06<00:00,  4.02it/s][A[A

 96% 22/23 [00:06<00:00,  4.40it/s][A[A

100% 23/23 [00:07<00:00,  4.47it/s][A[A100% 23/23 [00:07<00:00,  3.27it/s]
Meta loss on this task batch = 4.5879e-01, PNorm = 34.7679, GNorm = 0.4177

100% 19/19 [03:13<00:00,  9.88s/it][A100% 19/19 [03:13<00:00, 10.17s/it]
Took 193.27416253089905 seconds to complete one epoch of meta training

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/20 [00:00<?, ?it/s][A[A


  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 31.62it/s]


  5% 1/20 [00:00<00:03,  6.03it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 21.00it/s]


 10% 2/20 [00:00<00:03,  5.34it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:01<00:00,  1.82it/s][A[A[A100% 3/3 [00:01<00:00,  2.64it/s]


 15% 3/20 [00:01<00:08,  1.96it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 16.16it/s]


 20% 4/20 [00:01<00:06,  2.36it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 15.31it/s][A[A[A100% 4/4 [00:00<00:00, 17.71it/s]


 25% 5/20 [00:02<00:06,  2.43it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.45it/s][A[A[A100% 4/4 [00:00<00:00, 16.16it/s]


 30% 6/20 [00:02<00:05,  2.46it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.91it/s][A[A[A100% 4/4 [00:00<00:00, 20.76it/s]


 35% 7/20 [00:03<00:05,  2.55it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 17.70it/s][A[A[A100% 4/4 [00:00<00:00, 20.13it/s]


 40% 8/20 [00:03<00:04,  2.63it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.85it/s][A[A[A100% 4/4 [00:00<00:00, 23.59it/s]


 45% 9/20 [00:04<00:07,  1.51it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 17.95it/s][A[A[A100% 4/4 [00:00<00:00, 19.98it/s]


 50% 10/20 [00:05<00:05,  1.75it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 17.01it/s][A[A[A100% 4/4 [00:00<00:00, 19.56it/s]


 55% 11/20 [00:05<00:04,  1.94it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.54it/s][A[A[A100% 4/4 [00:00<00:00, 23.00it/s]


 60% 12/20 [00:05<00:03,  2.19it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 15.44it/s][A[A[A100% 3/3 [00:00<00:00, 14.67it/s]


 65% 13/20 [00:06<00:03,  2.33it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.97it/s][A[A[A100% 3/3 [00:00<00:00, 14.57it/s]


 70% 14/20 [00:06<00:02,  2.44it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 14.69it/s][A[A[A100% 4/4 [00:00<00:00, 19.15it/s]


 75% 15/20 [00:06<00:01,  2.52it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.89it/s][A[A[A100% 3/3 [00:00<00:00, 18.40it/s]


 80% 16/20 [00:07<00:01,  2.68it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 12.42it/s][A[A[A100% 3/3 [00:00<00:00, 15.34it/s]


 85% 17/20 [00:08<00:02,  1.49it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 22.48it/s]


 90% 18/20 [00:08<00:01,  1.82it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 15.93it/s][A[A[A100% 3/3 [00:00<00:00, 20.31it/s]


 95% 19/20 [00:09<00:00,  2.09it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 13.63it/s]


100% 20/20 [00:09<00:00,  2.46it/s][A[A100% 20/20 [00:09<00:00,  2.14it/s]

100% 1/1 [00:09<00:00,  9.36s/it][A100% 1/1 [00:09<00:00,  9.36s/it]
Took 202.6389331817627 seconds to complete one epoch of meta training and validating
Meta validation score auc = 0.546103
Found better MAML checkpoint after meta validation, saving now
  3% 1/30 [03:22<1:37:57, 202.68s/it]Epoch 1

  0% 0/19 [00:00<?, ?it/s][A

  0% 0/32 [00:00<?, ?it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  3% 1/32 [00:00<00:04,  6.21it/s][A[A

  6% 2/32 [00:00<00:05,  5.63it/s][A[A

  9% 3/32 [00:00<00:05,  5.77it/s][A[A

 12% 4/32 [00:00<00:04,  6.09it/s][A[A

 16% 5/32 [00:00<00:04,  6.46it/s][A[A

 19% 6/32 [00:00<00:04,  6.25it/s][A[A

 22% 7/32 [00:02<00:11,  2.20it/s][A[A

 25% 8/32 [00:02<00:08,  2.67it/s][A[A

 28% 9/32 [00:02<00:06,  3.37it/s][A[A

 31% 10/32 [00:02<00:05,  4.08it/s][A[A

 34% 11/32 [00:02<00:04,  4.37it/s][A[A

 38% 12/32 [00:02<00:04,  4.31it/s][A[A

 41% 13/32 [00:03<00:04,  4.35it/s][A[A

 44% 14/32 [00:03<00:04,  4.38it/s][A[A

 47% 15/32 [00:03<00:03,  5.00it/s][A[A

 50% 16/32 [00:03<00:03,  5.05it/s][A[A

 53% 17/32 [00:03<00:02,  5.19it/s][A[A

 56% 18/32 [00:04<00:02,  5.36it/s][A[A

 59% 19/32 [00:04<00:02,  5.68it/s][A[A

 62% 20/32 [00:04<00:02,  5.61it/s][A[A

 66% 21/32 [00:04<00:01,  6.29it/s][A[A

 69% 22/32 [00:04<00:01,  6.11it/s][A[A

 72% 23/32 [00:04<00:01,  6.40it/s][A[A

 75% 24/32 [00:05<00:01,  6.08it/s][A[A

 78% 25/32 [00:06<00:03,  2.15it/s][A[A

 81% 26/32 [00:06<00:02,  2.60it/s][A[A

 84% 27/32 [00:06<00:01,  3.21it/s][A[A

 88% 28/32 [00:06<00:01,  3.80it/s][A[A

 91% 29/32 [00:06<00:00,  3.98it/s][A[A

 94% 30/32 [00:07<00:00,  4.86it/s][A[A

 97% 31/32 [00:07<00:00,  5.18it/s][A[A

100% 32/32 [00:07<00:00,  5.43it/s][A[A100% 32/32 [00:07<00:00,  4.34it/s]
Meta loss on this task batch = 5.8518e-01, PNorm = 34.7907, GNorm = 0.8643

  5% 1/19 [00:08<02:24,  8.06s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:03,  8.65it/s][A[A

  6% 2/32 [00:00<00:03,  8.22it/s][A[A

  9% 3/32 [00:00<00:04,  7.23it/s][A[A

 12% 4/32 [00:00<00:04,  6.52it/s][A[A

 16% 5/32 [00:00<00:04,  6.06it/s][A[A

 19% 6/32 [00:00<00:04,  6.36it/s][A[A

 22% 7/32 [00:01<00:03,  6.69it/s][A[A

 25% 8/32 [00:01<00:03,  6.65it/s][A[A

 28% 9/32 [00:01<00:03,  6.64it/s][A[A

 31% 10/32 [00:01<00:03,  6.92it/s][A[A

 34% 11/32 [00:01<00:02,  7.17it/s][A[A

 38% 12/32 [00:01<00:02,  6.68it/s][A[A

 41% 13/32 [00:01<00:02,  6.86it/s][A[A

 44% 14/32 [00:02<00:02,  6.45it/s][A[A

 47% 15/32 [00:03<00:07,  2.21it/s][A[A

 50% 16/32 [00:03<00:05,  2.72it/s][A[A

 53% 17/32 [00:03<00:04,  3.20it/s][A[A

 56% 18/32 [00:03<00:04,  3.48it/s][A[A

 59% 19/32 [00:04<00:03,  4.09it/s][A[A

 62% 20/32 [00:04<00:02,  4.15it/s][A[A

 66% 21/32 [00:04<00:02,  4.72it/s][A[A

 69% 22/32 [00:04<00:01,  5.24it/s][A[A

 72% 23/32 [00:04<00:01,  5.61it/s][A[A

 75% 24/32 [00:04<00:01,  5.95it/s][A[A

 78% 25/32 [00:05<00:01,  5.64it/s][A[A

 81% 26/32 [00:05<00:01,  5.41it/s][A[A

 84% 27/32 [00:05<00:00,  5.61it/s][A[A

 88% 28/32 [00:05<00:00,  5.67it/s][A[A

 91% 29/32 [00:05<00:00,  6.26it/s][A[A

 94% 30/32 [00:05<00:00,  6.05it/s][A[A

 97% 31/32 [00:05<00:00,  6.22it/s][A[A

100% 32/32 [00:07<00:00,  2.23it/s][A[A100% 32/32 [00:07<00:00,  4.50it/s]
Meta loss on this task batch = 5.2329e-01, PNorm = 34.8112, GNorm = 0.2453

 11% 2/19 [00:15<02:15,  7.97s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.87it/s][A[A

  6% 2/32 [00:00<00:05,  5.64it/s][A[A

  9% 3/32 [00:00<00:05,  5.69it/s][A[A

 12% 4/32 [00:00<00:05,  5.48it/s][A[A

 16% 5/32 [00:00<00:04,  5.68it/s][A[A

 19% 6/32 [00:01<00:04,  5.52it/s][A[A

 22% 7/32 [00:01<00:04,  5.53it/s][A[A

 25% 8/32 [00:01<00:04,  5.93it/s][A[A

 28% 9/32 [00:01<00:03,  6.17it/s][A[A

 31% 10/32 [00:01<00:04,  5.48it/s][A[A

 34% 11/32 [00:02<00:04,  5.15it/s][A[A

 38% 12/32 [00:02<00:03,  5.26it/s][A[A

 41% 13/32 [00:02<00:03,  5.13it/s][A[A

 44% 14/32 [00:03<00:08,  2.01it/s][A[A

 47% 15/32 [00:03<00:06,  2.43it/s][A[A

 50% 16/32 [00:03<00:05,  3.05it/s][A[A

 53% 17/32 [00:04<00:04,  3.43it/s][A[A

 56% 18/32 [00:04<00:03,  4.06it/s][A[A

 59% 19/32 [00:04<00:02,  4.57it/s][A[A

 62% 20/32 [00:04<00:02,  5.20it/s][A[A

 66% 21/32 [00:04<00:01,  5.87it/s][A[A

 69% 22/32 [00:04<00:01,  6.22it/s][A[A

 72% 23/32 [00:05<00:01,  5.95it/s][A[A

 75% 24/32 [00:05<00:01,  5.51it/s][A[A

 78% 25/32 [00:05<00:01,  6.09it/s][A[A

 81% 26/32 [00:05<00:00,  6.57it/s][A[A

 84% 27/32 [00:05<00:00,  5.64it/s][A[A

 88% 28/32 [00:05<00:00,  5.82it/s][A[A

 91% 29/32 [00:07<00:01,  2.13it/s][A[A

 94% 30/32 [00:07<00:00,  2.50it/s][A[A

 97% 31/32 [00:07<00:00,  3.20it/s][A[A

100% 32/32 [00:07<00:00,  3.62it/s][A[A100% 32/32 [00:07<00:00,  4.23it/s]
Meta loss on this task batch = 5.1178e-01, PNorm = 34.8313, GNorm = 0.0283

 16% 3/19 [00:24<02:09,  8.07s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.90it/s][A[A

  6% 2/32 [00:00<00:06,  4.89it/s][A[A

  9% 3/32 [00:00<00:05,  5.11it/s][A[A

 12% 4/32 [00:00<00:05,  5.21it/s][A[A

 16% 5/32 [00:00<00:05,  5.07it/s][A[A

 19% 6/32 [00:01<00:05,  5.04it/s][A[A

 22% 7/32 [00:01<00:04,  5.65it/s][A[A

 25% 8/32 [00:01<00:04,  5.73it/s][A[A

 28% 9/32 [00:01<00:03,  6.19it/s][A[A

 31% 10/32 [00:01<00:03,  6.30it/s][A[A

 34% 11/32 [00:02<00:09,  2.21it/s][A[A

 38% 12/32 [00:03<00:07,  2.78it/s][A[A

 41% 13/32 [00:03<00:05,  3.25it/s][A[A

 44% 14/32 [00:03<00:04,  3.75it/s][A[A

 47% 15/32 [00:03<00:03,  4.34it/s][A[A

 50% 16/32 [00:03<00:03,  4.78it/s][A[A

 53% 17/32 [00:03<00:02,  5.30it/s][A[A

 56% 18/32 [00:04<00:02,  5.08it/s][A[A

 59% 19/32 [00:04<00:02,  5.54it/s][A[A

 62% 20/32 [00:04<00:02,  5.62it/s][A[A

 66% 21/32 [00:04<00:02,  5.42it/s][A[A

 69% 22/32 [00:04<00:01,  5.03it/s][A[A

 72% 23/32 [00:05<00:01,  4.91it/s][A[A

 75% 24/32 [00:05<00:01,  4.82it/s][A[A

 78% 25/32 [00:05<00:01,  5.35it/s][A[A

 81% 26/32 [00:05<00:01,  5.94it/s][A[A

 84% 27/32 [00:05<00:00,  5.99it/s][A[A

 88% 28/32 [00:05<00:00,  5.83it/s][A[A

 91% 29/32 [00:06<00:01,  2.18it/s][A[A

 94% 30/32 [00:07<00:00,  2.58it/s][A[A

 97% 31/32 [00:07<00:00,  3.20it/s][A[A

100% 32/32 [00:07<00:00,  3.62it/s][A[A100% 32/32 [00:07<00:00,  4.25it/s]
Meta loss on this task batch = 5.7905e-01, PNorm = 34.8505, GNorm = 0.0415

 21% 4/19 [00:32<02:01,  8.12s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  6.01it/s][A[A

  6% 2/32 [00:00<00:04,  6.54it/s][A[A

  9% 3/32 [00:00<00:04,  6.60it/s][A[A

 12% 4/32 [00:00<00:04,  6.16it/s][A[A

 16% 5/32 [00:00<00:04,  5.70it/s][A[A

 19% 6/32 [00:01<00:04,  5.45it/s][A[A

 22% 7/32 [00:01<00:04,  5.16it/s][A[A

 25% 8/32 [00:01<00:04,  5.62it/s][A[A

 28% 9/32 [00:01<00:04,  5.45it/s][A[A

 31% 10/32 [00:02<00:10,  2.12it/s][A[A

 34% 11/32 [00:02<00:08,  2.61it/s][A[A

 38% 12/32 [00:03<00:06,  3.06it/s][A[A

 41% 13/32 [00:03<00:05,  3.41it/s][A[A

 44% 14/32 [00:03<00:04,  4.05it/s][A[A

 47% 15/32 [00:03<00:04,  4.21it/s][A[A

 50% 16/32 [00:03<00:03,  5.03it/s][A[A

 53% 17/32 [00:04<00:03,  4.59it/s][A[A

 56% 18/32 [00:04<00:02,  4.78it/s][A[A

 59% 19/32 [00:04<00:02,  4.96it/s][A[A

 62% 20/32 [00:04<00:02,  4.75it/s][A[A

 66% 21/32 [00:04<00:02,  5.00it/s][A[A

 69% 22/32 [00:04<00:01,  5.71it/s][A[A

 72% 23/32 [00:05<00:01,  5.36it/s][A[A

 75% 24/32 [00:05<00:01,  4.90it/s][A[A

 78% 25/32 [00:06<00:03,  1.99it/s][A[A

 81% 26/32 [00:06<00:02,  2.54it/s][A[A

 84% 27/32 [00:06<00:01,  3.06it/s][A[A

 88% 28/32 [00:07<00:01,  3.68it/s][A[A

 91% 29/32 [00:07<00:00,  4.17it/s][A[A

 94% 30/32 [00:07<00:00,  4.72it/s][A[A

 97% 31/32 [00:07<00:00,  4.99it/s][A[A

100% 32/32 [00:07<00:00,  5.04it/s][A[A100% 32/32 [00:07<00:00,  4.14it/s]
Meta loss on this task batch = 5.0905e-01, PNorm = 34.8714, GNorm = 0.2938

 26% 5/19 [00:40<01:54,  8.21s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  7.21it/s][A[A

  6% 2/32 [00:00<00:04,  6.86it/s][A[A

  9% 3/32 [00:00<00:04,  6.92it/s][A[A

 12% 4/32 [00:00<00:04,  6.81it/s][A[A

 16% 5/32 [00:00<00:04,  5.71it/s][A[A

 19% 6/32 [00:01<00:04,  5.36it/s][A[A

 22% 7/32 [00:01<00:04,  5.43it/s][A[A

 25% 8/32 [00:01<00:04,  5.19it/s][A[A

 28% 9/32 [00:01<00:04,  4.72it/s][A[A

 31% 10/32 [00:02<00:11,  2.00it/s][A[A

 34% 11/32 [00:03<00:08,  2.47it/s][A[A

 38% 12/32 [00:03<00:06,  2.97it/s][A[A

 41% 13/32 [00:03<00:05,  3.62it/s][A[A

 44% 14/32 [00:03<00:04,  3.80it/s][A[A

 47% 15/32 [00:03<00:04,  4.06it/s][A[A

 50% 16/32 [00:04<00:03,  4.22it/s][A[A

 53% 17/32 [00:04<00:03,  4.86it/s][A[A

 56% 18/32 [00:04<00:02,  5.20it/s][A[A

 59% 19/32 [00:04<00:02,  5.07it/s][A[A

 62% 20/32 [00:04<00:02,  5.18it/s][A[A

 66% 21/32 [00:04<00:02,  5.23it/s][A[A

 69% 22/32 [00:05<00:01,  5.08it/s][A[A

 72% 23/32 [00:06<00:04,  2.04it/s][A[A

 75% 24/32 [00:06<00:03,  2.47it/s][A[A

 78% 25/32 [00:06<00:02,  2.85it/s][A[A

 81% 26/32 [00:06<00:01,  3.47it/s][A[A

 84% 27/32 [00:07<00:01,  3.68it/s][A[A

 88% 28/32 [00:07<00:01,  3.78it/s][A[A

 91% 29/32 [00:07<00:00,  3.94it/s][A[A

 94% 30/32 [00:07<00:00,  4.24it/s][A[A

 97% 31/32 [00:07<00:00,  4.82it/s][A[A

100% 32/32 [00:08<00:00,  4.90it/s][A[A100% 32/32 [00:08<00:00,  3.95it/s]
Meta loss on this task batch = 4.7534e-01, PNorm = 34.8945, GNorm = 0.4600

 32% 6/19 [00:49<01:49,  8.40s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.24it/s][A[A

  6% 2/32 [00:00<00:05,  5.59it/s][A[A

  9% 3/32 [00:00<00:05,  5.25it/s][A[A

 12% 4/32 [00:00<00:05,  4.92it/s][A[A

 16% 5/32 [00:00<00:04,  5.55it/s][A[A

 19% 6/32 [00:02<00:12,  2.07it/s][A[A

 22% 7/32 [00:02<00:09,  2.56it/s][A[A

 25% 8/32 [00:02<00:08,  2.98it/s][A[A

 28% 9/32 [00:02<00:06,  3.45it/s][A[A

 31% 10/32 [00:02<00:05,  3.79it/s][A[A

 34% 11/32 [00:03<00:04,  4.29it/s][A[A

 38% 12/32 [00:03<00:04,  4.45it/s][A[A

 41% 13/32 [00:03<00:03,  4.79it/s][A[A

 44% 14/32 [00:03<00:03,  5.23it/s][A[A

 47% 15/32 [00:03<00:03,  4.93it/s][A[A

 50% 16/32 [00:04<00:03,  4.75it/s][A[A

 53% 17/32 [00:04<00:03,  4.96it/s][A[A

 56% 18/32 [00:04<00:02,  4.94it/s][A[A

 59% 19/32 [00:04<00:02,  5.19it/s][A[A

 62% 20/32 [00:04<00:02,  4.87it/s][A[A

 66% 21/32 [00:05<00:02,  4.92it/s][A[A

 69% 22/32 [00:05<00:02,  4.85it/s][A[A

 72% 23/32 [00:05<00:01,  4.76it/s][A[A

 75% 24/32 [00:06<00:04,  1.97it/s][A[A

 78% 25/32 [00:06<00:02,  2.39it/s][A[A

 81% 26/32 [00:07<00:02,  2.86it/s][A[A

 84% 27/32 [00:07<00:01,  3.40it/s][A[A

 88% 28/32 [00:07<00:01,  3.68it/s][A[A

 91% 29/32 [00:07<00:00,  4.21it/s][A[A

 94% 30/32 [00:07<00:00,  4.48it/s][A[A

 97% 31/32 [00:07<00:00,  4.52it/s][A[A

100% 32/32 [00:08<00:00,  4.66it/s][A[A100% 32/32 [00:08<00:00,  3.91it/s]
Meta loss on this task batch = 4.7668e-01, PNorm = 34.9177, GNorm = 0.1591

 37% 7/19 [00:58<01:42,  8.56s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.41it/s][A[A

  6% 2/32 [00:00<00:06,  4.51it/s][A[A

  9% 3/32 [00:00<00:06,  4.54it/s][A[A

 12% 4/32 [00:00<00:06,  4.60it/s][A[A

 16% 5/32 [00:01<00:05,  4.69it/s][A[A

 19% 6/32 [00:02<00:13,  1.96it/s][A[A

 22% 7/32 [00:02<00:10,  2.47it/s][A[A

 25% 8/32 [00:02<00:08,  2.88it/s][A[A

 28% 9/32 [00:02<00:06,  3.32it/s][A[A

 31% 10/32 [00:02<00:05,  3.89it/s][A[A

 34% 11/32 [00:03<00:05,  4.12it/s][A[A

 38% 12/32 [00:03<00:04,  4.41it/s][A[A

 41% 13/32 [00:03<00:03,  4.82it/s][A[A

 44% 14/32 [00:03<00:03,  5.23it/s][A[A

 47% 15/32 [00:03<00:03,  5.04it/s][A[A

 50% 16/32 [00:04<00:03,  4.96it/s][A[A

 53% 17/32 [00:04<00:03,  4.92it/s][A[A

 56% 18/32 [00:04<00:02,  5.05it/s][A[A

 59% 19/32 [00:04<00:02,  4.97it/s][A[A

 62% 20/32 [00:04<00:02,  5.14it/s][A[A

 66% 21/32 [00:05<00:02,  5.35it/s][A[A

 69% 22/32 [00:06<00:04,  2.01it/s][A[A

 72% 23/32 [00:06<00:03,  2.53it/s][A[A

 75% 24/32 [00:06<00:02,  2.94it/s][A[A

 78% 25/32 [00:06<00:02,  3.20it/s][A[A

 81% 26/32 [00:07<00:01,  3.41it/s][A[A

 84% 27/32 [00:07<00:01,  3.64it/s][A[A

 88% 28/32 [00:07<00:01,  3.85it/s][A[A

 91% 29/32 [00:07<00:00,  4.11it/s][A[A

 94% 30/32 [00:08<00:00,  4.32it/s][A[A

 97% 31/32 [00:08<00:00,  4.47it/s][A[A

100% 32/32 [00:08<00:00,  4.50it/s][A[A100% 32/32 [00:08<00:00,  3.78it/s]
Meta loss on this task batch = 3.9444e-01, PNorm = 34.9428, GNorm = 0.3961

 42% 8/19 [01:07<01:36,  8.76s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:38,  1.24s/it][A[A

  6% 2/32 [00:01<00:28,  1.07it/s][A[A

  9% 3/32 [00:01<00:20,  1.39it/s][A[A

 12% 4/32 [00:01<00:16,  1.73it/s][A[A

 16% 5/32 [00:02<00:12,  2.12it/s][A[A

 19% 6/32 [00:02<00:10,  2.53it/s][A[A

 22% 7/32 [00:02<00:08,  2.91it/s][A[A

 25% 8/32 [00:02<00:07,  3.27it/s][A[A

 28% 9/32 [00:03<00:06,  3.57it/s][A[A

 31% 10/32 [00:03<00:05,  3.79it/s][A[A

 34% 11/32 [00:03<00:05,  3.99it/s][A[A

 38% 12/32 [00:03<00:04,  4.17it/s][A[A

 41% 13/32 [00:04<00:10,  1.87it/s][A[A

 44% 14/32 [00:05<00:07,  2.29it/s][A[A

 47% 15/32 [00:05<00:06,  2.68it/s][A[A

 50% 16/32 [00:05<00:05,  3.02it/s][A[A

 53% 17/32 [00:05<00:04,  3.33it/s][A[A

 56% 18/32 [00:06<00:03,  3.68it/s][A[A

 59% 19/32 [00:06<00:03,  3.88it/s][A[A

 62% 20/32 [00:06<00:02,  4.06it/s][A[A

 66% 21/32 [00:06<00:02,  4.28it/s][A[A

 69% 22/32 [00:06<00:02,  4.38it/s][A[A

 72% 23/32 [00:07<00:02,  4.45it/s][A[A

 75% 24/32 [00:08<00:04,  1.92it/s][A[A

 78% 25/32 [00:08<00:03,  2.32it/s][A[A

 81% 26/32 [00:08<00:02,  2.66it/s][A[A

 84% 27/32 [00:08<00:01,  3.03it/s][A[A

 88% 28/32 [00:09<00:01,  3.39it/s][A[A

 91% 29/32 [00:09<00:00,  3.59it/s][A[A

 94% 30/32 [00:09<00:00,  3.79it/s][A[A

 97% 31/32 [00:09<00:00,  4.02it/s][A[A

100% 32/32 [00:10<00:00,  4.20it/s][A[A100% 32/32 [00:10<00:00,  3.17it/s]
Meta loss on this task batch = 2.5536e-01, PNorm = 34.9724, GNorm = 0.6658

 47% 9/19 [01:18<01:34,  9.40s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:37,  1.22s/it][A[A

  6% 2/32 [00:01<00:27,  1.09it/s][A[A

  9% 3/32 [00:01<00:20,  1.41it/s][A[A

 12% 4/32 [00:01<00:15,  1.78it/s][A[A

 16% 5/32 [00:02<00:12,  2.18it/s][A[A

 19% 6/32 [00:02<00:10,  2.56it/s][A[A

 22% 7/32 [00:02<00:08,  2.97it/s][A[A

 25% 8/32 [00:02<00:07,  3.27it/s][A[A

 28% 9/32 [00:03<00:06,  3.47it/s][A[A

 31% 10/32 [00:03<00:05,  3.70it/s][A[A

 34% 11/32 [00:03<00:05,  3.93it/s][A[A

 38% 12/32 [00:03<00:04,  4.08it/s][A[A

 41% 13/32 [00:03<00:04,  4.16it/s][A[A

 44% 14/32 [00:05<00:09,  1.88it/s][A[A

 47% 15/32 [00:05<00:07,  2.30it/s][A[A

 50% 16/32 [00:05<00:05,  2.69it/s][A[A

 53% 17/32 [00:05<00:04,  3.06it/s][A[A

 56% 18/32 [00:06<00:04,  3.37it/s][A[A

 59% 19/32 [00:06<00:03,  3.72it/s][A[A

 62% 20/32 [00:06<00:03,  3.93it/s][A[A

 66% 21/32 [00:06<00:02,  4.04it/s][A[A

 69% 22/32 [00:06<00:02,  4.10it/s][A[A

 72% 23/32 [00:07<00:02,  4.14it/s][A[A

 75% 24/32 [00:07<00:01,  4.32it/s][A[A

 78% 25/32 [00:08<00:03,  1.88it/s][A[A

 81% 26/32 [00:08<00:02,  2.24it/s][A[A

 84% 27/32 [00:09<00:01,  2.74it/s][A[A

 88% 28/32 [00:09<00:01,  3.07it/s][A[A

 91% 29/32 [00:09<00:00,  3.42it/s][A[A

 94% 30/32 [00:09<00:00,  3.64it/s][A[A

 97% 31/32 [00:09<00:00,  3.85it/s][A[A

100% 32/32 [00:10<00:00,  4.26it/s][A[A100% 32/32 [00:10<00:00,  3.17it/s]
Meta loss on this task batch = 2.7188e-01, PNorm = 35.0041, GNorm = 0.2649

 53% 10/19 [01:29<01:28,  9.84s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.32it/s][A[A

  6% 2/32 [00:00<00:06,  4.36it/s][A[A

  9% 3/32 [00:00<00:06,  4.36it/s][A[A

 12% 4/32 [00:00<00:06,  4.32it/s][A[A

 16% 5/32 [00:02<00:14,  1.91it/s][A[A

 19% 6/32 [00:02<00:10,  2.38it/s][A[A

 22% 7/32 [00:02<00:09,  2.73it/s][A[A

 25% 8/32 [00:02<00:07,  3.06it/s][A[A

 28% 9/32 [00:03<00:06,  3.33it/s][A[A

 31% 10/32 [00:03<00:06,  3.55it/s][A[A

 34% 11/32 [00:03<00:05,  3.79it/s][A[A

 38% 12/32 [00:03<00:05,  3.93it/s][A[A

 41% 13/32 [00:03<00:04,  4.01it/s][A[A

 44% 14/32 [00:04<00:04,  4.05it/s][A[A

 47% 15/32 [00:04<00:04,  4.08it/s][A[A

 50% 16/32 [00:05<00:08,  1.87it/s][A[A

 53% 17/32 [00:05<00:06,  2.24it/s][A[A

 56% 18/32 [00:06<00:05,  2.63it/s][A[A

 59% 19/32 [00:06<00:04,  2.97it/s][A[A

 62% 20/32 [00:06<00:03,  3.25it/s][A[A

 66% 21/32 [00:06<00:03,  3.53it/s][A[A

 69% 22/32 [00:07<00:02,  3.74it/s][A[A

 72% 23/32 [00:07<00:02,  3.86it/s][A[A

 75% 24/32 [00:07<00:02,  3.96it/s][A[A

 78% 25/32 [00:07<00:01,  4.23it/s][A[A

 81% 26/32 [00:07<00:01,  4.25it/s][A[A

 84% 27/32 [00:08<00:01,  4.29it/s][A[A

 88% 28/32 [00:08<00:00,  4.28it/s][A[A

 91% 29/32 [00:09<00:01,  1.87it/s][A[A

 94% 30/32 [00:09<00:00,  2.27it/s][A[A

 97% 31/32 [00:10<00:00,  2.62it/s][A[A

100% 32/32 [00:10<00:00,  2.94it/s][A[A100% 32/32 [00:10<00:00,  3.09it/s]
Meta loss on this task batch = 6.2336e-01, PNorm = 35.0283, GNorm = 0.6214

 58% 11/19 [01:40<01:21, 10.25s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.86it/s][A[A

  6% 2/32 [00:00<00:07,  3.88it/s][A[A

  9% 3/32 [00:00<00:07,  3.95it/s][A[A

 12% 4/32 [00:00<00:06,  4.00it/s][A[A

 16% 5/32 [00:02<00:14,  1.82it/s][A[A

 19% 6/32 [00:02<00:11,  2.18it/s][A[A

 22% 7/32 [00:02<00:09,  2.52it/s][A[A

 25% 8/32 [00:02<00:08,  2.86it/s][A[A

 28% 9/32 [00:03<00:07,  3.17it/s][A[A

 31% 10/32 [00:03<00:06,  3.37it/s][A[A

 34% 11/32 [00:04<00:12,  1.72it/s][A[A

 38% 12/32 [00:04<00:09,  2.10it/s][A[A

 41% 13/32 [00:05<00:07,  2.48it/s][A[A

 44% 14/32 [00:05<00:06,  2.84it/s][A[A

 47% 15/32 [00:05<00:05,  3.16it/s][A[A

 50% 16/32 [00:05<00:04,  3.48it/s][A[A

 53% 17/32 [00:06<00:04,  3.65it/s][A[A

 56% 18/32 [00:06<00:03,  3.78it/s][A[A

 59% 19/32 [00:06<00:03,  3.89it/s][A[A

 62% 20/32 [00:06<00:03,  4.00it/s][A[A

 66% 21/32 [00:08<00:06,  1.82it/s][A[A

 69% 22/32 [00:08<00:04,  2.19it/s][A[A

 72% 23/32 [00:08<00:03,  2.55it/s][A[A

 75% 24/32 [00:08<00:02,  3.00it/s][A[A

 78% 25/32 [00:08<00:02,  3.32it/s][A[A

 81% 26/32 [00:09<00:01,  3.64it/s][A[A

 84% 27/32 [00:09<00:01,  3.76it/s][A[A

 88% 28/32 [00:09<00:01,  3.93it/s][A[A

 91% 29/32 [00:09<00:00,  4.07it/s][A[A

 94% 30/32 [00:10<00:00,  4.13it/s][A[A

 97% 31/32 [00:10<00:00,  4.19it/s][A[A

100% 32/32 [00:11<00:00,  1.86it/s][A[A100% 32/32 [00:11<00:00,  2.76it/s]
Meta loss on this task batch = 6.5516e-01, PNorm = 35.0430, GNorm = 0.9210

 63% 12/19 [01:53<01:16, 10.90s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.18it/s][A[A

  6% 2/32 [00:00<00:07,  4.25it/s][A[A

  9% 3/32 [00:00<00:06,  4.27it/s][A[A

 12% 4/32 [00:00<00:06,  4.28it/s][A[A

 16% 5/32 [00:01<00:06,  4.28it/s][A[A

 19% 6/32 [00:01<00:06,  4.23it/s][A[A

 22% 7/32 [00:01<00:05,  4.51it/s][A[A

 25% 8/32 [00:01<00:05,  4.71it/s][A[A

 28% 9/32 [00:01<00:04,  5.01it/s][A[A

 31% 10/32 [00:02<00:04,  4.76it/s][A[A

 34% 11/32 [00:03<00:10,  1.92it/s][A[A

 38% 12/32 [00:03<00:08,  2.27it/s][A[A

 41% 13/32 [00:03<00:07,  2.62it/s][A[A

 44% 14/32 [00:04<00:06,  2.93it/s][A[A

 47% 15/32 [00:04<00:05,  3.21it/s][A[A

 50% 16/32 [00:04<00:04,  3.43it/s][A[A

 53% 17/32 [00:04<00:03,  3.84it/s][A[A

 56% 18/32 [00:06<00:07,  1.82it/s][A[A

 59% 19/32 [00:06<00:05,  2.19it/s][A[A

 62% 20/32 [00:06<00:04,  2.53it/s][A[A

 66% 21/32 [00:06<00:03,  2.85it/s][A[A

 69% 22/32 [00:07<00:03,  3.14it/s][A[A

 72% 23/32 [00:07<00:02,  3.35it/s][A[A

 75% 24/32 [00:07<00:02,  3.53it/s][A[A

 78% 25/32 [00:08<00:04,  1.75it/s][A[A

 81% 26/32 [00:09<00:02,  2.14it/s][A[A

 84% 27/32 [00:09<00:01,  2.62it/s][A[A

 88% 28/32 [00:09<00:01,  2.96it/s][A[A

 91% 29/32 [00:09<00:00,  3.20it/s][A[A

 94% 30/32 [00:09<00:00,  3.46it/s][A[A

 97% 31/32 [00:10<00:00,  3.81it/s][A[A

100% 32/32 [00:11<00:00,  1.79it/s][A[A100% 32/32 [00:11<00:00,  2.81it/s]
Meta loss on this task batch = 6.2757e-01, PNorm = 35.0540, GNorm = 0.4440

 68% 13/19 [02:05<01:07, 11.30s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.22it/s][A[A

  6% 2/32 [00:00<00:05,  5.32it/s][A[A

  9% 3/32 [00:00<00:05,  4.93it/s][A[A

 12% 4/32 [00:00<00:05,  4.72it/s][A[A

 16% 5/32 [00:01<00:05,  4.54it/s][A[A

 19% 6/32 [00:01<00:05,  4.79it/s][A[A

 22% 7/32 [00:01<00:05,  4.58it/s][A[A

 25% 8/32 [00:01<00:04,  5.01it/s][A[A

 28% 9/32 [00:01<00:04,  4.73it/s][A[A

 31% 10/32 [00:02<00:04,  4.57it/s][A[A

 34% 11/32 [00:02<00:04,  4.39it/s][A[A

 38% 12/32 [00:03<00:10,  1.87it/s][A[A

 41% 13/32 [00:03<00:08,  2.23it/s][A[A

 44% 14/32 [00:04<00:06,  2.59it/s][A[A

 47% 15/32 [00:04<00:05,  2.92it/s][A[A

 50% 16/32 [00:04<00:04,  3.23it/s][A[A

 53% 17/32 [00:04<00:04,  3.48it/s][A[A

 56% 18/32 [00:05<00:03,  3.81it/s][A[A

 59% 19/32 [00:05<00:03,  4.05it/s][A[A

 62% 20/32 [00:05<00:02,  4.15it/s][A[A

 66% 21/32 [00:05<00:02,  4.20it/s][A[A

 69% 22/32 [00:06<00:05,  1.86it/s][A[A

 72% 23/32 [00:07<00:04,  2.24it/s][A[A

 75% 24/32 [00:07<00:02,  2.70it/s][A[A

 78% 25/32 [00:07<00:02,  3.05it/s][A[A

 81% 26/32 [00:07<00:01,  3.34it/s][A[A

 84% 27/32 [00:08<00:01,  3.60it/s][A[A

 88% 28/32 [00:08<00:01,  3.79it/s][A[A

 91% 29/32 [00:08<00:00,  3.91it/s][A[A

 94% 30/32 [00:08<00:00,  4.18it/s][A[A

 97% 31/32 [00:08<00:00,  4.41it/s][A[A

100% 32/32 [00:09<00:00,  4.63it/s][A[A100% 32/32 [00:09<00:00,  3.51it/s]
Meta loss on this task batch = 5.8720e-01, PNorm = 35.0641, GNorm = 0.2141

 74% 14/19 [02:15<00:54, 10.88s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.38it/s][A[A

  6% 2/32 [00:00<00:06,  4.59it/s][A[A

  9% 3/32 [00:00<00:06,  4.61it/s][A[A

 12% 4/32 [00:00<00:05,  4.99it/s][A[A

 16% 5/32 [00:00<00:05,  5.14it/s][A[A

 19% 6/32 [00:02<00:13,  1.93it/s][A[A

 22% 7/32 [00:02<00:11,  2.27it/s][A[A

 25% 8/32 [00:02<00:08,  2.78it/s][A[A

 28% 9/32 [00:02<00:07,  3.13it/s][A[A

 31% 10/32 [00:03<00:06,  3.65it/s][A[A

 34% 11/32 [00:03<00:05,  3.88it/s][A[A

 38% 12/32 [00:03<00:04,  4.27it/s][A[A

 41% 13/32 [00:03<00:04,  4.41it/s][A[A

 44% 14/32 [00:03<00:04,  4.37it/s][A[A

 47% 15/32 [00:04<00:03,  4.68it/s][A[A

 50% 16/32 [00:04<00:03,  4.69it/s][A[A

 53% 17/32 [00:04<00:03,  4.75it/s][A[A

 56% 18/32 [00:04<00:02,  4.97it/s][A[A

 59% 19/32 [00:05<00:06,  2.07it/s][A[A

 62% 20/32 [00:06<00:04,  2.49it/s][A[A

 66% 21/32 [00:06<00:03,  3.00it/s][A[A

 69% 22/32 [00:06<00:02,  3.46it/s][A[A

 72% 23/32 [00:06<00:02,  3.58it/s][A[A

 75% 24/32 [00:06<00:02,  3.89it/s][A[A

 78% 25/32 [00:07<00:01,  4.04it/s][A[A

 81% 26/32 [00:07<00:01,  3.90it/s][A[A

 84% 27/32 [00:07<00:01,  4.09it/s][A[A

 88% 28/32 [00:07<00:00,  4.11it/s][A[A

 91% 29/32 [00:08<00:00,  4.16it/s][A[A

 94% 30/32 [00:09<00:01,  1.85it/s][A[A

 97% 31/32 [00:09<00:00,  2.31it/s][A[A

100% 32/32 [00:09<00:00,  2.69it/s][A[A100% 32/32 [00:09<00:00,  3.29it/s]
Meta loss on this task batch = 4.8206e-01, PNorm = 35.0746, GNorm = 0.0315

 79% 15/19 [02:25<00:43, 10.76s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.13it/s][A[A

  6% 2/32 [00:00<00:06,  4.47it/s][A[A

  9% 3/32 [00:00<00:06,  4.72it/s][A[A

 12% 4/32 [00:00<00:05,  4.95it/s][A[A

 16% 5/32 [00:00<00:05,  5.08it/s][A[A

 19% 6/32 [00:01<00:05,  4.88it/s][A[A

 22% 7/32 [00:01<00:05,  4.99it/s][A[A

 25% 8/32 [00:01<00:05,  4.71it/s][A[A

 28% 9/32 [00:01<00:04,  4.85it/s][A[A

 31% 10/32 [00:02<00:04,  4.64it/s][A[A

 34% 11/32 [00:03<00:10,  1.93it/s][A[A

 38% 12/32 [00:03<00:08,  2.36it/s][A[A

 41% 13/32 [00:03<00:06,  2.86it/s][A[A

 44% 14/32 [00:03<00:05,  3.36it/s][A[A

 47% 15/32 [00:04<00:04,  3.68it/s][A[A

 50% 16/32 [00:04<00:03,  4.22it/s][A[A

 53% 17/32 [00:04<00:03,  4.30it/s][A[A

 56% 18/32 [00:04<00:02,  4.71it/s][A[A

 59% 19/32 [00:04<00:02,  4.85it/s][A[A

 62% 20/32 [00:04<00:02,  4.95it/s][A[A

 66% 21/32 [00:05<00:02,  5.08it/s][A[A

 69% 22/32 [00:05<00:01,  5.07it/s][A[A

 72% 23/32 [00:05<00:01,  4.77it/s][A[A

 75% 24/32 [00:05<00:01,  4.57it/s][A[A

 78% 25/32 [00:05<00:01,  4.98it/s][A[A

 81% 26/32 [00:07<00:02,  2.00it/s][A[A

 84% 27/32 [00:07<00:02,  2.39it/s][A[A

 88% 28/32 [00:07<00:01,  2.82it/s][A[A

 91% 29/32 [00:07<00:00,  3.22it/s][A[A

 94% 30/32 [00:08<00:00,  3.49it/s][A[A

 97% 31/32 [00:08<00:00,  3.73it/s][A[A

100% 32/32 [00:08<00:00,  4.03it/s][A[A100% 32/32 [00:08<00:00,  3.77it/s]
Meta loss on this task batch = 5.6567e-01, PNorm = 35.0848, GNorm = 0.0510

 84% 16/19 [02:35<00:30, 10.31s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.73it/s][A[A

  6% 2/32 [00:00<00:07,  3.82it/s][A[A

  9% 3/32 [00:00<00:07,  3.93it/s][A[A

 12% 4/32 [00:00<00:06,  4.24it/s][A[A

 16% 5/32 [00:02<00:14,  1.91it/s][A[A

 19% 6/32 [00:02<00:10,  2.46it/s][A[A

 22% 7/32 [00:02<00:08,  3.00it/s][A[A

 25% 8/32 [00:02<00:07,  3.29it/s][A[A

 28% 9/32 [00:02<00:06,  3.64it/s][A[A

 31% 10/32 [00:03<00:05,  4.33it/s][A[A

 34% 11/32 [00:03<00:04,  4.58it/s][A[A

 38% 12/32 [00:03<00:04,  4.70it/s][A[A

 41% 13/32 [00:03<00:03,  5.11it/s][A[A

 44% 14/32 [00:03<00:03,  5.36it/s][A[A

 47% 15/32 [00:03<00:03,  5.29it/s][A[A

 50% 16/32 [00:04<00:03,  5.11it/s][A[A

 53% 17/32 [00:04<00:02,  5.23it/s][A[A

 56% 18/32 [00:04<00:02,  5.04it/s][A[A

 59% 19/32 [00:04<00:02,  4.85it/s][A[A

 62% 20/32 [00:05<00:06,  1.96it/s][A[A

 66% 21/32 [00:06<00:04,  2.43it/s][A[A

 69% 22/32 [00:06<00:03,  2.76it/s][A[A

 72% 23/32 [00:06<00:02,  3.18it/s][A[A

 75% 24/32 [00:06<00:02,  3.33it/s][A[A

 78% 25/32 [00:07<00:01,  3.63it/s][A[A

 81% 26/32 [00:07<00:01,  3.77it/s][A[A

 84% 27/32 [00:07<00:01,  4.23it/s][A[A

 88% 28/32 [00:07<00:00,  4.60it/s][A[A

 91% 29/32 [00:07<00:00,  4.44it/s][A[A

 94% 30/32 [00:08<00:00,  4.24it/s][A[A

 97% 31/32 [00:09<00:00,  1.92it/s][A[A

100% 32/32 [00:09<00:00,  2.38it/s][A[A100% 32/32 [00:09<00:00,  3.35it/s]
Meta loss on this task batch = 4.6634e-01, PNorm = 35.0953, GNorm = 0.0644

 89% 17/19 [02:45<00:20, 10.31s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.36it/s][A[A

  6% 2/32 [00:00<00:05,  5.19it/s][A[A

  9% 3/32 [00:00<00:05,  4.96it/s][A[A

 12% 4/32 [00:00<00:05,  4.93it/s][A[A

 16% 5/32 [00:01<00:05,  4.85it/s][A[A

 19% 6/32 [00:01<00:05,  4.96it/s][A[A

 22% 7/32 [00:01<00:05,  4.65it/s][A[A

 25% 8/32 [00:01<00:05,  4.44it/s][A[A

 28% 9/32 [00:01<00:05,  4.54it/s][A[A

 31% 10/32 [00:02<00:04,  4.52it/s][A[A

 34% 11/32 [00:02<00:04,  4.88it/s][A[A

 38% 12/32 [00:03<00:09,  2.00it/s][A[A

 41% 13/32 [00:03<00:08,  2.34it/s][A[A

 44% 14/32 [00:03<00:06,  2.92it/s][A[A

 47% 15/32 [00:04<00:05,  3.23it/s][A[A

 50% 16/32 [00:04<00:04,  3.69it/s][A[A

 53% 17/32 [00:04<00:03,  4.36it/s][A[A

 56% 18/32 [00:04<00:03,  4.65it/s][A[A

 59% 19/32 [00:04<00:02,  4.60it/s][A[A

 62% 20/32 [00:05<00:02,  4.56it/s][A[A

 66% 21/32 [00:05<00:02,  4.57it/s][A[A

 69% 22/32 [00:05<00:01,  5.19it/s][A[A

 72% 23/32 [00:05<00:01,  4.64it/s][A[A

 75% 24/32 [00:05<00:01,  4.36it/s][A[A

 78% 25/32 [00:06<00:01,  4.82it/s][A[A

 81% 26/32 [00:07<00:03,  1.97it/s][A[A

 84% 27/32 [00:07<00:02,  2.33it/s][A[A

 88% 28/32 [00:07<00:01,  2.71it/s][A[A

 91% 29/32 [00:08<00:00,  3.08it/s][A[A

 94% 30/32 [00:08<00:00,  3.56it/s][A[A

 97% 31/32 [00:08<00:00,  4.02it/s][A[A

100% 32/32 [00:08<00:00,  4.12it/s][A[A100% 32/32 [00:08<00:00,  3.72it/s]
Meta loss on this task batch = 5.2743e-01, PNorm = 35.1061, GNorm = 0.0661

 95% 18/19 [02:54<00:10, 10.02s/it][A

  0% 0/23 [00:00<?, ?it/s][A[A

  4% 1/23 [00:00<00:05,  4.22it/s][A[A

  9% 2/23 [00:00<00:04,  4.59it/s][A[A

 13% 3/23 [00:00<00:04,  4.66it/s][A[A

 17% 4/23 [00:00<00:03,  4.98it/s][A[A

 22% 5/23 [00:00<00:03,  5.57it/s][A[A

 26% 6/23 [00:01<00:03,  5.30it/s][A[A

 30% 7/23 [00:01<00:02,  5.37it/s][A[A

 35% 8/23 [00:01<00:02,  5.02it/s][A[A

 39% 9/23 [00:02<00:06,  2.10it/s][A[A

 43% 10/23 [00:02<00:05,  2.43it/s][A[A

 48% 11/23 [00:03<00:04,  2.96it/s][A[A

 52% 12/23 [00:03<00:03,  3.31it/s][A[A

 57% 13/23 [00:03<00:02,  3.95it/s][A[A

 61% 14/23 [00:03<00:02,  4.36it/s][A[A

 65% 15/23 [00:03<00:02,  3.64it/s][A[A

 70% 16/23 [00:04<00:01,  3.69it/s][A[A

 74% 17/23 [00:04<00:01,  4.21it/s][A[A

 78% 18/23 [00:04<00:01,  4.01it/s][A[A

 83% 19/23 [00:05<00:02,  1.85it/s][A[A

 87% 20/23 [00:06<00:01,  2.40it/s][A[A

 91% 21/23 [00:06<00:00,  2.94it/s][A[A

 96% 22/23 [00:06<00:00,  3.61it/s][A[A

100% 23/23 [00:06<00:00,  4.17it/s][A[A100% 23/23 [00:06<00:00,  3.55it/s]
Meta loss on this task batch = 4.5093e-01, PNorm = 35.1178, GNorm = 0.1312

100% 19/19 [03:01<00:00,  9.13s/it][A100% 19/19 [03:01<00:00,  9.57s/it]
Took 181.75249481201172 seconds to complete one epoch of meta training

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/20 [00:00<?, ?it/s][A[A


  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 32.23it/s]


  5% 1/20 [00:00<00:02,  8.42it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 20.16it/s]


 10% 2/20 [00:00<00:02,  6.20it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 17.53it/s][A[A[A100% 3/3 [00:00<00:00, 19.46it/s]


 15% 3/20 [00:00<00:03,  4.94it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 15.61it/s]


 20% 4/20 [00:00<00:03,  4.88it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 15.20it/s][A[A[A100% 4/4 [00:00<00:00, 17.54it/s]


 25% 5/20 [00:01<00:03,  3.84it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.59it/s][A[A[A100% 4/4 [00:00<00:00, 16.34it/s]


 30% 6/20 [00:01<00:04,  3.31it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.39it/s][A[A[A100% 4/4 [00:00<00:00, 21.01it/s]


 35% 7/20 [00:02<00:04,  3.14it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.85it/s][A[A[A


100% 4/4 [00:00<00:00, 19.48it/s][A[A[A100% 4/4 [00:00<00:00, 19.21it/s]


 40% 8/20 [00:02<00:04,  2.99it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.73it/s][A[A[A100% 4/4 [00:00<00:00, 23.22it/s]


 45% 9/20 [00:03<00:06,  1.58it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 18.64it/s][A[A[A


100% 4/4 [00:00<00:00, 18.61it/s][A[A[A100% 4/4 [00:00<00:00, 18.57it/s]


 50% 10/20 [00:04<00:05,  1.79it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 16.29it/s][A[A[A100% 4/4 [00:00<00:00, 18.60it/s]


 55% 11/20 [00:04<00:04,  1.97it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.21it/s][A[A[A100% 4/4 [00:00<00:00, 21.55it/s]


 60% 12/20 [00:04<00:03,  2.19it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.56it/s][A[A[A100% 3/3 [00:00<00:00, 13.88it/s]


 65% 13/20 [00:05<00:03,  2.29it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.12it/s][A[A[A


100% 3/3 [00:01<00:00,  2.70it/s][A[A[A100% 3/3 [00:01<00:00,  2.47it/s]


 70% 14/20 [00:06<00:04,  1.38it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 14.09it/s][A[A[A100% 4/4 [00:00<00:00, 18.37it/s]


 75% 15/20 [00:07<00:03,  1.61it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 13.84it/s][A[A[A100% 3/3 [00:00<00:00, 17.14it/s]


 80% 16/20 [00:07<00:02,  1.85it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 11.74it/s][A[A[A100% 3/3 [00:00<00:00, 14.44it/s]


 85% 17/20 [00:07<00:01,  2.00it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 23.10it/s]


 90% 18/20 [00:08<00:00,  2.33it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 16.02it/s][A[A[A100% 3/3 [00:00<00:00, 20.41it/s]


 95% 19/20 [00:08<00:00,  2.57it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 13.85it/s]


100% 20/20 [00:08<00:00,  2.94it/s][A[A100% 20/20 [00:08<00:00,  2.34it/s]

100% 1/1 [00:08<00:00,  8.56s/it][A100% 1/1 [00:08<00:00,  8.56s/it]
Took 190.31190371513367 seconds to complete one epoch of meta training and validating
Meta validation score auc = 0.582557
Found better MAML checkpoint after meta validation, saving now
  7% 2/30 [06:33<1:32:51, 198.99s/it]Epoch 2

  0% 0/19 [00:00<?, ?it/s][A

  0% 0/32 [00:00<?, ?it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  3% 1/32 [00:00<00:06,  4.81it/s][A[A

  6% 2/32 [00:00<00:06,  4.52it/s][A[A

  9% 3/32 [00:00<00:06,  4.54it/s][A[A

 12% 4/32 [00:01<00:13,  2.01it/s][A[A

 16% 5/32 [00:01<00:10,  2.57it/s][A[A

 19% 6/32 [00:02<00:08,  3.05it/s][A[A

 22% 7/32 [00:02<00:06,  3.58it/s][A[A

 25% 8/32 [00:02<00:06,  3.92it/s][A[A

 28% 9/32 [00:02<00:04,  4.65it/s][A[A

 31% 10/32 [00:02<00:04,  5.30it/s][A[A

 34% 11/32 [00:03<00:04,  4.80it/s][A[A

 38% 12/32 [00:03<00:04,  4.47it/s][A[A

 41% 13/32 [00:03<00:04,  4.23it/s][A[A

 44% 14/32 [00:03<00:04,  4.45it/s][A[A

 47% 15/32 [00:03<00:03,  4.84it/s][A[A

 50% 16/32 [00:04<00:03,  4.81it/s][A[A

 53% 17/32 [00:04<00:03,  4.86it/s][A[A

 56% 18/32 [00:04<00:02,  5.24it/s][A[A

 59% 19/32 [00:05<00:06,  2.07it/s][A[A

 62% 20/32 [00:05<00:04,  2.46it/s][A[A

 66% 21/32 [00:05<00:03,  3.14it/s][A[A

 69% 22/32 [00:06<00:02,  3.57it/s][A[A

 72% 23/32 [00:06<00:02,  4.14it/s][A[A

 75% 24/32 [00:06<00:01,  4.41it/s][A[A

 78% 25/32 [00:06<00:01,  4.66it/s][A[A

 81% 26/32 [00:06<00:01,  4.67it/s][A[A

 84% 27/32 [00:07<00:00,  5.07it/s][A[A

 88% 28/32 [00:07<00:00,  5.26it/s][A[A

 91% 29/32 [00:07<00:00,  5.39it/s][A[A

 94% 30/32 [00:07<00:00,  6.07it/s][A[A

 97% 31/32 [00:07<00:00,  5.90it/s][A[A

100% 32/32 [00:07<00:00,  5.89it/s][A[A100% 32/32 [00:07<00:00,  4.06it/s]
Meta loss on this task batch = 5.4094e-01, PNorm = 35.1295, GNorm = 0.1027

  5% 1/19 [00:08<02:34,  8.60s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:03,  7.75it/s][A[A

  6% 2/32 [00:00<00:04,  7.50it/s][A[A

  9% 3/32 [00:00<00:04,  6.78it/s][A[A

 12% 4/32 [00:01<00:13,  2.13it/s][A[A

 16% 5/32 [00:01<00:10,  2.54it/s][A[A

 19% 6/32 [00:02<00:08,  3.14it/s][A[A

 22% 7/32 [00:02<00:06,  3.86it/s][A[A

 25% 8/32 [00:02<00:05,  4.55it/s][A[A

 28% 9/32 [00:02<00:04,  4.95it/s][A[A

 31% 10/32 [00:02<00:04,  5.43it/s][A[A

 34% 11/32 [00:02<00:03,  5.37it/s][A[A

 38% 12/32 [00:02<00:03,  5.37it/s][A[A

 41% 13/32 [00:03<00:03,  5.73it/s][A[A

 44% 14/32 [00:03<00:03,  5.40it/s][A[A

 47% 15/32 [00:03<00:03,  5.18it/s][A[A

 50% 16/32 [00:03<00:03,  5.11it/s][A[A

 53% 17/32 [00:03<00:02,  5.09it/s][A[A

 56% 18/32 [00:04<00:02,  4.94it/s][A[A

 59% 19/32 [00:04<00:02,  5.33it/s][A[A

 62% 20/32 [00:04<00:02,  5.39it/s][A[A

 66% 21/32 [00:04<00:02,  5.18it/s][A[A

 69% 22/32 [00:05<00:04,  2.10it/s][A[A

 72% 23/32 [00:05<00:03,  2.65it/s][A[A

 75% 24/32 [00:06<00:02,  3.29it/s][A[A

 78% 25/32 [00:06<00:01,  3.74it/s][A[A

 81% 26/32 [00:06<00:01,  4.02it/s][A[A

 84% 27/32 [00:06<00:01,  4.48it/s][A[A

 88% 28/32 [00:06<00:00,  4.49it/s][A[A

 91% 29/32 [00:06<00:00,  5.22it/s][A[A

 94% 30/32 [00:07<00:00,  5.23it/s][A[A

 97% 31/32 [00:07<00:00,  5.12it/s][A[A

100% 32/32 [00:07<00:00,  5.56it/s][A[A100% 32/32 [00:07<00:00,  4.25it/s]
Meta loss on this task batch = 5.3151e-01, PNorm = 35.1417, GNorm = 0.1136

 11% 2/19 [00:16<02:24,  8.49s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.71it/s][A[A

  6% 2/32 [00:00<00:05,  5.36it/s][A[A

  9% 3/32 [00:00<00:05,  5.15it/s][A[A

 12% 4/32 [00:00<00:05,  5.21it/s][A[A

 16% 5/32 [00:00<00:04,  5.47it/s][A[A

 19% 6/32 [00:01<00:05,  5.17it/s][A[A

 22% 7/32 [00:02<00:12,  1.99it/s][A[A

 25% 8/32 [00:02<00:09,  2.52it/s][A[A

 28% 9/32 [00:02<00:07,  3.09it/s][A[A

 31% 10/32 [00:02<00:06,  3.38it/s][A[A

 34% 11/32 [00:03<00:05,  3.58it/s][A[A

 38% 12/32 [00:03<00:05,  3.95it/s][A[A

 41% 13/32 [00:03<00:04,  4.33it/s][A[A

 44% 14/32 [00:03<00:04,  4.49it/s][A[A

 47% 15/32 [00:03<00:03,  4.46it/s][A[A

 50% 16/32 [00:04<00:03,  5.08it/s][A[A

 53% 17/32 [00:04<00:03,  4.85it/s][A[A

 56% 18/32 [00:04<00:02,  5.44it/s][A[A

 59% 19/32 [00:04<00:02,  5.68it/s][A[A

 62% 20/32 [00:04<00:01,  6.03it/s][A[A

 66% 21/32 [00:04<00:01,  6.45it/s][A[A

 69% 22/32 [00:06<00:04,  2.22it/s][A[A

 72% 23/32 [00:06<00:03,  2.72it/s][A[A

 75% 24/32 [00:06<00:02,  3.09it/s][A[A

 78% 25/32 [00:06<00:01,  3.56it/s][A[A

 81% 26/32 [00:06<00:01,  4.01it/s][A[A

 84% 27/32 [00:06<00:01,  4.44it/s][A[A

 88% 28/32 [00:07<00:00,  4.80it/s][A[A

 91% 29/32 [00:07<00:00,  4.98it/s][A[A

 94% 30/32 [00:07<00:00,  4.67it/s][A[A

 97% 31/32 [00:07<00:00,  5.41it/s][A[A

100% 32/32 [00:07<00:00,  5.24it/s][A[A100% 32/32 [00:07<00:00,  4.07it/s]
Meta loss on this task batch = 5.2740e-01, PNorm = 35.1554, GNorm = 0.1330

 16% 3/19 [00:25<02:16,  8.52s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.44it/s][A[A

  6% 2/32 [00:00<00:06,  4.51it/s][A[A

  9% 3/32 [00:00<00:06,  4.82it/s][A[A

 12% 4/32 [00:00<00:05,  5.17it/s][A[A

 16% 5/32 [00:00<00:05,  5.17it/s][A[A

 19% 6/32 [00:01<00:05,  4.89it/s][A[A

 22% 7/32 [00:01<00:04,  5.48it/s][A[A

 25% 8/32 [00:01<00:04,  5.41it/s][A[A

 28% 9/32 [00:01<00:03,  5.83it/s][A[A

 31% 10/32 [00:02<00:10,  2.14it/s][A[A

 34% 11/32 [00:02<00:07,  2.70it/s][A[A

 38% 12/32 [00:03<00:05,  3.36it/s][A[A

 41% 13/32 [00:03<00:04,  3.82it/s][A[A

 44% 14/32 [00:03<00:04,  4.02it/s][A[A

 47% 15/32 [00:03<00:03,  4.61it/s][A[A

 50% 16/32 [00:03<00:03,  4.58it/s][A[A

 53% 17/32 [00:04<00:03,  4.98it/s][A[A

 56% 18/32 [00:04<00:02,  5.27it/s][A[A

 59% 19/32 [00:04<00:02,  5.71it/s][A[A

 62% 20/32 [00:04<00:02,  5.32it/s][A[A

 66% 21/32 [00:04<00:02,  5.10it/s][A[A

 69% 22/32 [00:04<00:02,  4.76it/s][A[A

 72% 23/32 [00:05<00:01,  4.52it/s][A[A

 75% 24/32 [00:06<00:04,  1.93it/s][A[A

 78% 25/32 [00:06<00:02,  2.49it/s][A[A

 81% 26/32 [00:06<00:01,  3.15it/s][A[A

 84% 27/32 [00:06<00:01,  3.49it/s][A[A

 88% 28/32 [00:07<00:01,  3.84it/s][A[A

 91% 29/32 [00:07<00:00,  4.53it/s][A[A

 94% 30/32 [00:07<00:00,  4.50it/s][A[A

 97% 31/32 [00:07<00:00,  5.08it/s][A[A

100% 32/32 [00:07<00:00,  4.97it/s][A[A100% 32/32 [00:07<00:00,  4.09it/s]
Meta loss on this task batch = 5.8362e-01, PNorm = 35.1686, GNorm = 0.0415

 21% 4/19 [00:33<02:07,  8.52s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.56it/s][A[A

  6% 2/32 [00:00<00:05,  5.22it/s][A[A

  9% 3/32 [00:00<00:05,  5.57it/s][A[A

 12% 4/32 [00:00<00:05,  4.99it/s][A[A

 16% 5/32 [00:00<00:05,  4.87it/s][A[A

 19% 6/32 [00:02<00:12,  2.02it/s][A[A

 22% 7/32 [00:02<00:10,  2.39it/s][A[A

 25% 8/32 [00:02<00:08,  2.99it/s][A[A

 28% 9/32 [00:02<00:06,  3.39it/s][A[A

 31% 10/32 [00:02<00:05,  3.93it/s][A[A

 34% 11/32 [00:03<00:04,  4.27it/s][A[A

 38% 12/32 [00:03<00:04,  4.45it/s][A[A

 41% 13/32 [00:03<00:04,  4.32it/s][A[A

 44% 14/32 [00:03<00:03,  4.90it/s][A[A

 47% 15/32 [00:03<00:03,  4.97it/s][A[A

 50% 16/32 [00:04<00:02,  5.34it/s][A[A

 53% 17/32 [00:04<00:02,  5.03it/s][A[A

 56% 18/32 [00:04<00:02,  5.08it/s][A[A

 59% 19/32 [00:04<00:02,  4.75it/s][A[A

 62% 20/32 [00:04<00:02,  4.66it/s][A[A

 66% 21/32 [00:05<00:02,  4.78it/s][A[A

 69% 22/32 [00:06<00:04,  2.01it/s][A[A

 72% 23/32 [00:06<00:03,  2.50it/s][A[A

 75% 24/32 [00:06<00:02,  2.85it/s][A[A

 78% 25/32 [00:06<00:02,  3.22it/s][A[A

 81% 26/32 [00:07<00:01,  3.87it/s][A[A

 84% 27/32 [00:07<00:01,  4.30it/s][A[A

 88% 28/32 [00:07<00:00,  4.85it/s][A[A

 91% 29/32 [00:07<00:00,  4.87it/s][A[A

 94% 30/32 [00:07<00:00,  5.27it/s][A[A

 97% 31/32 [00:07<00:00,  5.33it/s][A[A

100% 32/32 [00:08<00:00,  5.04it/s][A[A100% 32/32 [00:08<00:00,  3.95it/s]
Meta loss on this task batch = 5.1796e-01, PNorm = 35.1827, GNorm = 0.1152

 26% 5/19 [00:42<02:00,  8.62s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  6.82it/s][A[A

  6% 2/32 [00:00<00:04,  6.10it/s][A[A

  9% 3/32 [00:00<00:04,  6.35it/s][A[A

 12% 4/32 [00:00<00:04,  6.00it/s][A[A

 16% 5/32 [00:00<00:04,  5.55it/s][A[A

 19% 6/32 [00:01<00:05,  5.04it/s][A[A

 22% 7/32 [00:02<00:12,  2.04it/s][A[A

 25% 8/32 [00:02<00:09,  2.51it/s][A[A

 28% 9/32 [00:02<00:08,  2.87it/s][A[A

 31% 10/32 [00:02<00:06,  3.24it/s][A[A

 34% 11/32 [00:03<00:05,  3.54it/s][A[A

 38% 12/32 [00:03<00:05,  3.80it/s][A[A

 41% 13/32 [00:03<00:04,  4.38it/s][A[A

 44% 14/32 [00:03<00:04,  4.46it/s][A[A

 47% 15/32 [00:03<00:03,  4.51it/s][A[A

 50% 16/32 [00:04<00:03,  4.34it/s][A[A

 53% 17/32 [00:04<00:03,  4.90it/s][A[A

 56% 18/32 [00:04<00:02,  4.87it/s][A[A

 59% 19/32 [00:05<00:06,  2.02it/s][A[A

 62% 20/32 [00:05<00:04,  2.48it/s][A[A

 66% 21/32 [00:06<00:03,  2.95it/s][A[A

 69% 22/32 [00:06<00:03,  3.32it/s][A[A

 72% 23/32 [00:06<00:02,  3.59it/s][A[A

 75% 24/32 [00:06<00:01,  4.02it/s][A[A

 78% 25/32 [00:06<00:01,  4.16it/s][A[A

 81% 26/32 [00:07<00:01,  4.77it/s][A[A

 84% 27/32 [00:07<00:01,  4.56it/s][A[A

 88% 28/32 [00:07<00:00,  4.61it/s][A[A

 91% 29/32 [00:07<00:00,  4.68it/s][A[A

 94% 30/32 [00:07<00:00,  4.90it/s][A[A

 97% 31/32 [00:08<00:00,  5.40it/s][A[A

100% 32/32 [00:08<00:00,  5.34it/s][A[A100% 32/32 [00:08<00:00,  3.87it/s]
Meta loss on this task batch = 4.5298e-01, PNorm = 35.1981, GNorm = 0.1616

 32% 6/19 [00:51<01:53,  8.74s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:36,  1.19s/it][A[A

  6% 2/32 [00:01<00:26,  1.12it/s][A[A

  9% 3/32 [00:01<00:19,  1.46it/s][A[A

 12% 4/32 [00:01<00:15,  1.83it/s][A[A

 16% 5/32 [00:01<00:11,  2.37it/s][A[A

 19% 6/32 [00:02<00:09,  2.83it/s][A[A

 22% 7/32 [00:02<00:07,  3.19it/s][A[A

 25% 8/32 [00:02<00:06,  3.70it/s][A[A

 28% 9/32 [00:02<00:05,  4.14it/s][A[A

 31% 10/32 [00:02<00:05,  4.31it/s][A[A

 34% 11/32 [00:03<00:04,  4.37it/s][A[A

 38% 12/32 [00:03<00:04,  4.38it/s][A[A

 41% 13/32 [00:03<00:04,  4.52it/s][A[A

 44% 14/32 [00:03<00:03,  4.99it/s][A[A

 47% 15/32 [00:03<00:03,  4.75it/s][A[A

 50% 16/32 [00:05<00:08,  1.92it/s][A[A

 53% 17/32 [00:05<00:06,  2.41it/s][A[A

 56% 18/32 [00:05<00:04,  2.80it/s][A[A

 59% 19/32 [00:05<00:03,  3.32it/s][A[A

 62% 20/32 [00:05<00:03,  3.60it/s][A[A

 66% 21/32 [00:06<00:02,  3.83it/s][A[A

 69% 22/32 [00:06<00:02,  4.12it/s][A[A

 72% 23/32 [00:06<00:02,  4.15it/s][A[A

 75% 24/32 [00:06<00:01,  4.22it/s][A[A

 78% 25/32 [00:07<00:01,  4.60it/s][A[A

 81% 26/32 [00:07<00:01,  4.95it/s][A[A

 84% 27/32 [00:07<00:01,  4.85it/s][A[A

 88% 28/32 [00:07<00:00,  4.61it/s][A[A

 91% 29/32 [00:07<00:00,  4.98it/s][A[A

 94% 30/32 [00:09<00:01,  2.00it/s][A[A

 97% 31/32 [00:09<00:00,  2.52it/s][A[A

100% 32/32 [00:09<00:00,  2.89it/s][A[A100% 32/32 [00:09<00:00,  3.40it/s]
Meta loss on this task batch = 4.7035e-01, PNorm = 35.2138, GNorm = 0.0802

 37% 7/19 [01:01<01:49,  9.17s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.05it/s][A[A

  6% 2/32 [00:00<00:07,  4.04it/s][A[A

  9% 3/32 [00:00<00:07,  4.03it/s][A[A

 12% 4/32 [00:00<00:06,  4.06it/s][A[A

 16% 5/32 [00:01<00:06,  4.22it/s][A[A

 19% 6/32 [00:01<00:06,  4.08it/s][A[A

 22% 7/32 [00:01<00:05,  4.53it/s][A[A

 25% 8/32 [00:01<00:05,  4.31it/s][A[A

 28% 9/32 [00:02<00:05,  4.27it/s][A[A

 31% 10/32 [00:02<00:05,  4.31it/s][A[A

 34% 11/32 [00:02<00:04,  4.76it/s][A[A

 38% 12/32 [00:03<00:10,  1.97it/s][A[A

 41% 13/32 [00:03<00:07,  2.42it/s][A[A

 44% 14/32 [00:04<00:06,  2.96it/s][A[A

 47% 15/32 [00:04<00:04,  3.45it/s][A[A

 50% 16/32 [00:04<00:04,  3.63it/s][A[A

 53% 17/32 [00:04<00:04,  3.72it/s][A[A

 56% 18/32 [00:04<00:03,  3.94it/s][A[A

 59% 19/32 [00:05<00:03,  3.97it/s][A[A

 62% 20/32 [00:05<00:02,  4.32it/s][A[A

 66% 21/32 [00:05<00:02,  4.47it/s][A[A

 69% 22/32 [00:05<00:02,  4.38it/s][A[A

 72% 23/32 [00:06<00:01,  4.77it/s][A[A

 75% 24/32 [00:07<00:04,  1.92it/s][A[A

 78% 25/32 [00:07<00:03,  2.31it/s][A[A

 81% 26/32 [00:07<00:02,  2.77it/s][A[A

 84% 27/32 [00:07<00:01,  3.04it/s][A[A

 88% 28/32 [00:08<00:01,  3.26it/s][A[A

 91% 29/32 [00:08<00:00,  3.51it/s][A[A

 94% 30/32 [00:08<00:00,  3.73it/s][A[A

 97% 31/32 [00:08<00:00,  3.89it/s][A[A

100% 32/32 [00:09<00:00,  3.97it/s][A[A100% 32/32 [00:09<00:00,  3.51it/s]
Meta loss on this task batch = 3.6149e-01, PNorm = 35.2299, GNorm = 0.0456

 42% 8/19 [01:11<01:43,  9.39s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.09it/s][A[A

  6% 2/32 [00:01<00:16,  1.85it/s][A[A

  9% 3/32 [00:01<00:12,  2.24it/s][A[A

 12% 4/32 [00:01<00:10,  2.59it/s][A[A

 16% 5/32 [00:02<00:09,  2.88it/s][A[A

 19% 6/32 [00:02<00:08,  3.19it/s][A[A

 22% 7/32 [00:02<00:07,  3.47it/s][A[A

 25% 8/32 [00:02<00:06,  3.68it/s][A[A

 28% 9/32 [00:03<00:06,  3.79it/s][A[A

 31% 10/32 [00:03<00:05,  3.89it/s][A[A

 34% 11/32 [00:03<00:05,  4.02it/s][A[A

 38% 12/32 [00:03<00:04,  4.06it/s][A[A

 41% 13/32 [00:04<00:04,  4.06it/s][A[A

 44% 14/32 [00:05<00:10,  1.80it/s][A[A

 47% 15/32 [00:05<00:07,  2.19it/s][A[A

 50% 16/32 [00:05<00:06,  2.58it/s][A[A

 53% 17/32 [00:06<00:05,  2.93it/s][A[A

 56% 18/32 [00:06<00:04,  3.28it/s][A[A

 59% 19/32 [00:06<00:03,  3.49it/s][A[A

 62% 20/32 [00:06<00:03,  3.60it/s][A[A

 66% 21/32 [00:07<00:02,  3.76it/s][A[A

 69% 22/32 [00:07<00:02,  3.87it/s][A[A

 72% 23/32 [00:07<00:02,  3.95it/s][A[A

 75% 24/32 [00:08<00:04,  1.82it/s][A[A

 78% 25/32 [00:08<00:03,  2.22it/s][A[A

 81% 26/32 [00:09<00:02,  2.64it/s][A[A

 84% 27/32 [00:09<00:01,  2.93it/s][A[A

 88% 28/32 [00:09<00:01,  3.25it/s][A[A

 91% 29/32 [00:09<00:00,  3.56it/s][A[A

 94% 30/32 [00:10<00:00,  3.76it/s][A[A

 97% 31/32 [00:10<00:00,  3.89it/s][A[A

100% 32/32 [00:10<00:00,  4.06it/s][A[A100% 32/32 [00:10<00:00,  3.03it/s]
Meta loss on this task batch = 2.1288e-01, PNorm = 35.2450, GNorm = 0.0635

 47% 9/19 [01:23<01:39,  9.99s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  3.88it/s][A[A

  6% 2/32 [00:00<00:07,  4.06it/s][A[A

  9% 3/32 [00:00<00:06,  4.16it/s][A[A

 12% 4/32 [00:01<00:15,  1.84it/s][A[A

 16% 5/32 [00:02<00:12,  2.21it/s][A[A

 19% 6/32 [00:02<00:09,  2.60it/s][A[A

 22% 7/32 [00:02<00:08,  2.94it/s][A[A

 25% 8/32 [00:02<00:07,  3.20it/s][A[A

 28% 9/32 [00:03<00:06,  3.40it/s][A[A

 31% 10/32 [00:03<00:06,  3.62it/s][A[A

 34% 11/32 [00:03<00:05,  3.80it/s][A[A

 38% 12/32 [00:03<00:05,  3.82it/s][A[A

 41% 13/32 [00:05<00:10,  1.82it/s][A[A

 44% 14/32 [00:05<00:08,  2.19it/s][A[A

 47% 15/32 [00:05<00:06,  2.53it/s][A[A

 50% 16/32 [00:05<00:05,  2.88it/s][A[A

 53% 17/32 [00:06<00:04,  3.16it/s][A[A

 56% 18/32 [00:06<00:04,  3.40it/s][A[A

 59% 19/32 [00:06<00:03,  3.65it/s][A[A

 62% 20/32 [00:06<00:03,  3.83it/s][A[A

 66% 21/32 [00:07<00:02,  3.95it/s][A[A

 69% 22/32 [00:07<00:02,  4.03it/s][A[A

 72% 23/32 [00:07<00:02,  4.06it/s][A[A

 75% 24/32 [00:07<00:01,  4.03it/s][A[A

 78% 25/32 [00:07<00:01,  4.14it/s][A[A

 81% 26/32 [00:08<00:01,  4.42it/s][A[A

 84% 27/32 [00:08<00:01,  4.81it/s][A[A

 88% 28/32 [00:08<00:00,  4.72it/s][A[A

 91% 29/32 [00:09<00:01,  1.91it/s][A[A

 94% 30/32 [00:10<00:00,  2.29it/s][A[A

 97% 31/32 [00:10<00:00,  2.67it/s][A[A

100% 32/32 [00:10<00:00,  3.14it/s][A[A100% 32/32 [00:10<00:00,  3.06it/s]
Meta loss on this task batch = 2.9343e-01, PNorm = 35.2554, GNorm = 0.2253

 53% 10/19 [01:34<01:33, 10.37s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.19it/s][A[A

  6% 2/32 [00:00<00:07,  4.22it/s][A[A

  9% 3/32 [00:00<00:06,  4.61it/s][A[A

 12% 4/32 [00:00<00:05,  4.77it/s][A[A

 16% 5/32 [00:01<00:05,  4.61it/s][A[A

 19% 6/32 [00:01<00:05,  4.97it/s][A[A

 22% 7/32 [00:02<00:12,  1.94it/s][A[A

 25% 8/32 [00:02<00:10,  2.29it/s][A[A

 28% 9/32 [00:02<00:08,  2.79it/s][A[A

 31% 10/32 [00:03<00:07,  3.08it/s][A[A

 34% 11/32 [00:03<00:06,  3.33it/s][A[A

 38% 12/32 [00:03<00:05,  3.57it/s][A[A

 41% 13/32 [00:03<00:05,  3.77it/s][A[A

 44% 14/32 [00:05<00:09,  1.81it/s][A[A

 47% 15/32 [00:05<00:07,  2.24it/s][A[A

 50% 16/32 [00:05<00:06,  2.59it/s][A[A

 53% 17/32 [00:05<00:04,  3.07it/s][A[A

 56% 18/32 [00:05<00:04,  3.33it/s][A[A

 59% 19/32 [00:06<00:03,  3.51it/s][A[A

 62% 20/32 [00:06<00:03,  3.98it/s][A[A

 66% 21/32 [00:06<00:02,  4.30it/s][A[A

 69% 22/32 [00:06<00:02,  4.18it/s][A[A

 72% 23/32 [00:07<00:02,  4.15it/s][A[A

 75% 24/32 [00:07<00:01,  4.29it/s][A[A

 78% 25/32 [00:07<00:01,  4.47it/s][A[A

 81% 26/32 [00:08<00:03,  1.89it/s][A[A

 84% 27/32 [00:08<00:02,  2.26it/s][A[A

 88% 28/32 [00:09<00:01,  2.69it/s][A[A

 91% 29/32 [00:09<00:01,  3.00it/s][A[A

 94% 30/32 [00:09<00:00,  3.26it/s][A[A

 97% 31/32 [00:09<00:00,  3.75it/s][A[A

100% 32/32 [00:10<00:00,  4.01it/s][A[A100% 32/32 [00:10<00:00,  3.19it/s]
Meta loss on this task batch = 7.6944e-01, PNorm = 35.2551, GNorm = 0.7182

 58% 11/19 [01:45<01:24, 10.51s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.32it/s][A[A

  6% 2/32 [00:00<00:06,  4.43it/s][A[A

  9% 3/32 [00:00<00:06,  4.38it/s][A[A

 12% 4/32 [00:00<00:06,  4.33it/s][A[A

 16% 5/32 [00:02<00:14,  1.89it/s][A[A

 19% 6/32 [00:02<00:11,  2.30it/s][A[A

 22% 7/32 [00:02<00:09,  2.64it/s][A[A

 25% 8/32 [00:02<00:08,  2.97it/s][A[A

 28% 9/32 [00:03<00:07,  3.24it/s][A[A

 31% 10/32 [00:03<00:05,  3.69it/s][A[A

 34% 11/32 [00:03<00:05,  3.79it/s][A[A

 38% 12/32 [00:03<00:04,  4.06it/s][A[A

 41% 13/32 [00:04<00:10,  1.83it/s][A[A

 44% 14/32 [00:05<00:07,  2.27it/s][A[A

 47% 15/32 [00:05<00:06,  2.67it/s][A[A

 50% 16/32 [00:05<00:05,  3.03it/s][A[A

 53% 17/32 [00:05<00:04,  3.25it/s][A[A

 56% 18/32 [00:06<00:04,  3.44it/s][A[A

 59% 19/32 [00:06<00:03,  3.71it/s][A[A

 62% 20/32 [00:06<00:02,  4.12it/s][A[A

 66% 21/32 [00:06<00:02,  4.27it/s][A[A

 69% 22/32 [00:06<00:02,  4.24it/s][A[A

 72% 23/32 [00:07<00:02,  4.22it/s][A[A

 75% 24/32 [00:08<00:04,  1.86it/s][A[A

 78% 25/32 [00:08<00:03,  2.23it/s][A[A

 81% 26/32 [00:08<00:02,  2.57it/s][A[A

 84% 27/32 [00:09<00:01,  2.93it/s][A[A

 88% 28/32 [00:09<00:01,  3.40it/s][A[A

 91% 29/32 [00:09<00:00,  3.79it/s][A[A

 94% 30/32 [00:09<00:00,  4.02it/s][A[A

 97% 31/32 [00:09<00:00,  4.27it/s][A[A

100% 32/32 [00:10<00:00,  4.19it/s][A[A100% 32/32 [00:10<00:00,  3.14it/s]
Meta loss on this task batch = 5.3868e-01, PNorm = 35.2568, GNorm = 0.0384

 63% 12/19 [01:56<01:14, 10.66s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:38,  1.23s/it][A[A

  6% 2/32 [00:01<00:27,  1.07it/s][A[A

  9% 3/32 [00:01<00:20,  1.38it/s][A[A

 12% 4/32 [00:01<00:16,  1.74it/s][A[A

 16% 5/32 [00:02<00:12,  2.20it/s][A[A

 19% 6/32 [00:02<00:10,  2.57it/s][A[A

 22% 7/32 [00:02<00:08,  2.93it/s][A[A

 25% 8/32 [00:02<00:06,  3.44it/s][A[A

 28% 9/32 [00:02<00:06,  3.75it/s][A[A

 31% 10/32 [00:03<00:05,  3.84it/s][A[A

 34% 11/32 [00:03<00:05,  4.16it/s][A[A

 38% 12/32 [00:03<00:04,  4.22it/s][A[A

 41% 13/32 [00:03<00:04,  4.24it/s][A[A

 44% 14/32 [00:04<00:04,  4.29it/s][A[A

 47% 15/32 [00:04<00:03,  4.50it/s][A[A

 50% 16/32 [00:04<00:03,  4.67it/s][A[A

 53% 17/32 [00:05<00:07,  1.97it/s][A[A

 56% 18/32 [00:05<00:05,  2.36it/s][A[A

 59% 19/32 [00:06<00:04,  2.74it/s][A[A

 62% 20/32 [00:06<00:03,  3.02it/s][A[A

 66% 21/32 [00:06<00:03,  3.27it/s][A[A

 69% 22/32 [00:06<00:02,  3.50it/s][A[A

 72% 23/32 [00:07<00:02,  3.79it/s][A[A

 75% 24/32 [00:07<00:01,  4.13it/s][A[A

 78% 25/32 [00:07<00:01,  4.50it/s][A[A

 81% 26/32 [00:07<00:01,  4.40it/s][A[A

 84% 27/32 [00:07<00:01,  4.57it/s][A[A

 88% 28/32 [00:09<00:02,  1.90it/s][A[A

 91% 29/32 [00:09<00:01,  2.30it/s][A[A

 94% 30/32 [00:09<00:00,  2.79it/s][A[A

 97% 31/32 [00:09<00:00,  3.15it/s][A[A

100% 32/32 [00:09<00:00,  3.58it/s][A[A100% 32/32 [00:09<00:00,  3.22it/s]
Meta loss on this task batch = 6.0574e-01, PNorm = 35.2569, GNorm = 0.1687

 68% 13/19 [02:07<01:04, 10.69s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.65it/s][A[A

  6% 2/32 [00:00<00:05,  5.54it/s][A[A

  9% 3/32 [00:00<00:05,  5.11it/s][A[A

 12% 4/32 [00:00<00:05,  4.80it/s][A[A

 16% 5/32 [00:01<00:05,  4.76it/s][A[A

 19% 6/32 [00:01<00:05,  4.88it/s][A[A

 22% 7/32 [00:01<00:05,  4.78it/s][A[A

 25% 8/32 [00:01<00:05,  4.78it/s][A[A

 28% 9/32 [00:02<00:11,  1.94it/s][A[A

 31% 10/32 [00:03<00:09,  2.29it/s][A[A

 34% 11/32 [00:03<00:07,  2.66it/s][A[A

 38% 12/32 [00:03<00:06,  2.98it/s][A[A

 41% 13/32 [00:03<00:05,  3.38it/s][A[A

 44% 14/32 [00:04<00:05,  3.58it/s][A[A

 47% 15/32 [00:04<00:04,  3.90it/s][A[A

 50% 16/32 [00:04<00:03,  4.06it/s][A[A

 53% 17/32 [00:04<00:03,  4.08it/s][A[A

 56% 18/32 [00:05<00:07,  1.83it/s][A[A

 59% 19/32 [00:06<00:05,  2.23it/s][A[A

 62% 20/32 [00:06<00:04,  2.59it/s][A[A

 66% 21/32 [00:06<00:03,  3.04it/s][A[A

 69% 22/32 [00:06<00:02,  3.40it/s][A[A

 72% 23/32 [00:07<00:02,  3.67it/s][A[A

 75% 24/32 [00:07<00:02,  3.90it/s][A[A

 78% 25/32 [00:07<00:01,  3.98it/s][A[A

 81% 26/32 [00:07<00:01,  4.17it/s][A[A

 84% 27/32 [00:07<00:01,  4.40it/s][A[A

 88% 28/32 [00:08<00:00,  4.42it/s][A[A

 91% 29/32 [00:09<00:01,  1.91it/s][A[A

 94% 30/32 [00:09<00:00,  2.32it/s][A[A

 97% 31/32 [00:09<00:00,  2.67it/s][A[A

100% 32/32 [00:10<00:00,  2.91it/s][A[A100% 32/32 [00:10<00:00,  3.16it/s]
Meta loss on this task batch = 5.8257e-01, PNorm = 35.2571, GNorm = 0.0448

 74% 14/19 [02:18<00:53, 10.76s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  6.02it/s][A[A

  6% 2/32 [00:00<00:05,  5.14it/s][A[A

  9% 3/32 [00:00<00:06,  4.77it/s][A[A

 12% 4/32 [00:00<00:06,  4.62it/s][A[A

 16% 5/32 [00:01<00:05,  4.67it/s][A[A

 19% 6/32 [00:01<00:05,  4.65it/s][A[A

 22% 7/32 [00:02<00:13,  1.92it/s][A[A

 25% 8/32 [00:02<00:10,  2.39it/s][A[A

 28% 9/32 [00:03<00:08,  2.70it/s][A[A

 31% 10/32 [00:03<00:06,  3.17it/s][A[A

 34% 11/32 [00:03<00:05,  3.68it/s][A[A

 38% 12/32 [00:03<00:04,  4.23it/s][A[A

 41% 13/32 [00:03<00:04,  4.29it/s][A[A

 44% 14/32 [00:03<00:04,  4.26it/s][A[A

 47% 15/32 [00:04<00:03,  4.60it/s][A[A

 50% 16/32 [00:04<00:03,  4.67it/s][A[A

 53% 17/32 [00:04<00:03,  4.76it/s][A[A

 56% 18/32 [00:04<00:02,  4.70it/s][A[A

 59% 19/32 [00:04<00:02,  5.19it/s][A[A

 62% 20/32 [00:06<00:06,  1.99it/s][A[A

 66% 21/32 [00:06<00:04,  2.47it/s][A[A

 69% 22/32 [00:06<00:03,  2.85it/s][A[A

 72% 23/32 [00:06<00:02,  3.05it/s][A[A

 75% 24/32 [00:07<00:02,  3.39it/s][A[A

 78% 25/32 [00:07<00:01,  3.87it/s][A[A

 81% 26/32 [00:07<00:01,  3.96it/s][A[A

 84% 27/32 [00:07<00:01,  4.00it/s][A[A

 88% 28/32 [00:07<00:00,  4.32it/s][A[A

 91% 29/32 [00:08<00:00,  4.24it/s][A[A

 94% 30/32 [00:08<00:00,  4.26it/s][A[A

 97% 31/32 [00:08<00:00,  4.73it/s][A[A

100% 32/32 [00:08<00:00,  4.61it/s][A[A100% 32/32 [00:08<00:00,  3.65it/s]
Meta loss on this task batch = 5.0713e-01, PNorm = 35.2583, GNorm = 0.0705

 79% 15/19 [02:27<00:41, 10.39s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:36,  1.19s/it][A[A

  6% 2/32 [00:01<00:26,  1.13it/s][A[A

  9% 3/32 [00:01<00:19,  1.49it/s][A[A

 12% 4/32 [00:01<00:14,  1.91it/s][A[A

 16% 5/32 [00:01<00:11,  2.36it/s][A[A

 19% 6/32 [00:02<00:09,  2.73it/s][A[A

 22% 7/32 [00:02<00:07,  3.21it/s][A[A

 25% 8/32 [00:02<00:06,  3.70it/s][A[A

 28% 9/32 [00:02<00:05,  4.12it/s][A[A

 31% 10/32 [00:02<00:05,  4.15it/s][A[A

 34% 11/32 [00:04<00:11,  1.86it/s][A[A

 38% 12/32 [00:04<00:08,  2.23it/s][A[A

 41% 13/32 [00:04<00:07,  2.61it/s][A[A

 44% 14/32 [00:04<00:06,  2.98it/s][A[A

 47% 15/32 [00:05<00:05,  3.28it/s][A[A

 50% 16/32 [00:05<00:04,  3.80it/s][A[A

 53% 17/32 [00:05<00:03,  3.90it/s][A[A

 56% 18/32 [00:05<00:03,  4.25it/s][A[A

 59% 19/32 [00:05<00:03,  4.21it/s][A[A

 62% 20/32 [00:06<00:02,  4.29it/s][A[A

 66% 21/32 [00:06<00:02,  4.32it/s][A[A

 69% 22/32 [00:07<00:05,  1.94it/s][A[A

 72% 23/32 [00:07<00:03,  2.33it/s][A[A

 75% 24/32 [00:08<00:03,  2.65it/s][A[A

 78% 25/32 [00:08<00:02,  3.24it/s][A[A

 81% 26/32 [00:08<00:01,  3.66it/s][A[A

 84% 27/32 [00:08<00:01,  3.71it/s][A[A

 88% 28/32 [00:08<00:01,  3.88it/s][A[A

 91% 29/32 [00:09<00:00,  3.76it/s][A[A

 94% 30/32 [00:09<00:00,  3.87it/s][A[A

 97% 31/32 [00:09<00:00,  3.99it/s][A[A

100% 32/32 [00:10<00:00,  1.84it/s][A[A100% 32/32 [00:10<00:00,  2.96it/s]
Meta loss on this task batch = 5.6963e-01, PNorm = 35.2605, GNorm = 0.0308

 84% 16/19 [02:39<00:32, 10.75s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  3.88it/s][A[A

  6% 2/32 [00:00<00:07,  3.86it/s][A[A

  9% 3/32 [00:00<00:06,  4.19it/s][A[A

 12% 4/32 [00:00<00:06,  4.46it/s][A[A

 16% 5/32 [00:01<00:06,  4.44it/s][A[A

 19% 6/32 [00:01<00:05,  5.07it/s][A[A

 22% 7/32 [00:01<00:04,  5.11it/s][A[A

 25% 8/32 [00:01<00:05,  4.64it/s][A[A

 28% 9/32 [00:01<00:05,  4.52it/s][A[A

 31% 10/32 [00:02<00:04,  5.03it/s][A[A

 34% 11/32 [00:02<00:04,  4.86it/s][A[A

 38% 12/32 [00:03<00:09,  2.02it/s][A[A

 41% 13/32 [00:03<00:07,  2.46it/s][A[A

 44% 14/32 [00:03<00:06,  2.89it/s][A[A

 47% 15/32 [00:04<00:05,  3.33it/s][A[A

 50% 16/32 [00:04<00:04,  3.71it/s][A[A

 53% 17/32 [00:04<00:03,  4.13it/s][A[A

 56% 18/32 [00:04<00:03,  4.17it/s][A[A

 59% 19/32 [00:04<00:03,  4.05it/s][A[A

 62% 20/32 [00:05<00:02,  4.14it/s][A[A

 66% 21/32 [00:05<00:02,  4.26it/s][A[A

 69% 22/32 [00:05<00:02,  4.26it/s][A[A

 72% 23/32 [00:05<00:02,  4.44it/s][A[A

 75% 24/32 [00:07<00:04,  1.86it/s][A[A

 78% 25/32 [00:07<00:03,  2.28it/s][A[A

 81% 26/32 [00:07<00:02,  2.67it/s][A[A

 84% 27/32 [00:07<00:01,  3.19it/s][A[A

 88% 28/32 [00:07<00:01,  3.65it/s][A[A

 91% 29/32 [00:08<00:00,  3.83it/s][A[A

 94% 30/32 [00:08<00:00,  3.93it/s][A[A

 97% 31/32 [00:08<00:00,  4.17it/s][A[A

100% 32/32 [00:08<00:00,  4.18it/s][A[A100% 32/32 [00:08<00:00,  3.63it/s]
Meta loss on this task batch = 4.5372e-01, PNorm = 35.2660, GNorm = 0.2036

 89% 17/19 [02:48<00:20, 10.40s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.29it/s][A[A

  6% 2/32 [00:00<00:06,  4.98it/s][A[A

  9% 3/32 [00:00<00:05,  4.89it/s][A[A

 12% 4/32 [00:00<00:05,  4.71it/s][A[A

 16% 5/32 [00:01<00:06,  4.48it/s][A[A

 19% 6/32 [00:01<00:05,  4.45it/s][A[A

 22% 7/32 [00:02<00:13,  1.88it/s][A[A

 25% 8/32 [00:02<00:10,  2.24it/s][A[A

 28% 9/32 [00:03<00:08,  2.64it/s][A[A

 31% 10/32 [00:03<00:07,  2.99it/s][A[A

 34% 11/32 [00:03<00:06,  3.33it/s][A[A

 38% 12/32 [00:03<00:05,  3.79it/s][A[A

 41% 13/32 [00:03<00:04,  3.90it/s][A[A

 44% 14/32 [00:04<00:03,  4.50it/s][A[A

 47% 15/32 [00:04<00:03,  4.31it/s][A[A

 50% 16/32 [00:04<00:03,  4.42it/s][A[A

 53% 17/32 [00:04<00:02,  5.07it/s][A[A

 56% 18/32 [00:05<00:06,  2.00it/s][A[A

 59% 19/32 [00:06<00:05,  2.36it/s][A[A

 62% 20/32 [00:06<00:04,  2.75it/s][A[A

 66% 21/32 [00:06<00:03,  3.08it/s][A[A

 69% 22/32 [00:06<00:02,  3.72it/s][A[A

 72% 23/32 [00:06<00:02,  3.88it/s][A[A

 75% 24/32 [00:07<00:02,  3.74it/s][A[A

 78% 25/32 [00:07<00:01,  3.93it/s][A[A

 81% 26/32 [00:07<00:01,  4.12it/s][A[A

 84% 27/32 [00:07<00:01,  3.91it/s][A[A

 88% 28/32 [00:08<00:00,  4.19it/s][A[A

 91% 29/32 [00:09<00:01,  1.94it/s][A[A

 94% 30/32 [00:09<00:00,  2.31it/s][A[A

 97% 31/32 [00:09<00:00,  2.74it/s][A[A

100% 32/32 [00:10<00:00,  2.97it/s][A[A100% 32/32 [00:10<00:00,  3.19it/s]
Meta loss on this task batch = 5.2519e-01, PNorm = 35.2733, GNorm = 0.1152

 95% 18/19 [02:59<00:10, 10.53s/it][A

  0% 0/23 [00:00<?, ?it/s][A[A

  4% 1/23 [00:00<00:05,  4.10it/s][A[A

  9% 2/23 [00:00<00:04,  4.51it/s][A[A

 13% 3/23 [00:00<00:04,  4.55it/s][A[A

 17% 4/23 [00:00<00:03,  5.03it/s][A[A

 22% 5/23 [00:00<00:03,  5.63it/s][A[A

 26% 6/23 [00:01<00:03,  5.32it/s][A[A

 30% 7/23 [00:02<00:08,  1.99it/s][A[A

 35% 8/23 [00:02<00:06,  2.35it/s][A[A

 39% 9/23 [00:02<00:04,  2.96it/s][A[A

 43% 10/23 [00:02<00:04,  3.19it/s][A[A

 48% 11/23 [00:03<00:03,  3.62it/s][A[A

 52% 12/23 [00:03<00:02,  3.78it/s][A[A

 57% 13/23 [00:03<00:02,  4.43it/s][A[A

 61% 14/23 [00:03<00:01,  4.68it/s][A[A

 65% 15/23 [00:04<00:02,  3.77it/s][A[A

 70% 16/23 [00:04<00:01,  3.81it/s][A[A

 74% 17/23 [00:05<00:03,  1.90it/s][A[A

 78% 18/23 [00:05<00:02,  2.33it/s][A[A

 83% 19/23 [00:05<00:01,  2.67it/s][A[A

 87% 20/23 [00:06<00:00,  3.34it/s][A[A

 91% 21/23 [00:06<00:00,  3.86it/s][A[A

 96% 22/23 [00:06<00:00,  4.57it/s][A[A

100% 23/23 [00:06<00:00,  5.04it/s][A[A100% 23/23 [00:06<00:00,  3.52it/s]
Meta loss on this task batch = 4.6565e-01, PNorm = 35.2827, GNorm = 0.2138

100% 19/19 [03:06<00:00,  9.49s/it][A100% 19/19 [03:06<00:00,  9.82s/it]
Took 186.66995120048523 seconds to complete one epoch of meta training

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/20 [00:00<?, ?it/s][A[A


  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 32.65it/s]


  5% 1/20 [00:00<00:02,  8.46it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 21.35it/s]


 10% 2/20 [00:00<00:02,  6.50it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 18.21it/s][A[A[A100% 3/3 [00:00<00:00, 20.32it/s]


 15% 3/20 [00:00<00:03,  5.19it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 15.52it/s]


 20% 4/20 [00:00<00:03,  4.93it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 15.05it/s][A[A[A100% 4/4 [00:00<00:00, 17.40it/s]


 25% 5/20 [00:01<00:03,  3.84it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.85it/s][A[A[A100% 4/4 [00:00<00:00, 16.57it/s]


 30% 6/20 [00:01<00:04,  3.36it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.85it/s][A[A[A100% 4/4 [00:00<00:00, 21.57it/s]


 35% 7/20 [00:02<00:07,  1.65it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.53it/s][A[A[A


100% 4/4 [00:00<00:00, 19.30it/s][A[A[A100% 4/4 [00:00<00:00, 19.13it/s]


 40% 8/20 [00:03<00:06,  1.85it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.95it/s][A[A[A100% 4/4 [00:00<00:00, 23.65it/s]


 45% 9/20 [00:03<00:05,  2.10it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.40it/s][A[A[A


100% 4/4 [00:00<00:00, 19.35it/s][A[A[A100% 4/4 [00:00<00:00, 19.29it/s]


 50% 10/20 [00:04<00:04,  2.25it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 16.87it/s][A[A[A100% 4/4 [00:00<00:00, 19.14it/s]


 55% 11/20 [00:04<00:03,  2.38it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 17.80it/s][A[A[A100% 4/4 [00:00<00:00, 21.06it/s]


 60% 12/20 [00:04<00:03,  2.48it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.12it/s][A[A[A


100% 3/3 [00:01<00:00,  2.68it/s][A[A[A100% 3/3 [00:01<00:00,  2.46it/s]


 65% 13/20 [00:06<00:04,  1.43it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 13.75it/s][A[A[A100% 3/3 [00:00<00:00, 13.49it/s]


 70% 14/20 [00:06<00:03,  1.64it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 14.02it/s][A[A[A100% 4/4 [00:00<00:00, 18.24it/s]


 75% 15/20 [00:06<00:02,  1.83it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 13.81it/s][A[A[A100% 3/3 [00:00<00:00, 17.04it/s]


 80% 16/20 [00:07<00:01,  2.05it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 11.94it/s][A[A[A100% 3/3 [00:00<00:00, 14.67it/s]


 85% 17/20 [00:07<00:01,  2.16it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 22.31it/s]


 90% 18/20 [00:07<00:00,  2.51it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 15.51it/s][A[A[A100% 3/3 [00:00<00:00, 19.72it/s]


 95% 19/20 [00:08<00:00,  2.69it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 13.73it/s]


100% 20/20 [00:08<00:00,  3.00it/s][A[A100% 20/20 [00:08<00:00,  2.34it/s]

100% 1/1 [00:08<00:00,  8.53s/it][A100% 1/1 [00:08<00:00,  8.53s/it]
Took 195.20389699935913 seconds to complete one epoch of meta training and validating
Meta validation score auc = 0.590154
Found better MAML checkpoint after meta validation, saving now
 10% 3/30 [09:48<1:29:02, 197.87s/it]Epoch 3

  0% 0/19 [00:00<?, ?it/s][A

  0% 0/32 [00:00<?, ?it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  3% 1/32 [00:01<00:35,  1.16s/it][A[A

  6% 2/32 [00:01<00:26,  1.14it/s][A[A

  9% 3/32 [00:01<00:19,  1.50it/s][A[A

 12% 4/32 [00:01<00:14,  1.91it/s][A[A

 16% 5/32 [00:01<00:11,  2.36it/s][A[A

 19% 6/32 [00:02<00:09,  2.80it/s][A[A

 22% 7/32 [00:02<00:07,  3.25it/s][A[A

 25% 8/32 [00:02<00:06,  3.64it/s][A[A

 28% 9/32 [00:02<00:05,  4.05it/s][A[A

 31% 10/32 [00:02<00:05,  4.39it/s][A[A

 34% 11/32 [00:03<00:04,  4.57it/s][A[A

 38% 12/32 [00:03<00:04,  4.38it/s][A[A

 41% 13/32 [00:03<00:04,  4.42it/s][A[A

 44% 14/32 [00:04<00:09,  1.88it/s][A[A

 47% 15/32 [00:04<00:06,  2.43it/s][A[A

 50% 16/32 [00:05<00:05,  2.86it/s][A[A

 53% 17/32 [00:05<00:04,  3.34it/s][A[A

 56% 18/32 [00:05<00:03,  3.83it/s][A[A

 59% 19/32 [00:05<00:03,  4.16it/s][A[A

 62% 20/32 [00:05<00:02,  4.47it/s][A[A

 66% 21/32 [00:06<00:02,  4.83it/s][A[A

 69% 22/32 [00:06<00:01,  5.05it/s][A[A

 72% 23/32 [00:06<00:01,  5.55it/s][A[A

 75% 24/32 [00:06<00:01,  5.56it/s][A[A

 78% 25/32 [00:06<00:01,  5.53it/s][A[A

 81% 26/32 [00:06<00:01,  5.21it/s][A[A

 84% 27/32 [00:07<00:00,  5.62it/s][A[A

 88% 28/32 [00:07<00:00,  5.67it/s][A[A

 91% 29/32 [00:08<00:01,  2.03it/s][A[A

 94% 30/32 [00:08<00:00,  2.55it/s][A[A

 97% 31/32 [00:08<00:00,  3.09it/s][A[A

100% 32/32 [00:08<00:00,  3.52it/s][A[A100% 32/32 [00:08<00:00,  3.56it/s]
Meta loss on this task batch = 5.2453e-01, PNorm = 35.2941, GNorm = 0.1102

  5% 1/19 [00:09<02:55,  9.74s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.73it/s][A[A

  6% 2/32 [00:00<00:05,  5.47it/s][A[A

  9% 3/32 [00:00<00:05,  4.93it/s][A[A

 12% 4/32 [00:00<00:05,  5.14it/s][A[A

 16% 5/32 [00:00<00:04,  5.54it/s][A[A

 19% 6/32 [00:01<00:04,  5.28it/s][A[A

 22% 7/32 [00:01<00:04,  5.48it/s][A[A

 25% 8/32 [00:01<00:04,  5.73it/s][A[A

 28% 9/32 [00:01<00:04,  5.64it/s][A[A

 31% 10/32 [00:01<00:03,  5.83it/s][A[A

 34% 11/32 [00:01<00:03,  6.27it/s][A[A

 38% 12/32 [00:02<00:03,  5.35it/s][A[A

 41% 13/32 [00:02<00:03,  5.51it/s][A[A

 44% 14/32 [00:02<00:03,  5.54it/s][A[A

 47% 15/32 [00:02<00:02,  5.77it/s][A[A

 50% 16/32 [00:02<00:02,  6.18it/s][A[A

 53% 17/32 [00:04<00:06,  2.16it/s][A[A

 56% 18/32 [00:04<00:05,  2.55it/s][A[A

 59% 19/32 [00:04<00:04,  3.17it/s][A[A

 62% 20/32 [00:04<00:03,  3.46it/s][A[A

 66% 21/32 [00:04<00:02,  4.11it/s][A[A

 69% 22/32 [00:04<00:02,  4.41it/s][A[A

 72% 23/32 [00:05<00:01,  4.84it/s][A[A

 75% 24/32 [00:05<00:01,  5.17it/s][A[A

 78% 25/32 [00:05<00:01,  5.19it/s][A[A

 81% 26/32 [00:05<00:01,  5.05it/s][A[A

 84% 27/32 [00:05<00:00,  5.01it/s][A[A

 88% 28/32 [00:06<00:00,  5.21it/s][A[A

 91% 29/32 [00:06<00:00,  5.44it/s][A[A

 94% 30/32 [00:06<00:00,  5.54it/s][A[A

 97% 31/32 [00:06<00:00,  5.90it/s][A[A

100% 32/32 [00:07<00:00,  2.15it/s][A[A100% 32/32 [00:07<00:00,  4.18it/s]
Meta loss on this task batch = 5.0634e-01, PNorm = 35.3075, GNorm = 0.1019

 11% 2/19 [00:18<02:38,  9.33s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.82it/s][A[A

  6% 2/32 [00:00<00:04,  6.07it/s][A[A

  9% 3/32 [00:00<00:04,  5.92it/s][A[A

 12% 4/32 [00:00<00:04,  5.67it/s][A[A

 16% 5/32 [00:00<00:05,  5.38it/s][A[A

 19% 6/32 [00:01<00:04,  5.36it/s][A[A

 22% 7/32 [00:01<00:04,  5.95it/s][A[A

 25% 8/32 [00:01<00:04,  6.00it/s][A[A

 28% 9/32 [00:01<00:04,  5.69it/s][A[A

 31% 10/32 [00:01<00:04,  5.35it/s][A[A

 34% 11/32 [00:01<00:04,  5.25it/s][A[A

 38% 12/32 [00:02<00:03,  5.34it/s][A[A

 41% 13/32 [00:02<00:03,  5.19it/s][A[A

 44% 14/32 [00:02<00:03,  5.06it/s][A[A

 47% 15/32 [00:02<00:03,  5.55it/s][A[A

 50% 16/32 [00:03<00:07,  2.09it/s][A[A

 53% 17/32 [00:04<00:05,  2.65it/s][A[A

 56% 18/32 [00:04<00:04,  3.20it/s][A[A

 59% 19/32 [00:04<00:03,  3.69it/s][A[A

 62% 20/32 [00:04<00:02,  4.23it/s][A[A

 66% 21/32 [00:04<00:02,  4.47it/s][A[A

 69% 22/32 [00:04<00:02,  4.68it/s][A[A

 72% 23/32 [00:05<00:01,  4.89it/s][A[A

 75% 24/32 [00:05<00:01,  5.02it/s][A[A

 78% 25/32 [00:05<00:01,  5.64it/s][A[A

 81% 26/32 [00:05<00:00,  6.21it/s][A[A

 84% 27/32 [00:05<00:00,  5.47it/s][A[A

 88% 28/32 [00:05<00:00,  5.25it/s][A[A

 91% 29/32 [00:06<00:00,  5.29it/s][A[A

 94% 30/32 [00:07<00:00,  2.00it/s][A[A

 97% 31/32 [00:07<00:00,  2.50it/s][A[A

100% 32/32 [00:07<00:00,  2.97it/s][A[A100% 32/32 [00:07<00:00,  4.14it/s]
Meta loss on this task batch = 5.1623e-01, PNorm = 35.3209, GNorm = 0.0181

 16% 3/19 [00:26<02:25,  9.07s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.10it/s][A[A

  6% 2/32 [00:00<00:06,  4.98it/s][A[A

  9% 3/32 [00:00<00:05,  5.08it/s][A[A

 12% 4/32 [00:00<00:05,  5.19it/s][A[A

 16% 5/32 [00:00<00:05,  4.98it/s][A[A

 19% 6/32 [00:01<00:04,  5.53it/s][A[A

 22% 7/32 [00:01<00:04,  5.42it/s][A[A

 25% 8/32 [00:01<00:04,  5.65it/s][A[A

 28% 9/32 [00:01<00:04,  5.66it/s][A[A

 31% 10/32 [00:01<00:03,  5.88it/s][A[A

 34% 11/32 [00:02<00:09,  2.15it/s][A[A

 38% 12/32 [00:03<00:07,  2.63it/s][A[A

 41% 13/32 [00:03<00:06,  3.12it/s][A[A

 44% 14/32 [00:03<00:04,  3.67it/s][A[A

 47% 15/32 [00:03<00:04,  4.16it/s][A[A

 50% 16/32 [00:03<00:03,  4.61it/s][A[A

 53% 17/32 [00:03<00:02,  5.11it/s][A[A

 56% 18/32 [00:04<00:02,  4.96it/s][A[A

 59% 19/32 [00:04<00:02,  5.34it/s][A[A

 62% 20/32 [00:04<00:02,  5.49it/s][A[A

 66% 21/32 [00:04<00:01,  5.82it/s][A[A

 69% 22/32 [00:04<00:01,  5.20it/s][A[A

 72% 23/32 [00:05<00:01,  5.01it/s][A[A

 75% 24/32 [00:05<00:01,  5.11it/s][A[A

 78% 25/32 [00:05<00:01,  5.34it/s][A[A

 81% 26/32 [00:05<00:01,  5.39it/s][A[A

 84% 27/32 [00:05<00:00,  5.58it/s][A[A

 88% 28/32 [00:06<00:01,  2.11it/s][A[A

 91% 29/32 [00:07<00:01,  2.62it/s][A[A

 94% 30/32 [00:07<00:00,  3.08it/s][A[A

 97% 31/32 [00:07<00:00,  3.48it/s][A[A

100% 32/32 [00:07<00:00,  3.99it/s][A[A100% 32/32 [00:07<00:00,  4.16it/s]
Meta loss on this task batch = 5.8708e-01, PNorm = 35.3310, GNorm = 0.0613

 21% 4/19 [00:35<02:13,  8.88s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  6.03it/s][A[A

  6% 2/32 [00:00<00:05,  5.81it/s][A[A

  9% 3/32 [00:00<00:04,  6.01it/s][A[A

 12% 4/32 [00:00<00:04,  5.80it/s][A[A

 16% 5/32 [00:00<00:04,  5.48it/s][A[A

 19% 6/32 [00:01<00:04,  5.45it/s][A[A

 22% 7/32 [00:01<00:04,  5.12it/s][A[A

 25% 8/32 [00:01<00:04,  5.30it/s][A[A

 28% 9/32 [00:01<00:04,  5.59it/s][A[A

 31% 10/32 [00:01<00:04,  5.44it/s][A[A

 34% 11/32 [00:02<00:03,  5.51it/s][A[A

 38% 12/32 [00:03<00:09,  2.04it/s][A[A

 41% 13/32 [00:03<00:07,  2.48it/s][A[A

 44% 14/32 [00:03<00:06,  2.92it/s][A[A

 47% 15/32 [00:03<00:05,  3.29it/s][A[A

 50% 16/32 [00:03<00:03,  4.08it/s][A[A

 53% 17/32 [00:04<00:03,  3.98it/s][A[A

 56% 18/32 [00:04<00:03,  4.21it/s][A[A

 59% 19/32 [00:04<00:02,  4.50it/s][A[A

 62% 20/32 [00:04<00:02,  4.42it/s][A[A

 66% 21/32 [00:05<00:02,  4.77it/s][A[A

 69% 22/32 [00:05<00:01,  5.50it/s][A[A

 72% 23/32 [00:05<00:01,  5.22it/s][A[A

 75% 24/32 [00:05<00:01,  5.05it/s][A[A

 78% 25/32 [00:05<00:01,  4.83it/s][A[A

 81% 26/32 [00:06<00:03,  1.98it/s][A[A

 84% 27/32 [00:07<00:02,  2.42it/s][A[A

 88% 28/32 [00:07<00:01,  2.99it/s][A[A

 91% 29/32 [00:07<00:00,  3.51it/s][A[A

 94% 30/32 [00:07<00:00,  3.81it/s][A[A

 97% 31/32 [00:07<00:00,  4.08it/s][A[A

100% 32/32 [00:08<00:00,  4.31it/s][A[A100% 32/32 [00:08<00:00,  3.95it/s]
Meta loss on this task batch = 5.1531e-01, PNorm = 35.3391, GNorm = 0.0713

 26% 5/19 [00:43<02:04,  8.87s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.07it/s][A[A

  6% 2/32 [00:00<00:05,  5.40it/s][A[A

  9% 3/32 [00:00<00:05,  5.65it/s][A[A

 12% 4/32 [00:00<00:04,  5.74it/s][A[A

 16% 5/32 [00:00<00:05,  5.16it/s][A[A

 19% 6/32 [00:01<00:04,  5.24it/s][A[A

 22% 7/32 [00:02<00:12,  2.03it/s][A[A

 25% 8/32 [00:02<00:09,  2.45it/s][A[A

 28% 9/32 [00:02<00:08,  2.75it/s][A[A

 31% 10/32 [00:02<00:06,  3.22it/s][A[A

 34% 11/32 [00:03<00:05,  3.67it/s][A[A

 38% 12/32 [00:03<00:04,  4.11it/s][A[A

 41% 13/32 [00:03<00:04,  4.35it/s][A[A

 44% 14/32 [00:03<00:04,  4.30it/s][A[A

 47% 15/32 [00:03<00:03,  4.42it/s][A[A

 50% 16/32 [00:04<00:03,  4.69it/s][A[A

 53% 17/32 [00:04<00:03,  4.86it/s][A[A

 56% 18/32 [00:04<00:02,  5.10it/s][A[A

 59% 19/32 [00:05<00:06,  2.01it/s][A[A

 62% 20/32 [00:05<00:04,  2.55it/s][A[A

 66% 21/32 [00:06<00:03,  3.00it/s][A[A

 69% 22/32 [00:06<00:02,  3.37it/s][A[A

 72% 23/32 [00:06<00:02,  3.84it/s][A[A

 75% 24/32 [00:06<00:01,  4.08it/s][A[A

 78% 25/32 [00:06<00:01,  4.23it/s][A[A

 81% 26/32 [00:07<00:01,  4.53it/s][A[A

 84% 27/32 [00:07<00:01,  4.53it/s][A[A

 88% 28/32 [00:07<00:00,  4.41it/s][A[A

 91% 29/32 [00:07<00:00,  4.35it/s][A[A

 94% 30/32 [00:07<00:00,  4.52it/s][A[A

 97% 31/32 [00:08<00:00,  5.05it/s][A[A

100% 32/32 [00:09<00:00,  2.02it/s][A[A100% 32/32 [00:09<00:00,  3.45it/s]
Meta loss on this task batch = 4.6119e-01, PNorm = 35.3463, GNorm = 0.0203

 32% 6/19 [00:53<01:59,  9.22s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  6.36it/s][A[A

  6% 2/32 [00:00<00:04,  6.43it/s][A[A

  9% 3/32 [00:00<00:05,  5.65it/s][A[A

 12% 4/32 [00:00<00:05,  5.06it/s][A[A

 16% 5/32 [00:00<00:05,  5.09it/s][A[A

 19% 6/32 [00:01<00:05,  5.10it/s][A[A

 22% 7/32 [00:01<00:04,  5.18it/s][A[A

 25% 8/32 [00:01<00:04,  5.00it/s][A[A

 28% 9/32 [00:01<00:04,  4.98it/s][A[A

 31% 10/32 [00:02<00:04,  4.79it/s][A[A

 34% 11/32 [00:02<00:04,  5.11it/s][A[A

 38% 12/32 [00:02<00:03,  5.50it/s][A[A

 41% 13/32 [00:03<00:09,  2.10it/s][A[A

 44% 14/32 [00:03<00:07,  2.56it/s][A[A

 47% 15/32 [00:03<00:05,  2.95it/s][A[A

 50% 16/32 [00:04<00:04,  3.28it/s][A[A

 53% 17/32 [00:04<00:04,  3.73it/s][A[A

 56% 18/32 [00:04<00:03,  4.17it/s][A[A

 59% 19/32 [00:04<00:03,  4.30it/s][A[A

 62% 20/32 [00:04<00:02,  4.39it/s][A[A

 66% 21/32 [00:05<00:02,  4.56it/s][A[A

 69% 22/32 [00:05<00:02,  4.56it/s][A[A

 72% 23/32 [00:05<00:01,  4.57it/s][A[A

 75% 24/32 [00:05<00:01,  4.93it/s][A[A

 78% 25/32 [00:06<00:03,  2.01it/s][A[A

 81% 26/32 [00:07<00:02,  2.49it/s][A[A

 84% 27/32 [00:07<00:01,  3.02it/s][A[A

 88% 28/32 [00:07<00:01,  3.48it/s][A[A

 91% 29/32 [00:07<00:00,  4.07it/s][A[A

 94% 30/32 [00:07<00:00,  4.36it/s][A[A

 97% 31/32 [00:07<00:00,  4.52it/s][A[A

100% 32/32 [00:08<00:00,  4.78it/s][A[A100% 32/32 [00:08<00:00,  3.93it/s]
Meta loss on this task batch = 4.8154e-01, PNorm = 35.3538, GNorm = 0.0263

 37% 7/19 [01:02<01:49,  9.12s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.23it/s][A[A

  6% 2/32 [00:00<00:05,  5.08it/s][A[A

  9% 3/32 [00:00<00:05,  4.97it/s][A[A

 12% 4/32 [00:00<00:05,  4.82it/s][A[A

 16% 5/32 [00:01<00:05,  4.82it/s][A[A

 19% 6/32 [00:01<00:05,  5.17it/s][A[A

 22% 7/32 [00:01<00:04,  5.44it/s][A[A

 25% 8/32 [00:02<00:11,  2.02it/s][A[A

 28% 9/32 [00:02<00:09,  2.46it/s][A[A

 31% 10/32 [00:02<00:07,  3.01it/s][A[A

 34% 11/32 [00:03<00:06,  3.36it/s][A[A

 38% 12/32 [00:03<00:05,  3.74it/s][A[A

 41% 13/32 [00:03<00:04,  4.19it/s][A[A

 44% 14/32 [00:03<00:03,  4.67it/s][A[A

 47% 15/32 [00:03<00:03,  4.70it/s][A[A

 50% 16/32 [00:04<00:03,  5.00it/s][A[A

 53% 17/32 [00:04<00:03,  4.83it/s][A[A

 56% 18/32 [00:04<00:02,  4.99it/s][A[A

 59% 19/32 [00:04<00:02,  4.81it/s][A[A

 62% 20/32 [00:04<00:02,  4.92it/s][A[A

 66% 21/32 [00:06<00:05,  2.02it/s][A[A

 69% 22/32 [00:06<00:04,  2.39it/s][A[A

 72% 23/32 [00:06<00:03,  2.94it/s][A[A

 75% 24/32 [00:06<00:02,  3.30it/s][A[A

 78% 25/32 [00:06<00:01,  3.50it/s][A[A

 81% 26/32 [00:07<00:01,  3.69it/s][A[A

 84% 27/32 [00:07<00:01,  4.16it/s][A[A

 88% 28/32 [00:07<00:00,  4.50it/s][A[A

 91% 29/32 [00:07<00:00,  4.62it/s][A[A

 94% 30/32 [00:07<00:00,  4.85it/s][A[A

 97% 31/32 [00:08<00:00,  4.94it/s][A[A

100% 32/32 [00:08<00:00,  5.10it/s][A[A100% 32/32 [00:08<00:00,  3.87it/s]
Meta loss on this task batch = 3.6021e-01, PNorm = 35.3625, GNorm = 0.0643

 42% 8/19 [01:11<01:39,  9.09s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.11it/s][A[A

  6% 2/32 [00:00<00:05,  5.19it/s][A[A

  9% 3/32 [00:00<00:05,  5.20it/s][A[A

 12% 4/32 [00:01<00:13,  2.05it/s][A[A

 16% 5/32 [00:01<00:10,  2.54it/s][A[A

 19% 6/32 [00:02<00:08,  3.02it/s][A[A

 22% 7/32 [00:02<00:07,  3.53it/s][A[A

 25% 8/32 [00:02<00:06,  3.89it/s][A[A

 28% 9/32 [00:02<00:05,  4.19it/s][A[A

 31% 10/32 [00:02<00:04,  4.44it/s][A[A

 34% 11/32 [00:03<00:04,  4.63it/s][A[A

 38% 12/32 [00:03<00:04,  4.85it/s][A[A

 41% 13/32 [00:03<00:03,  5.09it/s][A[A

 44% 14/32 [00:03<00:03,  5.28it/s][A[A

 47% 15/32 [00:03<00:03,  5.23it/s][A[A

 50% 16/32 [00:03<00:03,  5.22it/s][A[A

 53% 17/32 [00:04<00:02,  5.38it/s][A[A

 56% 18/32 [00:04<00:02,  5.58it/s][A[A

 59% 19/32 [00:05<00:06,  2.08it/s][A[A

 62% 20/32 [00:05<00:04,  2.54it/s][A[A

 66% 21/32 [00:05<00:03,  2.97it/s][A[A

 69% 22/32 [00:06<00:02,  3.41it/s][A[A

 72% 23/32 [00:06<00:02,  3.94it/s][A[A

 75% 24/32 [00:06<00:01,  4.36it/s][A[A

 78% 25/32 [00:06<00:01,  4.64it/s][A[A

 81% 26/32 [00:06<00:01,  4.68it/s][A[A

 84% 27/32 [00:06<00:01,  4.88it/s][A[A

 88% 28/32 [00:07<00:00,  4.94it/s][A[A

 91% 29/32 [00:07<00:00,  4.99it/s][A[A

 94% 30/32 [00:07<00:00,  5.03it/s][A[A

 97% 31/32 [00:07<00:00,  5.03it/s][A[A

100% 32/32 [00:07<00:00,  4.94it/s][A[A100% 32/32 [00:07<00:00,  4.00it/s]
Meta loss on this task batch = 2.0613e-01, PNorm = 35.3727, GNorm = 0.0827

 47% 9/19 [01:20<01:29,  8.99s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.28it/s][A[A

  6% 2/32 [00:00<00:05,  5.41it/s][A[A

  9% 3/32 [00:01<00:14,  2.07it/s][A[A

 12% 4/32 [00:01<00:10,  2.55it/s][A[A

 16% 5/32 [00:01<00:08,  3.06it/s][A[A

 19% 6/32 [00:02<00:07,  3.51it/s][A[A

 22% 7/32 [00:02<00:06,  3.88it/s][A[A

 25% 8/32 [00:02<00:05,  4.25it/s][A[A

 28% 9/32 [00:02<00:05,  4.55it/s][A[A

 31% 10/32 [00:02<00:04,  4.72it/s][A[A

 34% 11/32 [00:03<00:04,  4.82it/s][A[A

 38% 12/32 [00:03<00:04,  4.98it/s][A[A

 41% 13/32 [00:03<00:03,  5.04it/s][A[A

 44% 14/32 [00:03<00:03,  5.08it/s][A[A

 47% 15/32 [00:03<00:03,  5.25it/s][A[A

 50% 16/32 [00:03<00:02,  5.43it/s][A[A

 53% 17/32 [00:05<00:07,  2.04it/s][A[A

 56% 18/32 [00:05<00:05,  2.51it/s][A[A

 59% 19/32 [00:05<00:04,  3.03it/s][A[A

 62% 20/32 [00:05<00:03,  3.53it/s][A[A

 66% 21/32 [00:05<00:02,  4.03it/s][A[A

 69% 22/32 [00:06<00:02,  4.36it/s][A[A

 72% 23/32 [00:06<00:01,  4.73it/s][A[A

 75% 24/32 [00:06<00:01,  4.84it/s][A[A

 78% 25/32 [00:06<00:01,  5.01it/s][A[A

 81% 26/32 [00:06<00:01,  4.76it/s][A[A

 84% 27/32 [00:06<00:00,  5.08it/s][A[A

 88% 28/32 [00:08<00:02,  1.95it/s][A[A

 91% 29/32 [00:08<00:01,  2.43it/s][A[A

 94% 30/32 [00:08<00:00,  2.78it/s][A[A

 97% 31/32 [00:08<00:00,  3.16it/s][A[A

100% 32/32 [00:09<00:00,  3.66it/s][A[A100% 32/32 [00:09<00:00,  3.54it/s]
Meta loss on this task batch = 2.0746e-01, PNorm = 35.3876, GNorm = 0.2286

 53% 10/19 [01:30<01:23,  9.22s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.95it/s][A[A

  6% 2/32 [00:00<00:06,  4.66it/s][A[A

  9% 3/32 [00:00<00:06,  4.47it/s][A[A

 12% 4/32 [00:00<00:06,  4.35it/s][A[A

 16% 5/32 [00:02<00:14,  1.87it/s][A[A

 19% 6/32 [00:02<00:11,  2.31it/s][A[A

 22% 7/32 [00:02<00:08,  2.82it/s][A[A

 25% 8/32 [00:02<00:07,  3.18it/s][A[A

 28% 9/32 [00:03<00:06,  3.41it/s][A[A

 31% 10/32 [00:03<00:06,  3.62it/s][A[A

 34% 11/32 [00:03<00:05,  3.81it/s][A[A

 38% 12/32 [00:03<00:05,  3.97it/s][A[A

 41% 13/32 [00:03<00:04,  4.02it/s][A[A

 44% 14/32 [00:04<00:04,  4.08it/s][A[A

 47% 15/32 [00:04<00:04,  4.11it/s][A[A

 50% 16/32 [00:04<00:03,  4.58it/s][A[A

 53% 17/32 [00:05<00:07,  1.91it/s][A[A

 56% 18/32 [00:05<00:05,  2.40it/s][A[A

 59% 19/32 [00:06<00:04,  2.85it/s][A[A

 62% 20/32 [00:06<00:03,  3.15it/s][A[A

 66% 21/32 [00:06<00:03,  3.40it/s][A[A

 69% 22/32 [00:06<00:02,  3.66it/s][A[A

 72% 23/32 [00:07<00:02,  3.85it/s][A[A

 75% 24/32 [00:07<00:02,  3.88it/s][A[A

 78% 25/32 [00:07<00:01,  4.14it/s][A[A

 81% 26/32 [00:07<00:01,  4.40it/s][A[A

 84% 27/32 [00:08<00:02,  1.94it/s][A[A

 88% 28/32 [00:09<00:01,  2.33it/s][A[A

 91% 29/32 [00:09<00:01,  2.77it/s][A[A

 94% 30/32 [00:09<00:00,  3.14it/s][A[A

 97% 31/32 [00:09<00:00,  3.40it/s][A[A

100% 32/32 [00:10<00:00,  3.65it/s][A[A100% 32/32 [00:10<00:00,  3.18it/s]
Meta loss on this task batch = 6.3993e-01, PNorm = 35.3940, GNorm = 0.3840

 58% 11/19 [01:41<01:17,  9.72s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.85it/s][A[A

  6% 2/32 [00:00<00:07,  3.90it/s][A[A

  9% 3/32 [00:00<00:07,  3.93it/s][A[A

 12% 4/32 [00:01<00:07,  3.98it/s][A[A

 16% 5/32 [00:01<00:06,  4.01it/s][A[A

 19% 6/32 [00:01<00:06,  4.08it/s][A[A

 22% 7/32 [00:01<00:05,  4.54it/s][A[A

 25% 8/32 [00:01<00:05,  4.51it/s][A[A

 28% 9/32 [00:02<00:05,  4.58it/s][A[A

 31% 10/32 [00:03<00:11,  1.91it/s][A[A

 34% 11/32 [00:03<00:08,  2.41it/s][A[A

 38% 12/32 [00:03<00:07,  2.73it/s][A[A

 41% 13/32 [00:03<00:06,  3.11it/s][A[A

 44% 14/32 [00:04<00:05,  3.37it/s][A[A

 47% 15/32 [00:04<00:04,  3.58it/s][A[A

 50% 16/32 [00:04<00:03,  4.07it/s][A[A

 53% 17/32 [00:04<00:03,  4.28it/s][A[A

 56% 18/32 [00:05<00:03,  4.32it/s][A[A

 59% 19/32 [00:05<00:03,  4.26it/s][A[A

 62% 20/32 [00:05<00:02,  4.19it/s][A[A

 66% 21/32 [00:06<00:05,  1.85it/s][A[A

 69% 22/32 [00:06<00:04,  2.33it/s][A[A

 72% 23/32 [00:07<00:03,  2.77it/s][A[A

 75% 24/32 [00:07<00:02,  3.36it/s][A[A

 78% 25/32 [00:07<00:01,  3.56it/s][A[A

 81% 26/32 [00:07<00:01,  3.95it/s][A[A

 84% 27/32 [00:07<00:01,  4.01it/s][A[A

 88% 28/32 [00:08<00:01,  3.99it/s][A[A

 91% 29/32 [00:08<00:00,  4.11it/s][A[A

 94% 30/32 [00:08<00:00,  4.15it/s][A[A

 97% 31/32 [00:09<00:00,  1.85it/s][A[A

100% 32/32 [00:10<00:00,  2.21it/s][A[A100% 32/32 [00:10<00:00,  3.15it/s]
Meta loss on this task batch = 5.8480e-01, PNorm = 35.3966, GNorm = 0.2415

 63% 12/19 [01:52<01:10, 10.09s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.10it/s][A[A

  6% 2/32 [00:00<00:06,  4.44it/s][A[A

  9% 3/32 [00:00<00:06,  4.37it/s][A[A

 12% 4/32 [00:00<00:06,  4.50it/s][A[A

 16% 5/32 [00:01<00:06,  4.40it/s][A[A

 19% 6/32 [00:01<00:05,  4.49it/s][A[A

 22% 7/32 [00:01<00:05,  4.72it/s][A[A

 25% 8/32 [00:01<00:04,  4.85it/s][A[A

 28% 9/32 [00:01<00:04,  5.15it/s][A[A

 31% 10/32 [00:03<00:11,  1.98it/s][A[A

 34% 11/32 [00:03<00:08,  2.33it/s][A[A

 38% 12/32 [00:03<00:07,  2.72it/s][A[A

 41% 13/32 [00:03<00:06,  3.03it/s][A[A

 44% 14/32 [00:03<00:05,  3.54it/s][A[A

 47% 15/32 [00:04<00:04,  3.70it/s][A[A

 50% 16/32 [00:04<00:04,  3.83it/s][A[A

 53% 17/32 [00:04<00:03,  4.16it/s][A[A

 56% 18/32 [00:04<00:03,  3.93it/s][A[A

 59% 19/32 [00:06<00:07,  1.78it/s][A[A

 62% 20/32 [00:06<00:05,  2.23it/s][A[A

 66% 21/32 [00:06<00:04,  2.59it/s][A[A

 69% 22/32 [00:06<00:03,  2.90it/s][A[A

 72% 23/32 [00:07<00:02,  3.17it/s][A[A

 75% 24/32 [00:07<00:02,  3.40it/s][A[A

 78% 25/32 [00:07<00:01,  3.57it/s][A[A

 81% 26/32 [00:07<00:01,  3.89it/s][A[A

 84% 27/32 [00:08<00:01,  4.26it/s][A[A

 88% 28/32 [00:08<00:00,  4.69it/s][A[A

 91% 29/32 [00:08<00:00,  4.53it/s][A[A

 94% 30/32 [00:08<00:00,  4.44it/s][A[A

 97% 31/32 [00:08<00:00,  4.94it/s][A[A

100% 32/32 [00:09<00:00,  4.67it/s][A[A100% 32/32 [00:09<00:00,  3.53it/s]
Meta loss on this task batch = 6.1249e-01, PNorm = 35.3970, GNorm = 0.2245

 68% 13/19 [02:02<01:00, 10.02s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:37,  1.20s/it][A[A

  6% 2/32 [00:01<00:26,  1.12it/s][A[A

  9% 3/32 [00:01<00:20,  1.44it/s][A[A

 12% 4/32 [00:01<00:15,  1.80it/s][A[A

 16% 5/32 [00:02<00:12,  2.16it/s][A[A

 19% 6/32 [00:02<00:09,  2.62it/s][A[A

 22% 7/32 [00:02<00:08,  2.96it/s][A[A

 25% 8/32 [00:02<00:06,  3.53it/s][A[A

 28% 9/32 [00:02<00:06,  3.71it/s][A[A

 31% 10/32 [00:03<00:05,  4.09it/s][A[A

 34% 11/32 [00:03<00:05,  4.13it/s][A[A

 38% 12/32 [00:03<00:04,  4.41it/s][A[A

 41% 13/32 [00:03<00:04,  4.38it/s][A[A

 44% 14/32 [00:03<00:04,  4.33it/s][A[A

 47% 15/32 [00:04<00:04,  4.24it/s][A[A

 50% 16/32 [00:04<00:03,  4.18it/s][A[A

 53% 17/32 [00:04<00:03,  4.42it/s][A[A

 56% 18/32 [00:05<00:07,  1.94it/s][A[A

 59% 19/32 [00:06<00:05,  2.42it/s][A[A

 62% 20/32 [00:06<00:04,  2.92it/s][A[A

 66% 21/32 [00:06<00:03,  3.21it/s][A[A

 69% 22/32 [00:06<00:02,  3.45it/s][A[A

 72% 23/32 [00:06<00:02,  3.63it/s][A[A

 75% 24/32 [00:07<00:01,  4.26it/s][A[A

 78% 25/32 [00:07<00:01,  4.37it/s][A[A

 81% 26/32 [00:07<00:01,  4.33it/s][A[A

 84% 27/32 [00:07<00:01,  4.30it/s][A[A

 88% 28/32 [00:08<00:00,  4.26it/s][A[A

 91% 29/32 [00:08<00:00,  4.33it/s][A[A

 94% 30/32 [00:09<00:01,  1.90it/s][A[A

 97% 31/32 [00:09<00:00,  2.32it/s][A[A

100% 32/32 [00:09<00:00,  2.78it/s][A[A100% 32/32 [00:09<00:00,  3.25it/s]
Meta loss on this task batch = 5.7638e-01, PNorm = 35.3971, GNorm = 0.0780

 74% 14/19 [02:12<00:51, 10.20s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.06it/s][A[A

  6% 2/32 [00:00<00:06,  4.36it/s][A[A

  9% 3/32 [00:00<00:06,  4.37it/s][A[A

 12% 4/32 [00:00<00:05,  4.68it/s][A[A

 16% 5/32 [00:01<00:05,  4.90it/s][A[A

 19% 6/32 [00:01<00:05,  4.49it/s][A[A

 22% 7/32 [00:01<00:05,  4.31it/s][A[A

 25% 8/32 [00:01<00:05,  4.49it/s][A[A

 28% 9/32 [00:01<00:05,  4.44it/s][A[A

 31% 10/32 [00:02<00:04,  4.86it/s][A[A

 34% 11/32 [00:03<00:10,  1.97it/s][A[A

 38% 12/32 [00:03<00:08,  2.35it/s][A[A

 41% 13/32 [00:03<00:06,  2.85it/s][A[A

 44% 14/32 [00:03<00:05,  3.36it/s][A[A

 47% 15/32 [00:04<00:04,  3.74it/s][A[A

 50% 16/32 [00:04<00:04,  3.99it/s][A[A

 53% 17/32 [00:04<00:03,  4.26it/s][A[A

 56% 18/32 [00:04<00:03,  4.56it/s][A[A

 59% 19/32 [00:04<00:02,  4.68it/s][A[A

 62% 20/32 [00:05<00:02,  5.09it/s][A[A

 66% 21/32 [00:05<00:02,  5.07it/s][A[A

 69% 22/32 [00:05<00:01,  5.17it/s][A[A

 72% 23/32 [00:05<00:01,  4.62it/s][A[A

 75% 24/32 [00:05<00:01,  4.55it/s][A[A

 78% 25/32 [00:07<00:03,  1.92it/s][A[A

 81% 26/32 [00:07<00:02,  2.25it/s][A[A

 84% 27/32 [00:07<00:01,  2.79it/s][A[A

 88% 28/32 [00:07<00:01,  3.09it/s][A[A

 91% 29/32 [00:08<00:00,  3.35it/s][A[A

 94% 30/32 [00:08<00:00,  3.56it/s][A[A

 97% 31/32 [00:08<00:00,  3.97it/s][A[A

100% 32/32 [00:08<00:00,  4.13it/s][A[A100% 32/32 [00:08<00:00,  3.66it/s]
Meta loss on this task batch = 5.0984e-01, PNorm = 35.3968, GNorm = 0.0733

 79% 15/19 [02:22<00:39,  9.99s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  3.96it/s][A[A

  6% 2/32 [00:01<00:15,  1.89it/s][A[A

  9% 3/32 [00:01<00:12,  2.35it/s][A[A

 12% 4/32 [00:01<00:09,  2.80it/s][A[A

 16% 5/32 [00:01<00:08,  3.26it/s][A[A

 19% 6/32 [00:02<00:07,  3.54it/s][A[A

 22% 7/32 [00:02<00:06,  3.88it/s][A[A

 25% 8/32 [00:02<00:06,  3.95it/s][A[A

 28% 9/32 [00:02<00:05,  4.26it/s][A[A

 31% 10/32 [00:03<00:05,  4.38it/s][A[A

 34% 11/32 [00:03<00:04,  4.61it/s][A[A

 38% 12/32 [00:04<00:10,  1.95it/s][A[A

 41% 13/32 [00:04<00:07,  2.42it/s][A[A

 44% 14/32 [00:04<00:06,  2.90it/s][A[A

 47% 15/32 [00:05<00:05,  3.30it/s][A[A

 50% 16/32 [00:05<00:04,  3.60it/s][A[A

 53% 17/32 [00:05<00:03,  3.81it/s][A[A

 56% 18/32 [00:05<00:03,  4.29it/s][A[A

 59% 19/32 [00:05<00:02,  4.49it/s][A[A

 62% 20/32 [00:06<00:02,  4.62it/s][A[A

 66% 21/32 [00:06<00:02,  4.74it/s][A[A

 69% 22/32 [00:06<00:02,  4.70it/s][A[A

 72% 23/32 [00:06<00:01,  4.53it/s][A[A

 75% 24/32 [00:06<00:01,  4.39it/s][A[A

 78% 25/32 [00:08<00:03,  1.97it/s][A[A

 81% 26/32 [00:08<00:02,  2.41it/s][A[A

 84% 27/32 [00:08<00:01,  2.76it/s][A[A

 88% 28/32 [00:08<00:01,  3.16it/s][A[A

 91% 29/32 [00:08<00:00,  3.51it/s][A[A

 94% 30/32 [00:09<00:00,  3.70it/s][A[A

 97% 31/32 [00:09<00:00,  3.89it/s][A[A

100% 32/32 [00:09<00:00,  4.19it/s][A[A100% 32/32 [00:09<00:00,  3.33it/s]
Meta loss on this task batch = 5.6765e-01, PNorm = 35.3969, GNorm = 0.0646

 84% 16/19 [02:32<00:30, 10.11s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.86it/s][A[A

  6% 2/32 [00:00<00:07,  3.90it/s][A[A

  9% 3/32 [00:00<00:07,  4.04it/s][A[A

 12% 4/32 [00:00<00:06,  4.27it/s][A[A

 16% 5/32 [00:01<00:05,  4.55it/s][A[A

 19% 6/32 [00:02<00:13,  1.94it/s][A[A

 22% 7/32 [00:02<00:10,  2.45it/s][A[A

 25% 8/32 [00:02<00:08,  2.77it/s][A[A

 28% 9/32 [00:02<00:07,  3.18it/s][A[A

 31% 10/32 [00:03<00:05,  3.86it/s][A[A

 34% 11/32 [00:03<00:05,  4.15it/s][A[A

 38% 12/32 [00:03<00:04,  4.24it/s][A[A

 41% 13/32 [00:03<00:03,  4.75it/s][A[A

 44% 14/32 [00:03<00:03,  5.01it/s][A[A

 47% 15/32 [00:04<00:03,  5.01it/s][A[A

 50% 16/32 [00:04<00:03,  4.98it/s][A[A

 53% 17/32 [00:04<00:03,  4.70it/s][A[A

 56% 18/32 [00:04<00:02,  4.89it/s][A[A

 59% 19/32 [00:04<00:02,  5.15it/s][A[A

 62% 20/32 [00:05<00:02,  4.98it/s][A[A

 66% 21/32 [00:05<00:02,  5.41it/s][A[A

 69% 22/32 [00:06<00:04,  2.06it/s][A[A

 72% 23/32 [00:06<00:03,  2.50it/s][A[A

 75% 24/32 [00:06<00:02,  2.91it/s][A[A

 78% 25/32 [00:06<00:02,  3.41it/s][A[A

 81% 26/32 [00:07<00:01,  3.83it/s][A[A

 84% 27/32 [00:07<00:01,  4.12it/s][A[A

 88% 28/32 [00:07<00:00,  4.30it/s][A[A

 91% 29/32 [00:07<00:00,  4.52it/s][A[A

 94% 30/32 [00:07<00:00,  4.44it/s][A[A

 97% 31/32 [00:08<00:00,  4.78it/s][A[A

100% 32/32 [00:08<00:00,  4.92it/s][A[A100% 32/32 [00:08<00:00,  3.83it/s]
Meta loss on this task batch = 4.6762e-01, PNorm = 35.3989, GNorm = 0.0893

 89% 17/19 [02:41<00:19,  9.82s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.88it/s][A[A

  6% 2/32 [00:00<00:06,  4.82it/s][A[A

  9% 3/32 [00:01<00:14,  1.94it/s][A[A

 12% 4/32 [00:01<00:11,  2.37it/s][A[A

 16% 5/32 [00:02<00:09,  2.89it/s][A[A

 19% 6/32 [00:02<00:07,  3.39it/s][A[A

 22% 7/32 [00:02<00:07,  3.55it/s][A[A

 25% 8/32 [00:02<00:06,  3.65it/s][A[A

 28% 9/32 [00:02<00:05,  4.14it/s][A[A

 31% 10/32 [00:03<00:05,  4.28it/s][A[A

 34% 11/32 [00:03<00:04,  4.70it/s][A[A

 38% 12/32 [00:03<00:04,  4.81it/s][A[A

 41% 13/32 [00:03<00:04,  4.48it/s][A[A

 44% 14/32 [00:03<00:03,  4.58it/s][A[A

 47% 15/32 [00:05<00:08,  1.91it/s][A[A

 50% 16/32 [00:05<00:06,  2.36it/s][A[A

 53% 17/32 [00:05<00:05,  2.81it/s][A[A

 56% 18/32 [00:05<00:04,  3.29it/s][A[A

 59% 19/32 [00:05<00:03,  3.60it/s][A[A

 62% 20/32 [00:06<00:03,  3.80it/s][A[A

 66% 21/32 [00:06<00:02,  4.23it/s][A[A

 69% 22/32 [00:06<00:02,  4.45it/s][A[A

 72% 23/32 [00:06<00:02,  4.21it/s][A[A

 75% 24/32 [00:07<00:01,  4.07it/s][A[A

 78% 25/32 [00:07<00:01,  4.62it/s][A[A

 81% 26/32 [00:07<00:01,  4.73it/s][A[A

 84% 27/32 [00:08<00:02,  1.95it/s][A[A

 88% 28/32 [00:08<00:01,  2.31it/s][A[A

 91% 29/32 [00:09<00:01,  2.68it/s][A[A

 94% 30/32 [00:09<00:00,  3.18it/s][A[A

 97% 31/32 [00:09<00:00,  3.69it/s][A[A

100% 32/32 [00:09<00:00,  4.18it/s][A[A100% 32/32 [00:09<00:00,  3.32it/s]
Meta loss on this task batch = 5.3268e-01, PNorm = 35.4010, GNorm = 0.0319

 95% 18/19 [02:52<00:09,  9.98s/it][A

  0% 0/23 [00:00<?, ?it/s][A[A

  4% 1/23 [00:00<00:05,  4.30it/s][A[A

  9% 2/23 [00:00<00:04,  4.43it/s][A[A

 13% 3/23 [00:00<00:04,  4.55it/s][A[A

 17% 4/23 [00:00<00:03,  4.92it/s][A[A

 22% 5/23 [00:00<00:03,  5.41it/s][A[A

 26% 6/23 [00:01<00:03,  5.16it/s][A[A

 30% 7/23 [00:01<00:02,  5.42it/s][A[A

 35% 8/23 [00:01<00:02,  5.58it/s][A[A

 39% 9/23 [00:01<00:02,  5.68it/s][A[A

 43% 10/23 [00:01<00:02,  4.94it/s][A[A

 48% 11/23 [00:02<00:02,  5.66it/s][A[A

 52% 12/23 [00:02<00:01,  5.72it/s][A[A

 57% 13/23 [00:03<00:04,  2.05it/s][A[A

 61% 14/23 [00:03<00:03,  2.55it/s][A[A

 65% 15/23 [00:03<00:03,  2.61it/s][A[A

 70% 16/23 [00:04<00:02,  2.88it/s][A[A

 74% 17/23 [00:04<00:01,  3.17it/s][A[A

 78% 18/23 [00:04<00:01,  3.33it/s][A[A

 83% 19/23 [00:04<00:01,  3.86it/s][A[A

 87% 20/23 [00:05<00:00,  4.19it/s][A[A

 91% 21/23 [00:06<00:01,  1.90it/s][A[A

 96% 22/23 [00:06<00:00,  2.35it/s][A[A

100% 23/23 [00:06<00:00,  2.72it/s][A[A100% 23/23 [00:06<00:00,  3.43it/s]
Meta loss on this task batch = 4.3773e-01, PNorm = 35.4043, GNorm = 0.0517

100% 19/19 [02:59<00:00,  9.18s/it][A100% 19/19 [02:59<00:00,  9.44s/it]
Took 179.36144161224365 seconds to complete one epoch of meta training

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/20 [00:00<?, ?it/s][A[A


  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 32.58it/s]


  5% 1/20 [00:00<00:03,  5.68it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 20.48it/s]


 10% 2/20 [00:00<00:03,  5.51it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 17.89it/s][A[A[A100% 3/3 [00:00<00:00, 19.95it/s]


 15% 3/20 [00:00<00:03,  4.68it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 15.32it/s]


 20% 4/20 [00:00<00:03,  4.72it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 15.33it/s][A[A[A100% 4/4 [00:00<00:00, 17.62it/s]


 25% 5/20 [00:01<00:03,  3.84it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.57it/s][A[A[A100% 4/4 [00:00<00:00, 16.16it/s]


 30% 6/20 [00:01<00:04,  3.28it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.85it/s][A[A[A100% 4/4 [00:00<00:00, 20.57it/s]


 35% 7/20 [00:02<00:04,  3.10it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.99it/s][A[A[A


100% 4/4 [00:00<00:00, 19.63it/s][A[A[A100% 4/4 [00:00<00:00, 19.37it/s]


 40% 8/20 [00:02<00:03,  3.01it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:01<00:01,  1.80it/s][A[A[A100% 4/4 [00:01<00:00,  3.34it/s]


 45% 9/20 [00:03<00:07,  1.57it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.48it/s][A[A[A


100% 4/4 [00:00<00:00, 19.35it/s][A[A[A100% 4/4 [00:00<00:00, 19.25it/s]


 50% 10/20 [00:04<00:05,  1.80it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 16.17it/s][A[A[A100% 4/4 [00:00<00:00, 18.62it/s]


 55% 11/20 [00:04<00:04,  2.00it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.83it/s][A[A[A100% 4/4 [00:00<00:00, 22.09it/s]


 60% 12/20 [00:04<00:03,  2.17it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.99it/s][A[A[A100% 3/3 [00:00<00:00, 14.26it/s]


 65% 13/20 [00:05<00:03,  2.30it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.46it/s][A[A[A100% 3/3 [00:00<00:00, 13.86it/s]


 70% 14/20 [00:05<00:02,  2.38it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.84it/s][A[A[A100% 4/4 [00:00<00:00, 18.19it/s]


 75% 15/20 [00:05<00:02,  2.43it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.85it/s][A[A[A100% 3/3 [00:00<00:00, 18.22it/s]


 80% 16/20 [00:06<00:01,  2.59it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 11.84it/s][A[A[A100% 3/3 [00:00<00:00, 14.55it/s]


 85% 17/20 [00:06<00:01,  2.56it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 21.74it/s]


 90% 18/20 [00:06<00:00,  2.80it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 15.33it/s][A[A[A100% 3/3 [00:00<00:00, 19.63it/s]


 95% 19/20 [00:08<00:00,  1.55it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 13.18it/s]


100% 20/20 [00:08<00:00,  1.92it/s][A[A100% 20/20 [00:08<00:00,  2.34it/s]

100% 1/1 [00:08<00:00,  8.53s/it][A100% 1/1 [00:08<00:00,  8.53s/it]
Took 187.8939688205719 seconds to complete one epoch of meta training and validating
Meta validation score auc = 0.595285
Found better MAML checkpoint after meta validation, saving now
 13% 4/30 [12:56<1:24:27, 194.89s/it]Epoch 4

  0% 0/19 [00:00<?, ?it/s][A

  0% 0/32 [00:00<?, ?it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  3% 1/32 [00:00<00:06,  4.62it/s][A[A

  6% 2/32 [00:00<00:06,  4.31it/s][A[A

  9% 3/32 [00:00<00:06,  4.47it/s][A[A

 12% 4/32 [00:00<00:05,  4.97it/s][A[A

 16% 5/32 [00:00<00:04,  5.49it/s][A[A

 19% 6/32 [00:01<00:04,  5.46it/s][A[A

 22% 7/32 [00:01<00:04,  5.57it/s][A[A

 25% 8/32 [00:01<00:04,  5.46it/s][A[A

 28% 9/32 [00:01<00:03,  6.10it/s][A[A

 31% 10/32 [00:01<00:03,  6.52it/s][A[A

 34% 11/32 [00:03<00:10,  2.05it/s][A[A

 38% 12/32 [00:03<00:08,  2.37it/s][A[A

 41% 13/32 [00:03<00:07,  2.64it/s][A[A

 44% 14/32 [00:03<00:05,  3.01it/s][A[A

 47% 15/32 [00:03<00:04,  3.54it/s][A[A

 50% 16/32 [00:04<00:04,  3.83it/s][A[A

 53% 17/32 [00:04<00:03,  4.00it/s][A[A

 56% 18/32 [00:04<00:03,  4.49it/s][A[A

 59% 19/32 [00:04<00:02,  4.88it/s][A[A

 62% 20/32 [00:04<00:02,  4.75it/s][A[A

 66% 21/32 [00:06<00:05,  2.07it/s][A[A

 69% 22/32 [00:06<00:03,  2.51it/s][A[A

 72% 23/32 [00:06<00:02,  3.08it/s][A[A

 75% 24/32 [00:06<00:02,  3.41it/s][A[A

 78% 25/32 [00:06<00:01,  3.85it/s][A[A

 81% 26/32 [00:07<00:01,  4.09it/s][A[A

 84% 27/32 [00:07<00:01,  4.57it/s][A[A

 88% 28/32 [00:07<00:00,  5.05it/s][A[A

 91% 29/32 [00:07<00:00,  4.93it/s][A[A

 94% 30/32 [00:07<00:00,  5.65it/s][A[A

 97% 31/32 [00:07<00:00,  5.51it/s][A[A

100% 32/32 [00:08<00:00,  5.64it/s][A[A100% 32/32 [00:08<00:00,  3.98it/s]
Meta loss on this task batch = 5.3295e-01, PNorm = 35.4108, GNorm = 0.1448

  5% 1/19 [00:08<02:37,  8.76s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:03,  8.10it/s][A[A

  6% 2/32 [00:00<00:03,  7.66it/s][A[A

  9% 3/32 [00:00<00:04,  6.89it/s][A[A

 12% 4/32 [00:00<00:04,  5.82it/s][A[A

 16% 5/32 [00:01<00:13,  2.06it/s][A[A

 19% 6/32 [00:02<00:09,  2.62it/s][A[A

 22% 7/32 [00:02<00:07,  3.26it/s][A[A

 25% 8/32 [00:02<00:06,  3.88it/s][A[A

 28% 9/32 [00:02<00:05,  4.40it/s][A[A

 31% 10/32 [00:02<00:04,  4.93it/s][A[A

 34% 11/32 [00:02<00:04,  4.98it/s][A[A

 38% 12/32 [00:03<00:03,  5.06it/s][A[A

 41% 13/32 [00:03<00:03,  5.48it/s][A[A

 44% 14/32 [00:03<00:03,  5.18it/s][A[A

 47% 15/32 [00:03<00:03,  5.00it/s][A[A

 50% 16/32 [00:03<00:03,  5.00it/s][A[A

 53% 17/32 [00:04<00:03,  4.78it/s][A[A

 56% 18/32 [00:04<00:02,  4.69it/s][A[A

 59% 19/32 [00:04<00:02,  5.12it/s][A[A

 62% 20/32 [00:05<00:06,  1.99it/s][A[A

 66% 21/32 [00:05<00:04,  2.41it/s][A[A

 69% 22/32 [00:05<00:03,  2.99it/s][A[A

 72% 23/32 [00:06<00:02,  3.57it/s][A[A

 75% 24/32 [00:06<00:01,  4.14it/s][A[A

 78% 25/32 [00:06<00:01,  4.35it/s][A[A

 81% 26/32 [00:06<00:01,  4.51it/s][A[A

 84% 27/32 [00:06<00:01,  4.89it/s][A[A

 88% 28/32 [00:07<00:00,  4.76it/s][A[A

 91% 29/32 [00:07<00:00,  5.43it/s][A[A

 94% 30/32 [00:07<00:00,  5.24it/s][A[A

 97% 31/32 [00:07<00:00,  5.13it/s][A[A

100% 32/32 [00:07<00:00,  5.54it/s][A[A100% 32/32 [00:07<00:00,  4.13it/s]
Meta loss on this task batch = 5.2561e-01, PNorm = 35.4192, GNorm = 0.0905

 11% 2/19 [00:17<02:27,  8.66s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  6.13it/s][A[A

  6% 2/32 [00:00<00:05,  5.55it/s][A[A

  9% 3/32 [00:00<00:05,  5.21it/s][A[A

 12% 4/32 [00:00<00:05,  4.73it/s][A[A

 16% 5/32 [00:01<00:05,  5.04it/s][A[A

 19% 6/32 [00:02<00:13,  1.97it/s][A[A

 22% 7/32 [00:02<00:10,  2.38it/s][A[A

 25% 8/32 [00:02<00:08,  2.94it/s][A[A

 28% 9/32 [00:02<00:06,  3.53it/s][A[A

 31% 10/32 [00:03<00:05,  3.72it/s][A[A

 34% 11/32 [00:03<00:05,  3.86it/s][A[A

 38% 12/32 [00:03<00:04,  4.12it/s][A[A

 41% 13/32 [00:03<00:04,  4.35it/s][A[A

 44% 14/32 [00:03<00:04,  4.42it/s][A[A

 47% 15/32 [00:04<00:03,  4.43it/s][A[A

 50% 16/32 [00:04<00:03,  5.03it/s][A[A

 53% 17/32 [00:04<00:03,  4.75it/s][A[A

 56% 18/32 [00:04<00:02,  5.29it/s][A[A

 59% 19/32 [00:05<00:06,  2.07it/s][A[A

 62% 20/32 [00:05<00:04,  2.62it/s][A[A

 66% 21/32 [00:06<00:03,  3.28it/s][A[A

 69% 22/32 [00:06<00:02,  3.91it/s][A[A

 72% 23/32 [00:06<00:02,  4.01it/s][A[A

 75% 24/32 [00:06<00:01,  4.18it/s][A[A

 78% 25/32 [00:06<00:01,  4.47it/s][A[A

 81% 26/32 [00:07<00:01,  4.73it/s][A[A

 84% 27/32 [00:07<00:01,  4.72it/s][A[A

 88% 28/32 [00:07<00:00,  5.08it/s][A[A

 91% 29/32 [00:07<00:00,  4.87it/s][A[A

 94% 30/32 [00:07<00:00,  4.58it/s][A[A

 97% 31/32 [00:07<00:00,  5.39it/s][A[A

100% 32/32 [00:08<00:00,  5.21it/s][A[A100% 32/32 [00:08<00:00,  3.91it/s]
Meta loss on this task batch = 5.2395e-01, PNorm = 35.4308, GNorm = 0.1476

 16% 3/19 [00:26<02:19,  8.74s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.08it/s][A[A

  6% 2/32 [00:01<00:16,  1.87it/s][A[A

  9% 3/32 [00:01<00:12,  2.32it/s][A[A

 12% 4/32 [00:01<00:10,  2.68it/s][A[A

 16% 5/32 [00:02<00:08,  3.10it/s][A[A

 19% 6/32 [00:02<00:07,  3.39it/s][A[A

 22% 7/32 [00:02<00:06,  4.08it/s][A[A

 25% 8/32 [00:02<00:05,  4.34it/s][A[A

 28% 9/32 [00:02<00:04,  4.92it/s][A[A

 31% 10/32 [00:02<00:04,  5.26it/s][A[A

 34% 11/32 [00:03<00:03,  5.56it/s][A[A

 38% 12/32 [00:03<00:03,  6.00it/s][A[A

 41% 13/32 [00:03<00:03,  5.50it/s][A[A

 44% 14/32 [00:03<00:03,  5.22it/s][A[A

 47% 15/32 [00:03<00:03,  5.52it/s][A[A

 50% 16/32 [00:05<00:07,  2.02it/s][A[A

 53% 17/32 [00:05<00:05,  2.53it/s][A[A

 56% 18/32 [00:05<00:04,  3.03it/s][A[A

 59% 19/32 [00:05<00:03,  3.67it/s][A[A

 62% 20/32 [00:05<00:03,  3.89it/s][A[A

 66% 21/32 [00:05<00:02,  4.08it/s][A[A

 69% 22/32 [00:06<00:02,  4.46it/s][A[A

 72% 23/32 [00:06<00:02,  4.42it/s][A[A

 75% 24/32 [00:06<00:01,  4.47it/s][A[A

 78% 25/32 [00:06<00:01,  5.10it/s][A[A

 81% 26/32 [00:06<00:01,  5.77it/s][A[A

 84% 27/32 [00:07<00:00,  5.45it/s][A[A

 88% 28/32 [00:07<00:00,  5.26it/s][A[A

 91% 29/32 [00:07<00:00,  5.70it/s][A[A

 94% 30/32 [00:07<00:00,  5.24it/s][A[A

 97% 31/32 [00:07<00:00,  5.72it/s][A[A

100% 32/32 [00:07<00:00,  5.35it/s][A[A100% 32/32 [00:07<00:00,  4.01it/s]
Meta loss on this task batch = 5.8461e-01, PNorm = 35.4422, GNorm = 0.0742

 21% 4/19 [00:34<02:11,  8.73s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:38,  1.23s/it][A[A

  6% 2/32 [00:01<00:26,  1.11it/s][A[A

  9% 3/32 [00:01<00:19,  1.48it/s][A[A

 12% 4/32 [00:01<00:15,  1.82it/s][A[A

 16% 5/32 [00:01<00:12,  2.23it/s][A[A

 19% 6/32 [00:02<00:09,  2.76it/s][A[A

 22% 7/32 [00:02<00:07,  3.31it/s][A[A

 25% 8/32 [00:02<00:06,  3.93it/s][A[A

 28% 9/32 [00:02<00:05,  4.14it/s][A[A

 31% 10/32 [00:02<00:04,  4.58it/s][A[A

 34% 11/32 [00:03<00:04,  4.57it/s][A[A

 38% 12/32 [00:03<00:04,  4.71it/s][A[A

 41% 13/32 [00:03<00:04,  4.62it/s][A[A

 44% 14/32 [00:03<00:03,  5.13it/s][A[A

 47% 15/32 [00:03<00:03,  4.90it/s][A[A

 50% 16/32 [00:04<00:03,  5.19it/s][A[A

 53% 17/32 [00:05<00:07,  1.93it/s][A[A

 56% 18/32 [00:05<00:05,  2.37it/s][A[A

 59% 19/32 [00:05<00:04,  2.71it/s][A[A

 62% 20/32 [00:05<00:03,  3.04it/s][A[A

 66% 21/32 [00:06<00:03,  3.45it/s][A[A

 69% 22/32 [00:06<00:02,  3.92it/s][A[A

 72% 23/32 [00:06<00:02,  4.33it/s][A[A

 75% 24/32 [00:06<00:01,  4.23it/s][A[A

 78% 25/32 [00:06<00:01,  4.28it/s][A[A

 81% 26/32 [00:07<00:01,  4.84it/s][A[A

 84% 27/32 [00:07<00:00,  5.02it/s][A[A

 88% 28/32 [00:07<00:00,  5.42it/s][A[A

 91% 29/32 [00:07<00:00,  5.32it/s][A[A

 94% 30/32 [00:07<00:00,  5.68it/s][A[A

 97% 31/32 [00:07<00:00,  5.65it/s][A[A

100% 32/32 [00:09<00:00,  2.02it/s][A[A100% 32/32 [00:09<00:00,  3.47it/s]
Meta loss on this task batch = 5.2269e-01, PNorm = 35.4550, GNorm = 0.1241

 26% 5/19 [00:44<02:07,  9.10s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  6.67it/s][A[A

  6% 2/32 [00:00<00:04,  6.04it/s][A[A

  9% 3/32 [00:00<00:04,  6.28it/s][A[A

 12% 4/32 [00:00<00:04,  5.95it/s][A[A

 16% 5/32 [00:00<00:05,  4.93it/s][A[A

 19% 6/32 [00:01<00:05,  4.65it/s][A[A

 22% 7/32 [00:01<00:05,  4.91it/s][A[A

 25% 8/32 [00:01<00:04,  5.12it/s][A[A

 28% 9/32 [00:01<00:05,  4.60it/s][A[A

 31% 10/32 [00:02<00:04,  4.51it/s][A[A

 34% 11/32 [00:03<00:11,  1.90it/s][A[A

 38% 12/32 [00:03<00:08,  2.30it/s][A[A

 41% 13/32 [00:03<00:06,  2.87it/s][A[A

 44% 14/32 [00:03<00:05,  3.13it/s][A[A

 47% 15/32 [00:04<00:04,  3.43it/s][A[A

 50% 16/32 [00:04<00:04,  3.65it/s][A[A

 53% 17/32 [00:04<00:03,  4.27it/s][A[A

 56% 18/32 [00:04<00:03,  4.30it/s][A[A

 59% 19/32 [00:04<00:02,  4.56it/s][A[A

 62% 20/32 [00:05<00:02,  4.76it/s][A[A

 66% 21/32 [00:05<00:02,  4.80it/s][A[A

 69% 22/32 [00:05<00:02,  4.74it/s][A[A

 72% 23/32 [00:05<00:01,  4.65it/s][A[A

 75% 24/32 [00:06<00:04,  1.95it/s][A[A

 78% 25/32 [00:07<00:03,  2.33it/s][A[A

 81% 26/32 [00:07<00:02,  2.91it/s][A[A

 84% 27/32 [00:07<00:01,  3.18it/s][A[A

 88% 28/32 [00:07<00:01,  3.25it/s][A[A

 91% 29/32 [00:08<00:00,  3.47it/s][A[A

 94% 30/32 [00:08<00:00,  3.80it/s][A[A

 97% 31/32 [00:08<00:00,  4.35it/s][A[A

100% 32/32 [00:08<00:00,  4.51it/s][A[A100% 32/32 [00:08<00:00,  3.68it/s]
Meta loss on this task batch = 4.4785e-01, PNorm = 35.4689, GNorm = 0.1199

 32% 6/19 [00:54<01:59,  9.22s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.19it/s][A[A

  6% 2/32 [00:00<00:05,  5.33it/s][A[A

  9% 3/32 [00:00<00:05,  5.06it/s][A[A

 12% 4/32 [00:00<00:05,  4.77it/s][A[A

 16% 5/32 [00:00<00:05,  5.35it/s][A[A

 19% 6/32 [00:02<00:12,  2.05it/s][A[A

 22% 7/32 [00:02<00:10,  2.44it/s][A[A

 25% 8/32 [00:02<00:08,  2.93it/s][A[A

 28% 9/32 [00:02<00:06,  3.40it/s][A[A

 31% 10/32 [00:02<00:05,  3.71it/s][A[A

 34% 11/32 [00:03<00:05,  3.92it/s][A[A

 38% 12/32 [00:03<00:04,  4.03it/s][A[A

 41% 13/32 [00:03<00:04,  4.23it/s][A[A

 44% 14/32 [00:03<00:03,  4.71it/s][A[A

 47% 15/32 [00:04<00:03,  4.45it/s][A[A

 50% 16/32 [00:04<00:03,  4.33it/s][A[A

 53% 17/32 [00:05<00:07,  1.88it/s][A[A

 56% 18/32 [00:05<00:06,  2.27it/s][A[A

 59% 19/32 [00:05<00:04,  2.78it/s][A[A

 62% 20/32 [00:06<00:03,  3.08it/s][A[A

 66% 21/32 [00:06<00:03,  3.37it/s][A[A

 69% 22/32 [00:06<00:02,  3.59it/s][A[A

 72% 23/32 [00:06<00:02,  3.76it/s][A[A

 75% 24/32 [00:07<00:02,  3.99it/s][A[A

 78% 25/32 [00:07<00:01,  4.37it/s][A[A

 81% 26/32 [00:07<00:01,  4.32it/s][A[A

 84% 27/32 [00:07<00:01,  4.32it/s][A[A

 88% 28/32 [00:07<00:00,  4.21it/s][A[A

 91% 29/32 [00:08<00:00,  4.62it/s][A[A

 94% 30/32 [00:09<00:01,  1.96it/s][A[A

 97% 31/32 [00:09<00:00,  2.48it/s][A[A

100% 32/32 [00:09<00:00,  2.85it/s][A[A100% 32/32 [00:09<00:00,  3.29it/s]
Meta loss on this task batch = 4.6273e-01, PNorm = 35.4821, GNorm = 0.0321

 37% 7/19 [01:04<01:55,  9.60s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.00it/s][A[A

  6% 2/32 [00:00<00:07,  3.91it/s][A[A

  9% 3/32 [00:00<00:07,  3.95it/s][A[A

 12% 4/32 [00:01<00:07,  3.99it/s][A[A

 16% 5/32 [00:01<00:06,  4.06it/s][A[A

 19% 6/32 [00:01<00:06,  3.98it/s][A[A

 22% 7/32 [00:02<00:13,  1.89it/s][A[A

 25% 8/32 [00:02<00:10,  2.24it/s][A[A

 28% 9/32 [00:03<00:08,  2.61it/s][A[A

 31% 10/32 [00:03<00:07,  2.94it/s][A[A

 34% 11/32 [00:03<00:06,  3.37it/s][A[A

 38% 12/32 [00:03<00:05,  3.73it/s][A[A

 41% 13/32 [00:04<00:04,  4.03it/s][A[A

 44% 14/32 [00:04<00:03,  4.51it/s][A[A

 47% 15/32 [00:04<00:03,  4.83it/s][A[A

 50% 16/32 [00:04<00:03,  4.73it/s][A[A

 53% 17/32 [00:04<00:03,  4.45it/s][A[A

 56% 18/32 [00:05<00:03,  4.43it/s][A[A

 59% 19/32 [00:05<00:03,  4.25it/s][A[A

 62% 20/32 [00:06<00:06,  1.87it/s][A[A

 66% 21/32 [00:06<00:04,  2.27it/s][A[A

 69% 22/32 [00:06<00:03,  2.79it/s][A[A

 72% 23/32 [00:07<00:02,  3.33it/s][A[A

 75% 24/32 [00:07<00:02,  3.51it/s][A[A

 78% 25/32 [00:07<00:01,  3.66it/s][A[A

 81% 26/32 [00:07<00:01,  4.06it/s][A[A

 84% 27/32 [00:07<00:01,  4.39it/s][A[A

 88% 28/32 [00:08<00:00,  4.65it/s][A[A

 91% 29/32 [00:08<00:00,  4.88it/s][A[A

 94% 30/32 [00:08<00:00,  5.03it/s][A[A

 97% 31/32 [00:08<00:00,  5.01it/s][A[A

100% 32/32 [00:08<00:00,  5.10it/s][A[A100% 32/32 [00:08<00:00,  3.59it/s]
Meta loss on this task batch = 3.2593e-01, PNorm = 35.4987, GNorm = 0.1208

 42% 8/19 [01:14<01:45,  9.62s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.28it/s][A[A

  6% 2/32 [00:00<00:05,  5.25it/s][A[A

  9% 3/32 [00:00<00:05,  5.25it/s][A[A

 12% 4/32 [00:00<00:05,  5.29it/s][A[A

 16% 5/32 [00:01<00:13,  2.04it/s][A[A

 19% 6/32 [00:02<00:10,  2.50it/s][A[A

 22% 7/32 [00:02<00:08,  2.97it/s][A[A

 25% 8/32 [00:02<00:07,  3.40it/s][A[A

 28% 9/32 [00:02<00:06,  3.74it/s][A[A

 31% 10/32 [00:02<00:05,  4.09it/s][A[A

 34% 11/32 [00:03<00:04,  4.39it/s][A[A

 38% 12/32 [00:03<00:04,  4.66it/s][A[A

 41% 13/32 [00:03<00:03,  4.84it/s][A[A

 44% 14/32 [00:03<00:03,  4.85it/s][A[A

 47% 15/32 [00:03<00:03,  4.87it/s][A[A

 50% 16/32 [00:04<00:03,  5.03it/s][A[A

 53% 17/32 [00:04<00:02,  5.23it/s][A[A

 56% 18/32 [00:04<00:02,  5.31it/s][A[A

 59% 19/32 [00:04<00:02,  5.34it/s][A[A

 62% 20/32 [00:04<00:02,  5.39it/s][A[A

 66% 21/32 [00:04<00:02,  5.30it/s][A[A

 69% 22/32 [00:05<00:01,  5.25it/s][A[A

 72% 23/32 [00:06<00:04,  1.97it/s][A[A

 75% 24/32 [00:06<00:03,  2.42it/s][A[A

 78% 25/32 [00:06<00:02,  2.81it/s][A[A

 81% 26/32 [00:07<00:01,  3.32it/s][A[A

 84% 27/32 [00:07<00:01,  3.71it/s][A[A

 88% 28/32 [00:07<00:00,  4.04it/s][A[A

 91% 29/32 [00:07<00:00,  4.33it/s][A[A

 94% 30/32 [00:07<00:00,  4.62it/s][A[A

 97% 31/32 [00:07<00:00,  4.77it/s][A[A

100% 32/32 [00:08<00:00,  4.82it/s][A[A100% 32/32 [00:08<00:00,  3.91it/s]
Meta loss on this task batch = 3.4480e-02, PNorm = 35.5209, GNorm = 0.1680

 47% 9/19 [01:23<01:34,  9.43s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.99it/s][A[A

  6% 2/32 [00:00<00:06,  4.72it/s][A[A

  9% 3/32 [00:00<00:05,  4.86it/s][A[A

 12% 4/32 [00:00<00:05,  4.89it/s][A[A

 16% 5/32 [00:02<00:13,  2.00it/s][A[A

 19% 6/32 [00:02<00:10,  2.45it/s][A[A

 22% 7/32 [00:02<00:08,  2.93it/s][A[A

 25% 8/32 [00:02<00:07,  3.41it/s][A[A

 28% 9/32 [00:02<00:05,  3.86it/s][A[A

 31% 10/32 [00:02<00:05,  4.20it/s][A[A

 34% 11/32 [00:03<00:04,  4.45it/s][A[A

 38% 12/32 [00:03<00:04,  4.66it/s][A[A

 41% 13/32 [00:03<00:03,  4.76it/s][A[A

 44% 14/32 [00:03<00:03,  4.94it/s][A[A

 47% 15/32 [00:03<00:03,  4.98it/s][A[A

 50% 16/32 [00:04<00:03,  4.87it/s][A[A

 53% 17/32 [00:04<00:02,  5.03it/s][A[A

 56% 18/32 [00:04<00:02,  5.07it/s][A[A

 59% 19/32 [00:04<00:02,  4.79it/s][A[A

 62% 20/32 [00:04<00:02,  4.90it/s][A[A

 66% 21/32 [00:05<00:02,  5.05it/s][A[A

 69% 22/32 [00:05<00:01,  5.17it/s][A[A

 72% 23/32 [00:05<00:01,  5.28it/s][A[A

 75% 24/32 [00:05<00:01,  5.30it/s][A[A

 78% 25/32 [00:05<00:01,  5.35it/s][A[A

 81% 26/32 [00:07<00:02,  2.02it/s][A[A

 84% 27/32 [00:07<00:02,  2.39it/s][A[A

 88% 28/32 [00:07<00:01,  2.73it/s][A[A

 91% 29/32 [00:07<00:00,  3.02it/s][A[A

 94% 30/32 [00:08<00:00,  3.30it/s][A[A

 97% 31/32 [00:08<00:00,  3.56it/s][A[A

100% 32/32 [00:08<00:00,  3.73it/s][A[A100% 32/32 [00:08<00:00,  3.76it/s]
Meta loss on this task batch = 1.7127e-01, PNorm = 35.5413, GNorm = 0.0457

 53% 10/19 [01:32<01:24,  9.40s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.83it/s][A[A

  6% 2/32 [00:01<00:16,  1.78it/s][A[A

  9% 3/32 [00:01<00:12,  2.24it/s][A[A

 12% 4/32 [00:01<00:10,  2.71it/s][A[A

 16% 5/32 [00:02<00:08,  3.05it/s][A[A

 19% 6/32 [00:02<00:07,  3.29it/s][A[A

 22% 7/32 [00:02<00:07,  3.48it/s][A[A

 25% 8/32 [00:02<00:06,  3.63it/s][A[A

 28% 9/32 [00:03<00:05,  4.01it/s][A[A

 31% 10/32 [00:03<00:05,  4.01it/s][A[A

 34% 11/32 [00:04<00:11,  1.83it/s][A[A

 38% 12/32 [00:04<00:09,  2.19it/s][A[A

 41% 13/32 [00:05<00:07,  2.53it/s][A[A

 44% 14/32 [00:05<00:06,  2.86it/s][A[A

 47% 15/32 [00:05<00:05,  3.32it/s][A[A

 50% 16/32 [00:05<00:04,  3.54it/s][A[A

 53% 17/32 [00:05<00:03,  3.89it/s][A[A

 56% 18/32 [00:06<00:03,  3.97it/s][A[A

 59% 19/32 [00:06<00:03,  4.08it/s][A[A

 62% 20/32 [00:07<00:06,  1.90it/s][A[A

 66% 21/32 [00:07<00:04,  2.36it/s][A[A

 69% 22/32 [00:07<00:03,  2.74it/s][A[A

 72% 23/32 [00:08<00:02,  3.08it/s][A[A

 75% 24/32 [00:08<00:02,  3.30it/s][A[A

 78% 25/32 [00:08<00:01,  3.52it/s][A[A

 81% 26/32 [00:08<00:01,  3.67it/s][A[A

 84% 27/32 [00:09<00:01,  3.74it/s][A[A

 88% 28/32 [00:09<00:01,  3.94it/s][A[A

 91% 29/32 [00:09<00:00,  3.97it/s][A[A

 94% 30/32 [00:09<00:00,  4.06it/s][A[A

 97% 31/32 [00:10<00:00,  4.36it/s][A[A

100% 32/32 [00:11<00:00,  1.87it/s][A[A100% 32/32 [00:11<00:00,  2.82it/s]
Meta loss on this task batch = 5.8996e-01, PNorm = 35.5522, GNorm = 0.3336

 58% 11/19 [01:44<01:21, 10.22s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  3.98it/s][A[A

  6% 2/32 [00:00<00:07,  4.03it/s][A[A

  9% 3/32 [00:00<00:07,  4.01it/s][A[A

 12% 4/32 [00:00<00:06,  4.38it/s][A[A

 16% 5/32 [00:01<00:06,  4.23it/s][A[A

 19% 6/32 [00:02<00:14,  1.84it/s][A[A

 22% 7/32 [00:02<00:11,  2.23it/s][A[A

 25% 8/32 [00:02<00:09,  2.62it/s][A[A

 28% 9/32 [00:03<00:07,  2.96it/s][A[A

 31% 10/32 [00:03<00:06,  3.26it/s][A[A

 34% 11/32 [00:03<00:06,  3.45it/s][A[A

 38% 12/32 [00:03<00:05,  3.73it/s][A[A

 41% 13/32 [00:04<00:04,  3.84it/s][A[A

 44% 14/32 [00:04<00:04,  4.10it/s][A[A

 47% 15/32 [00:04<00:04,  4.11it/s][A[A

 50% 16/32 [00:05<00:08,  1.85it/s][A[A

 53% 17/32 [00:06<00:06,  2.18it/s][A[A

 56% 18/32 [00:06<00:05,  2.52it/s][A[A

 59% 19/32 [00:06<00:04,  2.83it/s][A[A

 62% 20/32 [00:06<00:03,  3.32it/s][A[A

 66% 21/32 [00:06<00:03,  3.49it/s][A[A

 69% 22/32 [00:07<00:02,  3.64it/s][A[A

 72% 23/32 [00:07<00:02,  3.81it/s][A[A

 75% 24/32 [00:07<00:02,  3.93it/s][A[A

 78% 25/32 [00:07<00:01,  4.03it/s][A[A

 81% 26/32 [00:08<00:01,  4.07it/s][A[A

 84% 27/32 [00:09<00:02,  1.83it/s][A[A

 88% 28/32 [00:09<00:01,  2.23it/s][A[A

 91% 29/32 [00:09<00:01,  2.71it/s][A[A

 94% 30/32 [00:10<00:00,  3.07it/s][A[A

 97% 31/32 [00:10<00:00,  3.47it/s][A[A

100% 32/32 [00:10<00:00,  3.64it/s][A[A100% 32/32 [00:10<00:00,  3.06it/s]
Meta loss on this task batch = 5.7307e-01, PNorm = 35.5524, GNorm = 0.4264

 63% 12/19 [01:56<01:13, 10.54s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.68it/s][A[A

  6% 2/32 [00:00<00:07,  3.79it/s][A[A

  9% 3/32 [00:00<00:07,  3.91it/s][A[A

 12% 4/32 [00:00<00:07,  3.98it/s][A[A

 16% 5/32 [00:02<00:14,  1.88it/s][A[A

 19% 6/32 [00:02<00:11,  2.24it/s][A[A

 22% 7/32 [00:02<00:09,  2.64it/s][A[A

 25% 8/32 [00:02<00:08,  2.94it/s][A[A

 28% 9/32 [00:03<00:06,  3.32it/s][A[A

 31% 10/32 [00:03<00:06,  3.55it/s][A[A

 34% 11/32 [00:03<00:05,  4.00it/s][A[A

 38% 12/32 [00:03<00:04,  4.08it/s][A[A

 41% 13/32 [00:03<00:04,  4.11it/s][A[A

 44% 14/32 [00:05<00:09,  1.82it/s][A[A

 47% 15/32 [00:05<00:07,  2.20it/s][A[A

 50% 16/32 [00:05<00:06,  2.64it/s][A[A

 53% 17/32 [00:05<00:05,  2.96it/s][A[A

 56% 18/32 [00:06<00:04,  3.44it/s][A[A

 59% 19/32 [00:06<00:03,  3.61it/s][A[A

 62% 20/32 [00:06<00:03,  3.75it/s][A[A

 66% 21/32 [00:06<00:02,  3.83it/s][A[A

 69% 22/32 [00:08<00:05,  1.81it/s][A[A

 72% 23/32 [00:08<00:04,  2.17it/s][A[A

 75% 24/32 [00:08<00:03,  2.59it/s][A[A

 78% 25/32 [00:08<00:02,  2.99it/s][A[A

 81% 26/32 [00:09<00:01,  3.23it/s][A[A

 84% 27/32 [00:09<00:01,  3.47it/s][A[A

 88% 28/32 [00:09<00:01,  3.58it/s][A[A

 91% 29/32 [00:09<00:00,  3.67it/s][A[A

 94% 30/32 [00:09<00:00,  4.00it/s][A[A

 97% 31/32 [00:11<00:00,  1.83it/s][A[A

100% 32/32 [00:11<00:00,  2.30it/s][A[A100% 32/32 [00:11<00:00,  2.81it/s]
Meta loss on this task batch = 6.2964e-01, PNorm = 35.5465, GNorm = 0.3296

 68% 13/19 [02:08<01:06, 11.03s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  3.96it/s][A[A

  6% 2/32 [00:00<00:07,  3.98it/s][A[A

  9% 3/32 [00:00<00:07,  4.09it/s][A[A

 12% 4/32 [00:00<00:06,  4.20it/s][A[A

 16% 5/32 [00:01<00:06,  4.15it/s][A[A

 19% 6/32 [00:01<00:06,  4.21it/s][A[A

 22% 7/32 [00:01<00:06,  4.15it/s][A[A

 25% 8/32 [00:01<00:05,  4.19it/s][A[A

 28% 9/32 [00:02<00:05,  4.53it/s][A[A

 31% 10/32 [00:02<00:05,  4.33it/s][A[A

 34% 11/32 [00:03<00:11,  1.86it/s][A[A

 38% 12/32 [00:03<00:09,  2.20it/s][A[A

 41% 13/32 [00:04<00:07,  2.61it/s][A[A

 44% 14/32 [00:04<00:06,  2.92it/s][A[A

 47% 15/32 [00:04<00:05,  3.19it/s][A[A

 50% 16/32 [00:04<00:04,  3.39it/s][A[A

 53% 17/32 [00:05<00:04,  3.59it/s][A[A

 56% 18/32 [00:05<00:03,  3.78it/s][A[A

 59% 19/32 [00:05<00:03,  3.98it/s][A[A

 62% 20/32 [00:05<00:03,  3.96it/s][A[A

 66% 21/32 [00:06<00:05,  1.83it/s][A[A

 69% 22/32 [00:07<00:04,  2.19it/s][A[A

 72% 23/32 [00:07<00:03,  2.55it/s][A[A

 75% 24/32 [00:07<00:02,  2.92it/s][A[A

 78% 25/32 [00:07<00:02,  3.21it/s][A[A

 81% 26/32 [00:08<00:01,  3.43it/s][A[A

 84% 27/32 [00:08<00:01,  3.63it/s][A[A

 88% 28/32 [00:08<00:01,  3.83it/s][A[A

 91% 29/32 [00:08<00:00,  3.95it/s][A[A

 94% 30/32 [00:09<00:00,  4.10it/s][A[A

 97% 31/32 [00:10<00:00,  1.83it/s][A[A

100% 32/32 [00:10<00:00,  2.16it/s][A[A100% 32/32 [00:10<00:00,  3.01it/s]
Meta loss on this task batch = 5.7781e-01, PNorm = 35.5400, GNorm = 0.1090

 74% 14/19 [02:19<00:55, 11.17s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.76it/s][A[A

  6% 2/32 [00:00<00:06,  4.41it/s][A[A

  9% 3/32 [00:00<00:06,  4.22it/s][A[A

 12% 4/32 [00:00<00:06,  4.21it/s][A[A

 16% 5/32 [00:01<00:06,  4.35it/s][A[A

 19% 6/32 [00:01<00:06,  4.20it/s][A[A

 22% 7/32 [00:01<00:05,  4.17it/s][A[A

 25% 8/32 [00:01<00:05,  4.48it/s][A[A

 28% 9/32 [00:02<00:05,  4.27it/s][A[A

 31% 10/32 [00:03<00:11,  1.91it/s][A[A

 34% 11/32 [00:03<00:08,  2.42it/s][A[A

 38% 12/32 [00:03<00:06,  2.93it/s][A[A

 41% 13/32 [00:03<00:05,  3.25it/s][A[A

 44% 14/32 [00:04<00:05,  3.46it/s][A[A

 47% 15/32 [00:04<00:04,  3.92it/s][A[A

 50% 16/32 [00:04<00:03,  4.08it/s][A[A

 53% 17/32 [00:04<00:03,  4.03it/s][A[A

 56% 18/32 [00:05<00:03,  4.14it/s][A[A

 59% 19/32 [00:05<00:02,  4.66it/s][A[A

 62% 20/32 [00:05<00:02,  4.47it/s][A[A

 66% 21/32 [00:05<00:02,  4.81it/s][A[A

 69% 22/32 [00:05<00:02,  4.67it/s][A[A

 72% 23/32 [00:07<00:04,  1.92it/s][A[A

 75% 24/32 [00:07<00:03,  2.34it/s][A[A

 78% 25/32 [00:07<00:02,  2.85it/s][A[A

 81% 26/32 [00:07<00:01,  3.12it/s][A[A

 84% 27/32 [00:07<00:01,  3.40it/s][A[A

 88% 28/32 [00:08<00:01,  3.84it/s][A[A

 91% 29/32 [00:08<00:00,  3.89it/s][A[A

 94% 30/32 [00:08<00:00,  3.93it/s][A[A

 97% 31/32 [00:08<00:00,  3.99it/s][A[A

100% 32/32 [00:09<00:00,  4.06it/s][A[A100% 32/32 [00:09<00:00,  3.53it/s]
Meta loss on this task batch = 4.8874e-01, PNorm = 35.5352, GNorm = 0.0492

 79% 15/19 [02:29<00:43, 10.77s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.41it/s][A[A

  6% 2/32 [00:00<00:06,  4.35it/s][A[A

  9% 3/32 [00:00<00:06,  4.29it/s][A[A

 12% 4/32 [00:01<00:14,  1.87it/s][A[A

 16% 5/32 [00:02<00:11,  2.31it/s][A[A

 19% 6/32 [00:02<00:09,  2.67it/s][A[A

 22% 7/32 [00:02<00:08,  2.98it/s][A[A

 25% 8/32 [00:02<00:07,  3.39it/s][A[A

 28% 9/32 [00:03<00:06,  3.58it/s][A[A

 31% 10/32 [00:03<00:05,  3.71it/s][A[A

 34% 11/32 [00:03<00:05,  3.81it/s][A[A

 38% 12/32 [00:03<00:05,  3.93it/s][A[A

 41% 13/32 [00:05<00:10,  1.83it/s][A[A

 44% 14/32 [00:05<00:08,  2.19it/s][A[A

 47% 15/32 [00:05<00:06,  2.59it/s][A[A

 50% 16/32 [00:05<00:05,  3.16it/s][A[A

 53% 17/32 [00:05<00:04,  3.43it/s][A[A

 56% 18/32 [00:06<00:03,  3.81it/s][A[A

 59% 19/32 [00:06<00:03,  3.91it/s][A[A

 62% 20/32 [00:06<00:02,  4.03it/s][A[A

 66% 21/32 [00:06<00:02,  4.08it/s][A[A

 69% 22/32 [00:06<00:02,  4.33it/s][A[A

 72% 23/32 [00:07<00:02,  4.24it/s][A[A

 75% 24/32 [00:07<00:01,  4.07it/s][A[A

 78% 25/32 [00:08<00:03,  1.90it/s][A[A

 81% 26/32 [00:08<00:02,  2.25it/s][A[A

 84% 27/32 [00:09<00:01,  2.55it/s][A[A

 88% 28/32 [00:09<00:01,  2.89it/s][A[A

 91% 29/32 [00:09<00:00,  3.05it/s][A[A

 94% 30/32 [00:09<00:00,  3.33it/s][A[A

 97% 31/32 [00:10<00:00,  3.55it/s][A[A

100% 32/32 [00:10<00:00,  3.68it/s][A[A100% 32/32 [00:10<00:00,  3.06it/s]
Meta loss on this task batch = 5.5515e-01, PNorm = 35.5324, GNorm = 0.0328

 84% 16/19 [02:40<00:32, 10.92s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.41it/s][A[A

  6% 2/32 [00:00<00:06,  4.79it/s][A[A

  9% 3/32 [00:01<00:14,  2.01it/s][A[A

 12% 4/32 [00:01<00:11,  2.47it/s][A[A

 16% 5/32 [00:02<00:09,  2.84it/s][A[A

 19% 6/32 [00:02<00:07,  3.46it/s][A[A

 22% 7/32 [00:02<00:06,  3.80it/s][A[A

 25% 8/32 [00:02<00:06,  3.71it/s][A[A

 28% 9/32 [00:02<00:06,  3.79it/s][A[A

 31% 10/32 [00:03<00:05,  4.26it/s][A[A

 34% 11/32 [00:03<00:04,  4.28it/s][A[A

 38% 12/32 [00:03<00:04,  4.45it/s][A[A

 41% 13/32 [00:03<00:04,  4.62it/s][A[A

 44% 14/32 [00:03<00:03,  4.60it/s][A[A

 47% 15/32 [00:04<00:03,  4.48it/s][A[A

 50% 16/32 [00:04<00:03,  4.35it/s][A[A

 53% 17/32 [00:04<00:03,  4.66it/s][A[A

 56% 18/32 [00:05<00:07,  1.94it/s][A[A

 59% 19/32 [00:06<00:05,  2.26it/s][A[A

 62% 20/32 [00:06<00:04,  2.64it/s][A[A

 66% 21/32 [00:06<00:03,  3.03it/s][A[A

 69% 22/32 [00:06<00:03,  3.30it/s][A[A

 72% 23/32 [00:07<00:02,  3.47it/s][A[A

 75% 24/32 [00:07<00:02,  3.52it/s][A[A

 78% 25/32 [00:07<00:01,  3.77it/s][A[A

 81% 26/32 [00:07<00:01,  3.87it/s][A[A

 84% 27/32 [00:07<00:01,  4.28it/s][A[A

 88% 28/32 [00:08<00:00,  4.53it/s][A[A

 91% 29/32 [00:08<00:00,  4.48it/s][A[A

 94% 30/32 [00:09<00:01,  1.98it/s][A[A

 97% 31/32 [00:09<00:00,  2.41it/s][A[A

100% 32/32 [00:09<00:00,  2.75it/s][A[A100% 32/32 [00:09<00:00,  3.21it/s]
Meta loss on this task batch = 4.4545e-01, PNorm = 35.5326, GNorm = 0.0825

 89% 17/19 [02:51<00:21, 10.87s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.01it/s][A[A

  6% 2/32 [00:00<00:06,  4.76it/s][A[A

  9% 3/32 [00:00<00:06,  4.63it/s][A[A

 12% 4/32 [00:00<00:06,  4.51it/s][A[A

 16% 5/32 [00:01<00:06,  4.30it/s][A[A

 19% 6/32 [00:01<00:05,  4.41it/s][A[A

 22% 7/32 [00:02<00:13,  1.87it/s][A[A

 25% 8/32 [00:02<00:10,  2.22it/s][A[A

 28% 9/32 [00:03<00:08,  2.64it/s][A[A

 31% 10/32 [00:03<00:07,  3.02it/s][A[A

 34% 11/32 [00:03<00:06,  3.37it/s][A[A

 38% 12/32 [00:03<00:05,  3.79it/s][A[A

 41% 13/32 [00:03<00:05,  3.79it/s][A[A

 44% 14/32 [00:04<00:04,  4.39it/s][A[A

 47% 15/32 [00:04<00:04,  4.22it/s][A[A

 50% 16/32 [00:04<00:03,  4.27it/s][A[A

 53% 17/32 [00:04<00:03,  4.86it/s][A[A

 56% 18/32 [00:04<00:02,  4.72it/s][A[A

 59% 19/32 [00:06<00:06,  1.93it/s][A[A

 62% 20/32 [00:06<00:05,  2.32it/s][A[A

 66% 21/32 [00:06<00:04,  2.69it/s][A[A

 69% 22/32 [00:06<00:02,  3.34it/s][A[A

 72% 23/32 [00:07<00:02,  3.43it/s][A[A

 75% 24/32 [00:07<00:02,  3.41it/s][A[A

 78% 25/32 [00:07<00:01,  3.62it/s][A[A

 81% 26/32 [00:08<00:03,  1.78it/s][A[A

 84% 27/32 [00:09<00:02,  2.11it/s][A[A

 88% 28/32 [00:09<00:01,  2.61it/s][A[A

 91% 29/32 [00:09<00:00,  3.14it/s][A[A

 94% 30/32 [00:09<00:00,  3.36it/s][A[A

 97% 31/32 [00:09<00:00,  3.73it/s][A[A

100% 32/32 [00:10<00:00,  3.79it/s][A[A100% 32/32 [00:10<00:00,  3.16it/s]
Meta loss on this task batch = 5.1496e-01, PNorm = 35.5353, GNorm = 0.1071

 95% 18/19 [03:02<00:10, 10.89s/it][A

  0% 0/23 [00:00<?, ?it/s][A[A

  4% 1/23 [00:00<00:05,  4.19it/s][A[A

  9% 2/23 [00:00<00:04,  4.63it/s][A[A

 13% 3/23 [00:00<00:04,  4.67it/s][A[A

 17% 4/23 [00:00<00:04,  4.74it/s][A[A

 22% 5/23 [00:00<00:03,  5.34it/s][A[A

 26% 6/23 [00:01<00:03,  5.12it/s][A[A

 30% 7/23 [00:02<00:08,  1.97it/s][A[A

 35% 8/23 [00:02<00:06,  2.32it/s][A[A

 39% 9/23 [00:02<00:04,  2.92it/s][A[A

 43% 10/23 [00:03<00:04,  3.12it/s][A[A

 48% 11/23 [00:03<00:03,  3.51it/s][A[A

 52% 12/23 [00:03<00:02,  3.73it/s][A[A

 57% 13/23 [00:03<00:02,  4.38it/s][A[A

 61% 14/23 [00:03<00:01,  4.67it/s][A[A

 65% 15/23 [00:05<00:04,  1.78it/s][A[A

 70% 16/23 [00:05<00:03,  2.11it/s][A[A

 74% 17/23 [00:05<00:02,  2.63it/s][A[A

 78% 18/23 [00:05<00:01,  3.07it/s][A[A

 83% 19/23 [00:06<00:01,  3.35it/s][A[A

 87% 20/23 [00:06<00:00,  4.06it/s][A[A

 91% 21/23 [00:06<00:00,  4.51it/s][A[A

 96% 22/23 [00:06<00:00,  5.18it/s][A[A

100% 23/23 [00:06<00:00,  5.55it/s][A[A100% 23/23 [00:06<00:00,  3.48it/s]
Meta loss on this task batch = 4.4133e-01, PNorm = 35.5393, GNorm = 0.0767

100% 19/19 [03:09<00:00,  9.76s/it][A100% 19/19 [03:09<00:00,  9.99s/it]
Took 189.79455590248108 seconds to complete one epoch of meta training

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/20 [00:00<?, ?it/s][A[A


  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 32.76it/s]


  5% 1/20 [00:00<00:02,  8.31it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 20.03it/s]


 10% 2/20 [00:00<00:02,  6.33it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 18.14it/s][A[A[A100% 3/3 [00:00<00:00, 19.87it/s]


 15% 3/20 [00:00<00:03,  5.05it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 14.40it/s]


 20% 4/20 [00:00<00:03,  4.77it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 14.96it/s][A[A[A100% 4/4 [00:00<00:00, 17.30it/s]


 25% 5/20 [00:01<00:03,  3.82it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 25% 1/4 [00:01<00:03,  1.08s/it][A[A[A


 75% 3/4 [00:01<00:00,  1.28it/s][A[A[A100% 4/4 [00:01<00:00,  3.16it/s]


 30% 6/20 [00:02<00:08,  1.64it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.79it/s][A[A[A100% 4/4 [00:00<00:00, 20.51it/s]


 35% 7/20 [00:03<00:06,  1.87it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 18.76it/s][A[A[A


100% 4/4 [00:00<00:00, 18.50it/s][A[A[A100% 4/4 [00:00<00:00, 18.30it/s]


 40% 8/20 [00:03<00:05,  2.04it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.82it/s][A[A[A100% 4/4 [00:00<00:00, 23.48it/s]


 45% 9/20 [00:03<00:04,  2.25it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 17.95it/s][A[A[A100% 4/4 [00:00<00:00, 20.02it/s]


 50% 10/20 [00:04<00:04,  2.38it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 16.63it/s][A[A[A100% 4/4 [00:00<00:00, 19.10it/s]


 55% 11/20 [00:04<00:03,  2.48it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.73it/s][A[A[A100% 4/4 [00:00<00:00, 22.05it/s]


 60% 12/20 [00:04<00:03,  2.57it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.23it/s][A[A[A


100% 3/3 [00:01<00:00,  2.70it/s][A[A[A100% 3/3 [00:01<00:00,  2.48it/s]


 65% 13/20 [00:06<00:04,  1.45it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 13.68it/s][A[A[A100% 3/3 [00:00<00:00, 13.46it/s]


 70% 14/20 [00:06<00:03,  1.66it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.29it/s][A[A[A100% 4/4 [00:00<00:00, 17.36it/s]


 75% 15/20 [00:07<00:02,  1.84it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.66it/s][A[A[A100% 3/3 [00:00<00:00, 17.95it/s]


 80% 16/20 [00:07<00:01,  2.11it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:01<00:00,  1.73it/s][A[A[A100% 3/3 [00:01<00:00,  2.51it/s]


 85% 17/20 [00:08<00:02,  1.34it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 22.64it/s]


 90% 18/20 [00:08<00:01,  1.69it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 16.21it/s][A[A[A100% 3/3 [00:00<00:00, 20.46it/s]


 95% 19/20 [00:09<00:00,  2.04it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 12.91it/s]


100% 20/20 [00:09<00:00,  2.38it/s][A[A100% 20/20 [00:09<00:00,  2.11it/s]

100% 1/1 [00:09<00:00,  9.50s/it][A100% 1/1 [00:09<00:00,  9.50s/it]
Took 199.2924735546112 seconds to complete one epoch of meta training and validating
Meta validation score auc = 0.589920
 17% 5/30 [16:15<1:21:45, 196.21s/it]Epoch 5

  0% 0/19 [00:00<?, ?it/s][A

  0% 0/32 [00:00<?, ?it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  3% 1/32 [00:00<00:05,  6.05it/s][A[A

  6% 2/32 [00:00<00:05,  5.66it/s][A[A

  9% 3/32 [00:00<00:05,  5.73it/s][A[A

 12% 4/32 [00:00<00:04,  5.98it/s][A[A

 16% 5/32 [00:00<00:04,  6.35it/s][A[A

 19% 6/32 [00:01<00:04,  6.11it/s][A[A

 22% 7/32 [00:01<00:04,  6.11it/s][A[A

 25% 8/32 [00:01<00:04,  5.77it/s][A[A

 28% 9/32 [00:01<00:03,  6.31it/s][A[A

 31% 10/32 [00:01<00:03,  6.64it/s][A[A

 34% 11/32 [00:01<00:03,  6.02it/s][A[A

 38% 12/32 [00:03<00:09,  2.03it/s][A[A

 41% 13/32 [00:03<00:07,  2.42it/s][A[A

 44% 14/32 [00:03<00:06,  2.84it/s][A[A

 47% 15/32 [00:03<00:04,  3.47it/s][A[A

 50% 16/32 [00:03<00:04,  3.74it/s][A[A

 53% 17/32 [00:04<00:03,  4.26it/s][A[A

 56% 18/32 [00:04<00:03,  4.63it/s][A[A

 59% 19/32 [00:04<00:02,  5.02it/s][A[A

 62% 20/32 [00:04<00:02,  5.06it/s][A[A

 66% 21/32 [00:04<00:01,  5.72it/s][A[A

 69% 22/32 [00:04<00:01,  5.61it/s][A[A

 72% 23/32 [00:05<00:01,  5.89it/s][A[A

 75% 24/32 [00:05<00:01,  5.98it/s][A[A

 78% 25/32 [00:05<00:01,  5.67it/s][A[A

 81% 26/32 [00:05<00:01,  5.37it/s][A[A

 84% 27/32 [00:05<00:00,  5.64it/s][A[A

 88% 28/32 [00:05<00:00,  5.67it/s][A[A

 91% 29/32 [00:06<00:00,  5.66it/s][A[A

 94% 30/32 [00:06<00:00,  6.35it/s][A[A

 97% 31/32 [00:06<00:00,  6.14it/s][A[A

100% 32/32 [00:06<00:00,  5.96it/s][A[A100% 32/32 [00:06<00:00,  4.88it/s]
Meta loss on this task batch = 5.1805e-01, PNorm = 35.5454, GNorm = 0.0779

  5% 1/19 [00:07<02:10,  7.24s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  7.64it/s][A[A

  6% 2/32 [00:01<00:13,  2.31it/s][A[A

  9% 3/32 [00:01<00:10,  2.79it/s][A[A

 12% 4/32 [00:01<00:08,  3.20it/s][A[A

 16% 5/32 [00:01<00:07,  3.60it/s][A[A

 19% 6/32 [00:02<00:06,  4.17it/s][A[A

 22% 7/32 [00:02<00:05,  4.82it/s][A[A

 25% 8/32 [00:02<00:04,  5.30it/s][A[A

 28% 9/32 [00:02<00:04,  5.52it/s][A[A

 31% 10/32 [00:02<00:03,  5.95it/s][A[A

 34% 11/32 [00:02<00:03,  6.29it/s][A[A

 38% 12/32 [00:02<00:03,  6.06it/s][A[A

 41% 13/32 [00:03<00:03,  6.29it/s][A[A

 44% 14/32 [00:03<00:03,  5.99it/s][A[A

 47% 15/32 [00:03<00:02,  6.06it/s][A[A

 50% 16/32 [00:03<00:02,  5.86it/s][A[A

 53% 17/32 [00:03<00:02,  6.04it/s][A[A

 56% 18/32 [00:03<00:02,  5.45it/s][A[A

 59% 19/32 [00:04<00:02,  5.85it/s][A[A

 62% 20/32 [00:04<00:02,  5.79it/s][A[A

 66% 21/32 [00:05<00:05,  2.16it/s][A[A

 69% 22/32 [00:05<00:03,  2.72it/s][A[A

 72% 23/32 [00:05<00:02,  3.31it/s][A[A

 75% 24/32 [00:05<00:02,  3.97it/s][A[A

 78% 25/32 [00:06<00:01,  4.32it/s][A[A

 81% 26/32 [00:06<00:01,  4.46it/s][A[A

 84% 27/32 [00:06<00:01,  4.86it/s][A[A

 88% 28/32 [00:06<00:00,  5.05it/s][A[A

 91% 29/32 [00:06<00:00,  5.72it/s][A[A

 94% 30/32 [00:06<00:00,  5.99it/s][A[A

 97% 31/32 [00:06<00:00,  6.16it/s][A[A

100% 32/32 [00:07<00:00,  6.44it/s][A[A100% 32/32 [00:07<00:00,  4.48it/s]
Meta loss on this task batch = 5.2884e-01, PNorm = 35.5511, GNorm = 0.0409

 11% 2/19 [00:15<02:05,  7.41s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.55it/s][A[A

  6% 2/32 [00:00<00:05,  5.46it/s][A[A

  9% 3/32 [00:00<00:05,  5.51it/s][A[A

 12% 4/32 [00:00<00:04,  5.99it/s][A[A

 16% 5/32 [00:00<00:04,  6.04it/s][A[A

 19% 6/32 [00:01<00:04,  5.84it/s][A[A

 22% 7/32 [00:01<00:04,  5.66it/s][A[A

 25% 8/32 [00:02<00:11,  2.14it/s][A[A

 28% 9/32 [00:02<00:08,  2.69it/s][A[A

 31% 10/32 [00:02<00:07,  3.07it/s][A[A

 34% 11/32 [00:02<00:06,  3.37it/s][A[A

 38% 12/32 [00:03<00:05,  3.83it/s][A[A

 41% 13/32 [00:03<00:04,  4.31it/s][A[A

 44% 14/32 [00:03<00:03,  4.87it/s][A[A

 47% 15/32 [00:03<00:03,  4.88it/s][A[A

 50% 16/32 [00:03<00:02,  5.47it/s][A[A

 53% 17/32 [00:03<00:02,  5.23it/s][A[A

 56% 18/32 [00:04<00:02,  5.69it/s][A[A

 59% 19/32 [00:04<00:02,  5.96it/s][A[A

 62% 20/32 [00:04<00:01,  6.42it/s][A[A

 66% 21/32 [00:04<00:01,  6.79it/s][A[A

 69% 22/32 [00:04<00:01,  6.95it/s][A[A

 72% 23/32 [00:04<00:01,  7.14it/s][A[A

 75% 24/32 [00:05<00:01,  6.08it/s][A[A

 78% 25/32 [00:06<00:03,  2.24it/s][A[A

 81% 26/32 [00:06<00:02,  2.86it/s][A[A

 84% 27/32 [00:06<00:01,  3.36it/s][A[A

 88% 28/32 [00:06<00:01,  3.89it/s][A[A

 91% 29/32 [00:06<00:00,  4.48it/s][A[A

 94% 30/32 [00:06<00:00,  4.43it/s][A[A

 97% 31/32 [00:07<00:00,  5.23it/s][A[A

100% 32/32 [00:07<00:00,  5.28it/s][A[A100% 32/32 [00:07<00:00,  4.41it/s]
Meta loss on this task batch = 5.2199e-01, PNorm = 35.5600, GNorm = 0.1344

 16% 3/19 [00:22<02:01,  7.56s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.31it/s][A[A

  6% 2/32 [00:00<00:05,  5.17it/s][A[A

  9% 3/32 [00:00<00:05,  5.34it/s][A[A

 12% 4/32 [00:00<00:04,  6.02it/s][A[A

 16% 5/32 [00:00<00:04,  5.64it/s][A[A

 19% 6/32 [00:01<00:04,  5.44it/s][A[A

 22% 7/32 [00:01<00:04,  6.03it/s][A[A

 25% 8/32 [00:01<00:03,  6.02it/s][A[A

 28% 9/32 [00:01<00:03,  6.49it/s][A[A

 31% 10/32 [00:01<00:03,  6.55it/s][A[A

 34% 11/32 [00:02<00:09,  2.25it/s][A[A

 38% 12/32 [00:02<00:07,  2.84it/s][A[A

 41% 13/32 [00:03<00:05,  3.43it/s][A[A

 44% 14/32 [00:03<00:04,  3.93it/s][A[A

 47% 15/32 [00:03<00:03,  4.57it/s][A[A

 50% 16/32 [00:03<00:03,  4.92it/s][A[A

 53% 17/32 [00:03<00:02,  5.36it/s][A[A

 56% 18/32 [00:03<00:02,  5.56it/s][A[A

 59% 19/32 [00:03<00:02,  5.96it/s][A[A

 62% 20/32 [00:04<00:02,  5.92it/s][A[A

 66% 21/32 [00:04<00:01,  5.70it/s][A[A

 69% 22/32 [00:04<00:01,  5.12it/s][A[A

 72% 23/32 [00:04<00:01,  4.96it/s][A[A

 75% 24/32 [00:05<00:01,  4.86it/s][A[A

 78% 25/32 [00:05<00:01,  5.46it/s][A[A

 81% 26/32 [00:05<00:00,  6.10it/s][A[A

 84% 27/32 [00:05<00:00,  6.13it/s][A[A

 88% 28/32 [00:05<00:00,  5.91it/s][A[A

 91% 29/32 [00:05<00:00,  6.40it/s][A[A

 94% 30/32 [00:06<00:00,  2.10it/s][A[A

 97% 31/32 [00:07<00:00,  2.66it/s][A[A

100% 32/32 [00:07<00:00,  3.13it/s][A[A100% 32/32 [00:07<00:00,  4.38it/s]
Meta loss on this task batch = 5.9511e-01, PNorm = 35.5685, GNorm = 0.0630

 21% 4/19 [00:30<01:55,  7.69s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.77it/s][A[A

  6% 2/32 [00:00<00:04,  6.30it/s][A[A

  9% 3/32 [00:00<00:04,  6.32it/s][A[A

 12% 4/32 [00:00<00:04,  5.96it/s][A[A

 16% 5/32 [00:00<00:04,  5.57it/s][A[A

 19% 6/32 [00:01<00:04,  5.72it/s][A[A

 22% 7/32 [00:01<00:04,  5.30it/s][A[A

 25% 8/32 [00:01<00:04,  5.76it/s][A[A

 28% 9/32 [00:01<00:04,  5.53it/s][A[A

 31% 10/32 [00:01<00:03,  5.75it/s][A[A

 34% 11/32 [00:01<00:03,  5.93it/s][A[A

 38% 12/32 [00:03<00:09,  2.10it/s][A[A

 41% 13/32 [00:03<00:07,  2.51it/s][A[A

 44% 14/32 [00:03<00:05,  3.11it/s][A[A

 47% 15/32 [00:03<00:04,  3.72it/s][A[A

 50% 16/32 [00:03<00:03,  4.52it/s][A[A

 53% 17/32 [00:03<00:02,  5.09it/s][A[A

 56% 18/32 [00:04<00:02,  5.09it/s][A[A

 59% 19/32 [00:04<00:02,  5.14it/s][A[A

 62% 20/32 [00:04<00:02,  5.58it/s][A[A

 66% 21/32 [00:04<00:01,  5.58it/s][A[A

 69% 22/32 [00:04<00:01,  6.22it/s][A[A

 72% 23/32 [00:04<00:01,  6.06it/s][A[A

 75% 24/32 [00:05<00:01,  5.24it/s][A[A

 78% 25/32 [00:05<00:01,  5.18it/s][A[A

 81% 26/32 [00:05<00:01,  5.63it/s][A[A

 84% 27/32 [00:05<00:00,  5.64it/s][A[A

 88% 28/32 [00:05<00:00,  5.94it/s][A[A

 91% 29/32 [00:06<00:01,  2.14it/s][A[A

 94% 30/32 [00:07<00:00,  2.68it/s][A[A

 97% 31/32 [00:07<00:00,  3.16it/s][A[A

100% 32/32 [00:07<00:00,  3.54it/s][A[A100% 32/32 [00:07<00:00,  4.29it/s]
Meta loss on this task batch = 5.0767e-01, PNorm = 35.5783, GNorm = 0.1036

 26% 5/19 [00:39<01:49,  7.82s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  6.77it/s][A[A

  6% 2/32 [00:00<00:04,  6.48it/s][A[A

  9% 3/32 [00:00<00:04,  6.55it/s][A[A

 12% 4/32 [00:00<00:04,  6.48it/s][A[A

 16% 5/32 [00:00<00:04,  6.36it/s][A[A

 19% 6/32 [00:01<00:04,  5.54it/s][A[A

 22% 7/32 [00:01<00:04,  5.53it/s][A[A

 25% 8/32 [00:01<00:04,  5.55it/s][A[A

 28% 9/32 [00:01<00:03,  5.95it/s][A[A

 31% 10/32 [00:01<00:03,  5.82it/s][A[A

 34% 11/32 [00:01<00:03,  5.59it/s][A[A

 38% 12/32 [00:02<00:03,  5.58it/s][A[A

 41% 13/32 [00:02<00:03,  5.93it/s][A[A

 44% 14/32 [00:03<00:08,  2.17it/s][A[A

 47% 15/32 [00:03<00:06,  2.59it/s][A[A

 50% 16/32 [00:03<00:05,  2.95it/s][A[A

 53% 17/32 [00:03<00:04,  3.56it/s][A[A

 56% 18/32 [00:04<00:03,  4.11it/s][A[A

 59% 19/32 [00:04<00:02,  4.56it/s][A[A

 62% 20/32 [00:04<00:02,  4.76it/s][A[A

 66% 21/32 [00:04<00:02,  5.27it/s][A[A

 69% 22/32 [00:04<00:01,  5.06it/s][A[A

 72% 23/32 [00:04<00:01,  5.20it/s][A[A

 75% 24/32 [00:05<00:01,  5.65it/s][A[A

 78% 25/32 [00:05<00:01,  5.21it/s][A[A

 81% 26/32 [00:05<00:01,  5.69it/s][A[A

 84% 27/32 [00:05<00:00,  5.18it/s][A[A

 88% 28/32 [00:05<00:00,  5.40it/s][A[A

 91% 29/32 [00:06<00:00,  5.80it/s][A[A

 94% 30/32 [00:06<00:00,  6.15it/s][A[A

 97% 31/32 [00:06<00:00,  6.28it/s][A[A

100% 32/32 [00:07<00:00,  2.22it/s][A[A100% 32/32 [00:07<00:00,  4.29it/s]
Meta loss on this task batch = 4.6440e-01, PNorm = 35.5904, GNorm = 0.1736

 32% 6/19 [00:47<01:42,  7.92s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.97it/s][A[A

  6% 2/32 [00:00<00:05,  5.38it/s][A[A

  9% 3/32 [00:00<00:05,  5.27it/s][A[A

 12% 4/32 [00:00<00:04,  5.63it/s][A[A

 16% 5/32 [00:00<00:04,  6.11it/s][A[A

 19% 6/32 [00:00<00:04,  6.38it/s][A[A

 22% 7/32 [00:01<00:04,  6.09it/s][A[A

 25% 8/32 [00:01<00:03,  6.08it/s][A[A

 28% 9/32 [00:01<00:03,  5.91it/s][A[A

 31% 10/32 [00:01<00:04,  5.45it/s][A[A

 34% 11/32 [00:01<00:03,  5.53it/s][A[A

 38% 12/32 [00:02<00:03,  5.27it/s][A[A

 41% 13/32 [00:02<00:03,  5.35it/s][A[A

 44% 14/32 [00:02<00:03,  5.69it/s][A[A

 47% 15/32 [00:02<00:03,  5.10it/s][A[A

 50% 16/32 [00:03<00:08,  1.99it/s][A[A

 53% 17/32 [00:04<00:05,  2.58it/s][A[A

 56% 18/32 [00:04<00:04,  2.99it/s][A[A

 59% 19/32 [00:04<00:03,  3.50it/s][A[A

 62% 20/32 [00:04<00:03,  3.64it/s][A[A

 66% 21/32 [00:04<00:02,  3.97it/s][A[A

 69% 22/32 [00:05<00:02,  4.38it/s][A[A

 72% 23/32 [00:05<00:02,  4.38it/s][A[A

 75% 24/32 [00:05<00:01,  4.43it/s][A[A

 78% 25/32 [00:05<00:01,  4.71it/s][A[A

 81% 26/32 [00:05<00:01,  5.44it/s][A[A

 84% 27/32 [00:05<00:00,  5.53it/s][A[A

 88% 28/32 [00:07<00:01,  2.00it/s][A[A

 91% 29/32 [00:07<00:01,  2.53it/s][A[A

 94% 30/32 [00:07<00:00,  2.97it/s][A[A

 97% 31/32 [00:07<00:00,  3.54it/s][A[A

100% 32/32 [00:07<00:00,  3.85it/s][A[A100% 32/32 [00:07<00:00,  4.05it/s]
Meta loss on this task batch = 4.7724e-01, PNorm = 35.6042, GNorm = 0.0573

 37% 7/19 [00:55<01:37,  8.13s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.31it/s][A[A

  6% 2/32 [00:00<00:06,  4.44it/s][A[A

  9% 3/32 [00:00<00:06,  4.44it/s][A[A

 12% 4/32 [00:00<00:06,  4.45it/s][A[A

 16% 5/32 [00:01<00:05,  4.63it/s][A[A

 19% 6/32 [00:01<00:05,  4.68it/s][A[A

 22% 7/32 [00:01<00:04,  5.12it/s][A[A

 25% 8/32 [00:01<00:04,  4.95it/s][A[A

 28% 9/32 [00:01<00:04,  5.00it/s][A[A

 31% 10/32 [00:02<00:04,  5.45it/s][A[A

 34% 11/32 [00:02<00:03,  5.69it/s][A[A

 38% 12/32 [00:03<00:09,  2.10it/s][A[A

 41% 13/32 [00:03<00:07,  2.61it/s][A[A

 44% 14/32 [00:03<00:05,  3.16it/s][A[A

 47% 15/32 [00:03<00:04,  3.64it/s][A[A

 50% 16/32 [00:04<00:04,  3.89it/s][A[A

 53% 17/32 [00:04<00:03,  4.14it/s][A[A

 56% 18/32 [00:04<00:03,  4.46it/s][A[A

 59% 19/32 [00:04<00:02,  4.54it/s][A[A

 62% 20/32 [00:04<00:02,  5.12it/s][A[A

 66% 21/32 [00:04<00:02,  5.18it/s][A[A

 69% 22/32 [00:05<00:02,  4.77it/s][A[A

 72% 23/32 [00:05<00:01,  5.14it/s][A[A

 75% 24/32 [00:05<00:01,  4.96it/s][A[A

 78% 25/32 [00:05<00:01,  5.12it/s][A[A

 81% 26/32 [00:06<00:02,  2.02it/s][A[A

 84% 27/32 [00:07<00:02,  2.39it/s][A[A

 88% 28/32 [00:07<00:01,  2.78it/s][A[A

 91% 29/32 [00:07<00:00,  3.10it/s][A[A

 94% 30/32 [00:07<00:00,  3.35it/s][A[A

 97% 31/32 [00:08<00:00,  3.69it/s][A[A

100% 32/32 [00:08<00:00,  3.82it/s][A[A100% 32/32 [00:08<00:00,  3.82it/s]
Meta loss on this task batch = 3.7054e-01, PNorm = 35.6216, GNorm = 0.1565

 42% 8/19 [01:04<01:32,  8.43s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.30it/s][A[A

  6% 2/32 [00:00<00:06,  4.31it/s][A[A

  9% 3/32 [00:00<00:06,  4.29it/s][A[A

 12% 4/32 [00:00<00:06,  4.20it/s][A[A

 16% 5/32 [00:02<00:14,  1.89it/s][A[A

 19% 6/32 [00:02<00:11,  2.29it/s][A[A

 22% 7/32 [00:02<00:09,  2.66it/s][A[A

 25% 8/32 [00:02<00:08,  2.99it/s][A[A

 28% 9/32 [00:03<00:06,  3.37it/s][A[A

 31% 10/32 [00:03<00:06,  3.60it/s][A[A

 34% 11/32 [00:03<00:05,  3.77it/s][A[A

 38% 12/32 [00:03<00:05,  3.92it/s][A[A

 41% 13/32 [00:03<00:04,  4.03it/s][A[A

 44% 14/32 [00:04<00:04,  4.15it/s][A[A

 47% 15/32 [00:04<00:03,  4.33it/s][A[A

 50% 16/32 [00:04<00:03,  4.30it/s][A[A

 53% 17/32 [00:05<00:07,  1.89it/s][A[A

 56% 18/32 [00:06<00:06,  2.31it/s][A[A

 59% 19/32 [00:06<00:04,  2.73it/s][A[A

 62% 20/32 [00:06<00:03,  3.08it/s][A[A

 66% 21/32 [00:06<00:03,  3.45it/s][A[A

 69% 22/32 [00:06<00:02,  3.71it/s][A[A

 72% 23/32 [00:07<00:02,  4.01it/s][A[A

 75% 24/32 [00:07<00:01,  4.12it/s][A[A

 78% 25/32 [00:07<00:01,  4.19it/s][A[A

 81% 26/32 [00:07<00:01,  4.08it/s][A[A

 84% 27/32 [00:08<00:01,  4.27it/s][A[A

 88% 28/32 [00:08<00:00,  4.27it/s][A[A

 91% 29/32 [00:09<00:01,  1.87it/s][A[A

 94% 30/32 [00:09<00:00,  2.26it/s][A[A

 97% 31/32 [00:09<00:00,  2.69it/s][A[A

100% 32/32 [00:10<00:00,  3.05it/s][A[A100% 32/32 [00:10<00:00,  3.13it/s]
Meta loss on this task batch = 2.0881e-01, PNorm = 35.6436, GNorm = 0.1690

 47% 9/19 [01:16<01:32,  9.21s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.18it/s][A[A

  6% 2/32 [00:00<00:07,  4.27it/s][A[A

  9% 3/32 [00:00<00:06,  4.31it/s][A[A

 12% 4/32 [00:00<00:06,  4.28it/s][A[A

 16% 5/32 [00:01<00:06,  4.25it/s][A[A

 19% 6/32 [00:01<00:06,  4.32it/s][A[A

 22% 7/32 [00:01<00:05,  4.29it/s][A[A

 25% 8/32 [00:01<00:05,  4.31it/s][A[A

 28% 9/32 [00:03<00:12,  1.87it/s][A[A

 31% 10/32 [00:03<00:09,  2.24it/s][A[A

 34% 11/32 [00:03<00:08,  2.61it/s][A[A

 38% 12/32 [00:03<00:06,  2.99it/s][A[A

 41% 13/32 [00:04<00:05,  3.27it/s][A[A

 44% 14/32 [00:04<00:05,  3.52it/s][A[A

 47% 15/32 [00:04<00:04,  3.76it/s][A[A

 50% 16/32 [00:04<00:04,  3.96it/s][A[A

 53% 17/32 [00:04<00:03,  3.96it/s][A[A

 56% 18/32 [00:05<00:03,  4.03it/s][A[A

 59% 19/32 [00:06<00:07,  1.82it/s][A[A

 62% 20/32 [00:06<00:05,  2.18it/s][A[A

 66% 21/32 [00:06<00:04,  2.56it/s][A[A

 69% 22/32 [00:07<00:03,  2.95it/s][A[A

 72% 23/32 [00:07<00:02,  3.32it/s][A[A

 75% 24/32 [00:07<00:02,  3.64it/s][A[A

 78% 25/32 [00:07<00:01,  3.87it/s][A[A

 81% 26/32 [00:07<00:01,  4.19it/s][A[A

 84% 27/32 [00:08<00:01,  4.86it/s][A[A

 88% 28/32 [00:08<00:00,  4.78it/s][A[A

 91% 29/32 [00:08<00:00,  4.70it/s][A[A

 94% 30/32 [00:08<00:00,  4.50it/s][A[A

 97% 31/32 [00:09<00:00,  4.39it/s][A[A

100% 32/32 [00:10<00:00,  1.99it/s][A[A100% 32/32 [00:10<00:00,  3.14it/s]
Meta loss on this task batch = 2.7533e-01, PNorm = 35.6671, GNorm = 0.0444

 53% 10/19 [01:27<01:27,  9.75s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  3.93it/s][A[A

  6% 2/32 [00:00<00:07,  4.20it/s][A[A

  9% 3/32 [00:00<00:06,  4.57it/s][A[A

 12% 4/32 [00:00<00:05,  4.78it/s][A[A

 16% 5/32 [00:01<00:05,  4.88it/s][A[A

 19% 6/32 [00:01<00:04,  5.61it/s][A[A

 22% 7/32 [00:01<00:04,  5.04it/s][A[A

 25% 8/32 [00:01<00:05,  4.80it/s][A[A

 28% 9/32 [00:01<00:04,  5.04it/s][A[A

 31% 10/32 [00:03<00:11,  1.96it/s][A[A

 34% 11/32 [00:03<00:09,  2.31it/s][A[A

 38% 12/32 [00:03<00:07,  2.75it/s][A[A

 41% 13/32 [00:03<00:06,  3.08it/s][A[A

 44% 14/32 [00:03<00:04,  3.70it/s][A[A

 47% 15/32 [00:04<00:04,  4.05it/s][A[A

 50% 16/32 [00:04<00:03,  4.23it/s][A[A

 53% 17/32 [00:04<00:03,  4.50it/s][A[A

 56% 18/32 [00:04<00:03,  4.41it/s][A[A

 59% 19/32 [00:04<00:02,  4.34it/s][A[A

 62% 20/32 [00:05<00:02,  4.67it/s][A[A

 66% 21/32 [00:05<00:02,  4.80it/s][A[A

 69% 22/32 [00:05<00:02,  4.64it/s][A[A

 72% 23/32 [00:05<00:02,  4.47it/s][A[A

 75% 24/32 [00:06<00:04,  1.92it/s][A[A

 78% 25/32 [00:07<00:02,  2.40it/s][A[A

 81% 26/32 [00:07<00:02,  2.73it/s][A[A

 84% 27/32 [00:07<00:01,  3.03it/s][A[A

 88% 28/32 [00:07<00:01,  3.44it/s][A[A

 91% 29/32 [00:08<00:00,  3.62it/s][A[A

 94% 30/32 [00:08<00:00,  3.83it/s][A[A

 97% 31/32 [00:08<00:00,  4.23it/s][A[A

100% 32/32 [00:08<00:00,  4.77it/s][A[A100% 32/32 [00:08<00:00,  3.70it/s]
Meta loss on this task batch = 6.9159e-01, PNorm = 35.6812, GNorm = 0.3158

 58% 11/19 [01:36<01:17,  9.65s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.27it/s][A[A

  6% 2/32 [00:00<00:05,  5.37it/s][A[A

  9% 3/32 [00:00<00:05,  5.21it/s][A[A

 12% 4/32 [00:00<00:05,  4.93it/s][A[A

 16% 5/32 [00:02<00:13,  1.94it/s][A[A

 19% 6/32 [00:02<00:11,  2.35it/s][A[A

 22% 7/32 [00:02<00:09,  2.69it/s][A[A

 25% 8/32 [00:02<00:07,  3.01it/s][A[A

 28% 9/32 [00:02<00:07,  3.28it/s][A[A

 31% 10/32 [00:03<00:05,  3.72it/s][A[A

 34% 11/32 [00:03<00:05,  3.87it/s][A[A

 38% 12/32 [00:03<00:04,  4.12it/s][A[A

 41% 13/32 [00:03<00:04,  4.12it/s][A[A

 44% 14/32 [00:04<00:04,  4.33it/s][A[A

 47% 15/32 [00:04<00:03,  4.30it/s][A[A

 50% 16/32 [00:04<00:03,  4.35it/s][A[A

 53% 17/32 [00:04<00:03,  4.29it/s][A[A

 56% 18/32 [00:05<00:03,  4.20it/s][A[A

 59% 19/32 [00:05<00:02,  4.71it/s][A[A

 62% 20/32 [00:05<00:02,  4.90it/s][A[A

 66% 21/32 [00:06<00:05,  1.97it/s][A[A

 69% 22/32 [00:06<00:04,  2.35it/s][A[A

 72% 23/32 [00:07<00:03,  2.70it/s][A[A

 75% 24/32 [00:07<00:02,  3.12it/s][A[A

 78% 25/32 [00:07<00:02,  3.48it/s][A[A

 81% 26/32 [00:07<00:01,  3.73it/s][A[A

 84% 27/32 [00:07<00:01,  4.00it/s][A[A

 88% 28/32 [00:08<00:00,  4.27it/s][A[A

 91% 29/32 [00:08<00:00,  4.51it/s][A[A

 94% 30/32 [00:08<00:00,  5.11it/s][A[A

 97% 31/32 [00:08<00:00,  5.06it/s][A[A

100% 32/32 [00:08<00:00,  4.80it/s][A[A100% 32/32 [00:08<00:00,  3.62it/s]
Meta loss on this task batch = 5.4876e-01, PNorm = 35.6942, GNorm = 0.0647

 63% 12/19 [01:46<01:07,  9.65s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.95it/s][A[A

  6% 2/32 [00:00<00:06,  4.71it/s][A[A

  9% 3/32 [00:01<00:14,  1.94it/s][A[A

 12% 4/32 [00:01<00:12,  2.32it/s][A[A

 16% 5/32 [00:02<00:09,  2.84it/s][A[A

 19% 6/32 [00:02<00:08,  3.11it/s][A[A

 22% 7/32 [00:02<00:07,  3.53it/s][A[A

 25% 8/32 [00:02<00:05,  4.29it/s][A[A

 28% 9/32 [00:02<00:05,  4.55it/s][A[A

 31% 10/32 [00:03<00:05,  4.39it/s][A[A

 34% 11/32 [00:03<00:04,  4.55it/s][A[A

 38% 12/32 [00:03<00:04,  4.41it/s][A[A

 41% 13/32 [00:03<00:04,  4.44it/s][A[A

 44% 14/32 [00:03<00:04,  4.29it/s][A[A

 47% 15/32 [00:04<00:03,  4.47it/s][A[A

 50% 16/32 [00:04<00:03,  4.61it/s][A[A

 53% 17/32 [00:04<00:02,  5.03it/s][A[A

 56% 18/32 [00:05<00:07,  1.96it/s][A[A

 59% 19/32 [00:05<00:05,  2.49it/s][A[A

 62% 20/32 [00:06<00:04,  2.82it/s][A[A

 66% 21/32 [00:06<00:03,  3.17it/s][A[A

 69% 22/32 [00:06<00:02,  3.55it/s][A[A

 72% 23/32 [00:06<00:02,  4.08it/s][A[A

 75% 24/32 [00:06<00:01,  4.28it/s][A[A

 78% 25/32 [00:07<00:01,  4.54it/s][A[A

 81% 26/32 [00:07<00:01,  4.42it/s][A[A

 84% 27/32 [00:07<00:01,  4.79it/s][A[A

 88% 28/32 [00:07<00:00,  4.55it/s][A[A

 91% 29/32 [00:08<00:00,  4.66it/s][A[A

 94% 30/32 [00:08<00:00,  4.79it/s][A[A

 97% 31/32 [00:08<00:00,  4.84it/s][A[A

100% 32/32 [00:08<00:00,  4.93it/s][A[A100% 32/32 [00:08<00:00,  3.72it/s]
Meta loss on this task batch = 5.9153e-01, PNorm = 35.7042, GNorm = 0.0996

 68% 13/19 [01:55<00:57,  9.56s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:03,  7.77it/s][A[A

  6% 2/32 [00:00<00:03,  7.57it/s][A[A

  9% 3/32 [00:01<00:13,  2.13it/s][A[A

 12% 4/32 [00:01<00:11,  2.49it/s][A[A

 16% 5/32 [00:01<00:09,  2.93it/s][A[A

 19% 6/32 [00:02<00:07,  3.52it/s][A[A

 22% 7/32 [00:02<00:06,  3.80it/s][A[A

 25% 8/32 [00:02<00:05,  4.34it/s][A[A

 28% 9/32 [00:02<00:05,  4.25it/s][A[A

 31% 10/32 [00:02<00:05,  4.24it/s][A[A

 34% 11/32 [00:03<00:04,  4.70it/s][A[A

 38% 12/32 [00:03<00:04,  4.50it/s][A[A

 41% 13/32 [00:03<00:04,  4.65it/s][A[A

 44% 14/32 [00:03<00:03,  4.71it/s][A[A

 47% 15/32 [00:03<00:03,  4.70it/s][A[A

 50% 16/32 [00:04<00:03,  5.05it/s][A[A

 53% 17/32 [00:04<00:03,  4.79it/s][A[A

 56% 18/32 [00:05<00:07,  1.94it/s][A[A

 59% 19/32 [00:05<00:05,  2.34it/s][A[A

 62% 20/32 [00:06<00:04,  2.68it/s][A[A

 66% 21/32 [00:06<00:03,  3.10it/s][A[A

 69% 22/32 [00:06<00:02,  3.60it/s][A[A

 72% 23/32 [00:06<00:02,  3.83it/s][A[A

 75% 24/32 [00:06<00:01,  4.10it/s][A[A

 78% 25/32 [00:07<00:01,  4.15it/s][A[A

 81% 26/32 [00:07<00:01,  4.45it/s][A[A

 84% 27/32 [00:07<00:01,  4.92it/s][A[A

 88% 28/32 [00:07<00:00,  4.95it/s][A[A

 91% 29/32 [00:07<00:00,  4.73it/s][A[A

 94% 30/32 [00:08<00:00,  4.69it/s][A[A

 97% 31/32 [00:09<00:00,  1.94it/s][A[A

100% 32/32 [00:09<00:00,  2.36it/s][A[A100% 32/32 [00:09<00:00,  3.35it/s]
Meta loss on this task batch = 5.8318e-01, PNorm = 35.7092, GNorm = 0.2435

 74% 14/19 [02:05<00:48,  9.78s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.71it/s][A[A

  6% 2/32 [00:00<00:05,  5.53it/s][A[A

  9% 3/32 [00:00<00:05,  5.15it/s][A[A

 12% 4/32 [00:00<00:05,  5.33it/s][A[A

 16% 5/32 [00:00<00:05,  5.32it/s][A[A

 19% 6/32 [00:01<00:05,  4.97it/s][A[A

 22% 7/32 [00:01<00:05,  4.47it/s][A[A

 25% 8/32 [00:01<00:05,  4.75it/s][A[A

 28% 9/32 [00:02<00:11,  1.92it/s][A[A

 31% 10/32 [00:03<00:09,  2.39it/s][A[A

 34% 11/32 [00:03<00:07,  2.88it/s][A[A

 38% 12/32 [00:03<00:05,  3.42it/s][A[A

 41% 13/32 [00:03<00:05,  3.72it/s][A[A

 44% 14/32 [00:03<00:04,  3.86it/s][A[A

 47% 15/32 [00:04<00:03,  4.25it/s][A[A

 50% 16/32 [00:04<00:03,  4.38it/s][A[A

 53% 17/32 [00:04<00:03,  4.80it/s][A[A

 56% 18/32 [00:04<00:02,  4.97it/s][A[A

 59% 19/32 [00:04<00:02,  5.38it/s][A[A

 62% 20/32 [00:04<00:02,  5.10it/s][A[A

 66% 21/32 [00:05<00:02,  5.24it/s][A[A

 69% 22/32 [00:05<00:01,  5.30it/s][A[A

 72% 23/32 [00:06<00:04,  1.95it/s][A[A

 75% 24/32 [00:06<00:03,  2.34it/s][A[A

 78% 25/32 [00:07<00:02,  2.85it/s][A[A

 81% 26/32 [00:07<00:01,  3.08it/s][A[A

 84% 27/32 [00:07<00:01,  3.39it/s][A[A

 88% 28/32 [00:07<00:01,  3.77it/s][A[A

 91% 29/32 [00:07<00:00,  3.86it/s][A[A

 94% 30/32 [00:08<00:00,  3.93it/s][A[A

 97% 31/32 [00:08<00:00,  4.69it/s][A[A

100% 32/32 [00:08<00:00,  4.49it/s][A[A100% 32/32 [00:08<00:00,  3.75it/s]
Meta loss on this task batch = 5.2450e-01, PNorm = 35.7081, GNorm = 0.2373

 79% 15/19 [02:14<00:38,  9.63s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:37,  1.22s/it][A[A

  6% 2/32 [00:01<00:26,  1.12it/s][A[A

  9% 3/32 [00:01<00:19,  1.51it/s][A[A

 12% 4/32 [00:01<00:14,  1.98it/s][A[A

 16% 5/32 [00:01<00:11,  2.43it/s][A[A

 19% 6/32 [00:02<00:09,  2.80it/s][A[A

 22% 7/32 [00:02<00:07,  3.38it/s][A[A

 25% 8/32 [00:02<00:06,  3.81it/s][A[A

 28% 9/32 [00:02<00:05,  4.48it/s][A[A

 31% 10/32 [00:02<00:05,  4.39it/s][A[A

 34% 11/32 [00:02<00:04,  4.26it/s][A[A

 38% 12/32 [00:03<00:04,  4.43it/s][A[A

 41% 13/32 [00:03<00:04,  4.66it/s][A[A

 44% 14/32 [00:03<00:03,  4.93it/s][A[A

 47% 15/32 [00:03<00:03,  4.83it/s][A[A

 50% 16/32 [00:03<00:03,  5.23it/s][A[A

 53% 17/32 [00:04<00:03,  4.83it/s][A[A

 56% 18/32 [00:04<00:02,  5.20it/s][A[A

 59% 19/32 [00:05<00:06,  2.01it/s][A[A

 62% 20/32 [00:05<00:04,  2.42it/s][A[A

 66% 21/32 [00:05<00:03,  2.87it/s][A[A

 69% 22/32 [00:06<00:03,  3.30it/s][A[A

 72% 23/32 [00:06<00:02,  3.54it/s][A[A

 75% 24/32 [00:06<00:02,  3.71it/s][A[A

 78% 25/32 [00:06<00:01,  4.50it/s][A[A

 81% 26/32 [00:06<00:01,  4.95it/s][A[A

 84% 27/32 [00:07<00:01,  4.65it/s][A[A

 88% 28/32 [00:07<00:00,  4.64it/s][A[A

 91% 29/32 [00:07<00:00,  4.66it/s][A[A

 94% 30/32 [00:08<00:01,  1.89it/s][A[A

 97% 31/32 [00:09<00:00,  2.25it/s][A[A

100% 32/32 [00:09<00:00,  2.69it/s][A[A100% 32/32 [00:09<00:00,  3.45it/s]
Meta loss on this task batch = 5.6954e-01, PNorm = 35.7035, GNorm = 0.1781

 84% 16/19 [02:24<00:29,  9.74s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.80it/s][A[A

  6% 2/32 [00:00<00:07,  3.88it/s][A[A

  9% 3/32 [00:00<00:06,  4.21it/s][A[A

 12% 4/32 [00:00<00:06,  4.40it/s][A[A

 16% 5/32 [00:01<00:05,  4.57it/s][A[A

 19% 6/32 [00:01<00:05,  5.17it/s][A[A

 22% 7/32 [00:01<00:04,  5.45it/s][A[A

 25% 8/32 [00:01<00:04,  5.01it/s][A[A

 28% 9/32 [00:02<00:11,  1.96it/s][A[A

 31% 10/32 [00:02<00:08,  2.57it/s][A[A

 34% 11/32 [00:03<00:06,  3.03it/s][A[A

 38% 12/32 [00:03<00:05,  3.45it/s][A[A

 41% 13/32 [00:03<00:04,  3.98it/s][A[A

 44% 14/32 [00:03<00:04,  4.38it/s][A[A

 47% 15/32 [00:03<00:03,  4.83it/s][A[A

 50% 16/32 [00:04<00:03,  5.16it/s][A[A

 53% 17/32 [00:04<00:02,  5.20it/s][A[A

 56% 18/32 [00:04<00:02,  4.99it/s][A[A

 59% 19/32 [00:04<00:02,  4.85it/s][A[A

 62% 20/32 [00:04<00:02,  4.68it/s][A[A

 66% 21/32 [00:05<00:02,  4.82it/s][A[A

 69% 22/32 [00:05<00:02,  4.56it/s][A[A

 72% 23/32 [00:05<00:01,  4.96it/s][A[A

 75% 24/32 [00:06<00:04,  1.89it/s][A[A

 78% 25/32 [00:06<00:03,  2.28it/s][A[A

 81% 26/32 [00:07<00:02,  2.61it/s][A[A

 84% 27/32 [00:07<00:01,  3.11it/s][A[A

 88% 28/32 [00:07<00:01,  3.54it/s][A[A

 91% 29/32 [00:07<00:00,  3.71it/s][A[A

 94% 30/32 [00:08<00:00,  3.79it/s][A[A

 97% 31/32 [00:08<00:00,  4.15it/s][A[A

100% 32/32 [00:08<00:00,  4.39it/s][A[A100% 32/32 [00:08<00:00,  3.77it/s]
Meta loss on this task batch = 4.6764e-01, PNorm = 35.6986, GNorm = 0.0730

 89% 17/19 [02:34<00:19,  9.59s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.05it/s][A[A

  6% 2/32 [00:00<00:06,  4.90it/s][A[A

  9% 3/32 [00:00<00:06,  4.75it/s][A[A

 12% 4/32 [00:00<00:05,  4.68it/s][A[A

 16% 5/32 [00:02<00:13,  1.94it/s][A[A

 19% 6/32 [00:02<00:10,  2.36it/s][A[A

 22% 7/32 [00:02<00:09,  2.69it/s][A[A

 25% 8/32 [00:02<00:08,  2.97it/s][A[A

 28% 9/32 [00:03<00:06,  3.35it/s][A[A

 31% 10/32 [00:03<00:06,  3.64it/s][A[A

 34% 11/32 [00:03<00:05,  4.14it/s][A[A

 38% 12/32 [00:03<00:04,  4.47it/s][A[A

 41% 13/32 [00:03<00:04,  4.66it/s][A[A

 44% 14/32 [00:03<00:03,  5.06it/s][A[A

 47% 15/32 [00:04<00:03,  4.67it/s][A[A

 50% 16/32 [00:04<00:03,  4.78it/s][A[A

 53% 17/32 [00:04<00:02,  5.34it/s][A[A

 56% 18/32 [00:04<00:02,  5.32it/s][A[A

 59% 19/32 [00:04<00:02,  5.02it/s][A[A

 62% 20/32 [00:06<00:06,  1.96it/s][A[A

 66% 21/32 [00:06<00:04,  2.34it/s][A[A

 69% 22/32 [00:06<00:03,  2.92it/s][A[A

 72% 23/32 [00:06<00:02,  3.49it/s][A[A

 75% 24/32 [00:06<00:02,  3.61it/s][A[A

 78% 25/32 [00:07<00:01,  4.18it/s][A[A

 81% 26/32 [00:07<00:01,  4.30it/s][A[A

 84% 27/32 [00:07<00:01,  4.16it/s][A[A

 88% 28/32 [00:07<00:00,  4.31it/s][A[A

 91% 29/32 [00:07<00:00,  4.59it/s][A[A

 94% 30/32 [00:08<00:00,  4.79it/s][A[A

 97% 31/32 [00:08<00:00,  4.99it/s][A[A

100% 32/32 [00:08<00:00,  4.71it/s][A[A100% 32/32 [00:08<00:00,  3.73it/s]
Meta loss on this task batch = 5.0657e-01, PNorm = 35.6941, GNorm = 0.0449

 95% 18/19 [02:43<00:09,  9.51s/it][A

  0% 0/23 [00:00<?, ?it/s][A[A

  4% 1/23 [00:00<00:05,  4.07it/s][A[A

  9% 2/23 [00:00<00:04,  4.47it/s][A[A

 13% 3/23 [00:01<00:10,  1.93it/s][A[A

 17% 4/23 [00:01<00:07,  2.50it/s][A[A

 22% 5/23 [00:01<00:05,  3.08it/s][A[A

 26% 6/23 [00:02<00:04,  3.43it/s][A[A

 30% 7/23 [00:02<00:04,  3.85it/s][A[A

 35% 8/23 [00:02<00:03,  3.96it/s][A[A

 39% 9/23 [00:02<00:03,  4.61it/s][A[A

 43% 10/23 [00:02<00:02,  4.37it/s][A[A

 48% 11/23 [00:03<00:02,  4.77it/s][A[A

 52% 12/23 [00:03<00:02,  4.67it/s][A[A

 57% 13/23 [00:03<00:01,  5.21it/s][A[A

 61% 14/23 [00:03<00:01,  5.27it/s][A[A

 65% 15/23 [00:04<00:02,  3.99it/s][A[A

 70% 16/23 [00:04<00:01,  3.97it/s][A[A

 74% 17/23 [00:05<00:03,  1.89it/s][A[A

 78% 18/23 [00:05<00:02,  2.32it/s][A[A

 83% 19/23 [00:05<00:01,  2.71it/s][A[A

 87% 20/23 [00:06<00:00,  3.37it/s][A[A

 91% 21/23 [00:06<00:00,  3.87it/s][A[A

 96% 22/23 [00:06<00:00,  4.55it/s][A[A

100% 23/23 [00:06<00:00,  5.03it/s][A[A100% 23/23 [00:06<00:00,  3.56it/s]
Meta loss on this task batch = 4.3205e-01, PNorm = 35.6916, GNorm = 0.0391

100% 19/19 [02:50<00:00,  8.75s/it][A100% 19/19 [02:50<00:00,  8.98s/it]
Took 170.54958367347717 seconds to complete one epoch of meta training

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/20 [00:00<?, ?it/s][A[A


  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 31.37it/s]


  5% 1/20 [00:00<00:02,  8.16it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 20.17it/s]


 10% 2/20 [00:00<00:02,  6.13it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 18.55it/s][A[A[A100% 3/3 [00:00<00:00, 20.15it/s]


 15% 3/20 [00:00<00:03,  5.09it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 14.96it/s]


 20% 4/20 [00:00<00:03,  5.00it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 15.07it/s][A[A[A100% 4/4 [00:00<00:00, 17.44it/s]


 25% 5/20 [00:01<00:03,  3.85it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.16it/s][A[A[A


 75% 3/4 [00:01<00:00,  2.63it/s][A[A[A100% 4/4 [00:01<00:00,  3.15it/s]


 30% 6/20 [00:02<00:08,  1.65it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.55it/s][A[A[A100% 4/4 [00:00<00:00, 20.35it/s]


 35% 7/20 [00:03<00:06,  1.88it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.65it/s][A[A[A


100% 4/4 [00:00<00:00, 19.27it/s][A[A[A100% 4/4 [00:00<00:00, 18.99it/s]


 40% 8/20 [00:03<00:05,  2.06it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.48it/s][A[A[A100% 4/4 [00:00<00:00, 23.06it/s]


 45% 9/20 [00:03<00:04,  2.27it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 18.70it/s][A[A[A


100% 4/4 [00:00<00:00, 18.63it/s][A[A[A100% 4/4 [00:00<00:00, 18.55it/s]


 50% 10/20 [00:04<00:04,  2.38it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 16.31it/s][A[A[A100% 4/4 [00:00<00:00, 18.51it/s]


 55% 11/20 [00:04<00:03,  2.45it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 17.87it/s][A[A[A100% 4/4 [00:00<00:00, 21.21it/s]


 60% 12/20 [00:04<00:03,  2.54it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.40it/s][A[A[A


100% 3/3 [00:01<00:00,  2.68it/s][A[A[A100% 3/3 [00:01<00:00,  2.46it/s]


 65% 13/20 [00:06<00:04,  1.45it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.06it/s][A[A[A100% 3/3 [00:00<00:00, 13.78it/s]


 70% 14/20 [00:06<00:03,  1.69it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.90it/s][A[A[A100% 4/4 [00:00<00:00, 18.08it/s]


 75% 15/20 [00:07<00:02,  1.88it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.21it/s][A[A[A100% 3/3 [00:00<00:00, 17.42it/s]


 80% 16/20 [00:07<00:01,  2.10it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 11.86it/s][A[A[A100% 3/3 [00:00<00:00, 14.42it/s]


 85% 17/20 [00:07<00:01,  2.19it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 22.48it/s]


 90% 18/20 [00:08<00:00,  2.47it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 15.59it/s][A[A[A100% 3/3 [00:00<00:00, 19.88it/s]


 95% 19/20 [00:08<00:00,  2.66it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 12.68it/s]


100% 20/20 [00:08<00:00,  2.98it/s][A[A100% 20/20 [00:08<00:00,  2.33it/s]

100% 1/1 [00:08<00:00,  8.60s/it][A100% 1/1 [00:08<00:00,  8.60s/it]
Took 179.14582419395447 seconds to complete one epoch of meta training and validating
Meta validation score auc = 0.600045
Found better MAML checkpoint after meta validation, saving now
 20% 6/30 [19:15<1:16:33, 191.40s/it]Epoch 6

  0% 0/19 [00:00<?, ?it/s][A

  0% 0/32 [00:00<?, ?it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  3% 1/32 [00:00<00:06,  4.60it/s][A[A

  6% 2/32 [00:00<00:07,  4.28it/s][A[A

  9% 3/32 [00:00<00:06,  4.34it/s][A[A

 12% 4/32 [00:00<00:06,  4.58it/s][A[A

 16% 5/32 [00:01<00:05,  4.79it/s][A[A

 19% 6/32 [00:01<00:05,  4.91it/s][A[A

 22% 7/32 [00:01<00:05,  4.98it/s][A[A

 25% 8/32 [00:01<00:04,  4.95it/s][A[A

 28% 9/32 [00:01<00:04,  5.06it/s][A[A

 31% 10/32 [00:02<00:04,  5.20it/s][A[A

 34% 11/32 [00:02<00:04,  4.68it/s][A[A

 38% 12/32 [00:02<00:04,  4.29it/s][A[A

 41% 13/32 [00:03<00:10,  1.81it/s][A[A

 44% 14/32 [00:04<00:08,  2.16it/s][A[A

 47% 15/32 [00:04<00:06,  2.66it/s][A[A

 50% 16/32 [00:04<00:05,  3.06it/s][A[A

 53% 17/32 [00:04<00:04,  3.38it/s][A[A

 56% 18/32 [00:04<00:03,  3.85it/s][A[A

 59% 19/32 [00:05<00:03,  4.13it/s][A[A

 62% 20/32 [00:05<00:02,  4.19it/s][A[A

 66% 21/32 [00:05<00:02,  4.53it/s][A[A

 69% 22/32 [00:05<00:02,  4.60it/s][A[A

 72% 23/32 [00:05<00:01,  4.77it/s][A[A

 75% 24/32 [00:06<00:01,  4.71it/s][A[A

 78% 25/32 [00:06<00:01,  4.85it/s][A[A

 81% 26/32 [00:07<00:03,  1.95it/s][A[A

 84% 27/32 [00:07<00:02,  2.37it/s][A[A

 88% 28/32 [00:07<00:01,  2.88it/s][A[A

 91% 29/32 [00:08<00:00,  3.19it/s][A[A

 94% 30/32 [00:08<00:00,  3.76it/s][A[A

 97% 31/32 [00:08<00:00,  3.90it/s][A[A

100% 32/32 [00:08<00:00,  4.14it/s][A[A100% 32/32 [00:08<00:00,  3.64it/s]
Meta loss on this task batch = 5.2803e-01, PNorm = 35.6917, GNorm = 0.0565

  5% 1/19 [00:09<02:52,  9.58s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.48it/s][A[A

  6% 2/32 [00:00<00:05,  5.17it/s][A[A

  9% 3/32 [00:00<00:06,  4.65it/s][A[A

 12% 4/32 [00:00<00:06,  4.57it/s][A[A

 16% 5/32 [00:01<00:05,  4.82it/s][A[A

 19% 6/32 [00:02<00:13,  1.96it/s][A[A

 22% 7/32 [00:02<00:10,  2.43it/s][A[A

 25% 8/32 [00:02<00:08,  2.96it/s][A[A

 28% 9/32 [00:02<00:06,  3.37it/s][A[A

 31% 10/32 [00:03<00:05,  3.86it/s][A[A

 34% 11/32 [00:03<00:04,  4.21it/s][A[A

 38% 12/32 [00:03<00:04,  4.07it/s][A[A

 41% 13/32 [00:03<00:04,  4.47it/s][A[A

 44% 14/32 [00:03<00:04,  4.45it/s][A[A

 47% 15/32 [00:04<00:03,  4.53it/s][A[A

 50% 16/32 [00:04<00:03,  4.94it/s][A[A

 53% 17/32 [00:04<00:03,  4.82it/s][A[A

 56% 18/32 [00:04<00:03,  4.58it/s][A[A

 59% 19/32 [00:04<00:02,  4.67it/s][A[A

 62% 20/32 [00:06<00:06,  1.91it/s][A[A

 66% 21/32 [00:06<00:04,  2.31it/s][A[A

 69% 22/32 [00:06<00:03,  2.77it/s][A[A

 72% 23/32 [00:06<00:02,  3.28it/s][A[A

 75% 24/32 [00:06<00:02,  3.78it/s][A[A

 78% 25/32 [00:07<00:01,  4.13it/s][A[A

 81% 26/32 [00:07<00:01,  4.23it/s][A[A

 84% 27/32 [00:07<00:01,  4.32it/s][A[A

 88% 28/32 [00:07<00:00,  4.24it/s][A[A

 91% 29/32 [00:07<00:00,  4.61it/s][A[A

 94% 30/32 [00:08<00:00,  4.62it/s][A[A

 97% 31/32 [00:08<00:00,  4.71it/s][A[A

100% 32/32 [00:08<00:00,  5.03it/s][A[A100% 32/32 [00:08<00:00,  3.75it/s]
Meta loss on this task batch = 5.1364e-01, PNorm = 35.6949, GNorm = 0.0798

 11% 2/19 [00:18<02:41,  9.50s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.54it/s][A[A

  6% 2/32 [00:01<00:14,  2.09it/s][A[A

  9% 3/32 [00:01<00:11,  2.44it/s][A[A

 12% 4/32 [00:01<00:10,  2.72it/s][A[A

 16% 5/32 [00:02<00:08,  3.11it/s][A[A

 19% 6/32 [00:02<00:07,  3.39it/s][A[A

 22% 7/32 [00:02<00:06,  3.92it/s][A[A

 25% 8/32 [00:02<00:05,  4.35it/s][A[A

 28% 9/32 [00:02<00:05,  4.48it/s][A[A

 31% 10/32 [00:03<00:04,  4.46it/s][A[A

 34% 11/32 [00:03<00:04,  4.38it/s][A[A

 38% 12/32 [00:03<00:04,  4.53it/s][A[A

 41% 13/32 [00:03<00:04,  4.61it/s][A[A

 44% 14/32 [00:03<00:03,  4.57it/s][A[A

 47% 15/32 [00:05<00:08,  1.98it/s][A[A

 50% 16/32 [00:05<00:06,  2.42it/s][A[A

 53% 17/32 [00:05<00:05,  2.78it/s][A[A

 56% 18/32 [00:05<00:04,  3.32it/s][A[A

 59% 19/32 [00:05<00:03,  3.82it/s][A[A

 62% 20/32 [00:06<00:02,  4.32it/s][A[A

 66% 21/32 [00:06<00:02,  4.55it/s][A[A

 69% 22/32 [00:06<00:02,  4.71it/s][A[A

 72% 23/32 [00:06<00:01,  4.50it/s][A[A

 75% 24/32 [00:06<00:01,  4.54it/s][A[A

 78% 25/32 [00:07<00:01,  4.73it/s][A[A

 81% 26/32 [00:07<00:01,  4.97it/s][A[A

 84% 27/32 [00:08<00:02,  1.94it/s][A[A

 88% 28/32 [00:08<00:01,  2.35it/s][A[A

 91% 29/32 [00:08<00:01,  2.75it/s][A[A

 94% 30/32 [00:09<00:00,  3.03it/s][A[A

 97% 31/32 [00:09<00:00,  3.56it/s][A[A

100% 32/32 [00:09<00:00,  3.83it/s][A[A100% 32/32 [00:09<00:00,  3.33it/s]
Meta loss on this task batch = 5.1269e-01, PNorm = 35.7025, GNorm = 0.1349

 16% 3/19 [00:29<02:36,  9.76s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.41it/s][A[A

  6% 2/32 [00:00<00:06,  4.53it/s][A[A

  9% 3/32 [00:00<00:06,  4.66it/s][A[A

 12% 4/32 [00:00<00:06,  4.47it/s][A[A

 16% 5/32 [00:01<00:06,  4.43it/s][A[A

 19% 6/32 [00:01<00:05,  4.74it/s][A[A

 22% 7/32 [00:02<00:12,  1.96it/s][A[A

 25% 8/32 [00:02<00:10,  2.32it/s][A[A

 28% 9/32 [00:02<00:08,  2.82it/s][A[A

 31% 10/32 [00:03<00:06,  3.15it/s][A[A

 34% 11/32 [00:03<00:05,  3.66it/s][A[A

 38% 12/32 [00:03<00:04,  4.06it/s][A[A

 41% 13/32 [00:03<00:04,  4.16it/s][A[A

 44% 14/32 [00:03<00:04,  4.28it/s][A[A

 47% 15/32 [00:04<00:03,  4.64it/s][A[A

 50% 16/32 [00:04<00:03,  4.61it/s][A[A

 53% 17/32 [00:04<00:03,  4.73it/s][A[A

 56% 18/32 [00:04<00:02,  4.71it/s][A[A

 59% 19/32 [00:04<00:02,  5.21it/s][A[A

 62% 20/32 [00:06<00:06,  2.00it/s][A[A

 66% 21/32 [00:06<00:04,  2.48it/s][A[A

 69% 22/32 [00:06<00:03,  2.84it/s][A[A

 72% 23/32 [00:06<00:02,  3.14it/s][A[A

 75% 24/32 [00:06<00:02,  3.50it/s][A[A

 78% 25/32 [00:07<00:01,  4.00it/s][A[A

 81% 26/32 [00:07<00:01,  4.31it/s][A[A

 84% 27/32 [00:07<00:01,  4.39it/s][A[A

 88% 28/32 [00:07<00:00,  4.56it/s][A[A

 91% 29/32 [00:07<00:00,  4.90it/s][A[A

 94% 30/32 [00:08<00:00,  4.66it/s][A[A

 97% 31/32 [00:08<00:00,  4.72it/s][A[A

100% 32/32 [00:08<00:00,  4.90it/s][A[A100% 32/32 [00:08<00:00,  3.74it/s]
Meta loss on this task batch = 5.8000e-01, PNorm = 35.7112, GNorm = 0.0928

 21% 4/19 [00:38<02:24,  9.63s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.67it/s][A[A

  6% 2/32 [00:01<00:15,  1.96it/s][A[A

  9% 3/32 [00:01<00:12,  2.35it/s][A[A

 12% 4/32 [00:01<00:10,  2.67it/s][A[A

 16% 5/32 [00:02<00:08,  3.07it/s][A[A

 19% 6/32 [00:02<00:07,  3.47it/s][A[A

 22% 7/32 [00:02<00:06,  3.71it/s][A[A

 25% 8/32 [00:02<00:05,  4.17it/s][A[A

 28% 9/32 [00:02<00:05,  4.34it/s][A[A

 31% 10/32 [00:03<00:04,  4.52it/s][A[A

 34% 11/32 [00:03<00:04,  4.60it/s][A[A

 38% 12/32 [00:03<00:04,  4.53it/s][A[A

 41% 13/32 [00:03<00:04,  4.44it/s][A[A

 44% 14/32 [00:05<00:09,  1.92it/s][A[A

 47% 15/32 [00:05<00:07,  2.32it/s][A[A

 50% 16/32 [00:05<00:05,  2.85it/s][A[A

 53% 17/32 [00:05<00:04,  3.09it/s][A[A

 56% 18/32 [00:05<00:04,  3.45it/s][A[A

 59% 19/32 [00:06<00:03,  3.63it/s][A[A

 62% 20/32 [00:06<00:03,  3.75it/s][A[A

 66% 21/32 [00:06<00:02,  3.89it/s][A[A

 69% 22/32 [00:06<00:02,  4.33it/s][A[A

 72% 23/32 [00:06<00:02,  4.40it/s][A[A

 75% 24/32 [00:07<00:01,  4.28it/s][A[A

 78% 25/32 [00:08<00:03,  1.87it/s][A[A

 81% 26/32 [00:08<00:02,  2.31it/s][A[A

 84% 27/32 [00:08<00:01,  2.76it/s][A[A

 88% 28/32 [00:09<00:01,  3.32it/s][A[A

 91% 29/32 [00:09<00:00,  3.55it/s][A[A

 94% 30/32 [00:09<00:00,  3.82it/s][A[A

 97% 31/32 [00:09<00:00,  4.13it/s][A[A

100% 32/32 [00:09<00:00,  4.23it/s][A[A100% 32/32 [00:09<00:00,  3.24it/s]
Meta loss on this task batch = 5.2872e-01, PNorm = 35.7221, GNorm = 0.1431

 26% 5/19 [00:49<02:19,  9.95s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.03it/s][A[A

  6% 2/32 [00:00<00:05,  5.04it/s][A[A

  9% 3/32 [00:00<00:05,  5.35it/s][A[A

 12% 4/32 [00:00<00:05,  5.00it/s][A[A

 16% 5/32 [00:02<00:14,  1.90it/s][A[A

 19% 6/32 [00:02<00:11,  2.27it/s][A[A

 22% 7/32 [00:02<00:09,  2.71it/s][A[A

 25% 8/32 [00:02<00:07,  3.10it/s][A[A

 28% 9/32 [00:03<00:07,  3.26it/s][A[A

 31% 10/32 [00:03<00:06,  3.46it/s][A[A

 34% 11/32 [00:03<00:05,  3.73it/s][A[A

 38% 12/32 [00:03<00:04,  4.02it/s][A[A

 41% 13/32 [00:03<00:04,  4.27it/s][A[A

 44% 14/32 [00:05<00:09,  1.86it/s][A[A

 47% 15/32 [00:05<00:07,  2.27it/s][A[A

 50% 16/32 [00:05<00:06,  2.65it/s][A[A

 53% 17/32 [00:05<00:04,  3.10it/s][A[A

 56% 18/32 [00:05<00:04,  3.43it/s][A[A

 59% 19/32 [00:06<00:03,  3.74it/s][A[A

 62% 20/32 [00:06<00:02,  4.05it/s][A[A

 66% 21/32 [00:06<00:02,  4.27it/s][A[A

 69% 22/32 [00:06<00:02,  4.43it/s][A[A

 72% 23/32 [00:07<00:02,  4.44it/s][A[A

 75% 24/32 [00:07<00:01,  4.48it/s][A[A

 78% 25/32 [00:08<00:03,  1.89it/s][A[A

 81% 26/32 [00:08<00:02,  2.35it/s][A[A

 84% 27/32 [00:08<00:01,  2.71it/s][A[A

 88% 28/32 [00:09<00:01,  2.90it/s][A[A

 91% 29/32 [00:09<00:00,  3.18it/s][A[A

 94% 30/32 [00:09<00:00,  3.56it/s][A[A

 97% 31/32 [00:09<00:00,  3.90it/s][A[A

100% 32/32 [00:10<00:00,  4.20it/s][A[A100% 32/32 [00:10<00:00,  3.19it/s]
Meta loss on this task batch = 4.6903e-01, PNorm = 35.7352, GNorm = 0.1659

 32% 6/19 [01:00<02:12, 10.22s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.07it/s][A[A

  6% 2/32 [00:00<00:05,  5.05it/s][A[A

  9% 3/32 [00:01<00:14,  1.97it/s][A[A

 12% 4/32 [00:01<00:12,  2.33it/s][A[A

 16% 5/32 [00:02<00:09,  2.77it/s][A[A

 19% 6/32 [00:02<00:08,  3.19it/s][A[A

 22% 7/32 [00:02<00:07,  3.48it/s][A[A

 25% 8/32 [00:02<00:06,  3.80it/s][A[A

 28% 9/32 [00:02<00:05,  4.06it/s][A[A

 31% 10/32 [00:03<00:05,  4.16it/s][A[A

 34% 11/32 [00:03<00:04,  4.25it/s][A[A

 38% 12/32 [00:03<00:04,  4.31it/s][A[A

 41% 13/32 [00:03<00:04,  4.25it/s][A[A

 44% 14/32 [00:05<00:09,  1.90it/s][A[A

 47% 15/32 [00:05<00:07,  2.29it/s][A[A

 50% 16/32 [00:05<00:06,  2.65it/s][A[A

 53% 17/32 [00:05<00:05,  3.00it/s][A[A

 56% 18/32 [00:05<00:04,  3.41it/s][A[A

 59% 19/32 [00:06<00:03,  3.69it/s][A[A

 62% 20/32 [00:06<00:03,  3.91it/s][A[A

 66% 21/32 [00:06<00:02,  4.12it/s][A[A

 69% 22/32 [00:06<00:02,  4.18it/s][A[A

 72% 23/32 [00:08<00:04,  1.84it/s][A[A

 75% 24/32 [00:08<00:03,  2.24it/s][A[A

 78% 25/32 [00:08<00:02,  2.67it/s][A[A

 81% 26/32 [00:08<00:01,  3.02it/s][A[A

 84% 27/32 [00:08<00:01,  3.37it/s][A[A

 88% 28/32 [00:09<00:01,  3.62it/s][A[A

 91% 29/32 [00:09<00:00,  3.89it/s][A[A

 94% 30/32 [00:09<00:00,  4.17it/s][A[A

 97% 31/32 [00:09<00:00,  4.35it/s][A[A

100% 32/32 [00:09<00:00,  4.57it/s][A[A100% 32/32 [00:09<00:00,  3.21it/s]
Meta loss on this task batch = 4.7462e-01, PNorm = 35.7490, GNorm = 0.0796

 37% 7/19 [01:10<02:04, 10.39s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.59it/s][A[A

  6% 2/32 [00:00<00:06,  4.33it/s][A[A

  9% 3/32 [00:00<00:06,  4.22it/s][A[A

 12% 4/32 [00:00<00:06,  4.16it/s][A[A

 16% 5/32 [00:01<00:06,  4.30it/s][A[A

 19% 6/32 [00:02<00:13,  1.90it/s][A[A

 22% 7/32 [00:02<00:10,  2.32it/s][A[A

 25% 8/32 [00:02<00:08,  2.67it/s][A[A

 28% 9/32 [00:03<00:07,  3.03it/s][A[A

 31% 10/32 [00:03<00:06,  3.31it/s][A[A

 34% 11/32 [00:03<00:05,  3.63it/s][A[A

 38% 12/32 [00:03<00:05,  3.92it/s][A[A

 41% 13/32 [00:03<00:04,  4.17it/s][A[A

 44% 14/32 [00:04<00:04,  4.29it/s][A[A

 47% 15/32 [00:04<00:03,  4.40it/s][A[A

 50% 16/32 [00:04<00:03,  4.44it/s][A[A

 53% 17/32 [00:04<00:03,  4.26it/s][A[A

 56% 18/32 [00:05<00:03,  4.35it/s][A[A

 59% 19/32 [00:05<00:03,  4.19it/s][A[A

 62% 20/32 [00:06<00:06,  1.89it/s][A[A

 66% 21/32 [00:06<00:04,  2.29it/s][A[A

 69% 22/32 [00:06<00:03,  2.65it/s][A[A

 72% 23/32 [00:07<00:02,  3.05it/s][A[A

 75% 24/32 [00:07<00:02,  3.28it/s][A[A

 78% 25/32 [00:07<00:01,  3.50it/s][A[A

 81% 26/32 [00:07<00:01,  3.65it/s][A[A

 84% 27/32 [00:08<00:01,  4.09it/s][A[A

 88% 28/32 [00:08<00:00,  4.33it/s][A[A

 91% 29/32 [00:08<00:00,  4.55it/s][A[A

 94% 30/32 [00:08<00:00,  4.73it/s][A[A

 97% 31/32 [00:08<00:00,  4.96it/s][A[A

100% 32/32 [00:09<00:00,  5.00it/s][A[A100% 32/32 [00:09<00:00,  3.52it/s]
Meta loss on this task batch = 3.8167e-01, PNorm = 35.7667, GNorm = 0.1730

 42% 8/19 [01:20<01:52, 10.24s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.24it/s][A[A

  6% 2/32 [00:01<00:14,  2.06it/s][A[A

  9% 3/32 [00:01<00:11,  2.48it/s][A[A

 12% 4/32 [00:01<00:09,  2.96it/s][A[A

 16% 5/32 [00:01<00:08,  3.37it/s][A[A

 19% 6/32 [00:02<00:06,  3.84it/s][A[A

 22% 7/32 [00:02<00:05,  4.23it/s][A[A

 25% 8/32 [00:02<00:05,  4.63it/s][A[A

 28% 9/32 [00:02<00:04,  4.78it/s][A[A

 31% 10/32 [00:02<00:04,  4.86it/s][A[A

 34% 11/32 [00:03<00:04,  4.89it/s][A[A

 38% 12/32 [00:03<00:03,  5.00it/s][A[A

 41% 13/32 [00:03<00:03,  5.01it/s][A[A

 44% 14/32 [00:03<00:03,  5.07it/s][A[A

 47% 15/32 [00:03<00:03,  5.18it/s][A[A

 50% 16/32 [00:04<00:02,  5.43it/s][A[A

 53% 17/32 [00:04<00:02,  5.44it/s][A[A

 56% 18/32 [00:05<00:06,  2.06it/s][A[A

 59% 19/32 [00:05<00:05,  2.49it/s][A[A

 62% 20/32 [00:05<00:04,  2.95it/s][A[A

 66% 21/32 [00:05<00:03,  3.40it/s][A[A

 69% 22/32 [00:06<00:02,  3.77it/s][A[A

 72% 23/32 [00:06<00:02,  4.06it/s][A[A

 75% 24/32 [00:06<00:01,  4.42it/s][A[A

 78% 25/32 [00:06<00:01,  4.84it/s][A[A

 81% 26/32 [00:06<00:01,  5.11it/s][A[A

 84% 27/32 [00:07<00:00,  5.08it/s][A[A

 88% 28/32 [00:07<00:00,  5.13it/s][A[A

 91% 29/32 [00:07<00:00,  5.14it/s][A[A

 94% 30/32 [00:08<00:00,  2.03it/s][A[A

 97% 31/32 [00:08<00:00,  2.46it/s][A[A

100% 32/32 [00:09<00:00,  2.96it/s][A[A100% 32/32 [00:09<00:00,  3.54it/s]
Meta loss on this task batch = 2.4401e-01, PNorm = 35.7913, GNorm = 0.2908

 47% 9/19 [01:30<01:40, 10.08s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.75it/s][A[A

  6% 2/32 [00:00<00:05,  5.03it/s][A[A

  9% 3/32 [00:00<00:05,  5.03it/s][A[A

 12% 4/32 [00:00<00:05,  5.03it/s][A[A

 16% 5/32 [00:00<00:05,  5.29it/s][A[A

 19% 6/32 [00:01<00:04,  5.21it/s][A[A

 22% 7/32 [00:01<00:04,  5.21it/s][A[A

 25% 8/32 [00:01<00:04,  5.37it/s][A[A

 28% 9/32 [00:01<00:04,  5.30it/s][A[A

 31% 10/32 [00:01<00:04,  5.24it/s][A[A

 34% 11/32 [00:02<00:04,  5.22it/s][A[A

 38% 12/32 [00:03<00:09,  2.00it/s][A[A

 41% 13/32 [00:03<00:07,  2.45it/s][A[A

 44% 14/32 [00:03<00:06,  2.90it/s][A[A

 47% 15/32 [00:03<00:05,  3.29it/s][A[A

 50% 16/32 [00:04<00:04,  3.84it/s][A[A

 53% 17/32 [00:04<00:03,  4.15it/s][A[A

 56% 18/32 [00:04<00:03,  4.39it/s][A[A

 59% 19/32 [00:04<00:02,  4.67it/s][A[A

 62% 20/32 [00:04<00:02,  4.87it/s][A[A

 66% 21/32 [00:05<00:02,  4.74it/s][A[A

 69% 22/32 [00:05<00:02,  4.93it/s][A[A

 72% 23/32 [00:05<00:01,  4.96it/s][A[A

 75% 24/32 [00:05<00:01,  5.06it/s][A[A

 78% 25/32 [00:06<00:03,  2.02it/s][A[A

 81% 26/32 [00:07<00:02,  2.40it/s][A[A

 84% 27/32 [00:07<00:01,  2.77it/s][A[A

 88% 28/32 [00:07<00:01,  3.10it/s][A[A

 91% 29/32 [00:07<00:00,  3.35it/s][A[A

 94% 30/32 [00:07<00:00,  3.58it/s][A[A

 97% 31/32 [00:08<00:00,  3.76it/s][A[A

100% 32/32 [00:08<00:00,  3.86it/s][A[A100% 32/32 [00:08<00:00,  3.78it/s]
Meta loss on this task batch = 2.9023e-01, PNorm = 35.8184, GNorm = 0.1059

 53% 10/19 [01:39<01:28,  9.81s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.20it/s][A[A

  6% 2/32 [00:00<00:06,  4.80it/s][A[A

  9% 3/32 [00:01<00:15,  1.91it/s][A[A

 12% 4/32 [00:01<00:12,  2.28it/s][A[A

 16% 5/32 [00:02<00:10,  2.65it/s][A[A

 19% 6/32 [00:02<00:08,  2.95it/s][A[A

 22% 7/32 [00:02<00:07,  3.43it/s][A[A

 25% 8/32 [00:02<00:06,  3.61it/s][A[A

 28% 9/32 [00:03<00:06,  3.71it/s][A[A

 31% 10/32 [00:03<00:05,  3.81it/s][A[A

 34% 11/32 [00:04<00:11,  1.79it/s][A[A

 38% 12/32 [00:04<00:09,  2.18it/s][A[A

 41% 13/32 [00:05<00:07,  2.52it/s][A[A

 44% 14/32 [00:05<00:06,  2.85it/s][A[A

 47% 15/32 [00:05<00:05,  3.14it/s][A[A

 50% 16/32 [00:05<00:04,  3.59it/s][A[A

 53% 17/32 [00:05<00:04,  3.75it/s][A[A

 56% 18/32 [00:06<00:03,  4.14it/s][A[A

 59% 19/32 [00:06<00:03,  4.08it/s][A[A

 62% 20/32 [00:06<00:02,  4.12it/s][A[A

 66% 21/32 [00:07<00:05,  1.84it/s][A[A

 69% 22/32 [00:08<00:04,  2.19it/s][A[A

 72% 23/32 [00:08<00:03,  2.53it/s][A[A

 75% 24/32 [00:08<00:02,  2.83it/s][A[A

 78% 25/32 [00:08<00:02,  3.11it/s][A[A

 81% 26/32 [00:09<00:01,  3.50it/s][A[A

 84% 27/32 [00:09<00:01,  3.87it/s][A[A

 88% 28/32 [00:09<00:01,  3.99it/s][A[A

 91% 29/32 [00:09<00:00,  4.09it/s][A[A

 94% 30/32 [00:11<00:01,  1.84it/s][A[A

 97% 31/32 [00:11<00:00,  2.21it/s][A[A

100% 32/32 [00:11<00:00,  2.58it/s][A[A100% 32/32 [00:11<00:00,  2.79it/s]
Meta loss on this task batch = 5.7656e-01, PNorm = 35.8436, GNorm = 0.0707

 58% 11/19 [01:52<01:24, 10.56s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.64it/s][A[A

  6% 2/32 [00:00<00:07,  3.77it/s][A[A

  9% 3/32 [00:00<00:07,  3.84it/s][A[A

 12% 4/32 [00:01<00:07,  3.91it/s][A[A

 16% 5/32 [00:01<00:06,  3.99it/s][A[A

 19% 6/32 [00:01<00:06,  4.03it/s][A[A

 22% 7/32 [00:02<00:13,  1.90it/s][A[A

 25% 8/32 [00:02<00:10,  2.26it/s][A[A

 28% 9/32 [00:03<00:08,  2.60it/s][A[A

 31% 10/32 [00:03<00:07,  2.93it/s][A[A

 34% 11/32 [00:03<00:06,  3.19it/s][A[A

 38% 12/32 [00:03<00:05,  3.39it/s][A[A

 41% 13/32 [00:04<00:05,  3.55it/s][A[A

 44% 14/32 [00:05<00:10,  1.74it/s][A[A

 47% 15/32 [00:05<00:08,  2.10it/s][A[A

 50% 16/32 [00:05<00:06,  2.57it/s][A[A

 53% 17/32 [00:06<00:05,  2.89it/s][A[A

 56% 18/32 [00:06<00:04,  3.18it/s][A[A

 59% 19/32 [00:06<00:03,  3.42it/s][A[A

 62% 20/32 [00:06<00:03,  3.57it/s][A[A

 66% 21/32 [00:07<00:02,  3.71it/s][A[A

 69% 22/32 [00:08<00:05,  1.85it/s][A[A

 72% 23/32 [00:08<00:04,  2.21it/s][A[A

 75% 24/32 [00:08<00:02,  2.69it/s][A[A

 78% 25/32 [00:08<00:02,  3.00it/s][A[A

 81% 26/32 [00:09<00:01,  3.33it/s][A[A

 84% 27/32 [00:09<00:01,  3.54it/s][A[A

 88% 28/32 [00:09<00:01,  3.69it/s][A[A

 91% 29/32 [00:09<00:00,  3.88it/s][A[A

 94% 30/32 [00:10<00:00,  3.99it/s][A[A

 97% 31/32 [00:10<00:00,  4.05it/s][A[A

100% 32/32 [00:10<00:00,  4.10it/s][A[A100% 32/32 [00:10<00:00,  3.03it/s]
Meta loss on this task batch = 5.9535e-01, PNorm = 35.8622, GNorm = 0.2480

 63% 12/19 [02:03<01:15, 10.81s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  3.98it/s][A[A

  6% 2/32 [00:00<00:07,  4.03it/s][A[A

  9% 3/32 [00:00<00:07,  4.08it/s][A[A

 12% 4/32 [00:01<00:15,  1.86it/s][A[A

 16% 5/32 [00:02<00:12,  2.24it/s][A[A

 19% 6/32 [00:02<00:09,  2.61it/s][A[A

 22% 7/32 [00:02<00:08,  2.93it/s][A[A

 25% 8/32 [00:02<00:07,  3.25it/s][A[A

 28% 9/32 [00:03<00:06,  3.53it/s][A[A

 31% 10/32 [00:03<00:05,  3.73it/s][A[A

 34% 11/32 [00:03<00:05,  3.91it/s][A[A

 38% 12/32 [00:03<00:04,  4.07it/s][A[A

 41% 13/32 [00:04<00:04,  4.07it/s][A[A

 44% 14/32 [00:04<00:04,  4.42it/s][A[A

 47% 15/32 [00:04<00:03,  4.35it/s][A[A

 50% 16/32 [00:05<00:08,  1.87it/s][A[A

 53% 17/32 [00:05<00:06,  2.23it/s][A[A

 56% 18/32 [00:06<00:05,  2.61it/s][A[A

 59% 19/32 [00:06<00:04,  2.93it/s][A[A

 62% 20/32 [00:06<00:03,  3.29it/s][A[A

 66% 21/32 [00:06<00:03,  3.52it/s][A[A

 69% 22/32 [00:07<00:02,  3.70it/s][A[A

 72% 23/32 [00:07<00:02,  3.84it/s][A[A

 75% 24/32 [00:08<00:04,  1.80it/s][A[A

 78% 25/32 [00:08<00:03,  2.18it/s][A[A

 81% 26/32 [00:09<00:02,  2.56it/s][A[A

 84% 27/32 [00:09<00:01,  2.96it/s][A[A

 88% 28/32 [00:09<00:01,  3.47it/s][A[A

 91% 29/32 [00:09<00:00,  3.65it/s][A[A

 94% 30/32 [00:09<00:00,  3.83it/s][A[A

 97% 31/32 [00:10<00:00,  4.20it/s][A[A

100% 32/32 [00:10<00:00,  4.22it/s][A[A100% 32/32 [00:10<00:00,  3.09it/s]
Meta loss on this task batch = 6.2940e-01, PNorm = 35.8706, GNorm = 0.4055

 68% 13/19 [02:14<01:05, 10.91s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.22it/s][A[A

  6% 2/32 [00:00<00:07,  4.21it/s][A[A

  9% 3/32 [00:00<00:06,  4.21it/s][A[A

 12% 4/32 [00:00<00:06,  4.16it/s][A[A

 16% 5/32 [00:01<00:06,  4.10it/s][A[A

 19% 6/32 [00:02<00:14,  1.86it/s][A[A

 22% 7/32 [00:02<00:11,  2.21it/s][A[A

 25% 8/32 [00:02<00:09,  2.61it/s][A[A

 28% 9/32 [00:03<00:07,  2.97it/s][A[A

 31% 10/32 [00:03<00:06,  3.27it/s][A[A

 34% 11/32 [00:03<00:05,  3.52it/s][A[A

 38% 12/32 [00:03<00:05,  3.99it/s][A[A

 41% 13/32 [00:04<00:04,  4.06it/s][A[A

 44% 14/32 [00:04<00:04,  4.11it/s][A[A

 47% 15/32 [00:04<00:04,  4.08it/s][A[A

 50% 16/32 [00:04<00:03,  4.02it/s][A[A

 53% 17/32 [00:05<00:03,  4.04it/s][A[A

 56% 18/32 [00:06<00:07,  1.87it/s][A[A

 59% 19/32 [00:06<00:05,  2.33it/s][A[A

 62% 20/32 [00:06<00:04,  2.79it/s][A[A

 66% 21/32 [00:06<00:03,  3.10it/s][A[A

 69% 22/32 [00:07<00:02,  3.36it/s][A[A

 72% 23/32 [00:07<00:02,  3.54it/s][A[A

 75% 24/32 [00:07<00:01,  4.00it/s][A[A

 78% 25/32 [00:08<00:03,  1.82it/s][A[A

 81% 26/32 [00:08<00:02,  2.17it/s][A[A

 84% 27/32 [00:09<00:01,  2.52it/s][A[A

 88% 28/32 [00:09<00:01,  2.84it/s][A[A

 91% 29/32 [00:09<00:00,  3.18it/s][A[A

 94% 30/32 [00:09<00:00,  3.47it/s][A[A

 97% 31/32 [00:10<00:00,  3.64it/s][A[A

100% 32/32 [00:10<00:00,  3.72it/s][A[A100% 32/32 [00:10<00:00,  3.07it/s]
Meta loss on this task batch = 6.0439e-01, PNorm = 35.8730, GNorm = 0.2248

 74% 14/19 [02:25<00:55, 11.01s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.39it/s][A[A

  6% 2/32 [00:00<00:07,  4.22it/s][A[A

  9% 3/32 [00:00<00:06,  4.21it/s][A[A

 12% 4/32 [00:00<00:06,  4.29it/s][A[A

 16% 5/32 [00:01<00:06,  4.32it/s][A[A

 19% 6/32 [00:02<00:14,  1.85it/s][A[A

 22% 7/32 [00:02<00:11,  2.20it/s][A[A

 25% 8/32 [00:02<00:09,  2.64it/s][A[A

 28% 9/32 [00:03<00:07,  2.92it/s][A[A

 31% 10/32 [00:03<00:06,  3.23it/s][A[A

 34% 11/32 [00:03<00:05,  3.59it/s][A[A

 38% 12/32 [00:03<00:05,  3.83it/s][A[A

 41% 13/32 [00:04<00:04,  4.16it/s][A[A

 44% 14/32 [00:04<00:04,  4.35it/s][A[A

 47% 15/32 [00:04<00:03,  4.50it/s][A[A

 50% 16/32 [00:04<00:03,  4.50it/s][A[A

 53% 17/32 [00:04<00:03,  4.42it/s][A[A

 56% 18/32 [00:05<00:03,  4.46it/s][A[A

 59% 19/32 [00:06<00:06,  1.93it/s][A[A

 62% 20/32 [00:06<00:05,  2.37it/s][A[A

 66% 21/32 [00:06<00:03,  2.80it/s][A[A

 69% 22/32 [00:06<00:03,  3.18it/s][A[A

 72% 23/32 [00:07<00:02,  3.36it/s][A[A

 75% 24/32 [00:07<00:02,  3.65it/s][A[A

 78% 25/32 [00:07<00:01,  3.81it/s][A[A

 81% 26/32 [00:07<00:01,  3.84it/s][A[A

 84% 27/32 [00:08<00:01,  4.17it/s][A[A

 88% 28/32 [00:09<00:02,  1.87it/s][A[A

 91% 29/32 [00:09<00:01,  2.23it/s][A[A

 94% 30/32 [00:09<00:00,  2.60it/s][A[A

 97% 31/32 [00:10<00:00,  2.94it/s][A[A

100% 32/32 [00:10<00:00,  3.20it/s][A[A100% 32/32 [00:10<00:00,  3.12it/s]
Meta loss on this task batch = 5.2906e-01, PNorm = 35.8648, GNorm = 0.3837

 79% 15/19 [02:36<00:44, 11.02s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.05it/s][A[A

  6% 2/32 [00:00<00:07,  4.15it/s][A[A

  9% 3/32 [00:01<00:15,  1.86it/s][A[A

 12% 4/32 [00:01<00:12,  2.23it/s][A[A

 16% 5/32 [00:02<00:10,  2.59it/s][A[A

 19% 6/32 [00:02<00:08,  2.95it/s][A[A

 22% 7/32 [00:02<00:07,  3.28it/s][A[A

 25% 8/32 [00:02<00:06,  3.54it/s][A[A

 28% 9/32 [00:03<00:06,  3.71it/s][A[A

 31% 10/32 [00:03<00:05,  3.82it/s][A[A

 34% 11/32 [00:03<00:05,  3.91it/s][A[A

 38% 12/32 [00:03<00:05,  3.99it/s][A[A

 41% 13/32 [00:04<00:04,  4.10it/s][A[A

 44% 14/32 [00:05<00:09,  1.87it/s][A[A

 47% 15/32 [00:05<00:07,  2.25it/s][A[A

 50% 16/32 [00:05<00:06,  2.64it/s][A[A

 53% 17/32 [00:05<00:05,  2.99it/s][A[A

 56% 18/32 [00:06<00:04,  3.46it/s][A[A

 59% 19/32 [00:06<00:03,  3.67it/s][A[A

 62% 20/32 [00:06<00:03,  3.86it/s][A[A

 66% 21/32 [00:06<00:02,  4.03it/s][A[A

 69% 22/32 [00:07<00:02,  4.29it/s][A[A

 72% 23/32 [00:07<00:02,  4.28it/s][A[A

 75% 24/32 [00:07<00:01,  4.06it/s][A[A

 78% 25/32 [00:08<00:03,  1.87it/s][A[A

 81% 26/32 [00:08<00:02,  2.24it/s][A[A

 84% 27/32 [00:09<00:01,  2.56it/s][A[A

 88% 28/32 [00:09<00:01,  2.93it/s][A[A

 91% 29/32 [00:09<00:00,  3.10it/s][A[A

 94% 30/32 [00:09<00:00,  3.36it/s][A[A

 97% 31/32 [00:10<00:00,  3.59it/s][A[A

100% 32/32 [00:10<00:00,  3.76it/s][A[A100% 32/32 [00:10<00:00,  3.06it/s]
Meta loss on this task batch = 5.5119e-01, PNorm = 35.8561, GNorm = 0.0885

 84% 16/19 [02:48<00:33, 11.10s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:38,  1.25s/it][A[A

  6% 2/32 [00:01<00:28,  1.05it/s][A[A

  9% 3/32 [00:01<00:21,  1.36it/s][A[A

 12% 4/32 [00:01<00:16,  1.74it/s][A[A

 16% 5/32 [00:02<00:13,  2.07it/s][A[A

 19% 6/32 [00:02<00:10,  2.48it/s][A[A

 22% 7/32 [00:02<00:08,  2.85it/s][A[A

 25% 8/32 [00:02<00:07,  3.09it/s][A[A

 28% 9/32 [00:03<00:06,  3.42it/s][A[A

 31% 10/32 [00:03<00:05,  3.94it/s][A[A

 34% 11/32 [00:03<00:05,  4.06it/s][A[A

 38% 12/32 [00:04<00:10,  1.85it/s][A[A

 41% 13/32 [00:04<00:08,  2.25it/s][A[A

 44% 14/32 [00:05<00:06,  2.64it/s][A[A

 47% 15/32 [00:05<00:05,  2.96it/s][A[A

 50% 16/32 [00:05<00:04,  3.23it/s][A[A

 53% 17/32 [00:05<00:04,  3.50it/s][A[A

 56% 18/32 [00:06<00:03,  3.85it/s][A[A

 59% 19/32 [00:06<00:03,  4.14it/s][A[A

 62% 20/32 [00:06<00:02,  4.25it/s][A[A

 66% 21/32 [00:06<00:02,  4.57it/s][A[A

 69% 22/32 [00:06<00:02,  4.77it/s][A[A

 72% 23/32 [00:08<00:04,  1.95it/s][A[A

 75% 24/32 [00:08<00:03,  2.26it/s][A[A

 78% 25/32 [00:08<00:02,  2.76it/s][A[A

 81% 26/32 [00:08<00:01,  3.22it/s][A[A

 84% 27/32 [00:08<00:01,  3.60it/s][A[A

 88% 28/32 [00:09<00:01,  3.91it/s][A[A

 91% 29/32 [00:09<00:00,  3.97it/s][A[A

 94% 30/32 [00:09<00:00,  3.93it/s][A[A

 97% 31/32 [00:09<00:00,  4.16it/s][A[A

100% 32/32 [00:10<00:00,  4.18it/s][A[A100% 32/32 [00:10<00:00,  3.16it/s]
Meta loss on this task batch = 4.3656e-01, PNorm = 35.8493, GNorm = 0.0313

 89% 17/19 [02:59<00:22, 11.05s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.95it/s][A[A

  6% 2/32 [00:01<00:15,  1.98it/s][A[A

  9% 3/32 [00:01<00:12,  2.36it/s][A[A

 12% 4/32 [00:01<00:10,  2.74it/s][A[A

 16% 5/32 [00:02<00:08,  3.22it/s][A[A

 19% 6/32 [00:02<00:07,  3.62it/s][A[A

 22% 7/32 [00:02<00:06,  3.71it/s][A[A

 25% 8/32 [00:02<00:06,  3.76it/s][A[A

 28% 9/32 [00:02<00:05,  4.08it/s][A[A

 31% 10/32 [00:03<00:05,  4.16it/s][A[A

 34% 11/32 [00:03<00:04,  4.24it/s][A[A

 38% 12/32 [00:03<00:04,  4.44it/s][A[A

 41% 13/32 [00:04<00:10,  1.87it/s][A[A

 44% 14/32 [00:05<00:07,  2.30it/s][A[A

 47% 15/32 [00:05<00:06,  2.60it/s][A[A

 50% 16/32 [00:05<00:05,  2.98it/s][A[A

 53% 17/32 [00:05<00:04,  3.40it/s][A[A

 56% 18/32 [00:05<00:03,  3.69it/s][A[A

 59% 19/32 [00:06<00:03,  3.82it/s][A[A

 62% 20/32 [00:06<00:03,  3.96it/s][A[A

 66% 21/32 [00:06<00:02,  4.41it/s][A[A

 69% 22/32 [00:06<00:02,  4.60it/s][A[A

 72% 23/32 [00:07<00:02,  4.33it/s][A[A

 75% 24/32 [00:08<00:04,  1.83it/s][A[A

 78% 25/32 [00:08<00:03,  2.22it/s][A[A

 81% 26/32 [00:08<00:02,  2.61it/s][A[A

 84% 27/32 [00:09<00:01,  2.79it/s][A[A

 88% 28/32 [00:09<00:01,  3.10it/s][A[A

 91% 29/32 [00:09<00:00,  3.40it/s][A[A

 94% 30/32 [00:09<00:00,  3.62it/s][A[A

 97% 31/32 [00:10<00:00,  3.94it/s][A[A

100% 32/32 [00:10<00:00,  4.24it/s][A[A100% 32/32 [00:10<00:00,  3.13it/s]
Meta loss on this task batch = 5.0775e-01, PNorm = 35.8443, GNorm = 0.0407

 95% 18/19 [03:10<00:11, 11.04s/it][A

  0% 0/23 [00:00<?, ?it/s][A[A

  4% 1/23 [00:00<00:05,  4.15it/s][A[A

  9% 2/23 [00:00<00:04,  4.30it/s][A[A

 13% 3/23 [00:00<00:04,  4.37it/s][A[A

 17% 4/23 [00:00<00:04,  4.55it/s][A[A

 22% 5/23 [00:01<00:03,  4.78it/s][A[A

 26% 6/23 [00:02<00:08,  1.96it/s][A[A

 30% 7/23 [00:02<00:06,  2.33it/s][A[A

 35% 8/23 [00:02<00:05,  2.67it/s][A[A

 39% 9/23 [00:02<00:04,  3.19it/s][A[A

 43% 10/23 [00:03<00:03,  3.35it/s][A[A

 48% 11/23 [00:03<00:03,  3.94it/s][A[A

 52% 12/23 [00:03<00:02,  4.06it/s][A[A

 57% 13/23 [00:03<00:02,  4.31it/s][A[A

 61% 14/23 [00:04<00:02,  4.29it/s][A[A

 65% 15/23 [00:05<00:04,  1.75it/s][A[A

 70% 16/23 [00:05<00:03,  2.09it/s][A[A

 74% 17/23 [00:05<00:02,  2.47it/s][A[A

 78% 18/23 [00:06<00:01,  2.77it/s][A[A

 83% 19/23 [00:06<00:01,  3.10it/s][A[A

 87% 20/23 [00:06<00:00,  3.57it/s][A[A

 91% 21/23 [00:06<00:00,  3.96it/s][A[A

 96% 22/23 [00:06<00:00,  4.33it/s][A[A

100% 23/23 [00:08<00:00,  1.90it/s][A[A100% 23/23 [00:08<00:00,  2.83it/s]
Meta loss on this task batch = 4.3783e-01, PNorm = 35.8416, GNorm = 0.0676

100% 19/19 [03:18<00:00, 10.34s/it][A100% 19/19 [03:18<00:00, 10.46s/it]
Took 198.79150104522705 seconds to complete one epoch of meta training

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/20 [00:00<?, ?it/s][A[A


  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 33.44it/s]


  5% 1/20 [00:00<00:03,  5.88it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 21.31it/s]


 10% 2/20 [00:00<00:03,  5.71it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 18.42it/s][A[A[A100% 3/3 [00:00<00:00, 20.53it/s]


 15% 3/20 [00:00<00:03,  4.85it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 16.00it/s]


 20% 4/20 [00:00<00:03,  4.77it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 15.35it/s][A[A[A100% 4/4 [00:00<00:00, 17.77it/s]


 25% 5/20 [00:01<00:03,  4.10it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.72it/s][A[A[A100% 4/4 [00:00<00:00, 16.46it/s]


 30% 6/20 [00:01<00:03,  3.73it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.93it/s][A[A[A100% 4/4 [00:00<00:00, 21.55it/s]


 35% 7/20 [00:01<00:03,  3.63it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 17.38it/s][A[A[A100% 4/4 [00:00<00:00, 19.74it/s]


 40% 8/20 [00:02<00:03,  3.44it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.63it/s][A[A[A100% 4/4 [00:00<00:00, 23.32it/s]


 45% 9/20 [00:03<00:06,  1.73it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.00it/s][A[A[A100% 4/4 [00:00<00:00, 20.08it/s]


 50% 10/20 [00:03<00:04,  2.00it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 17.18it/s][A[A[A100% 4/4 [00:00<00:00, 19.62it/s]


 55% 11/20 [00:04<00:04,  2.25it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.87it/s][A[A[A100% 4/4 [00:00<00:00, 22.31it/s]


 60% 12/20 [00:04<00:03,  2.54it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 15.06it/s][A[A[A100% 3/3 [00:00<00:00, 14.30it/s]


 65% 13/20 [00:04<00:02,  2.58it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.17it/s][A[A[A100% 3/3 [00:00<00:00, 13.88it/s]


 70% 14/20 [00:05<00:02,  2.58it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 14.35it/s][A[A[A100% 4/4 [00:00<00:00, 18.62it/s]


 75% 15/20 [00:05<00:01,  2.73it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.69it/s][A[A[A100% 3/3 [00:00<00:00, 18.15it/s]


 80% 16/20 [00:05<00:01,  2.81it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 12.52it/s][A[A[A100% 3/3 [00:00<00:00, 15.22it/s]


 85% 17/20 [00:07<00:01,  1.51it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 22.83it/s]


 90% 18/20 [00:07<00:01,  1.83it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 15.70it/s][A[A[A100% 3/3 [00:00<00:00, 19.94it/s]


 95% 19/20 [00:07<00:00,  2.11it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 13.75it/s]


100% 20/20 [00:07<00:00,  2.47it/s][A[A100% 20/20 [00:07<00:00,  2.53it/s]

100% 1/1 [00:07<00:00,  7.89s/it][A100% 1/1 [00:07<00:00,  7.89s/it]
Took 206.68410921096802 seconds to complete one epoch of meta training and validating
Meta validation score auc = 0.602850
Found better MAML checkpoint after meta validation, saving now
 23% 7/30 [22:42<1:15:08, 196.00s/it]Epoch 7

  0% 0/19 [00:00<?, ?it/s][A

  0% 0/32 [00:00<?, ?it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  3% 1/32 [00:00<00:05,  5.62it/s][A[A

  6% 2/32 [00:00<00:05,  5.30it/s][A[A

  9% 3/32 [00:00<00:05,  5.54it/s][A[A

 12% 4/32 [00:00<00:04,  5.86it/s][A[A

 16% 5/32 [00:01<00:12,  2.19it/s][A[A

 19% 6/32 [00:02<00:09,  2.67it/s][A[A

 22% 7/32 [00:02<00:07,  3.23it/s][A[A

 25% 8/32 [00:02<00:06,  3.64it/s][A[A

 28% 9/32 [00:02<00:05,  4.36it/s][A[A

 31% 10/32 [00:02<00:04,  5.03it/s][A[A

 34% 11/32 [00:02<00:04,  5.04it/s][A[A

 38% 12/32 [00:03<00:04,  4.64it/s][A[A

 41% 13/32 [00:03<00:04,  4.57it/s][A[A

 44% 14/32 [00:03<00:04,  4.45it/s][A[A

 47% 15/32 [00:03<00:03,  5.12it/s][A[A

 50% 16/32 [00:03<00:03,  5.02it/s][A[A

 53% 17/32 [00:04<00:02,  5.15it/s][A[A

 56% 18/32 [00:04<00:02,  5.35it/s][A[A

 59% 19/32 [00:05<00:06,  2.11it/s][A[A

 62% 20/32 [00:05<00:04,  2.58it/s][A[A

 66% 21/32 [00:05<00:03,  3.25it/s][A[A

 69% 22/32 [00:05<00:02,  3.74it/s][A[A

 75% 24/32 [00:06<00:01,  4.37it/s][A[A

 78% 25/32 [00:06<00:01,  4.68it/s][A[A

 81% 26/32 [00:06<00:01,  4.71it/s][A[A

 88% 28/32 [00:06<00:00,  5.34it/s][A[A

 91% 29/32 [00:06<00:00,  4.96it/s][A[A

 94% 30/32 [00:07<00:00,  5.73it/s][A[A

 97% 31/32 [00:07<00:00,  6.43it/s][A[A

100% 32/32 [00:07<00:00,  6.23it/s][A[A100% 32/32 [00:07<00:00,  4.33it/s]
Meta loss on this task batch = 5.1505e-01, PNorm = 35.8431, GNorm = 0.0926

  5% 1/19 [00:08<02:25,  8.07s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  7.63it/s][A[A

  6% 2/32 [00:00<00:03,  7.51it/s][A[A

  9% 3/32 [00:00<00:04,  6.91it/s][A[A

 12% 4/32 [00:00<00:04,  6.49it/s][A[A

 16% 5/32 [00:00<00:04,  6.06it/s][A[A

 19% 6/32 [00:00<00:04,  6.36it/s][A[A

 22% 7/32 [00:01<00:03,  6.71it/s][A[A

 25% 8/32 [00:01<00:03,  6.97it/s][A[A

 28% 9/32 [00:01<00:03,  6.86it/s][A[A

 31% 10/32 [00:02<00:09,  2.28it/s][A[A

 34% 11/32 [00:02<00:07,  2.86it/s][A[A

 38% 12/32 [00:02<00:05,  3.34it/s][A[A

 41% 13/32 [00:02<00:04,  3.98it/s][A[A

 44% 14/32 [00:03<00:04,  4.36it/s][A[A

 47% 15/32 [00:03<00:03,  4.77it/s][A[A

 50% 16/32 [00:03<00:03,  5.00it/s][A[A

 53% 17/32 [00:03<00:02,  5.13it/s][A[A

 56% 18/32 [00:03<00:02,  5.10it/s][A[A

 62% 20/32 [00:04<00:02,  5.35it/s][A[A

 66% 21/32 [00:04<00:01,  5.90it/s][A[A

 69% 22/32 [00:04<00:01,  6.29it/s][A[A

 72% 23/32 [00:04<00:01,  6.46it/s][A[A

 75% 24/32 [00:04<00:01,  6.67it/s][A[A

 78% 25/32 [00:04<00:01,  6.06it/s][A[A

 81% 26/32 [00:05<00:01,  5.69it/s][A[A

 84% 27/32 [00:05<00:00,  5.84it/s][A[A

 88% 28/32 [00:05<00:00,  5.85it/s][A[A

 91% 29/32 [00:05<00:00,  6.39it/s][A[A

 94% 30/32 [00:06<00:00,  2.16it/s][A[A

 97% 31/32 [00:06<00:00,  2.70it/s][A[A

100% 32/32 [00:07<00:00,  3.32it/s][A[A100% 32/32 [00:07<00:00,  4.54it/s]
Meta loss on this task batch = 5.1552e-01, PNorm = 35.8488, GNorm = 0.1108

 11% 2/19 [00:15<02:15,  7.96s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.58it/s][A[A

  6% 2/32 [00:00<00:05,  5.47it/s][A[A

  9% 3/32 [00:00<00:05,  5.67it/s][A[A

 12% 4/32 [00:00<00:05,  5.55it/s][A[A

 16% 5/32 [00:00<00:04,  5.69it/s][A[A

 19% 6/32 [00:01<00:04,  5.58it/s][A[A

 22% 7/32 [00:01<00:04,  5.54it/s][A[A

 25% 8/32 [00:01<00:04,  5.82it/s][A[A

 28% 9/32 [00:01<00:03,  6.05it/s][A[A

 31% 10/32 [00:01<00:03,  5.62it/s][A[A

 34% 11/32 [00:01<00:03,  5.99it/s][A[A

 38% 12/32 [00:02<00:03,  5.90it/s][A[A

 41% 13/32 [00:02<00:03,  5.41it/s][A[A

 44% 14/32 [00:02<00:03,  5.12it/s][A[A

 47% 15/32 [00:03<00:08,  2.02it/s][A[A

 50% 16/32 [00:03<00:06,  2.59it/s][A[A

 53% 17/32 [00:03<00:04,  3.21it/s][A[A

 56% 18/32 [00:04<00:03,  3.87it/s][A[A

 59% 19/32 [00:04<00:03,  4.31it/s][A[A

 62% 20/32 [00:04<00:02,  4.98it/s][A[A

 66% 21/32 [00:04<00:01,  5.61it/s][A[A

 69% 22/32 [00:04<00:01,  5.99it/s][A[A

 72% 23/32 [00:04<00:01,  5.78it/s][A[A

 75% 24/32 [00:05<00:01,  6.16it/s][A[A

 78% 25/32 [00:05<00:01,  6.63it/s][A[A

 81% 26/32 [00:05<00:00,  7.01it/s][A[A

 84% 27/32 [00:05<00:00,  5.92it/s][A[A

 88% 28/32 [00:05<00:00,  5.97it/s][A[A

 91% 29/32 [00:05<00:00,  5.81it/s][A[A

 94% 30/32 [00:06<00:00,  5.12it/s][A[A

 97% 31/32 [00:06<00:00,  5.90it/s][A[A

100% 32/32 [00:06<00:00,  5.68it/s][A[A100% 32/32 [00:06<00:00,  5.01it/s]
Meta loss on this task batch = 5.2319e-01, PNorm = 35.8577, GNorm = 0.0836

 16% 3/19 [00:22<02:03,  7.69s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.98it/s][A[A

  6% 2/32 [00:01<00:14,  2.00it/s][A[A

  9% 3/32 [00:01<00:11,  2.48it/s][A[A

 12% 4/32 [00:01<00:09,  2.97it/s][A[A

 16% 5/32 [00:01<00:08,  3.34it/s][A[A

 19% 6/32 [00:02<00:07,  3.68it/s][A[A

 22% 7/32 [00:02<00:05,  4.37it/s][A[A

 25% 8/32 [00:02<00:04,  5.13it/s][A[A

 28% 9/32 [00:02<00:04,  5.55it/s][A[A

 34% 11/32 [00:02<00:03,  6.21it/s][A[A

 38% 12/32 [00:02<00:03,  6.53it/s][A[A

 41% 13/32 [00:03<00:03,  6.14it/s][A[A

 44% 14/32 [00:03<00:02,  6.15it/s][A[A

 47% 15/32 [00:03<00:02,  6.38it/s][A[A

 50% 16/32 [00:03<00:02,  6.25it/s][A[A

 56% 18/32 [00:03<00:02,  6.30it/s][A[A

 59% 19/32 [00:04<00:02,  6.41it/s][A[A

 62% 20/32 [00:05<00:05,  2.19it/s][A[A

 66% 21/32 [00:05<00:04,  2.65it/s][A[A

 69% 22/32 [00:05<00:03,  3.01it/s][A[A

 72% 23/32 [00:05<00:02,  3.39it/s][A[A

 75% 24/32 [00:06<00:02,  3.80it/s][A[A

 78% 25/32 [00:06<00:01,  4.50it/s][A[A

 81% 26/32 [00:06<00:01,  5.23it/s][A[A

 84% 27/32 [00:06<00:00,  5.44it/s][A[A

 88% 28/32 [00:06<00:00,  5.48it/s][A[A

 91% 29/32 [00:06<00:00,  5.93it/s][A[A

 94% 30/32 [00:06<00:00,  6.21it/s][A[A

 97% 31/32 [00:07<00:00,  6.44it/s][A[A

100% 32/32 [00:07<00:00,  5.95it/s][A[A100% 32/32 [00:07<00:00,  4.42it/s]
Meta loss on this task batch = 5.8100e-01, PNorm = 35.8669, GNorm = 0.0614

 21% 4/19 [00:30<01:56,  7.76s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.52it/s][A[A

  6% 2/32 [00:00<00:05,  5.96it/s][A[A

 12% 4/32 [00:00<00:04,  6.21it/s][A[A

 16% 5/32 [00:00<00:04,  5.74it/s][A[A

 19% 6/32 [00:01<00:04,  5.59it/s][A[A

 22% 7/32 [00:01<00:04,  5.19it/s][A[A

 25% 8/32 [00:02<00:11,  2.11it/s][A[A

 28% 9/32 [00:02<00:08,  2.63it/s][A[A

 31% 10/32 [00:02<00:06,  3.18it/s][A[A

 34% 11/32 [00:02<00:05,  3.61it/s][A[A

 38% 12/32 [00:03<00:05,  3.90it/s][A[A

 41% 13/32 [00:03<00:04,  4.43it/s][A[A

 44% 14/32 [00:03<00:03,  4.98it/s][A[A

 47% 15/32 [00:03<00:03,  4.83it/s][A[A

 50% 16/32 [00:03<00:02,  5.60it/s][A[A

 53% 17/32 [00:03<00:03,  4.82it/s][A[A

 56% 18/32 [00:04<00:02,  4.94it/s][A[A

 59% 19/32 [00:04<00:02,  5.01it/s][A[A

 62% 20/32 [00:04<00:02,  4.69it/s][A[A

 66% 21/32 [00:04<00:02,  5.22it/s][A[A

 69% 22/32 [00:04<00:01,  5.93it/s][A[A

 72% 23/32 [00:05<00:01,  5.45it/s][A[A

 75% 24/32 [00:06<00:03,  2.12it/s][A[A

 78% 25/32 [00:06<00:02,  2.53it/s][A[A

 81% 26/32 [00:06<00:01,  3.13it/s][A[A

 84% 27/32 [00:06<00:01,  3.63it/s][A[A

 88% 28/32 [00:06<00:00,  4.21it/s][A[A

 91% 29/32 [00:07<00:00,  4.84it/s][A[A

 94% 30/32 [00:07<00:00,  5.28it/s][A[A

 97% 31/32 [00:07<00:00,  5.40it/s][A[A

100% 32/32 [00:07<00:00,  5.25it/s][A[A100% 32/32 [00:07<00:00,  4.22it/s]
Meta loss on this task batch = 5.1531e-01, PNorm = 35.8776, GNorm = 0.0807

 26% 5/19 [00:39<01:50,  7.91s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  6.93it/s][A[A

  6% 2/32 [00:00<00:04,  6.66it/s][A[A

  9% 3/32 [00:00<00:04,  6.74it/s][A[A

 12% 4/32 [00:00<00:03,  7.03it/s][A[A

 16% 5/32 [00:00<00:04,  5.75it/s][A[A

 19% 6/32 [00:01<00:04,  5.63it/s][A[A

 22% 7/32 [00:01<00:04,  5.61it/s][A[A

 25% 8/32 [00:01<00:04,  5.32it/s][A[A

 28% 9/32 [00:02<00:11,  1.97it/s][A[A

 31% 10/32 [00:02<00:09,  2.43it/s][A[A

 34% 11/32 [00:03<00:07,  2.86it/s][A[A

 38% 12/32 [00:03<00:06,  3.29it/s][A[A

 41% 13/32 [00:03<00:04,  3.88it/s][A[A

 44% 14/32 [00:03<00:04,  3.91it/s][A[A

 47% 15/32 [00:03<00:04,  4.10it/s][A[A

 50% 16/32 [00:04<00:03,  4.38it/s][A[A

 53% 17/32 [00:04<00:03,  4.94it/s][A[A

 56% 18/32 [00:04<00:02,  5.12it/s][A[A

 59% 19/32 [00:04<00:02,  4.88it/s][A[A

 62% 20/32 [00:04<00:02,  5.31it/s][A[A

 66% 21/32 [00:04<00:02,  5.15it/s][A[A

 69% 22/32 [00:05<00:02,  4.96it/s][A[A

 72% 23/32 [00:06<00:04,  2.01it/s][A[A

 75% 24/32 [00:06<00:03,  2.43it/s][A[A

 78% 25/32 [00:06<00:02,  2.81it/s][A[A

 81% 26/32 [00:06<00:01,  3.41it/s][A[A

 84% 27/32 [00:07<00:01,  3.59it/s][A[A

 88% 28/32 [00:07<00:01,  3.67it/s][A[A

 91% 29/32 [00:07<00:00,  3.80it/s][A[A

 94% 30/32 [00:07<00:00,  4.08it/s][A[A

100% 32/32 [00:08<00:00,  4.60it/s][A[A100% 32/32 [00:08<00:00,  3.90it/s]
Meta loss on this task batch = 4.6155e-01, PNorm = 35.8908, GNorm = 0.1373

 32% 6/19 [00:47<01:46,  8.22s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.83it/s][A[A

  6% 2/32 [00:00<00:04,  6.14it/s][A[A

  9% 3/32 [00:00<00:05,  5.39it/s][A[A

 12% 4/32 [00:00<00:05,  4.85it/s][A[A

 16% 5/32 [00:00<00:05,  5.32it/s][A[A

 19% 6/32 [00:01<00:05,  5.17it/s][A[A

 22% 7/32 [00:01<00:04,  5.24it/s][A[A

 25% 8/32 [00:02<00:11,  2.01it/s][A[A

 28% 9/32 [00:02<00:09,  2.47it/s][A[A

 31% 10/32 [00:02<00:07,  2.90it/s][A[A

 34% 11/32 [00:03<00:06,  3.42it/s][A[A

 38% 12/32 [00:03<00:05,  3.96it/s][A[A

 41% 13/32 [00:03<00:04,  4.57it/s][A[A

 44% 14/32 [00:03<00:03,  5.07it/s][A[A

 47% 15/32 [00:03<00:03,  5.47it/s][A[A

 50% 16/32 [00:03<00:03,  4.89it/s][A[A

 53% 17/32 [00:04<00:03,  4.87it/s][A[A

 56% 18/32 [00:04<00:02,  4.73it/s][A[A

 59% 19/32 [00:04<00:02,  4.92it/s][A[A

 62% 20/32 [00:04<00:02,  5.24it/s][A[A

 66% 21/32 [00:04<00:02,  5.01it/s][A[A

 69% 22/32 [00:05<00:02,  4.80it/s][A[A

 72% 23/32 [00:05<00:01,  4.65it/s][A[A

 75% 24/32 [00:05<00:01,  4.86it/s][A[A

 78% 25/32 [00:06<00:03,  1.96it/s][A[A

 81% 26/32 [00:07<00:02,  2.40it/s][A[A

 84% 27/32 [00:07<00:01,  2.88it/s][A[A

 88% 28/32 [00:07<00:01,  3.30it/s][A[A

 91% 29/32 [00:07<00:00,  4.11it/s][A[A

 94% 30/32 [00:07<00:00,  4.38it/s][A[A

 97% 31/32 [00:07<00:00,  4.43it/s][A[A

100% 32/32 [00:08<00:00,  4.43it/s][A[A100% 32/32 [00:08<00:00,  3.92it/s]
Meta loss on this task batch = 4.9068e-01, PNorm = 35.9060, GNorm = 0.1026

 37% 7/19 [00:56<01:41,  8.42s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.00it/s][A[A

  6% 2/32 [00:00<00:07,  4.19it/s][A[A

  9% 3/32 [00:00<00:06,  4.29it/s][A[A

 12% 4/32 [00:01<00:14,  1.89it/s][A[A

 16% 5/32 [00:02<00:11,  2.30it/s][A[A

 19% 6/32 [00:02<00:09,  2.70it/s][A[A

 22% 7/32 [00:02<00:07,  3.43it/s][A[A

 25% 8/32 [00:02<00:06,  3.62it/s][A[A

 28% 9/32 [00:02<00:05,  3.90it/s][A[A

 31% 10/32 [00:03<00:04,  4.49it/s][A[A

 34% 11/32 [00:03<00:04,  4.52it/s][A[A

 38% 12/32 [00:03<00:04,  4.59it/s][A[A

 41% 13/32 [00:03<00:03,  4.89it/s][A[A

 44% 14/32 [00:03<00:03,  5.64it/s][A[A

 47% 15/32 [00:03<00:03,  5.17it/s][A[A

 50% 16/32 [00:04<00:03,  5.19it/s][A[A

 53% 17/32 [00:04<00:03,  4.96it/s][A[A

 56% 18/32 [00:04<00:02,  4.99it/s][A[A

 59% 19/32 [00:04<00:02,  4.91it/s][A[A

 62% 20/32 [00:05<00:02,  4.99it/s][A[A

 66% 21/32 [00:05<00:02,  5.39it/s][A[A

 69% 22/32 [00:06<00:05,  1.99it/s][A[A

 72% 23/32 [00:06<00:03,  2.61it/s][A[A

 75% 24/32 [00:06<00:02,  2.98it/s][A[A

 78% 25/32 [00:06<00:02,  3.18it/s][A[A

 81% 26/32 [00:07<00:01,  3.33it/s][A[A

 84% 27/32 [00:07<00:01,  3.74it/s][A[A

 88% 28/32 [00:07<00:00,  4.13it/s][A[A

 91% 29/32 [00:07<00:00,  4.37it/s][A[A

 94% 30/32 [00:08<00:00,  4.57it/s][A[A

 97% 31/32 [00:08<00:00,  4.63it/s][A[A

100% 32/32 [00:08<00:00,  4.78it/s][A[A100% 32/32 [00:08<00:00,  3.80it/s]
Meta loss on this task batch = 3.7381e-01, PNorm = 35.9247, GNorm = 0.1309

 42% 8/19 [01:06<01:35,  8.64s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.03it/s][A[A

  6% 2/32 [00:01<00:14,  2.02it/s][A[A

  9% 3/32 [00:01<00:11,  2.48it/s][A[A

 12% 4/32 [00:01<00:09,  2.89it/s][A[A

 16% 5/32 [00:01<00:08,  3.32it/s][A[A

 19% 6/32 [00:02<00:06,  3.76it/s][A[A

 22% 7/32 [00:02<00:06,  4.11it/s][A[A

 25% 8/32 [00:02<00:05,  4.36it/s][A[A

 28% 9/32 [00:02<00:05,  4.51it/s][A[A

 31% 10/32 [00:02<00:04,  4.67it/s][A[A

 34% 11/32 [00:03<00:04,  4.76it/s][A[A

 38% 12/32 [00:03<00:04,  4.75it/s][A[A

 41% 13/32 [00:03<00:03,  4.84it/s][A[A

 44% 14/32 [00:03<00:03,  4.88it/s][A[A

 47% 15/32 [00:03<00:03,  4.76it/s][A[A

 50% 16/32 [00:04<00:03,  4.85it/s][A[A

 53% 17/32 [00:05<00:07,  1.97it/s][A[A

 56% 18/32 [00:05<00:05,  2.44it/s][A[A

 59% 19/32 [00:05<00:04,  2.90it/s][A[A

 62% 20/32 [00:05<00:03,  3.35it/s][A[A

 66% 21/32 [00:06<00:02,  3.76it/s][A[A

 69% 22/32 [00:06<00:02,  4.04it/s][A[A

 72% 23/32 [00:06<00:02,  4.44it/s][A[A

 75% 24/32 [00:06<00:01,  4.55it/s][A[A

 78% 25/32 [00:06<00:01,  4.67it/s][A[A

 81% 26/32 [00:07<00:01,  4.63it/s][A[A

 84% 27/32 [00:07<00:01,  4.70it/s][A[A

 88% 28/32 [00:07<00:00,  4.86it/s][A[A

 91% 29/32 [00:07<00:00,  5.01it/s][A[A

 94% 30/32 [00:07<00:00,  4.92it/s][A[A

 97% 31/32 [00:09<00:00,  1.96it/s][A[A

100% 32/32 [00:09<00:00,  2.41it/s][A[A100% 32/32 [00:09<00:00,  3.42it/s]
Meta loss on this task batch = 2.2235e-01, PNorm = 35.9467, GNorm = 0.1357

 47% 9/19 [01:16<01:30,  9.08s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.67it/s][A[A

  6% 2/32 [00:00<00:06,  4.92it/s][A[A

  9% 3/32 [00:00<00:05,  4.86it/s][A[A

 12% 4/32 [00:00<00:05,  4.86it/s][A[A

 16% 5/32 [00:01<00:05,  4.92it/s][A[A

 19% 6/32 [00:01<00:05,  4.92it/s][A[A

 22% 7/32 [00:01<00:05,  4.97it/s][A[A

 25% 8/32 [00:01<00:04,  4.85it/s][A[A

 28% 9/32 [00:01<00:04,  4.76it/s][A[A

 31% 10/32 [00:02<00:04,  4.88it/s][A[A

 34% 11/32 [00:02<00:04,  4.96it/s][A[A

 38% 12/32 [00:02<00:03,  5.03it/s][A[A

 41% 13/32 [00:02<00:03,  4.81it/s][A[A

 44% 14/32 [00:02<00:03,  4.93it/s][A[A

 47% 15/32 [00:04<00:08,  1.99it/s][A[A

 50% 16/32 [00:04<00:06,  2.48it/s][A[A

 53% 17/32 [00:04<00:05,  2.95it/s][A[A

 56% 18/32 [00:04<00:04,  3.36it/s][A[A

 59% 19/32 [00:04<00:03,  3.82it/s][A[A

 62% 20/32 [00:04<00:02,  4.25it/s][A[A

 66% 21/32 [00:05<00:02,  4.65it/s][A[A

 69% 22/32 [00:05<00:02,  4.76it/s][A[A

 72% 23/32 [00:05<00:01,  4.88it/s][A[A

 75% 24/32 [00:05<00:01,  5.06it/s][A[A

 78% 25/32 [00:05<00:01,  5.01it/s][A[A

 81% 26/32 [00:06<00:01,  4.58it/s][A[A

 84% 27/32 [00:06<00:01,  4.88it/s][A[A

 88% 28/32 [00:06<00:00,  4.66it/s][A[A

 91% 29/32 [00:06<00:00,  4.94it/s][A[A

 94% 30/32 [00:06<00:00,  4.63it/s][A[A

 97% 31/32 [00:07<00:00,  5.02it/s][A[A

100% 32/32 [00:08<00:00,  2.00it/s][A[A100% 32/32 [00:08<00:00,  3.83it/s]
Meta loss on this task batch = 2.3192e-01, PNorm = 35.9733, GNorm = 0.1539

 53% 10/19 [01:25<01:21,  9.10s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.86it/s][A[A

  6% 2/32 [00:00<00:06,  4.60it/s][A[A

  9% 3/32 [00:00<00:06,  4.49it/s][A[A

 12% 4/32 [00:00<00:06,  4.39it/s][A[A

 16% 5/32 [00:01<00:06,  4.38it/s][A[A

 19% 6/32 [00:01<00:05,  4.62it/s][A[A

 22% 7/32 [00:01<00:05,  4.93it/s][A[A

 25% 8/32 [00:01<00:04,  5.02it/s][A[A

 28% 9/32 [00:01<00:04,  4.78it/s][A[A

 31% 10/32 [00:02<00:04,  4.58it/s][A[A

 34% 11/32 [00:02<00:04,  4.42it/s][A[A

 38% 12/32 [00:02<00:04,  4.28it/s][A[A

 41% 13/32 [00:02<00:04,  4.19it/s][A[A

 44% 14/32 [00:03<00:04,  4.18it/s][A[A

 47% 15/32 [00:03<00:04,  4.12it/s][A[A

 50% 16/32 [00:03<00:03,  4.25it/s][A[A

 53% 17/32 [00:04<00:08,  1.85it/s][A[A

 56% 18/32 [00:05<00:06,  2.30it/s][A[A

 59% 19/32 [00:05<00:04,  2.90it/s][A[A

 62% 20/32 [00:05<00:03,  3.17it/s][A[A

 66% 21/32 [00:05<00:03,  3.42it/s][A[A

 69% 22/32 [00:05<00:02,  3.68it/s][A[A

 72% 23/32 [00:06<00:02,  3.98it/s][A[A

 75% 24/32 [00:06<00:02,  3.99it/s][A[A

 78% 25/32 [00:06<00:01,  4.29it/s][A[A

 81% 26/32 [00:06<00:01,  4.54it/s][A[A

 84% 27/32 [00:06<00:01,  4.68it/s][A[A

 88% 28/32 [00:07<00:00,  4.41it/s][A[A

 91% 29/32 [00:07<00:00,  4.78it/s][A[A

 94% 30/32 [00:08<00:01,  1.94it/s][A[A

 97% 31/32 [00:08<00:00,  2.30it/s][A[A

100% 32/32 [00:09<00:00,  2.66it/s][A[A100% 32/32 [00:09<00:00,  3.52it/s]
Meta loss on this task batch = 6.2694e-01, PNorm = 35.9978, GNorm = 0.0642

 58% 11/19 [01:35<01:14,  9.34s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.08it/s][A[A

  6% 2/32 [00:00<00:07,  4.07it/s][A[A

  9% 3/32 [00:00<00:07,  4.13it/s][A[A

 12% 4/32 [00:00<00:06,  4.13it/s][A[A

 16% 5/32 [00:01<00:06,  4.14it/s][A[A

 19% 6/32 [00:01<00:06,  4.19it/s][A[A

 22% 7/32 [00:01<00:05,  4.62it/s][A[A

 25% 8/32 [00:01<00:05,  4.56it/s][A[A

 28% 9/32 [00:01<00:04,  5.02it/s][A[A

 31% 10/32 [00:02<00:04,  4.67it/s][A[A

 34% 11/32 [00:02<00:04,  5.04it/s][A[A

 38% 12/32 [00:02<00:04,  4.69it/s][A[A

 41% 13/32 [00:02<00:03,  4.77it/s][A[A

 44% 14/32 [00:03<00:03,  4.54it/s][A[A

 47% 15/32 [00:03<00:03,  4.39it/s][A[A

 50% 16/32 [00:03<00:03,  4.73it/s][A[A

 53% 17/32 [00:04<00:07,  2.06it/s][A[A

 56% 18/32 [00:04<00:05,  2.45it/s][A[A

 59% 19/32 [00:05<00:04,  2.80it/s][A[A

 62% 20/32 [00:05<00:03,  3.11it/s][A[A

 66% 21/32 [00:05<00:03,  3.36it/s][A[A

 69% 22/32 [00:05<00:02,  3.83it/s][A[A

 72% 23/32 [00:05<00:02,  4.40it/s][A[A

 75% 24/32 [00:06<00:01,  4.56it/s][A[A

 78% 25/32 [00:06<00:01,  4.54it/s][A[A

 81% 26/32 [00:06<00:01,  4.51it/s][A[A

 84% 27/32 [00:06<00:01,  4.33it/s][A[A

 88% 28/32 [00:07<00:00,  4.22it/s][A[A

 91% 29/32 [00:07<00:00,  4.17it/s][A[A

 94% 30/32 [00:08<00:01,  1.84it/s][A[A

 97% 31/32 [00:08<00:00,  2.20it/s][A[A

100% 32/32 [00:09<00:00,  2.54it/s][A[A100% 32/32 [00:09<00:00,  3.54it/s]
Meta loss on this task batch = 5.5243e-01, PNorm = 36.0202, GNorm = 0.0342

 63% 12/19 [01:44<01:06,  9.49s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.85it/s][A[A

  6% 2/32 [00:00<00:07,  4.22it/s][A[A

  9% 3/32 [00:00<00:06,  4.63it/s][A[A

 12% 4/32 [00:00<00:05,  5.11it/s][A[A

 16% 5/32 [00:01<00:05,  4.76it/s][A[A

 19% 6/32 [00:01<00:05,  4.87it/s][A[A

 22% 7/32 [00:01<00:04,  5.22it/s][A[A

 25% 8/32 [00:01<00:04,  5.19it/s][A[A

 28% 9/32 [00:02<00:11,  2.07it/s][A[A

 31% 10/32 [00:02<00:08,  2.47it/s][A[A

 34% 11/32 [00:03<00:07,  2.79it/s][A[A

 38% 12/32 [00:03<00:06,  3.29it/s][A[A

 41% 13/32 [00:03<00:05,  3.52it/s][A[A

 44% 14/32 [00:03<00:04,  4.00it/s][A[A

 47% 15/32 [00:04<00:04,  4.03it/s][A[A

 50% 16/32 [00:04<00:03,  4.05it/s][A[A

 53% 17/32 [00:04<00:03,  4.31it/s][A[A

 56% 18/32 [00:04<00:03,  4.26it/s][A[A

 59% 19/32 [00:04<00:03,  4.27it/s][A[A

 62% 20/32 [00:05<00:02,  4.58it/s][A[A

 66% 21/32 [00:05<00:02,  4.35it/s][A[A

 69% 22/32 [00:06<00:05,  1.85it/s][A[A

 72% 23/32 [00:06<00:04,  2.21it/s][A[A

 75% 24/32 [00:07<00:03,  2.54it/s][A[A

 78% 25/32 [00:07<00:02,  2.86it/s][A[A

 81% 26/32 [00:07<00:01,  3.28it/s][A[A

 84% 27/32 [00:07<00:01,  3.71it/s][A[A

 88% 28/32 [00:07<00:00,  4.21it/s][A[A

 91% 29/32 [00:08<00:00,  4.17it/s][A[A

 94% 30/32 [00:09<00:01,  1.86it/s][A[A

 97% 31/32 [00:09<00:00,  2.27it/s][A[A

100% 32/32 [00:09<00:00,  2.60it/s][A[A100% 32/32 [00:09<00:00,  3.24it/s]
Meta loss on this task batch = 5.8036e-01, PNorm = 36.0416, GNorm = 0.0438

 68% 13/19 [01:55<00:59,  9.84s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.93it/s][A[A

  6% 2/32 [00:00<00:05,  5.08it/s][A[A

  9% 3/32 [00:00<00:05,  4.92it/s][A[A

 12% 4/32 [00:00<00:05,  4.72it/s][A[A

 16% 5/32 [00:01<00:06,  4.48it/s][A[A

 19% 6/32 [00:01<00:05,  4.71it/s][A[A

 22% 7/32 [00:01<00:05,  4.48it/s][A[A

 25% 8/32 [00:01<00:04,  4.95it/s][A[A

 28% 9/32 [00:01<00:04,  4.72it/s][A[A

 31% 10/32 [00:02<00:04,  4.92it/s][A[A

 34% 11/32 [00:02<00:04,  4.66it/s][A[A

 38% 12/32 [00:02<00:04,  4.73it/s][A[A

 41% 13/32 [00:02<00:04,  4.57it/s][A[A

 44% 14/32 [00:03<00:04,  4.45it/s][A[A

 47% 15/32 [00:04<00:09,  1.85it/s][A[A

 50% 16/32 [00:04<00:07,  2.22it/s][A[A

 53% 17/32 [00:04<00:05,  2.75it/s][A[A

 56% 18/32 [00:04<00:04,  3.16it/s][A[A

 59% 19/32 [00:05<00:03,  3.53it/s][A[A

 62% 20/32 [00:05<00:03,  3.94it/s][A[A

 66% 21/32 [00:05<00:02,  3.99it/s][A[A

 69% 22/32 [00:05<00:02,  4.01it/s][A[A

 72% 23/32 [00:06<00:02,  4.03it/s][A[A

 75% 24/32 [00:06<00:01,  4.25it/s][A[A

 78% 25/32 [00:06<00:01,  4.47it/s][A[A

 81% 26/32 [00:06<00:01,  4.33it/s][A[A

 84% 27/32 [00:07<00:02,  1.85it/s][A[A

 88% 28/32 [00:08<00:01,  2.20it/s][A[A

 91% 29/32 [00:08<00:01,  2.61it/s][A[A

 94% 30/32 [00:08<00:00,  3.00it/s][A[A

 97% 31/32 [00:08<00:00,  3.38it/s][A[A

100% 32/32 [00:09<00:00,  3.78it/s][A[A100% 32/32 [00:09<00:00,  3.55it/s]
Meta loss on this task batch = 5.7178e-01, PNorm = 36.0580, GNorm = 0.1443

 74% 14/19 [02:05<00:49,  9.83s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.30it/s][A[A

  6% 2/32 [00:00<00:06,  4.55it/s][A[A

  9% 3/32 [00:00<00:06,  4.51it/s][A[A

 12% 4/32 [00:00<00:05,  4.85it/s][A[A

 16% 5/32 [00:01<00:05,  5.00it/s][A[A

 19% 6/32 [00:01<00:05,  4.48it/s][A[A

 22% 7/32 [00:01<00:05,  4.60it/s][A[A

 25% 8/32 [00:01<00:04,  4.93it/s][A[A

 28% 9/32 [00:02<00:11,  1.93it/s][A[A

 31% 10/32 [00:03<00:08,  2.48it/s][A[A

 34% 11/32 [00:03<00:07,  2.86it/s][A[A

 38% 12/32 [00:03<00:05,  3.40it/s][A[A

 41% 13/32 [00:03<00:05,  3.77it/s][A[A

 44% 14/32 [00:03<00:04,  4.16it/s][A[A

 47% 15/32 [00:03<00:03,  4.55it/s][A[A

 50% 16/32 [00:04<00:03,  4.63it/s][A[A

 53% 17/32 [00:04<00:03,  4.71it/s][A[A

 56% 18/32 [00:04<00:02,  4.85it/s][A[A

 59% 19/32 [00:04<00:02,  5.23it/s][A[A

 62% 20/32 [00:04<00:02,  5.02it/s][A[A

 66% 21/32 [00:05<00:02,  5.21it/s][A[A

 69% 22/32 [00:05<00:01,  5.18it/s][A[A

 72% 23/32 [00:06<00:04,  1.94it/s][A[A

 75% 24/32 [00:06<00:03,  2.36it/s][A[A

 78% 25/32 [00:07<00:02,  2.72it/s][A[A

 81% 26/32 [00:07<00:01,  3.02it/s][A[A

 84% 27/32 [00:07<00:01,  3.37it/s][A[A

 88% 28/32 [00:07<00:01,  3.58it/s][A[A

 91% 29/32 [00:07<00:00,  3.87it/s][A[A

 94% 30/32 [00:08<00:00,  3.97it/s][A[A

 97% 31/32 [00:08<00:00,  4.32it/s][A[A

100% 32/32 [00:08<00:00,  4.67it/s][A[A100% 32/32 [00:08<00:00,  3.74it/s]
Meta loss on this task batch = 5.1130e-01, PNorm = 36.0652, GNorm = 0.2078

 79% 15/19 [02:14<00:38,  9.67s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.02it/s][A[A

  6% 2/32 [00:00<00:07,  4.28it/s][A[A

  9% 3/32 [00:00<00:06,  4.51it/s][A[A

 12% 4/32 [00:00<00:05,  4.72it/s][A[A

 16% 5/32 [00:00<00:05,  5.27it/s][A[A

 19% 6/32 [00:02<00:13,  1.98it/s][A[A

 22% 7/32 [00:02<00:10,  2.41it/s][A[A

 25% 8/32 [00:02<00:08,  2.74it/s][A[A

 28% 9/32 [00:02<00:07,  3.19it/s][A[A

 31% 10/32 [00:03<00:06,  3.54it/s][A[A

 34% 11/32 [00:03<00:05,  3.94it/s][A[A

 38% 12/32 [00:03<00:04,  4.19it/s][A[A

 41% 13/32 [00:03<00:04,  4.48it/s][A[A

 44% 14/32 [00:03<00:03,  4.74it/s][A[A

 47% 15/32 [00:04<00:03,  4.69it/s][A[A

 50% 16/32 [00:04<00:03,  5.06it/s][A[A

 53% 17/32 [00:04<00:03,  4.88it/s][A[A

 56% 18/32 [00:04<00:02,  5.18it/s][A[A

 59% 19/32 [00:04<00:02,  5.09it/s][A[A

 62% 20/32 [00:05<00:02,  5.01it/s][A[A

 66% 21/32 [00:06<00:05,  2.00it/s][A[A

 69% 22/32 [00:06<00:04,  2.43it/s][A[A

 72% 23/32 [00:06<00:03,  2.76it/s][A[A

 75% 24/32 [00:06<00:02,  3.05it/s][A[A

 78% 25/32 [00:07<00:01,  3.56it/s][A[A

 81% 26/32 [00:07<00:01,  3.85it/s][A[A

 84% 27/32 [00:07<00:01,  3.94it/s][A[A

 88% 28/32 [00:07<00:00,  4.11it/s][A[A

 91% 29/32 [00:07<00:00,  4.30it/s][A[A

 94% 30/32 [00:08<00:00,  4.27it/s][A[A

 97% 31/32 [00:08<00:00,  4.26it/s][A[A

100% 32/32 [00:08<00:00,  4.42it/s][A[A100% 32/32 [00:08<00:00,  3.71it/s]
Meta loss on this task batch = 5.8928e-01, PNorm = 36.0654, GNorm = 0.2088

 84% 16/19 [02:24<00:28,  9.59s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.74it/s][A[A

  6% 2/32 [00:00<00:07,  3.86it/s][A[A

  9% 3/32 [00:01<00:16,  1.81it/s][A[A

 12% 4/32 [00:01<00:12,  2.22it/s][A[A

 16% 5/32 [00:02<00:09,  2.75it/s][A[A

 19% 6/32 [00:02<00:07,  3.34it/s][A[A

 22% 7/32 [00:02<00:06,  3.99it/s][A[A

 25% 8/32 [00:02<00:06,  4.00it/s][A[A

 28% 9/32 [00:02<00:05,  4.10it/s][A[A

 31% 10/32 [00:03<00:04,  4.79it/s][A[A

 34% 11/32 [00:03<00:04,  4.88it/s][A[A

 38% 12/32 [00:03<00:04,  4.92it/s][A[A

 41% 13/32 [00:03<00:03,  5.46it/s][A[A

 44% 14/32 [00:03<00:03,  5.43it/s][A[A

 47% 15/32 [00:03<00:03,  5.27it/s][A[A

 50% 16/32 [00:04<00:03,  5.12it/s][A[A

 53% 17/32 [00:04<00:02,  5.17it/s][A[A

 56% 18/32 [00:04<00:02,  5.00it/s][A[A

 59% 19/32 [00:05<00:06,  1.96it/s][A[A

 62% 20/32 [00:05<00:05,  2.37it/s][A[A

 66% 21/32 [00:06<00:03,  2.82it/s][A[A

 69% 22/32 [00:06<00:03,  3.29it/s][A[A

 72% 23/32 [00:06<00:02,  3.61it/s][A[A

 75% 24/32 [00:06<00:02,  3.80it/s][A[A

 78% 25/32 [00:07<00:01,  4.15it/s][A[A

 81% 26/32 [00:07<00:01,  4.47it/s][A[A

 84% 27/32 [00:07<00:01,  4.72it/s][A[A

 88% 28/32 [00:07<00:00,  4.82it/s][A[A

 91% 29/32 [00:07<00:00,  5.21it/s][A[A

 94% 30/32 [00:07<00:00,  4.73it/s][A[A

 97% 31/32 [00:08<00:00,  4.94it/s][A[A

100% 32/32 [00:08<00:00,  5.01it/s][A[A100% 32/32 [00:08<00:00,  3.83it/s]
Meta loss on this task batch = 4.6844e-01, PNorm = 36.0643, GNorm = 0.0960

 89% 17/19 [02:33<00:18,  9.44s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.16it/s][A[A

  6% 2/32 [00:00<00:05,  5.06it/s][A[A

  9% 3/32 [00:01<00:14,  1.99it/s][A[A

 12% 4/32 [00:01<00:11,  2.40it/s][A[A

 16% 5/32 [00:02<00:09,  2.76it/s][A[A

 19% 6/32 [00:02<00:08,  3.12it/s][A[A

 22% 7/32 [00:02<00:07,  3.27it/s][A[A

 25% 8/32 [00:02<00:07,  3.40it/s][A[A

 28% 9/32 [00:03<00:06,  3.73it/s][A[A

 31% 10/32 [00:03<00:05,  3.87it/s][A[A

 34% 11/32 [00:03<00:04,  4.28it/s][A[A

 38% 12/32 [00:03<00:04,  4.56it/s][A[A

 41% 13/32 [00:03<00:04,  4.23it/s][A[A

 44% 14/32 [00:04<00:03,  4.76it/s][A[A

 47% 15/32 [00:05<00:08,  1.91it/s][A[A

 50% 16/32 [00:05<00:06,  2.36it/s][A[A

 53% 17/32 [00:05<00:05,  2.96it/s][A[A

 56% 18/32 [00:05<00:04,  3.36it/s][A[A

 59% 19/32 [00:06<00:03,  3.62it/s][A[A

 62% 20/32 [00:06<00:03,  3.77it/s][A[A

 66% 21/32 [00:06<00:02,  4.20it/s][A[A

 69% 22/32 [00:06<00:02,  4.75it/s][A[A

 72% 23/32 [00:06<00:02,  4.26it/s][A[A

 75% 24/32 [00:07<00:01,  4.06it/s][A[A

 78% 25/32 [00:07<00:01,  4.52it/s][A[A

 81% 26/32 [00:08<00:03,  1.93it/s][A[A

 84% 27/32 [00:08<00:02,  2.35it/s][A[A

 88% 28/32 [00:09<00:01,  2.69it/s][A[A

 91% 29/32 [00:09<00:01,  3.00it/s][A[A

 94% 30/32 [00:09<00:00,  3.46it/s][A[A

 97% 31/32 [00:09<00:00,  3.90it/s][A[A

100% 32/32 [00:09<00:00,  3.92it/s][A[A100% 32/32 [00:09<00:00,  3.24it/s]
Meta loss on this task batch = 5.3074e-01, PNorm = 36.0606, GNorm = 0.1240

 95% 18/19 [02:43<00:09,  9.80s/it][A

  0% 0/23 [00:00<?, ?it/s][A[A

  4% 1/23 [00:00<00:05,  4.02it/s][A[A

  9% 2/23 [00:00<00:04,  4.43it/s][A[A

 13% 3/23 [00:00<00:04,  4.54it/s][A[A

 17% 4/23 [00:00<00:03,  4.87it/s][A[A

 26% 6/23 [00:02<00:05,  2.97it/s][A[A

 30% 7/23 [00:02<00:04,  3.42it/s][A[A

 35% 8/23 [00:02<00:03,  3.83it/s][A[A

 39% 9/23 [00:02<00:03,  4.49it/s][A[A

 43% 10/23 [00:02<00:03,  4.29it/s][A[A

 48% 11/23 [00:03<00:02,  4.68it/s][A[A

 52% 12/23 [00:03<00:02,  4.95it/s][A[A

 57% 13/23 [00:03<00:01,  5.42it/s][A[A

 61% 14/23 [00:03<00:01,  5.84it/s][A[A

 65% 15/23 [00:03<00:01,  4.25it/s][A[A

 70% 16/23 [00:04<00:01,  4.00it/s][A[A

 74% 17/23 [00:04<00:01,  4.44it/s][A[A

 78% 18/23 [00:05<00:02,  1.85it/s][A[A

 83% 19/23 [00:05<00:01,  2.32it/s][A[A

 87% 20/23 [00:05<00:01,  2.94it/s][A[A

 91% 21/23 [00:06<00:00,  3.43it/s][A[A

 96% 22/23 [00:06<00:00,  4.12it/s][A[A

100% 23/23 [00:06<00:00,  4.66it/s][A[A100% 23/23 [00:06<00:00,  3.62it/s]
Meta loss on this task batch = 4.3311e-01, PNorm = 36.0564, GNorm = 0.0949

100% 19/19 [02:50<00:00,  8.92s/it][A100% 19/19 [02:50<00:00,  8.99s/it]
Took 170.78522157669067 seconds to complete one epoch of meta training

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/20 [00:00<?, ?it/s][A[A


  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 29.73it/s]


  5% 1/20 [00:00<00:02,  7.68it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A


100% 2/2 [00:00<00:00, 19.59it/s][A[A[A100% 2/2 [00:00<00:00, 19.53it/s]


 10% 2/20 [00:00<00:03,  5.81it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 16.81it/s][A[A[A100% 3/3 [00:00<00:00, 18.91it/s]


 15% 3/20 [00:00<00:03,  4.70it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 14.93it/s]


 20% 4/20 [00:00<00:03,  4.70it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 14.91it/s][A[A[A100% 4/4 [00:00<00:00, 17.14it/s]


 25% 5/20 [00:01<00:03,  3.79it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.08it/s][A[A[A


100% 4/4 [00:00<00:00, 14.55it/s][A[A[A100% 4/4 [00:00<00:00, 15.72it/s]


 30% 6/20 [00:01<00:04,  3.28it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.87it/s][A[A[A100% 4/4 [00:00<00:00, 20.48it/s]


 35% 7/20 [00:02<00:04,  3.09it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:01<00:01,  1.81it/s][A[A[A


100% 4/4 [00:01<00:00,  2.47it/s][A[A[A100% 4/4 [00:01<00:00,  3.26it/s]


 40% 8/20 [00:03<00:07,  1.56it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.16it/s][A[A[A100% 4/4 [00:00<00:00, 21.53it/s]


 45% 9/20 [00:03<00:06,  1.79it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.21it/s][A[A[A


100% 4/4 [00:00<00:00, 19.04it/s][A[A[A100% 4/4 [00:00<00:00, 18.90it/s]


 50% 10/20 [00:04<00:05,  1.97it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 15.97it/s][A[A[A100% 4/4 [00:00<00:00, 18.20it/s]


 55% 11/20 [00:04<00:04,  2.09it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 17.73it/s][A[A[A100% 4/4 [00:00<00:00, 21.01it/s]


 60% 12/20 [00:04<00:03,  2.25it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 13.92it/s][A[A[A100% 3/3 [00:00<00:00, 13.25it/s]


 65% 13/20 [00:05<00:03,  2.32it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 12.81it/s][A[A[A100% 3/3 [00:00<00:00, 12.49it/s]


 70% 14/20 [00:06<00:04,  1.37it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.23it/s][A[A[A100% 4/4 [00:00<00:00, 17.35it/s]


 75% 15/20 [00:07<00:03,  1.57it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 13.71it/s][A[A[A100% 3/3 [00:00<00:00, 17.01it/s]


 80% 16/20 [00:07<00:02,  1.82it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 11.91it/s][A[A[A100% 3/3 [00:00<00:00, 14.64it/s]


 85% 17/20 [00:07<00:01,  1.98it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 21.57it/s]


 90% 18/20 [00:08<00:00,  2.34it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.57it/s][A[A[A100% 3/3 [00:00<00:00, 18.56it/s]


 95% 19/20 [00:09<00:00,  1.43it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 12.37it/s]


100% 20/20 [00:09<00:00,  1.77it/s][A[A100% 20/20 [00:09<00:00,  2.04it/s]

100% 1/1 [00:09<00:00,  9.82s/it][A100% 1/1 [00:09<00:00,  9.82s/it]
Took 180.60297060012817 seconds to complete one epoch of meta training and validating
Meta validation score auc = 0.591351
 27% 8/30 [25:43<1:10:10, 191.39s/it]Epoch 8

  0% 0/19 [00:00<?, ?it/s][A

  0% 0/32 [00:00<?, ?it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  3% 1/32 [00:00<00:06,  4.50it/s][A[A

  6% 2/32 [00:00<00:07,  4.27it/s][A[A

  9% 3/32 [00:00<00:06,  4.33it/s][A[A

 12% 4/32 [00:00<00:05,  4.83it/s][A[A

 16% 5/32 [00:00<00:05,  5.35it/s][A[A

 19% 6/32 [00:01<00:04,  5.36it/s][A[A

 22% 7/32 [00:01<00:04,  5.50it/s][A[A

 25% 8/32 [00:01<00:04,  5.33it/s][A[A

 28% 9/32 [00:01<00:03,  5.95it/s][A[A

 31% 10/32 [00:01<00:03,  6.36it/s][A[A

 34% 11/32 [00:02<00:04,  5.17it/s][A[A

 38% 12/32 [00:02<00:04,  4.64it/s][A[A

 41% 13/32 [00:03<00:10,  1.86it/s][A[A

 44% 14/32 [00:03<00:07,  2.28it/s][A[A

 47% 15/32 [00:04<00:06,  2.79it/s][A[A

 50% 16/32 [00:04<00:05,  3.18it/s][A[A

 53% 17/32 [00:04<00:04,  3.57it/s][A[A

 56% 18/32 [00:04<00:03,  4.07it/s][A[A

 59% 19/32 [00:04<00:02,  4.58it/s][A[A

 62% 20/32 [00:04<00:02,  4.50it/s][A[A

 66% 21/32 [00:05<00:02,  5.28it/s][A[A

 69% 22/32 [00:05<00:01,  5.14it/s][A[A

 72% 23/32 [00:05<00:01,  5.05it/s][A[A

 75% 24/32 [00:05<00:01,  5.05it/s][A[A

 78% 25/32 [00:05<00:01,  5.17it/s][A[A

 81% 26/32 [00:06<00:01,  5.03it/s][A[A

 84% 27/32 [00:07<00:02,  1.96it/s][A[A

 88% 28/32 [00:07<00:01,  2.45it/s][A[A

 91% 29/32 [00:07<00:01,  2.99it/s][A[A

 94% 30/32 [00:07<00:00,  3.75it/s][A[A

 97% 31/32 [00:08<00:00,  3.81it/s][A[A

100% 32/32 [00:08<00:00,  4.24it/s][A[A100% 32/32 [00:08<00:00,  3.90it/s]
Meta loss on this task batch = 5.1720e-01, PNorm = 36.0535, GNorm = 0.0626

  5% 1/19 [00:08<02:40,  8.92s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:03,  8.10it/s][A[A

  6% 2/32 [00:00<00:03,  7.62it/s][A[A

  9% 3/32 [00:00<00:04,  6.85it/s][A[A

 12% 4/32 [00:00<00:04,  5.82it/s][A[A

 16% 5/32 [00:00<00:04,  5.44it/s][A[A

 19% 6/32 [00:01<00:04,  5.72it/s][A[A

 22% 7/32 [00:01<00:04,  6.16it/s][A[A

 25% 8/32 [00:01<00:03,  6.15it/s][A[A

 28% 9/32 [00:01<00:03,  6.12it/s][A[A

 31% 10/32 [00:01<00:03,  6.42it/s][A[A

 34% 11/32 [00:01<00:03,  5.97it/s][A[A

 38% 12/32 [00:02<00:03,  5.71it/s][A[A

 41% 13/32 [00:02<00:03,  5.95it/s][A[A

 44% 14/32 [00:02<00:03,  5.43it/s][A[A

 47% 15/32 [00:02<00:03,  5.18it/s][A[A

 50% 16/32 [00:03<00:07,  2.01it/s][A[A

 53% 17/32 [00:04<00:06,  2.43it/s][A[A

 56% 18/32 [00:04<00:05,  2.78it/s][A[A

 59% 19/32 [00:04<00:04,  3.23it/s][A[A

 62% 20/32 [00:04<00:03,  3.78it/s][A[A

 66% 21/32 [00:04<00:02,  4.00it/s][A[A

 69% 22/32 [00:04<00:02,  4.55it/s][A[A

 72% 23/32 [00:05<00:01,  4.99it/s][A[A

 75% 24/32 [00:05<00:01,  5.40it/s][A[A

 78% 25/32 [00:05<00:01,  5.33it/s][A[A

 81% 26/32 [00:05<00:01,  5.06it/s][A[A

 84% 27/32 [00:05<00:00,  5.25it/s][A[A

 88% 28/32 [00:06<00:00,  4.94it/s][A[A

 91% 29/32 [00:06<00:00,  5.55it/s][A[A

 94% 30/32 [00:06<00:00,  5.52it/s][A[A

 97% 31/32 [00:07<00:00,  2.04it/s][A[A

100% 32/32 [00:07<00:00,  2.58it/s][A[A100% 32/32 [00:07<00:00,  4.11it/s]
Meta loss on this task batch = 5.5396e-01, PNorm = 36.0469, GNorm = 0.1805

 11% 2/19 [00:17<02:29,  8.80s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.97it/s][A[A

  6% 2/32 [00:00<00:05,  5.41it/s][A[A

  9% 3/32 [00:00<00:05,  4.96it/s][A[A

 12% 4/32 [00:00<00:05,  4.97it/s][A[A

 16% 5/32 [00:01<00:05,  5.19it/s][A[A

 19% 6/32 [00:01<00:05,  4.92it/s][A[A

 22% 7/32 [00:01<00:05,  4.78it/s][A[A

 25% 8/32 [00:01<00:04,  5.21it/s][A[A

 28% 9/32 [00:01<00:04,  5.59it/s][A[A

 31% 10/32 [00:01<00:04,  5.12it/s][A[A

 34% 11/32 [00:02<00:04,  4.75it/s][A[A

 38% 12/32 [00:02<00:04,  4.83it/s][A[A

 41% 13/32 [00:03<00:09,  2.03it/s][A[A

 44% 14/32 [00:03<00:07,  2.44it/s][A[A

 47% 15/32 [00:04<00:06,  2.82it/s][A[A

 50% 16/32 [00:04<00:04,  3.48it/s][A[A

 53% 17/32 [00:04<00:04,  3.71it/s][A[A

 56% 18/32 [00:04<00:03,  4.29it/s][A[A

 59% 19/32 [00:04<00:02,  4.74it/s][A[A

 62% 20/32 [00:04<00:02,  5.21it/s][A[A

 66% 21/32 [00:04<00:01,  5.78it/s][A[A

 69% 22/32 [00:05<00:01,  6.07it/s][A[A

 72% 23/32 [00:05<00:01,  5.85it/s][A[A

 75% 24/32 [00:05<00:01,  5.28it/s][A[A

 78% 25/32 [00:05<00:01,  5.29it/s][A[A

 81% 26/32 [00:05<00:01,  5.36it/s][A[A

 84% 27/32 [00:06<00:00,  5.66it/s][A[A

 88% 28/32 [00:06<00:00,  5.76it/s][A[A

 91% 29/32 [00:06<00:00,  5.64it/s][A[A

 94% 30/32 [00:06<00:00,  5.03it/s][A[A

 97% 31/32 [00:07<00:00,  2.11it/s][A[A

100% 32/32 [00:08<00:00,  2.52it/s][A[A100% 32/32 [00:08<00:00,  4.00it/s]
Meta loss on this task batch = 5.2095e-01, PNorm = 36.0431, GNorm = 0.0174

 16% 3/19 [00:26<02:20,  8.77s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.11it/s][A[A

  6% 2/32 [00:00<00:07,  4.22it/s][A[A

  9% 3/32 [00:00<00:06,  4.51it/s][A[A

 12% 4/32 [00:00<00:05,  4.80it/s][A[A

 16% 5/32 [00:01<00:05,  4.78it/s][A[A

 19% 6/32 [00:01<00:05,  4.58it/s][A[A

 22% 7/32 [00:01<00:04,  5.25it/s][A[A

 25% 8/32 [00:01<00:05,  4.78it/s][A[A

 28% 9/32 [00:01<00:04,  5.33it/s][A[A

 31% 10/32 [00:02<00:04,  5.01it/s][A[A

 34% 11/32 [00:02<00:03,  5.45it/s][A[A

 38% 12/32 [00:02<00:03,  5.91it/s][A[A

 41% 13/32 [00:03<00:09,  2.10it/s][A[A

 44% 14/32 [00:03<00:07,  2.52it/s][A[A

 47% 15/32 [00:03<00:05,  3.12it/s][A[A

 50% 16/32 [00:04<00:04,  3.48it/s][A[A

 53% 17/32 [00:04<00:03,  3.84it/s][A[A

 56% 18/32 [00:04<00:03,  4.51it/s][A[A

 59% 19/32 [00:04<00:02,  5.09it/s][A[A

 62% 20/32 [00:04<00:02,  4.95it/s][A[A

 66% 21/32 [00:04<00:02,  4.75it/s][A[A

 69% 22/32 [00:05<00:02,  4.91it/s][A[A

 72% 23/32 [00:05<00:02,  4.47it/s][A[A

 75% 24/32 [00:05<00:01,  4.43it/s][A[A

 78% 25/32 [00:05<00:01,  5.07it/s][A[A

 81% 26/32 [00:06<00:02,  2.10it/s][A[A

 84% 27/32 [00:07<00:01,  2.52it/s][A[A

 88% 28/32 [00:07<00:01,  2.97it/s][A[A

 91% 29/32 [00:07<00:00,  3.62it/s][A[A

 94% 30/32 [00:07<00:00,  3.83it/s][A[A

 97% 31/32 [00:07<00:00,  4.45it/s][A[A

100% 32/32 [00:08<00:00,  4.41it/s][A[A100% 32/32 [00:08<00:00,  3.97it/s]
Meta loss on this task batch = 5.8940e-01, PNorm = 36.0404, GNorm = 0.0362

 21% 4/19 [00:34<02:11,  8.77s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.50it/s][A[A

  6% 2/32 [00:00<00:05,  5.14it/s][A[A

  9% 3/32 [00:00<00:05,  4.86it/s][A[A

 12% 4/32 [00:00<00:06,  4.52it/s][A[A

 16% 5/32 [00:01<00:05,  4.53it/s][A[A

 19% 6/32 [00:01<00:05,  4.91it/s][A[A

 22% 7/32 [00:01<00:04,  5.18it/s][A[A

 25% 8/32 [00:01<00:04,  5.65it/s][A[A

 28% 9/32 [00:01<00:04,  5.29it/s][A[A

 31% 10/32 [00:01<00:03,  5.59it/s][A[A

 34% 11/32 [00:03<00:10,  2.07it/s][A[A

 38% 12/32 [00:03<00:08,  2.49it/s][A[A

 41% 13/32 [00:03<00:06,  2.84it/s][A[A

 44% 14/32 [00:03<00:05,  3.44it/s][A[A

 47% 15/32 [00:03<00:04,  3.80it/s][A[A

 50% 16/32 [00:04<00:03,  4.30it/s][A[A

 53% 17/32 [00:04<00:03,  4.28it/s][A[A

 56% 18/32 [00:04<00:03,  4.46it/s][A[A

 59% 19/32 [00:04<00:03,  4.29it/s][A[A

 62% 20/32 [00:04<00:02,  4.28it/s][A[A

 66% 21/32 [00:05<00:02,  4.22it/s][A[A

 69% 22/32 [00:05<00:02,  4.54it/s][A[A

 72% 23/32 [00:05<00:01,  4.84it/s][A[A

 75% 24/32 [00:06<00:04,  1.91it/s][A[A

 78% 25/32 [00:07<00:03,  2.33it/s][A[A

 81% 26/32 [00:07<00:02,  2.92it/s][A[A

 84% 27/32 [00:07<00:01,  3.39it/s][A[A

 88% 28/32 [00:07<00:01,  3.97it/s][A[A

 91% 29/32 [00:07<00:00,  4.06it/s][A[A

 94% 30/32 [00:07<00:00,  4.64it/s][A[A

 97% 31/32 [00:08<00:00,  4.80it/s][A[A

100% 32/32 [00:08<00:00,  4.65it/s][A[A100% 32/32 [00:08<00:00,  3.84it/s]
Meta loss on this task batch = 5.1967e-01, PNorm = 36.0398, GNorm = 0.0477

 26% 5/19 [00:43<02:04,  8.87s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  6.44it/s][A[A

  6% 2/32 [00:00<00:05,  5.93it/s][A[A

  9% 3/32 [00:00<00:04,  6.15it/s][A[A

 12% 4/32 [00:00<00:05,  5.55it/s][A[A

 16% 5/32 [00:01<00:13,  2.04it/s][A[A

 19% 6/32 [00:02<00:10,  2.39it/s][A[A

 22% 7/32 [00:02<00:08,  2.87it/s][A[A

 25% 8/32 [00:02<00:07,  3.38it/s][A[A

 28% 9/32 [00:02<00:06,  3.62it/s][A[A

 31% 10/32 [00:03<00:05,  3.72it/s][A[A

 34% 11/32 [00:03<00:05,  3.79it/s][A[A

 38% 12/32 [00:03<00:05,  3.96it/s][A[A

 41% 13/32 [00:03<00:04,  4.51it/s][A[A

 44% 14/32 [00:03<00:04,  4.43it/s][A[A

 47% 15/32 [00:04<00:03,  4.46it/s][A[A

 50% 16/32 [00:05<00:08,  1.89it/s][A[A

 53% 17/32 [00:05<00:06,  2.42it/s][A[A

 56% 18/32 [00:05<00:04,  2.82it/s][A[A

 59% 19/32 [00:05<00:03,  3.46it/s][A[A

 62% 20/32 [00:06<00:03,  3.82it/s][A[A

 66% 21/32 [00:06<00:02,  4.12it/s][A[A

 69% 22/32 [00:06<00:02,  4.29it/s][A[A

 72% 23/32 [00:06<00:02,  4.34it/s][A[A

 75% 24/32 [00:06<00:01,  4.56it/s][A[A

 78% 25/32 [00:07<00:01,  4.47it/s][A[A

 81% 26/32 [00:07<00:01,  4.99it/s][A[A

 84% 27/32 [00:07<00:01,  4.67it/s][A[A

 88% 28/32 [00:07<00:00,  4.63it/s][A[A

 91% 29/32 [00:07<00:00,  4.53it/s][A[A

 94% 30/32 [00:09<00:01,  1.97it/s][A[A

 97% 31/32 [00:09<00:00,  2.40it/s][A[A

100% 32/32 [00:09<00:00,  2.87it/s][A[A100% 32/32 [00:09<00:00,  3.36it/s]
Meta loss on this task batch = 4.6188e-01, PNorm = 36.0428, GNorm = 0.1560

 32% 6/19 [00:54<02:00,  9.29s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.42it/s][A[A

  6% 2/32 [00:00<00:05,  5.40it/s][A[A

  9% 3/32 [00:00<00:05,  5.15it/s][A[A

 12% 4/32 [00:00<00:05,  4.93it/s][A[A

 16% 5/32 [00:00<00:04,  5.46it/s][A[A

 19% 6/32 [00:01<00:04,  5.37it/s][A[A

 22% 7/32 [00:01<00:05,  4.93it/s][A[A

 25% 8/32 [00:01<00:04,  5.13it/s][A[A

 28% 9/32 [00:01<00:04,  5.25it/s][A[A

 31% 10/32 [00:01<00:04,  5.02it/s][A[A

 34% 11/32 [00:02<00:04,  4.84it/s][A[A

 38% 12/32 [00:03<00:10,  1.95it/s][A[A

 41% 13/32 [00:03<00:08,  2.32it/s][A[A

 44% 14/32 [00:03<00:06,  2.87it/s][A[A

 47% 15/32 [00:04<00:05,  3.18it/s][A[A

 50% 16/32 [00:04<00:04,  3.42it/s][A[A

 53% 17/32 [00:04<00:03,  3.89it/s][A[A

 56% 18/32 [00:04<00:03,  3.95it/s][A[A

 59% 19/32 [00:04<00:02,  4.33it/s][A[A

 62% 20/32 [00:05<00:02,  4.26it/s][A[A

 66% 21/32 [00:05<00:02,  4.25it/s][A[A

 69% 22/32 [00:05<00:02,  4.44it/s][A[A

 72% 23/32 [00:05<00:02,  4.41it/s][A[A

 75% 24/32 [00:06<00:01,  4.44it/s][A[A

 78% 25/32 [00:06<00:01,  4.75it/s][A[A

 81% 26/32 [00:07<00:02,  2.01it/s][A[A

 84% 27/32 [00:07<00:02,  2.39it/s][A[A

 88% 28/32 [00:07<00:01,  2.76it/s][A[A

 91% 29/32 [00:08<00:00,  3.16it/s][A[A

 94% 30/32 [00:08<00:00,  3.56it/s][A[A

 97% 31/32 [00:08<00:00,  4.40it/s][A[A

100% 32/32 [00:08<00:00,  4.37it/s][A[A100% 32/32 [00:08<00:00,  3.74it/s]
Meta loss on this task batch = 4.9454e-01, PNorm = 36.0484, GNorm = 0.1326

 37% 7/19 [01:03<01:51,  9.31s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.08it/s][A[A

  6% 2/32 [00:00<00:07,  3.99it/s][A[A

  9% 3/32 [00:01<00:15,  1.82it/s][A[A

 12% 4/32 [00:02<00:13,  2.15it/s][A[A

 16% 5/32 [00:02<00:10,  2.54it/s][A[A

 19% 6/32 [00:02<00:09,  2.85it/s][A[A

 22% 7/32 [00:02<00:07,  3.23it/s][A[A

 25% 8/32 [00:02<00:07,  3.39it/s][A[A

 28% 9/32 [00:03<00:06,  3.57it/s][A[A

 31% 10/32 [00:03<00:05,  3.74it/s][A[A

 34% 11/32 [00:03<00:04,  4.36it/s][A[A

 38% 12/32 [00:03<00:04,  4.46it/s][A[A

 41% 13/32 [00:04<00:04,  4.64it/s][A[A

 44% 14/32 [00:04<00:03,  4.61it/s][A[A

 47% 15/32 [00:04<00:03,  4.90it/s][A[A

 50% 16/32 [00:04<00:03,  4.78it/s][A[A

 53% 17/32 [00:05<00:07,  1.90it/s][A[A

 56% 18/32 [00:06<00:06,  2.31it/s][A[A

 59% 19/32 [00:06<00:04,  2.63it/s][A[A

 62% 20/32 [00:06<00:03,  3.14it/s][A[A

 66% 21/32 [00:06<00:03,  3.39it/s][A[A

 69% 22/32 [00:06<00:02,  3.86it/s][A[A

 72% 23/32 [00:07<00:02,  4.01it/s][A[A

 75% 24/32 [00:07<00:02,  3.98it/s][A[A

 78% 25/32 [00:07<00:01,  4.07it/s][A[A

 81% 26/32 [00:07<00:01,  4.32it/s][A[A

 84% 27/32 [00:08<00:01,  4.62it/s][A[A

 88% 28/32 [00:08<00:00,  4.84it/s][A[A

 91% 29/32 [00:09<00:01,  2.00it/s][A[A

 94% 30/32 [00:09<00:00,  2.45it/s][A[A

 97% 31/32 [00:09<00:00,  2.89it/s][A[A

100% 32/32 [00:10<00:00,  3.31it/s][A[A100% 32/32 [00:10<00:00,  3.20it/s]
Meta loss on this task batch = 3.6673e-01, PNorm = 36.0621, GNorm = 0.2552

 42% 8/19 [01:14<01:47,  9.75s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.95it/s][A[A

  6% 2/32 [00:00<00:05,  5.04it/s][A[A

  9% 3/32 [00:00<00:05,  4.95it/s][A[A

 12% 4/32 [00:00<00:05,  5.01it/s][A[A

 16% 5/32 [00:00<00:05,  5.13it/s][A[A

 19% 6/32 [00:01<00:05,  5.12it/s][A[A

 22% 7/32 [00:01<00:04,  5.08it/s][A[A

 25% 8/32 [00:01<00:04,  5.06it/s][A[A

 28% 9/32 [00:01<00:04,  5.20it/s][A[A

 31% 10/32 [00:01<00:04,  5.16it/s][A[A

 34% 11/32 [00:03<00:10,  2.02it/s][A[A

 38% 12/32 [00:03<00:08,  2.47it/s][A[A

 41% 13/32 [00:03<00:06,  2.90it/s][A[A

 44% 14/32 [00:03<00:05,  3.33it/s][A[A

 47% 15/32 [00:03<00:04,  3.76it/s][A[A

 50% 16/32 [00:04<00:03,  4.12it/s][A[A

 53% 17/32 [00:04<00:03,  4.40it/s][A[A

 56% 18/32 [00:04<00:03,  4.63it/s][A[A

 59% 19/32 [00:04<00:02,  4.75it/s][A[A

 62% 20/32 [00:04<00:02,  4.78it/s][A[A

 66% 21/32 [00:05<00:02,  4.82it/s][A[A

 69% 22/32 [00:05<00:02,  4.91it/s][A[A

 72% 23/32 [00:05<00:01,  4.73it/s][A[A

 75% 24/32 [00:05<00:01,  5.05it/s][A[A

 78% 25/32 [00:06<00:03,  1.98it/s][A[A

 81% 26/32 [00:07<00:02,  2.46it/s][A[A

 84% 27/32 [00:07<00:01,  2.93it/s][A[A

 88% 28/32 [00:07<00:01,  3.38it/s][A[A

 91% 29/32 [00:07<00:00,  3.76it/s][A[A

 94% 30/32 [00:07<00:00,  4.13it/s][A[A

 97% 31/32 [00:08<00:00,  4.47it/s][A[A

100% 32/32 [00:08<00:00,  4.72it/s][A[A100% 32/32 [00:08<00:00,  3.89it/s]
Meta loss on this task batch = 1.4940e-01, PNorm = 36.0910, GNorm = 0.4946

 47% 9/19 [01:23<01:35,  9.54s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.20it/s][A[A

  6% 2/32 [00:00<00:05,  5.10it/s][A[A

  9% 3/32 [00:00<00:05,  5.19it/s][A[A

 12% 4/32 [00:00<00:05,  5.35it/s][A[A

 16% 5/32 [00:00<00:05,  5.35it/s][A[A

 19% 6/32 [00:01<00:04,  5.38it/s][A[A

 22% 7/32 [00:01<00:04,  5.42it/s][A[A

 25% 8/32 [00:01<00:04,  5.22it/s][A[A

 28% 9/32 [00:01<00:04,  5.40it/s][A[A

 31% 10/32 [00:02<00:10,  2.04it/s][A[A

 34% 11/32 [00:03<00:08,  2.49it/s][A[A

 38% 12/32 [00:03<00:06,  2.93it/s][A[A

 41% 13/32 [00:03<00:05,  3.44it/s][A[A

 44% 14/32 [00:03<00:04,  3.83it/s][A[A

 47% 15/32 [00:03<00:04,  4.11it/s][A[A

 50% 16/32 [00:04<00:03,  4.09it/s][A[A

 53% 17/32 [00:04<00:03,  4.31it/s][A[A

 56% 18/32 [00:04<00:03,  4.58it/s][A[A

 59% 19/32 [00:04<00:02,  4.53it/s][A[A

 62% 20/32 [00:04<00:02,  4.80it/s][A[A

 66% 21/32 [00:05<00:02,  4.85it/s][A[A

 69% 22/32 [00:05<00:01,  5.05it/s][A[A

 72% 23/32 [00:05<00:01,  5.13it/s][A[A

 75% 24/32 [00:05<00:01,  5.24it/s][A[A

 78% 25/32 [00:05<00:01,  5.12it/s][A[A

 81% 26/32 [00:07<00:02,  2.02it/s][A[A

 84% 27/32 [00:07<00:01,  2.51it/s][A[A

 88% 28/32 [00:07<00:01,  2.91it/s][A[A

 91% 29/32 [00:07<00:00,  3.16it/s][A[A

 94% 30/32 [00:07<00:00,  3.43it/s][A[A

 97% 31/32 [00:08<00:00,  3.65it/s][A[A

100% 32/32 [00:08<00:00,  4.01it/s][A[A100% 32/32 [00:08<00:00,  3.84it/s]
Meta loss on this task batch = 2.1783e-01, PNorm = 36.1302, GNorm = 0.2696

 53% 10/19 [01:32<01:24,  9.42s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  3.93it/s][A[A

  6% 2/32 [00:00<00:07,  3.99it/s][A[A

  9% 3/32 [00:00<00:06,  4.71it/s][A[A

 12% 4/32 [00:00<00:05,  4.85it/s][A[A

 16% 5/32 [00:01<00:05,  4.73it/s][A[A

 19% 6/32 [00:02<00:12,  2.01it/s][A[A

 22% 7/32 [00:02<00:10,  2.36it/s][A[A

 25% 8/32 [00:02<00:08,  2.70it/s][A[A

 28% 9/32 [00:02<00:06,  3.40it/s][A[A

 31% 10/32 [00:03<00:06,  3.58it/s][A[A

 34% 11/32 [00:03<00:05,  3.71it/s][A[A

 38% 12/32 [00:03<00:05,  3.91it/s][A[A

 41% 13/32 [00:03<00:04,  3.98it/s][A[A

 44% 14/32 [00:04<00:04,  4.07it/s][A[A

 47% 15/32 [00:04<00:03,  4.34it/s][A[A

 50% 16/32 [00:04<00:03,  4.21it/s][A[A

 53% 17/32 [00:05<00:07,  1.96it/s][A[A

 56% 18/32 [00:05<00:06,  2.33it/s][A[A

 59% 19/32 [00:06<00:04,  2.69it/s][A[A

 62% 20/32 [00:06<00:03,  3.42it/s][A[A

 66% 21/32 [00:06<00:02,  3.82it/s][A[A

 69% 22/32 [00:06<00:02,  3.95it/s][A[A

 72% 23/32 [00:06<00:02,  4.04it/s][A[A

 75% 24/32 [00:07<00:01,  4.20it/s][A[A

 78% 25/32 [00:07<00:01,  4.37it/s][A[A

 81% 26/32 [00:07<00:01,  4.34it/s][A[A

 84% 27/32 [00:07<00:01,  4.31it/s][A[A

 88% 28/32 [00:07<00:00,  4.48it/s][A[A

 91% 29/32 [00:08<00:00,  4.30it/s][A[A

 94% 30/32 [00:08<00:00,  4.32it/s][A[A

 97% 31/32 [00:09<00:00,  2.00it/s][A[A

100% 32/32 [00:09<00:00,  2.41it/s][A[A100% 32/32 [00:09<00:00,  3.28it/s]
Meta loss on this task batch = 6.6607e-01, PNorm = 36.1675, GNorm = 0.1549

 58% 11/19 [01:43<01:18,  9.76s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.19it/s][A[A

  6% 2/32 [00:00<00:06,  4.30it/s][A[A

  9% 3/32 [00:00<00:06,  4.32it/s][A[A

 12% 4/32 [00:00<00:06,  4.29it/s][A[A

 16% 5/32 [00:01<00:06,  4.29it/s][A[A

 19% 6/32 [00:01<00:05,  4.50it/s][A[A

 22% 7/32 [00:01<00:05,  4.42it/s][A[A

 25% 8/32 [00:01<00:05,  4.40it/s][A[A

 28% 9/32 [00:02<00:05,  4.41it/s][A[A

 31% 10/32 [00:02<00:04,  4.74it/s][A[A

 34% 11/32 [00:02<00:04,  4.48it/s][A[A

 38% 12/32 [00:02<00:04,  4.60it/s][A[A

 41% 13/32 [00:03<00:10,  1.88it/s][A[A

 44% 14/32 [00:04<00:07,  2.31it/s][A[A

 47% 15/32 [00:04<00:06,  2.66it/s][A[A

 50% 16/32 [00:04<00:05,  3.00it/s][A[A

 53% 17/32 [00:04<00:04,  3.23it/s][A[A

 56% 18/32 [00:05<00:04,  3.44it/s][A[A

 59% 19/32 [00:05<00:03,  3.74it/s][A[A

 62% 20/32 [00:05<00:02,  4.53it/s][A[A

 66% 21/32 [00:05<00:02,  4.57it/s][A[A

 69% 22/32 [00:06<00:05,  1.89it/s][A[A

 72% 23/32 [00:07<00:03,  2.27it/s][A[A

 75% 24/32 [00:07<00:03,  2.65it/s][A[A

 78% 25/32 [00:07<00:02,  2.99it/s][A[A

 81% 26/32 [00:07<00:01,  3.27it/s][A[A

 84% 27/32 [00:08<00:01,  3.49it/s][A[A

 88% 28/32 [00:08<00:01,  3.83it/s][A[A

 91% 29/32 [00:08<00:00,  4.14it/s][A[A

 94% 30/32 [00:08<00:00,  4.29it/s][A[A

 97% 31/32 [00:08<00:00,  4.52it/s][A[A

100% 32/32 [00:09<00:00,  4.40it/s][A[A100% 32/32 [00:09<00:00,  3.50it/s]
Meta loss on this task batch = 5.3567e-01, PNorm = 36.2067, GNorm = 0.0625

 63% 12/19 [01:53<01:08,  9.81s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.39it/s][A[A

  6% 2/32 [00:01<00:15,  1.88it/s][A[A

  9% 3/32 [00:01<00:12,  2.26it/s][A[A

 12% 4/32 [00:01<00:10,  2.63it/s][A[A

 16% 5/32 [00:02<00:08,  3.31it/s][A[A

 19% 6/32 [00:02<00:07,  3.52it/s][A[A

 22% 7/32 [00:02<00:06,  3.63it/s][A[A

 25% 8/32 [00:02<00:05,  4.10it/s][A[A

 28% 9/32 [00:02<00:05,  4.16it/s][A[A

 31% 10/32 [00:03<00:05,  4.13it/s][A[A

 34% 11/32 [00:03<00:04,  4.40it/s][A[A

 38% 12/32 [00:03<00:04,  4.34it/s][A[A

 41% 13/32 [00:03<00:04,  4.28it/s][A[A

 44% 14/32 [00:04<00:04,  4.19it/s][A[A

 47% 15/32 [00:04<00:03,  4.39it/s][A[A

 50% 16/32 [00:04<00:03,  4.62it/s][A[A

 53% 17/32 [00:05<00:07,  1.96it/s][A[A

 56% 18/32 [00:05<00:05,  2.45it/s][A[A

 59% 19/32 [00:06<00:04,  2.85it/s][A[A

 62% 20/32 [00:06<00:03,  3.10it/s][A[A

 66% 21/32 [00:06<00:03,  3.33it/s][A[A

 69% 22/32 [00:06<00:02,  3.53it/s][A[A

 72% 23/32 [00:07<00:02,  3.83it/s][A[A

 75% 24/32 [00:07<00:01,  4.16it/s][A[A

 78% 25/32 [00:07<00:01,  4.67it/s][A[A

 81% 26/32 [00:07<00:01,  4.46it/s][A[A

 84% 27/32 [00:07<00:01,  4.60it/s][A[A

 88% 28/32 [00:08<00:00,  4.46it/s][A[A

 91% 29/32 [00:09<00:01,  1.91it/s][A[A

 94% 30/32 [00:09<00:00,  2.34it/s][A[A

 97% 31/32 [00:09<00:00,  2.71it/s][A[A

100% 32/32 [00:09<00:00,  3.18it/s][A[A100% 32/32 [00:09<00:00,  3.22it/s]
Meta loss on this task batch = 5.7499e-01, PNorm = 36.2445, GNorm = 0.0636

 68% 13/19 [02:03<01:00, 10.09s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.58it/s][A[A

  6% 2/32 [00:00<00:05,  5.44it/s][A[A

  9% 3/32 [00:00<00:05,  5.06it/s][A[A

 12% 4/32 [00:00<00:05,  4.85it/s][A[A

 16% 5/32 [00:01<00:05,  4.83it/s][A[A

 19% 6/32 [00:01<00:05,  4.89it/s][A[A

 22% 7/32 [00:01<00:05,  4.83it/s][A[A

 25% 8/32 [00:01<00:05,  4.78it/s][A[A

 28% 9/32 [00:01<00:04,  5.19it/s][A[A

 31% 10/32 [00:02<00:04,  4.85it/s][A[A

 34% 11/32 [00:02<00:04,  4.75it/s][A[A

 38% 12/32 [00:02<00:04,  4.52it/s][A[A

 41% 13/32 [00:02<00:04,  4.70it/s][A[A

 44% 14/32 [00:02<00:03,  4.61it/s][A[A

 47% 15/32 [00:03<00:03,  4.76it/s][A[A

 50% 16/32 [00:04<00:08,  1.96it/s][A[A

 53% 17/32 [00:04<00:06,  2.34it/s][A[A

 56% 18/32 [00:04<00:05,  2.69it/s][A[A

 59% 19/32 [00:05<00:04,  3.02it/s][A[A

 62% 20/32 [00:05<00:03,  3.25it/s][A[A

 66% 21/32 [00:05<00:03,  3.62it/s][A[A

 69% 22/32 [00:05<00:02,  3.89it/s][A[A

 72% 23/32 [00:05<00:02,  3.98it/s][A[A

 75% 24/32 [00:06<00:01,  4.11it/s][A[A

 78% 25/32 [00:06<00:01,  4.11it/s][A[A

 81% 26/32 [00:06<00:01,  4.24it/s][A[A

 84% 27/32 [00:06<00:01,  4.38it/s][A[A

 88% 28/32 [00:07<00:00,  4.34it/s][A[A

 91% 29/32 [00:08<00:01,  1.89it/s][A[A

 94% 30/32 [00:08<00:00,  2.29it/s][A[A

 97% 31/32 [00:08<00:00,  2.63it/s][A[A

100% 32/32 [00:09<00:00,  2.88it/s][A[A100% 32/32 [00:09<00:00,  3.53it/s]
Meta loss on this task batch = 5.9060e-01, PNorm = 36.2770, GNorm = 0.1082

 74% 14/19 [02:13<00:50, 10.02s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  6.82it/s][A[A

  6% 2/32 [00:00<00:05,  5.47it/s][A[A

  9% 3/32 [00:00<00:05,  4.91it/s][A[A

 12% 4/32 [00:00<00:05,  4.72it/s][A[A

 16% 5/32 [00:01<00:05,  4.69it/s][A[A

 19% 6/32 [00:01<00:05,  4.62it/s][A[A

 22% 7/32 [00:02<00:13,  1.92it/s][A[A

 25% 8/32 [00:02<00:10,  2.39it/s][A[A

 28% 9/32 [00:03<00:08,  2.70it/s][A[A

 31% 10/32 [00:03<00:07,  3.05it/s][A[A

 34% 11/32 [00:03<00:05,  3.56it/s][A[A

 38% 12/32 [00:03<00:04,  4.01it/s][A[A

 41% 13/32 [00:03<00:04,  4.14it/s][A[A

 44% 14/32 [00:04<00:04,  4.13it/s][A[A

 47% 15/32 [00:04<00:03,  4.49it/s][A[A

 50% 16/32 [00:04<00:03,  4.48it/s][A[A

 53% 17/32 [00:04<00:03,  4.56it/s][A[A

 56% 18/32 [00:04<00:03,  4.51it/s][A[A

 59% 19/32 [00:05<00:02,  5.00it/s][A[A

 62% 20/32 [00:06<00:06,  1.95it/s][A[A

 66% 21/32 [00:06<00:04,  2.42it/s][A[A

 69% 22/32 [00:06<00:03,  2.80it/s][A[A

 72% 23/32 [00:06<00:03,  2.99it/s][A[A

 75% 24/32 [00:07<00:02,  3.32it/s][A[A

 78% 25/32 [00:07<00:01,  4.10it/s][A[A

 81% 26/32 [00:07<00:01,  4.07it/s][A[A

 84% 27/32 [00:07<00:01,  4.11it/s][A[A

 88% 28/32 [00:07<00:00,  4.40it/s][A[A

 91% 29/32 [00:08<00:00,  4.34it/s][A[A

 94% 30/32 [00:08<00:00,  4.28it/s][A[A

 97% 31/32 [00:08<00:00,  4.66it/s][A[A

100% 32/32 [00:08<00:00,  4.60it/s][A[A100% 32/32 [00:08<00:00,  3.61it/s]
Meta loss on this task batch = 4.8707e-01, PNorm = 36.3043, GNorm = 0.1169

 79% 15/19 [02:23<00:39,  9.90s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.82it/s][A[A

  6% 2/32 [00:01<00:14,  2.02it/s][A[A

  9% 3/32 [00:01<00:11,  2.51it/s][A[A

 12% 4/32 [00:01<00:09,  3.00it/s][A[A

 16% 5/32 [00:01<00:08,  3.25it/s][A[A

 19% 6/32 [00:02<00:07,  3.46it/s][A[A

 22% 7/32 [00:02<00:06,  3.83it/s][A[A

 25% 8/32 [00:02<00:05,  4.52it/s][A[A

 28% 9/32 [00:02<00:04,  4.85it/s][A[A

 31% 10/32 [00:02<00:04,  4.60it/s][A[A

 34% 11/32 [00:03<00:04,  4.48it/s][A[A

 38% 12/32 [00:03<00:04,  4.41it/s][A[A

 41% 13/32 [00:03<00:04,  4.39it/s][A[A

 44% 14/32 [00:04<00:09,  1.89it/s][A[A

 47% 15/32 [00:05<00:07,  2.26it/s][A[A

 50% 16/32 [00:05<00:05,  2.81it/s][A[A

 53% 17/32 [00:05<00:04,  3.13it/s][A[A

 56% 18/32 [00:05<00:03,  3.60it/s][A[A

 59% 19/32 [00:05<00:03,  3.78it/s][A[A

 62% 20/32 [00:06<00:03,  3.96it/s][A[A

 66% 21/32 [00:06<00:02,  4.07it/s][A[A

 69% 22/32 [00:06<00:02,  4.34it/s][A[A

 72% 23/32 [00:06<00:02,  4.30it/s][A[A

 75% 24/32 [00:08<00:04,  1.86it/s][A[A

 78% 25/32 [00:08<00:02,  2.37it/s][A[A

 81% 26/32 [00:08<00:02,  2.82it/s][A[A

 84% 27/32 [00:08<00:01,  3.08it/s][A[A

 88% 28/32 [00:08<00:01,  3.40it/s][A[A

 91% 29/32 [00:09<00:00,  3.44it/s][A[A

 94% 30/32 [00:09<00:00,  3.61it/s][A[A

 97% 31/32 [00:09<00:00,  3.75it/s][A[A

100% 32/32 [00:09<00:00,  3.86it/s][A[A100% 32/32 [00:09<00:00,  3.23it/s]
Meta loss on this task batch = 5.8772e-01, PNorm = 36.3230, GNorm = 0.2105

 84% 16/19 [02:34<00:30, 10.14s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.24it/s][A[A

  6% 2/32 [00:01<00:15,  1.95it/s][A[A

  9% 3/32 [00:01<00:12,  2.40it/s][A[A

 12% 4/32 [00:01<00:09,  2.85it/s][A[A

 16% 5/32 [00:02<00:08,  3.10it/s][A[A

 19% 6/32 [00:02<00:06,  3.76it/s][A[A

 22% 7/32 [00:02<00:06,  3.91it/s][A[A

 25% 8/32 [00:02<00:06,  3.87it/s][A[A

 28% 9/32 [00:02<00:05,  3.99it/s][A[A

 31% 10/32 [00:03<00:04,  4.59it/s][A[A

 34% 11/32 [00:03<00:04,  4.54it/s][A[A

 38% 12/32 [00:03<00:04,  4.76it/s][A[A

 41% 13/32 [00:04<00:09,  1.94it/s][A[A

 44% 14/32 [00:04<00:07,  2.36it/s][A[A

 47% 15/32 [00:05<00:06,  2.79it/s][A[A

 50% 16/32 [00:05<00:05,  3.17it/s][A[A

 53% 17/32 [00:05<00:04,  3.65it/s][A[A

 56% 18/32 [00:05<00:03,  3.79it/s][A[A

 59% 19/32 [00:06<00:03,  3.82it/s][A[A

 62% 20/32 [00:06<00:02,  4.03it/s][A[A

 66% 21/32 [00:06<00:02,  4.22it/s][A[A

 69% 22/32 [00:06<00:02,  4.25it/s][A[A

 72% 23/32 [00:06<00:02,  4.44it/s][A[A

 75% 24/32 [00:07<00:01,  4.17it/s][A[A

 78% 25/32 [00:08<00:03,  1.89it/s][A[A

 81% 26/32 [00:08<00:02,  2.27it/s][A[A

 84% 27/32 [00:08<00:01,  2.77it/s][A[A

 88% 28/32 [00:08<00:01,  3.23it/s][A[A

 91% 29/32 [00:09<00:00,  3.52it/s][A[A

 94% 30/32 [00:09<00:00,  3.99it/s][A[A

 97% 31/32 [00:09<00:00,  4.26it/s][A[A

100% 32/32 [00:09<00:00,  4.19it/s][A[A100% 32/32 [00:09<00:00,  3.26it/s]
Meta loss on this task batch = 4.2747e-01, PNorm = 36.3376, GNorm = 0.1097

 89% 17/19 [02:44<00:20, 10.28s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.35it/s][A[A

  6% 2/32 [00:00<00:05,  5.07it/s][A[A

  9% 3/32 [00:00<00:05,  4.91it/s][A[A

 12% 4/32 [00:00<00:05,  4.79it/s][A[A

 16% 5/32 [00:01<00:05,  4.60it/s][A[A

 19% 6/32 [00:01<00:05,  4.61it/s][A[A

 22% 7/32 [00:02<00:13,  1.91it/s][A[A

 25% 8/32 [00:02<00:10,  2.27it/s][A[A

 28% 9/32 [00:03<00:08,  2.67it/s][A[A

 31% 10/32 [00:03<00:07,  3.05it/s][A[A

 34% 11/32 [00:03<00:06,  3.41it/s][A[A

 38% 12/32 [00:03<00:05,  3.86it/s][A[A

 41% 13/32 [00:03<00:04,  3.98it/s][A[A

 44% 14/32 [00:03<00:03,  4.56it/s][A[A

 47% 15/32 [00:04<00:03,  4.35it/s][A[A

 50% 16/32 [00:04<00:03,  4.45it/s][A[A

 53% 17/32 [00:04<00:02,  5.12it/s][A[A

 56% 18/32 [00:05<00:06,  2.01it/s][A[A

 59% 19/32 [00:06<00:05,  2.38it/s][A[A

 62% 20/32 [00:06<00:04,  2.77it/s][A[A

 66% 21/32 [00:06<00:03,  3.12it/s][A[A

 69% 22/32 [00:06<00:02,  3.79it/s][A[A

 72% 23/32 [00:06<00:02,  3.94it/s][A[A

 75% 24/32 [00:07<00:02,  3.83it/s][A[A

 78% 25/32 [00:07<00:01,  4.04it/s][A[A

 81% 26/32 [00:07<00:01,  4.22it/s][A[A

 84% 27/32 [00:08<00:02,  1.85it/s][A[A

 88% 28/32 [00:08<00:01,  2.29it/s][A[A

 91% 29/32 [00:09<00:01,  2.98it/s][A[A

 94% 30/32 [00:09<00:00,  3.25it/s][A[A

 97% 31/32 [00:09<00:00,  3.61it/s][A[A

100% 32/32 [00:09<00:00,  3.69it/s][A[A100% 32/32 [00:09<00:00,  3.27it/s]
Meta loss on this task batch = 5.3761e-01, PNorm = 36.3451, GNorm = 0.2525

 95% 18/19 [02:55<00:10, 10.37s/it][A

  0% 0/23 [00:00<?, ?it/s][A[A

  4% 1/23 [00:00<00:05,  4.17it/s][A[A

  9% 2/23 [00:00<00:04,  4.56it/s][A[A

 13% 3/23 [00:00<00:04,  4.59it/s][A[A

 17% 4/23 [00:00<00:03,  5.09it/s][A[A

 22% 5/23 [00:01<00:08,  2.02it/s][A[A

 26% 6/23 [00:02<00:06,  2.45it/s][A[A

 30% 7/23 [00:02<00:05,  2.81it/s][A[A

 35% 8/23 [00:02<00:04,  3.11it/s][A[A

 39% 9/23 [00:02<00:03,  3.78it/s][A[A

 43% 10/23 [00:03<00:03,  3.80it/s][A[A

 48% 11/23 [00:03<00:02,  4.13it/s][A[A

 52% 12/23 [00:03<00:02,  4.15it/s][A[A

 57% 13/23 [00:03<00:02,  4.72it/s][A[A

 61% 14/23 [00:03<00:01,  4.64it/s][A[A

 65% 15/23 [00:04<00:02,  3.78it/s][A[A

 70% 16/23 [00:05<00:03,  1.78it/s][A[A

 74% 17/23 [00:05<00:02,  2.27it/s][A[A

 78% 18/23 [00:05<00:01,  2.69it/s][A[A

 83% 19/23 [00:06<00:01,  3.05it/s][A[A

 87% 20/23 [00:06<00:00,  3.73it/s][A[A

 91% 21/23 [00:06<00:00,  4.22it/s][A[A

 96% 22/23 [00:06<00:00,  4.90it/s][A[A

100% 23/23 [00:06<00:00,  5.32it/s][A[A100% 23/23 [00:06<00:00,  3.47it/s]
Meta loss on this task batch = 4.6202e-01, PNorm = 36.3477, GNorm = 0.1646

100% 19/19 [03:02<00:00,  9.42s/it][A100% 19/19 [03:02<00:00,  9.60s/it]
Took 182.398672580719 seconds to complete one epoch of meta training

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/20 [00:00<?, ?it/s][A[A


  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 33.01it/s]


  5% 1/20 [00:00<00:02,  8.37it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 20.48it/s]


 10% 2/20 [00:00<00:02,  6.41it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 18.53it/s][A[A[A100% 3/3 [00:00<00:00, 20.50it/s]


 15% 3/20 [00:00<00:03,  5.13it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 15.66it/s]


 20% 4/20 [00:00<00:03,  4.88it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 15.93it/s][A[A[A100% 4/4 [00:00<00:00, 18.16it/s]


 25% 5/20 [00:01<00:03,  3.88it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.92it/s][A[A[A100% 4/4 [00:00<00:00, 16.70it/s]


 30% 6/20 [00:01<00:04,  3.36it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:01<00:00,  2.61it/s][A[A[A100% 4/4 [00:01<00:00,  3.38it/s]


 35% 7/20 [00:02<00:07,  1.63it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.87it/s][A[A[A


100% 4/4 [00:00<00:00, 19.55it/s][A[A[A100% 4/4 [00:00<00:00, 19.32it/s]


 40% 8/20 [00:03<00:06,  1.86it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 20.42it/s][A[A[A100% 4/4 [00:00<00:00, 24.11it/s]


 45% 9/20 [00:03<00:05,  2.11it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.00it/s][A[A[A


100% 4/4 [00:00<00:00, 19.10it/s][A[A[A100% 4/4 [00:00<00:00, 19.14it/s]


 50% 10/20 [00:04<00:04,  2.26it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 16.69it/s][A[A[A100% 4/4 [00:00<00:00, 19.08it/s]


 55% 11/20 [00:04<00:03,  2.39it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.68it/s][A[A[A100% 4/4 [00:00<00:00, 22.08it/s]


 60% 12/20 [00:04<00:03,  2.51it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.64it/s][A[A[A100% 3/3 [00:00<00:00, 13.94it/s]


 65% 13/20 [00:05<00:02,  2.52it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.07it/s][A[A[A100% 3/3 [00:00<00:00, 13.75it/s]


 70% 14/20 [00:06<00:04,  1.45it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 14.17it/s][A[A[A100% 4/4 [00:00<00:00, 18.50it/s]


 75% 15/20 [00:06<00:03,  1.66it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.15it/s][A[A[A100% 3/3 [00:00<00:00, 17.57it/s]


 80% 16/20 [00:07<00:02,  1.91it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 12.14it/s][A[A[A100% 3/3 [00:00<00:00, 14.97it/s]


 85% 17/20 [00:07<00:01,  2.07it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 22.90it/s]


 90% 18/20 [00:07<00:00,  2.39it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 15.69it/s][A[A[A100% 3/3 [00:00<00:00, 20.00it/s]


 95% 19/20 [00:08<00:00,  2.61it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 14.28it/s]


100% 20/20 [00:08<00:00,  2.95it/s][A[A100% 20/20 [00:08<00:00,  2.36it/s]

100% 1/1 [00:08<00:00,  8.47s/it][A100% 1/1 [00:08<00:00,  8.47s/it]
Took 190.86567640304565 seconds to complete one epoch of meta training and validating
Meta validation score auc = 0.608217
Found better MAML checkpoint after meta validation, saving now
 30% 9/30 [28:53<1:06:56, 191.24s/it]Epoch 9

  0% 0/19 [00:00<?, ?it/s][A

  0% 0/32 [00:00<?, ?it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  3% 1/32 [00:00<00:05,  5.82it/s][A[A

  6% 2/32 [00:01<00:14,  2.06it/s][A[A

  9% 3/32 [00:01<00:11,  2.57it/s][A[A

 12% 4/32 [00:01<00:09,  3.03it/s][A[A

 16% 5/32 [00:01<00:07,  3.50it/s][A[A

 19% 6/32 [00:02<00:06,  3.90it/s][A[A

 22% 7/32 [00:02<00:05,  4.23it/s][A[A

 25% 8/32 [00:02<00:05,  4.47it/s][A[A

 28% 9/32 [00:02<00:04,  4.76it/s][A[A

 31% 10/32 [00:02<00:04,  4.92it/s][A[A

 34% 11/32 [00:03<00:04,  4.96it/s][A[A

 38% 12/32 [00:03<00:04,  4.59it/s][A[A

 41% 13/32 [00:03<00:04,  4.55it/s][A[A

 44% 14/32 [00:03<00:03,  4.50it/s][A[A

 47% 15/32 [00:03<00:03,  5.08it/s][A[A

 50% 16/32 [00:05<00:07,  2.01it/s][A[A

 53% 17/32 [00:05<00:06,  2.48it/s][A[A

 56% 18/32 [00:05<00:04,  2.99it/s][A[A

 59% 19/32 [00:05<00:03,  3.41it/s][A[A

 62% 20/32 [00:05<00:03,  3.84it/s][A[A

 66% 21/32 [00:06<00:02,  4.27it/s][A[A

 69% 22/32 [00:06<00:02,  4.61it/s][A[A

 75% 24/32 [00:06<00:01,  5.15it/s][A[A

 78% 25/32 [00:06<00:01,  5.21it/s][A[A

 81% 26/32 [00:06<00:01,  5.01it/s][A[A

 88% 28/32 [00:07<00:00,  5.60it/s][A[A

 91% 29/32 [00:07<00:00,  5.16it/s][A[A

 94% 30/32 [00:07<00:00,  5.41it/s][A[A

 97% 31/32 [00:07<00:00,  6.13it/s][A[A

100% 32/32 [00:07<00:00,  5.83it/s][A[A100% 32/32 [00:07<00:00,  4.08it/s]
Meta loss on this task batch = 5.4384e-01, PNorm = 36.3509, GNorm = 0.1287

  5% 1/19 [00:08<02:34,  8.56s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.60it/s][A[A

  6% 2/32 [00:00<00:05,  5.37it/s][A[A

  9% 3/32 [00:01<00:14,  1.99it/s][A[A

 12% 4/32 [00:01<00:11,  2.46it/s][A[A

 16% 5/32 [00:01<00:09,  2.94it/s][A[A

 19% 6/32 [00:02<00:07,  3.31it/s][A[A

 22% 7/32 [00:02<00:06,  3.80it/s][A[A

 25% 8/32 [00:02<00:05,  4.30it/s][A[A

 28% 9/32 [00:02<00:05,  4.53it/s][A[A

 31% 10/32 [00:02<00:04,  4.90it/s][A[A

 34% 11/32 [00:03<00:03,  5.47it/s][A[A

 38% 12/32 [00:03<00:04,  4.82it/s][A[A

 41% 13/32 [00:03<00:03,  5.09it/s][A[A

 44% 14/32 [00:03<00:03,  5.23it/s][A[A

 47% 15/32 [00:03<00:03,  5.42it/s][A[A

 50% 16/32 [00:03<00:02,  5.42it/s][A[A

 53% 17/32 [00:05<00:07,  2.07it/s][A[A

 56% 18/32 [00:05<00:05,  2.46it/s][A[A

 59% 19/32 [00:05<00:04,  3.18it/s][A[A

 62% 20/32 [00:05<00:03,  3.44it/s][A[A

 66% 21/32 [00:05<00:02,  4.07it/s][A[A

 69% 22/32 [00:06<00:02,  4.35it/s][A[A

 72% 23/32 [00:06<00:01,  4.80it/s][A[A

 75% 24/32 [00:06<00:01,  5.05it/s][A[A

 78% 25/32 [00:06<00:01,  5.07it/s][A[A

 81% 26/32 [00:06<00:01,  4.90it/s][A[A

 84% 27/32 [00:07<00:01,  4.84it/s][A[A

 88% 28/32 [00:07<00:00,  5.10it/s][A[A

 91% 29/32 [00:07<00:00,  5.36it/s][A[A

 94% 30/32 [00:07<00:00,  5.45it/s][A[A

 97% 31/32 [00:08<00:00,  2.13it/s][A[A

100% 32/32 [00:08<00:00,  2.64it/s][A[A100% 32/32 [00:08<00:00,  3.62it/s]
Meta loss on this task batch = 4.9551e-01, PNorm = 36.3567, GNorm = 0.0439

 11% 2/19 [00:18<02:30,  8.86s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.79it/s][A[A

  6% 2/32 [00:00<00:05,  5.66it/s][A[A

  9% 3/32 [00:00<00:04,  5.91it/s][A[A

 12% 4/32 [00:00<00:04,  5.66it/s][A[A

 16% 5/32 [00:00<00:04,  5.44it/s][A[A

 19% 6/32 [00:01<00:04,  5.39it/s][A[A

 22% 7/32 [00:01<00:04,  5.47it/s][A[A

 25% 8/32 [00:01<00:04,  5.63it/s][A[A

 28% 9/32 [00:01<00:04,  5.46it/s][A[A

 31% 10/32 [00:01<00:04,  5.21it/s][A[A

 34% 11/32 [00:02<00:04,  4.99it/s][A[A

 38% 12/32 [00:02<00:03,  5.18it/s][A[A

 41% 13/32 [00:02<00:03,  5.09it/s][A[A

 44% 14/32 [00:02<00:03,  5.01it/s][A[A

 47% 15/32 [00:02<00:03,  5.03it/s][A[A

 50% 16/32 [00:04<00:07,  2.02it/s][A[A

 53% 17/32 [00:04<00:06,  2.45it/s][A[A

 56% 18/32 [00:04<00:04,  3.00it/s][A[A

 59% 19/32 [00:04<00:03,  3.51it/s][A[A

 62% 20/32 [00:04<00:02,  4.02it/s][A[A

 66% 21/32 [00:04<00:02,  4.31it/s][A[A

 69% 22/32 [00:05<00:02,  4.56it/s][A[A

 72% 23/32 [00:05<00:01,  4.76it/s][A[A

 75% 24/32 [00:05<00:01,  4.72it/s][A[A

 78% 25/32 [00:05<00:01,  5.40it/s][A[A

 81% 26/32 [00:05<00:00,  6.01it/s][A[A

 84% 27/32 [00:05<00:00,  5.39it/s][A[A

 88% 28/32 [00:06<00:00,  5.21it/s][A[A

 91% 29/32 [00:06<00:00,  5.24it/s][A[A

 94% 30/32 [00:06<00:00,  4.91it/s][A[A

 97% 31/32 [00:07<00:00,  2.04it/s][A[A

100% 32/32 [00:07<00:00,  2.49it/s][A[A100% 32/32 [00:07<00:00,  4.01it/s]
Meta loss on this task batch = 5.0819e-01, PNorm = 36.3633, GNorm = 0.0453

 16% 3/19 [00:26<02:21,  8.83s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.08it/s][A[A

  6% 2/32 [00:00<00:06,  4.99it/s][A[A

  9% 3/32 [00:00<00:05,  5.06it/s][A[A

 12% 4/32 [00:00<00:05,  5.16it/s][A[A

 16% 5/32 [00:00<00:05,  4.98it/s][A[A

 19% 6/32 [00:01<00:05,  5.02it/s][A[A

 22% 7/32 [00:01<00:04,  5.11it/s][A[A

 25% 8/32 [00:01<00:04,  5.92it/s][A[A

 28% 9/32 [00:01<00:03,  5.89it/s][A[A

 34% 11/32 [00:01<00:03,  6.41it/s][A[A

 38% 12/32 [00:02<00:03,  6.12it/s][A[A

 41% 13/32 [00:02<00:03,  5.93it/s][A[A

 44% 14/32 [00:03<00:08,  2.16it/s][A[A

 47% 15/32 [00:03<00:06,  2.67it/s][A[A

 50% 16/32 [00:03<00:04,  3.22it/s][A[A

 56% 18/32 [00:04<00:03,  3.79it/s][A[A

 59% 19/32 [00:04<00:02,  4.36it/s][A[A

 62% 20/32 [00:04<00:02,  4.66it/s][A[A

 66% 21/32 [00:04<00:02,  4.84it/s][A[A

 69% 22/32 [00:04<00:01,  5.12it/s][A[A

 72% 23/32 [00:04<00:01,  5.01it/s][A[A

 75% 24/32 [00:05<00:01,  4.96it/s][A[A

 78% 25/32 [00:05<00:01,  5.24it/s][A[A

 81% 26/32 [00:05<00:01,  5.32it/s][A[A

 84% 27/32 [00:05<00:00,  5.54it/s][A[A

 88% 28/32 [00:05<00:00,  5.61it/s][A[A

 91% 29/32 [00:06<00:00,  5.72it/s][A[A

 94% 30/32 [00:06<00:00,  5.37it/s][A[A

 97% 31/32 [00:06<00:00,  5.29it/s][A[A

100% 32/32 [00:07<00:00,  2.02it/s][A[A100% 32/32 [00:07<00:00,  4.20it/s]
Meta loss on this task batch = 5.6800e-01, PNorm = 36.3697, GNorm = 0.0729

 21% 4/19 [00:35<02:10,  8.69s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.91it/s][A[A

  6% 2/32 [00:00<00:05,  5.69it/s][A[A

 12% 4/32 [00:00<00:04,  6.08it/s][A[A

 16% 5/32 [00:00<00:04,  5.64it/s][A[A

 19% 6/32 [00:01<00:04,  5.77it/s][A[A

 22% 7/32 [00:01<00:04,  5.85it/s][A[A

 25% 8/32 [00:01<00:04,  5.82it/s][A[A

 28% 9/32 [00:01<00:04,  5.68it/s][A[A

 31% 10/32 [00:01<00:03,  5.54it/s][A[A

 34% 11/32 [00:01<00:03,  5.64it/s][A[A

 38% 12/32 [00:02<00:03,  5.24it/s][A[A

 41% 13/32 [00:02<00:03,  4.83it/s][A[A

 44% 14/32 [00:02<00:03,  4.79it/s][A[A

 47% 15/32 [00:02<00:03,  4.71it/s][A[A

 50% 16/32 [00:03<00:07,  2.08it/s][A[A

 53% 17/32 [00:04<00:06,  2.41it/s][A[A

 56% 18/32 [00:04<00:04,  2.83it/s][A[A

 59% 19/32 [00:04<00:03,  3.28it/s][A[A

 62% 20/32 [00:04<00:03,  3.48it/s][A[A

 66% 21/32 [00:04<00:02,  4.13it/s][A[A

 69% 22/32 [00:05<00:02,  4.92it/s][A[A

 72% 23/32 [00:05<00:01,  5.20it/s][A[A

 75% 24/32 [00:05<00:01,  4.74it/s][A[A

 78% 25/32 [00:05<00:01,  4.59it/s][A[A

 81% 26/32 [00:05<00:01,  4.67it/s][A[A

 84% 27/32 [00:06<00:01,  4.73it/s][A[A

 88% 28/32 [00:06<00:00,  5.12it/s][A[A

 91% 29/32 [00:06<00:00,  5.59it/s][A[A

 94% 30/32 [00:06<00:00,  5.26it/s][A[A

 97% 31/32 [00:07<00:00,  2.03it/s][A[A

100% 32/32 [00:08<00:00,  2.45it/s][A[A100% 32/32 [00:08<00:00,  3.98it/s]
Meta loss on this task batch = 4.8575e-01, PNorm = 36.3781, GNorm = 0.0891

 26% 5/19 [00:44<02:02,  8.71s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.95it/s][A[A

  6% 2/32 [00:00<00:05,  5.18it/s][A[A

  9% 3/32 [00:00<00:05,  5.43it/s][A[A

 12% 4/32 [00:00<00:04,  5.89it/s][A[A

 16% 5/32 [00:00<00:05,  5.31it/s][A[A

 19% 6/32 [00:01<00:05,  5.03it/s][A[A

 22% 7/32 [00:01<00:04,  5.02it/s][A[A

 25% 8/32 [00:01<00:04,  5.27it/s][A[A

 28% 9/32 [00:01<00:04,  4.74it/s][A[A

 31% 10/32 [00:01<00:04,  5.01it/s][A[A

 34% 11/32 [00:02<00:04,  5.19it/s][A[A

 38% 12/32 [00:02<00:03,  5.34it/s][A[A

 41% 13/32 [00:02<00:03,  5.26it/s][A[A

 44% 14/32 [00:03<00:09,  1.98it/s][A[A

 47% 15/32 [00:03<00:07,  2.39it/s][A[A

 50% 16/32 [00:04<00:05,  2.77it/s][A[A

 53% 17/32 [00:04<00:04,  3.19it/s][A[A

 56% 18/32 [00:04<00:03,  3.65it/s][A[A

 59% 19/32 [00:04<00:03,  3.90it/s][A[A

 62% 20/32 [00:04<00:02,  4.24it/s][A[A

 66% 21/32 [00:05<00:02,  4.44it/s][A[A

 69% 22/32 [00:05<00:02,  4.53it/s][A[A

 72% 23/32 [00:05<00:01,  4.86it/s][A[A

 75% 24/32 [00:05<00:01,  4.83it/s][A[A

 78% 25/32 [00:05<00:01,  4.71it/s][A[A

 81% 26/32 [00:06<00:01,  4.85it/s][A[A

 84% 27/32 [00:06<00:01,  4.64it/s][A[A

 88% 28/32 [00:06<00:00,  4.58it/s][A[A

 91% 29/32 [00:07<00:01,  1.90it/s][A[A

 94% 30/32 [00:08<00:00,  2.33it/s][A[A

100% 32/32 [00:08<00:00,  2.90it/s][A[A100% 32/32 [00:08<00:00,  3.83it/s]
Meta loss on this task batch = 4.6384e-01, PNorm = 36.3880, GNorm = 0.1001

 32% 6/19 [00:53<01:54,  8.84s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.19it/s][A[A

  6% 2/32 [00:00<00:05,  5.69it/s][A[A

  9% 3/32 [00:00<00:05,  5.26it/s][A[A

 12% 4/32 [00:00<00:05,  4.84it/s][A[A

 16% 5/32 [00:00<00:05,  4.96it/s][A[A

 19% 6/32 [00:01<00:05,  5.00it/s][A[A

 22% 7/32 [00:01<00:04,  5.12it/s][A[A

 25% 8/32 [00:01<00:04,  5.24it/s][A[A

 28% 9/32 [00:01<00:04,  5.06it/s][A[A

 31% 10/32 [00:02<00:11,  1.98it/s][A[A

 34% 11/32 [00:03<00:08,  2.49it/s][A[A

 38% 12/32 [00:03<00:06,  2.93it/s][A[A

 41% 13/32 [00:03<00:05,  3.55it/s][A[A

 44% 14/32 [00:03<00:04,  3.91it/s][A[A

 47% 15/32 [00:03<00:04,  4.02it/s][A[A

 50% 16/32 [00:04<00:03,  4.06it/s][A[A

 53% 17/32 [00:04<00:03,  4.39it/s][A[A

 56% 18/32 [00:04<00:03,  4.58it/s][A[A

 59% 19/32 [00:04<00:02,  4.62it/s][A[A

 62% 20/32 [00:04<00:02,  4.45it/s][A[A

 66% 21/32 [00:05<00:02,  4.59it/s][A[A

 69% 22/32 [00:06<00:05,  1.91it/s][A[A

 72% 23/32 [00:06<00:03,  2.30it/s][A[A

 75% 24/32 [00:06<00:02,  2.69it/s][A[A

 78% 25/32 [00:07<00:02,  3.23it/s][A[A

 81% 26/32 [00:07<00:01,  3.69it/s][A[A

 84% 27/32 [00:07<00:01,  4.19it/s][A[A

 88% 28/32 [00:07<00:00,  4.25it/s][A[A

 91% 29/32 [00:07<00:00,  5.10it/s][A[A

 94% 30/32 [00:07<00:00,  5.08it/s][A[A

 97% 31/32 [00:08<00:00,  5.01it/s][A[A

100% 32/32 [00:08<00:00,  4.96it/s][A[A100% 32/32 [00:08<00:00,  3.84it/s]
Meta loss on this task batch = 4.8950e-01, PNorm = 36.4000, GNorm = 0.0931

 37% 7/19 [01:02<01:47,  8.92s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.21it/s][A[A

  6% 2/32 [00:00<00:06,  4.36it/s][A[A

  9% 3/32 [00:01<00:15,  1.89it/s][A[A

 12% 4/32 [00:01<00:12,  2.28it/s][A[A

 16% 5/32 [00:02<00:09,  2.73it/s][A[A

 19% 6/32 [00:02<00:08,  3.15it/s][A[A

 22% 7/32 [00:02<00:06,  3.94it/s][A[A

 25% 8/32 [00:02<00:05,  4.08it/s][A[A

 28% 9/32 [00:02<00:05,  4.31it/s][A[A

 31% 10/32 [00:02<00:04,  4.87it/s][A[A

 34% 11/32 [00:03<00:04,  4.84it/s][A[A

 38% 12/32 [00:03<00:04,  4.96it/s][A[A

 41% 13/32 [00:03<00:03,  5.22it/s][A[A

 44% 14/32 [00:03<00:03,  5.99it/s][A[A

 47% 15/32 [00:03<00:02,  5.88it/s][A[A

 50% 16/32 [00:04<00:03,  5.24it/s][A[A

 53% 17/32 [00:05<00:07,  2.02it/s][A[A

 56% 18/32 [00:05<00:05,  2.49it/s][A[A

 59% 19/32 [00:05<00:04,  2.92it/s][A[A

 62% 20/32 [00:05<00:03,  3.40it/s][A[A

 66% 21/32 [00:05<00:02,  4.00it/s][A[A

 69% 22/32 [00:06<00:02,  4.44it/s][A[A

 72% 23/32 [00:06<00:01,  5.30it/s][A[A

 75% 24/32 [00:06<00:01,  5.04it/s][A[A

 78% 25/32 [00:06<00:01,  4.73it/s][A[A

 81% 26/32 [00:06<00:01,  5.01it/s][A[A

 84% 27/32 [00:07<00:01,  4.83it/s][A[A

 88% 28/32 [00:07<00:00,  4.66it/s][A[A

 91% 29/32 [00:07<00:00,  4.60it/s][A[A

 94% 30/32 [00:07<00:00,  4.60it/s][A[A

 97% 31/32 [00:09<00:00,  1.93it/s][A[A

100% 32/32 [00:09<00:00,  2.32it/s][A[A100% 32/32 [00:09<00:00,  3.46it/s]
Meta loss on this task batch = 3.8259e-01, PNorm = 36.4152, GNorm = 0.1169

 42% 8/19 [01:12<01:41,  9.24s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.24it/s][A[A

  6% 2/32 [00:00<00:06,  4.39it/s][A[A

  9% 3/32 [00:00<00:06,  4.51it/s][A[A

 12% 4/32 [00:00<00:06,  4.51it/s][A[A

 16% 5/32 [00:01<00:05,  4.53it/s][A[A

 19% 6/32 [00:01<00:05,  4.50it/s][A[A

 22% 7/32 [00:01<00:05,  4.45it/s][A[A

 25% 8/32 [00:01<00:05,  4.53it/s][A[A

 28% 9/32 [00:01<00:05,  4.48it/s][A[A

 31% 10/32 [00:02<00:04,  4.51it/s][A[A

 34% 11/32 [00:02<00:04,  4.54it/s][A[A

 38% 12/32 [00:02<00:04,  4.36it/s][A[A

 41% 13/32 [00:03<00:10,  1.86it/s][A[A

 44% 14/32 [00:04<00:07,  2.27it/s][A[A

 47% 15/32 [00:04<00:06,  2.65it/s][A[A

 50% 16/32 [00:04<00:05,  3.02it/s][A[A

 53% 17/32 [00:04<00:04,  3.34it/s][A[A

 56% 18/32 [00:05<00:03,  3.67it/s][A[A

 59% 19/32 [00:05<00:03,  3.84it/s][A[A

 62% 20/32 [00:05<00:02,  4.01it/s][A[A

 66% 21/32 [00:05<00:02,  4.13it/s][A[A

 69% 22/32 [00:05<00:02,  4.29it/s][A[A

 72% 23/32 [00:06<00:02,  4.35it/s][A[A

 75% 24/32 [00:07<00:04,  1.87it/s][A[A

 78% 25/32 [00:07<00:03,  2.25it/s][A[A

 81% 26/32 [00:07<00:02,  2.62it/s][A[A

 84% 27/32 [00:08<00:01,  3.04it/s][A[A

 88% 28/32 [00:08<00:01,  3.40it/s][A[A

 91% 29/32 [00:08<00:00,  3.66it/s][A[A

 94% 30/32 [00:08<00:00,  3.90it/s][A[A

 97% 31/32 [00:08<00:00,  4.11it/s][A[A

100% 32/32 [00:09<00:00,  4.22it/s][A[A100% 32/32 [00:09<00:00,  3.49it/s]
Meta loss on this task batch = 2.2303e-01, PNorm = 36.4348, GNorm = 0.1508

 47% 9/19 [01:22<01:34,  9.46s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.07it/s][A[A

  6% 2/32 [00:00<00:07,  4.20it/s][A[A

  9% 3/32 [00:01<00:15,  1.87it/s][A[A

 12% 4/32 [00:01<00:12,  2.25it/s][A[A

 16% 5/32 [00:02<00:10,  2.64it/s][A[A

 19% 6/32 [00:02<00:08,  3.03it/s][A[A

 22% 7/32 [00:02<00:07,  3.37it/s][A[A

 25% 8/32 [00:02<00:06,  3.63it/s][A[A

 28% 9/32 [00:03<00:06,  3.81it/s][A[A

 31% 10/32 [00:03<00:05,  4.04it/s][A[A

 34% 11/32 [00:03<00:05,  4.18it/s][A[A

 38% 12/32 [00:03<00:04,  4.27it/s][A[A

 41% 13/32 [00:03<00:04,  4.25it/s][A[A

 44% 14/32 [00:05<00:09,  1.87it/s][A[A

 47% 15/32 [00:05<00:07,  2.26it/s][A[A

 50% 16/32 [00:05<00:06,  2.66it/s][A[A

 53% 17/32 [00:05<00:05,  2.98it/s][A[A

 56% 18/32 [00:06<00:04,  3.23it/s][A[A

 59% 19/32 [00:06<00:03,  3.53it/s][A[A

 62% 20/32 [00:06<00:03,  3.82it/s][A[A

 66% 21/32 [00:06<00:02,  3.99it/s][A[A

 69% 22/32 [00:06<00:02,  4.19it/s][A[A

 72% 23/32 [00:07<00:02,  4.35it/s][A[A

 75% 24/32 [00:07<00:01,  4.46it/s][A[A

 78% 25/32 [00:07<00:01,  4.54it/s][A[A

 81% 26/32 [00:07<00:01,  4.70it/s][A[A

 84% 27/32 [00:07<00:01,  4.90it/s][A[A

 88% 28/32 [00:09<00:02,  1.93it/s][A[A

 91% 29/32 [00:09<00:01,  2.31it/s][A[A

 94% 30/32 [00:09<00:00,  2.68it/s][A[A

 97% 31/32 [00:09<00:00,  3.00it/s][A[A

100% 32/32 [00:10<00:00,  3.46it/s][A[A100% 32/32 [00:10<00:00,  3.16it/s]
Meta loss on this task batch = 2.8135e-01, PNorm = 36.4585, GNorm = 0.1296

 53% 10/19 [01:33<01:29,  9.91s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.15it/s][A[A

  6% 2/32 [00:00<00:07,  4.22it/s][A[A

  9% 3/32 [00:00<00:06,  4.21it/s][A[A

 12% 4/32 [00:00<00:06,  4.51it/s][A[A

 16% 5/32 [00:01<00:06,  4.42it/s][A[A

 19% 6/32 [00:01<00:05,  4.66it/s][A[A

 22% 7/32 [00:01<00:05,  4.42it/s][A[A

 25% 8/32 [00:02<00:12,  1.87it/s][A[A

 28% 9/32 [00:03<00:10,  2.21it/s][A[A

 31% 10/32 [00:03<00:08,  2.55it/s][A[A

 34% 11/32 [00:03<00:07,  2.87it/s][A[A

 38% 12/32 [00:03<00:06,  3.16it/s][A[A

 41% 13/32 [00:04<00:05,  3.44it/s][A[A

 44% 14/32 [00:04<00:04,  3.64it/s][A[A

 47% 15/32 [00:04<00:04,  4.02it/s][A[A

 50% 16/32 [00:05<00:08,  1.87it/s][A[A

 53% 17/32 [00:05<00:06,  2.26it/s][A[A

 56% 18/32 [00:06<00:05,  2.63it/s][A[A

 59% 19/32 [00:06<00:04,  2.95it/s][A[A

 62% 20/32 [00:06<00:03,  3.21it/s][A[A

 66% 21/32 [00:06<00:03,  3.61it/s][A[A

 69% 22/32 [00:07<00:02,  3.71it/s][A[A

 72% 23/32 [00:07<00:02,  3.81it/s][A[A

 75% 24/32 [00:08<00:04,  1.79it/s][A[A

 78% 25/32 [00:08<00:03,  2.23it/s][A[A

 81% 26/32 [00:09<00:02,  2.59it/s][A[A

 84% 27/32 [00:09<00:01,  2.94it/s][A[A

 88% 28/32 [00:09<00:01,  3.30it/s][A[A

 91% 29/32 [00:09<00:00,  3.50it/s][A[A

 94% 30/32 [00:09<00:00,  3.78it/s][A[A

 97% 31/32 [00:10<00:00,  3.95it/s][A[A

100% 32/32 [00:10<00:00,  4.02it/s][A[A100% 32/32 [00:10<00:00,  3.08it/s]
Meta loss on this task batch = 5.7564e-01, PNorm = 36.4822, GNorm = 0.0403

 58% 11/19 [01:44<01:22, 10.30s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.74it/s][A[A

  6% 2/32 [00:00<00:07,  3.87it/s][A[A

  9% 3/32 [00:01<00:15,  1.81it/s][A[A

 12% 4/32 [00:01<00:12,  2.30it/s][A[A

 16% 5/32 [00:02<00:10,  2.66it/s][A[A

 19% 6/32 [00:02<00:08,  2.97it/s][A[A

 22% 7/32 [00:02<00:07,  3.25it/s][A[A

 25% 8/32 [00:02<00:06,  3.49it/s][A[A

 28% 9/32 [00:03<00:06,  3.66it/s][A[A

 31% 10/32 [00:03<00:05,  3.77it/s][A[A

 34% 11/32 [00:03<00:05,  3.90it/s][A[A

 38% 12/32 [00:04<00:10,  1.83it/s][A[A

 41% 13/32 [00:05<00:08,  2.20it/s][A[A

 44% 14/32 [00:05<00:06,  2.62it/s][A[A

 47% 15/32 [00:05<00:05,  2.94it/s][A[A

 50% 16/32 [00:05<00:04,  3.23it/s][A[A

 53% 17/32 [00:05<00:04,  3.45it/s][A[A

 56% 18/32 [00:06<00:03,  3.62it/s][A[A

 59% 19/32 [00:06<00:03,  3.72it/s][A[A

 62% 20/32 [00:06<00:03,  3.82it/s][A[A

 66% 21/32 [00:06<00:02,  3.95it/s][A[A

 69% 22/32 [00:08<00:05,  1.81it/s][A[A

 72% 23/32 [00:08<00:04,  2.19it/s][A[A

 75% 24/32 [00:08<00:03,  2.62it/s][A[A

 78% 25/32 [00:08<00:02,  2.97it/s][A[A

 81% 26/32 [00:09<00:01,  3.34it/s][A[A

 84% 27/32 [00:09<00:01,  3.56it/s][A[A

 88% 28/32 [00:09<00:01,  3.84it/s][A[A

 91% 29/32 [00:09<00:00,  4.24it/s][A[A

 94% 30/32 [00:09<00:00,  4.21it/s][A[A

 97% 31/32 [00:10<00:00,  4.41it/s][A[A

100% 32/32 [00:10<00:00,  4.37it/s][A[A100% 32/32 [00:10<00:00,  3.08it/s]
Meta loss on this task batch = 5.7029e-01, PNorm = 36.5037, GNorm = 0.0244

 63% 12/19 [01:55<01:14, 10.58s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:39,  1.26s/it][A[A

  6% 2/32 [00:01<00:28,  1.05it/s][A[A

  9% 3/32 [00:01<00:21,  1.36it/s][A[A

 12% 4/32 [00:01<00:16,  1.71it/s][A[A

 16% 5/32 [00:02<00:12,  2.09it/s][A[A

 19% 6/32 [00:02<00:10,  2.47it/s][A[A

 22% 7/32 [00:02<00:08,  3.02it/s][A[A

 25% 8/32 [00:02<00:06,  3.49it/s][A[A

 28% 9/32 [00:02<00:05,  4.05it/s][A[A

 31% 10/32 [00:03<00:05,  4.10it/s][A[A

 34% 11/32 [00:03<00:04,  4.46it/s][A[A

 38% 12/32 [00:03<00:04,  4.44it/s][A[A

 41% 13/32 [00:03<00:04,  4.33it/s][A[A

 44% 14/32 [00:04<00:04,  4.20it/s][A[A

 47% 15/32 [00:05<00:09,  1.87it/s][A[A

 50% 16/32 [00:05<00:06,  2.31it/s][A[A

 53% 17/32 [00:05<00:05,  2.79it/s][A[A

 56% 18/32 [00:05<00:04,  3.33it/s][A[A

 59% 19/32 [00:06<00:03,  3.51it/s][A[A

 62% 20/32 [00:06<00:03,  3.68it/s][A[A

 66% 21/32 [00:06<00:02,  3.86it/s][A[A

 69% 22/32 [00:06<00:02,  3.98it/s][A[A

 72% 23/32 [00:07<00:02,  3.99it/s][A[A

 75% 24/32 [00:07<00:01,  4.23it/s][A[A

 78% 25/32 [00:07<00:01,  4.17it/s][A[A

 81% 26/32 [00:07<00:01,  4.17it/s][A[A

 84% 27/32 [00:07<00:01,  4.54it/s][A[A

 88% 28/32 [00:09<00:02,  1.89it/s][A[A

 91% 29/32 [00:09<00:01,  2.24it/s][A[A

 94% 30/32 [00:09<00:00,  2.72it/s][A[A

 97% 31/32 [00:09<00:00,  3.16it/s][A[A

100% 32/32 [00:09<00:00,  3.71it/s][A[A100% 32/32 [00:09<00:00,  3.22it/s]
Meta loss on this task batch = 6.1677e-01, PNorm = 36.5202, GNorm = 0.0979

 68% 13/19 [02:06<01:03, 10.63s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.22it/s][A[A

  6% 2/32 [00:00<00:05,  5.30it/s][A[A

  9% 3/32 [00:00<00:05,  4.87it/s][A[A

 12% 4/32 [00:00<00:05,  4.69it/s][A[A

 16% 5/32 [00:02<00:14,  1.93it/s][A[A

 19% 6/32 [00:02<00:10,  2.39it/s][A[A

 22% 7/32 [00:02<00:09,  2.71it/s][A[A

 25% 8/32 [00:02<00:07,  3.26it/s][A[A

 28% 9/32 [00:02<00:06,  3.72it/s][A[A

 31% 10/32 [00:03<00:05,  3.89it/s][A[A

 34% 11/32 [00:03<00:05,  3.94it/s][A[A

 38% 12/32 [00:03<00:04,  4.01it/s][A[A

 41% 13/32 [00:03<00:04,  4.18it/s][A[A

 44% 14/32 [00:04<00:04,  4.18it/s][A[A

 47% 15/32 [00:05<00:09,  1.86it/s][A[A

 50% 16/32 [00:05<00:07,  2.21it/s][A[A

 53% 17/32 [00:05<00:05,  2.55it/s][A[A

 56% 18/32 [00:05<00:04,  2.96it/s][A[A

 59% 19/32 [00:06<00:03,  3.30it/s][A[A

 62% 20/32 [00:06<00:03,  3.47it/s][A[A

 66% 21/32 [00:06<00:02,  3.74it/s][A[A

 69% 22/32 [00:06<00:02,  3.88it/s][A[A

 72% 23/32 [00:07<00:02,  3.98it/s][A[A

 75% 24/32 [00:07<00:01,  4.25it/s][A[A

 78% 25/32 [00:08<00:03,  1.88it/s][A[A

 81% 26/32 [00:08<00:02,  2.27it/s][A[A

 84% 27/32 [00:09<00:01,  2.65it/s][A[A

 88% 28/32 [00:09<00:01,  2.97it/s][A[A

 91% 29/32 [00:09<00:00,  3.26it/s][A[A

 94% 30/32 [00:09<00:00,  3.60it/s][A[A

 97% 31/32 [00:09<00:00,  3.90it/s][A[A

100% 32/32 [00:10<00:00,  4.21it/s][A[A100% 32/32 [00:10<00:00,  3.16it/s]
Meta loss on this task batch = 5.7340e-01, PNorm = 36.5340, GNorm = 0.0664

 74% 14/19 [02:17<00:53, 10.71s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.47it/s][A[A

  6% 2/32 [00:00<00:06,  4.66it/s][A[A

  9% 3/32 [00:00<00:06,  4.59it/s][A[A

 12% 4/32 [00:00<00:05,  4.92it/s][A[A

 16% 5/32 [00:00<00:05,  5.12it/s][A[A

 19% 6/32 [00:01<00:05,  4.71it/s][A[A

 22% 7/32 [00:01<00:05,  4.46it/s][A[A

 25% 8/32 [00:02<00:12,  1.93it/s][A[A

 28% 9/32 [00:02<00:09,  2.32it/s][A[A

 31% 10/32 [00:03<00:07,  2.95it/s][A[A

 34% 11/32 [00:03<00:05,  3.54it/s][A[A

 38% 12/32 [00:03<00:05,  3.79it/s][A[A

 41% 13/32 [00:03<00:04,  4.07it/s][A[A

 44% 14/32 [00:03<00:04,  4.13it/s][A[A

 47% 15/32 [00:04<00:03,  4.39it/s][A[A

 50% 16/32 [00:04<00:03,  4.57it/s][A[A

 53% 17/32 [00:04<00:03,  4.67it/s][A[A

 56% 18/32 [00:04<00:02,  4.83it/s][A[A

 59% 19/32 [00:04<00:02,  4.81it/s][A[A

 62% 20/32 [00:05<00:02,  4.70it/s][A[A

 66% 21/32 [00:06<00:05,  1.96it/s][A[A

 69% 22/32 [00:06<00:04,  2.43it/s][A[A

 72% 23/32 [00:06<00:03,  2.69it/s][A[A

 75% 24/32 [00:06<00:02,  3.11it/s][A[A

 78% 25/32 [00:07<00:02,  3.42it/s][A[A

 81% 26/32 [00:07<00:01,  3.57it/s][A[A

 84% 27/32 [00:07<00:01,  3.85it/s][A[A

 88% 28/32 [00:07<00:00,  4.23it/s][A[A

 91% 29/32 [00:08<00:00,  4.23it/s][A[A

 94% 30/32 [00:08<00:00,  4.22it/s][A[A

 97% 31/32 [00:08<00:00,  4.46it/s][A[A

100% 32/32 [00:09<00:00,  1.88it/s][A[A100% 32/32 [00:09<00:00,  3.29it/s]
Meta loss on this task batch = 5.1073e-01, PNorm = 36.5450, GNorm = 0.0740

 79% 15/19 [02:27<00:42, 10.65s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.60it/s][A[A

  6% 2/32 [00:00<00:06,  4.82it/s][A[A

  9% 3/32 [00:00<00:05,  4.98it/s][A[A

 12% 4/32 [00:00<00:05,  5.08it/s][A[A

 16% 5/32 [00:00<00:04,  5.66it/s][A[A

 19% 6/32 [00:01<00:05,  5.17it/s][A[A

 22% 7/32 [00:01<00:04,  5.13it/s][A[A

 25% 8/32 [00:01<00:05,  4.78it/s][A[A

 28% 9/32 [00:01<00:04,  4.94it/s][A[A

 31% 10/32 [00:01<00:04,  4.73it/s][A[A

 34% 11/32 [00:03<00:10,  1.91it/s][A[A

 38% 12/32 [00:03<00:08,  2.35it/s][A[A

 41% 13/32 [00:03<00:06,  2.85it/s][A[A

 44% 14/32 [00:03<00:05,  3.36it/s][A[A

 47% 15/32 [00:04<00:04,  3.68it/s][A[A

 50% 16/32 [00:04<00:04,  3.83it/s][A[A

 53% 17/32 [00:04<00:03,  3.88it/s][A[A

 56% 18/32 [00:04<00:03,  4.32it/s][A[A

 59% 19/32 [00:04<00:02,  4.46it/s][A[A

 62% 20/32 [00:05<00:02,  4.55it/s][A[A

 66% 21/32 [00:05<00:02,  4.77it/s][A[A

 69% 22/32 [00:06<00:05,  1.97it/s][A[A

 72% 23/32 [00:06<00:03,  2.36it/s][A[A

 75% 24/32 [00:06<00:02,  2.71it/s][A[A

 78% 25/32 [00:07<00:02,  3.25it/s][A[A

 81% 26/32 [00:07<00:01,  3.61it/s][A[A

 84% 27/32 [00:07<00:01,  3.72it/s][A[A

 88% 28/32 [00:07<00:01,  3.94it/s][A[A

 91% 29/32 [00:07<00:00,  4.18it/s][A[A

 94% 30/32 [00:08<00:00,  4.20it/s][A[A

 97% 31/32 [00:08<00:00,  4.24it/s][A[A

100% 32/32 [00:08<00:00,  4.43it/s][A[A100% 32/32 [00:08<00:00,  3.70it/s]
Meta loss on this task batch = 5.6444e-01, PNorm = 36.5530, GNorm = 0.0646

 84% 16/19 [02:37<00:30, 10.28s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.16it/s][A[A

  6% 2/32 [00:00<00:06,  4.82it/s][A[A

  9% 3/32 [00:01<00:14,  1.99it/s][A[A

 12% 4/32 [00:01<00:11,  2.42it/s][A[A

 16% 5/32 [00:01<00:09,  2.96it/s][A[A

 19% 6/32 [00:02<00:07,  3.34it/s][A[A

 22% 7/32 [00:02<00:06,  3.99it/s][A[A

 25% 8/32 [00:02<00:05,  4.02it/s][A[A

 28% 9/32 [00:02<00:05,  4.22it/s][A[A

 31% 10/32 [00:02<00:04,  4.85it/s][A[A

 34% 11/32 [00:03<00:04,  4.97it/s][A[A

 38% 12/32 [00:03<00:04,  4.84it/s][A[A

 41% 13/32 [00:03<00:03,  5.43it/s][A[A

 44% 14/32 [00:03<00:03,  5.49it/s][A[A

 47% 15/32 [00:03<00:03,  5.24it/s][A[A

 50% 16/32 [00:04<00:03,  5.06it/s][A[A

 53% 17/32 [00:04<00:03,  4.79it/s][A[A

 56% 18/32 [00:04<00:02,  4.67it/s][A[A

 59% 19/32 [00:05<00:06,  1.93it/s][A[A

 62% 20/32 [00:05<00:05,  2.34it/s][A[A

 66% 21/32 [00:06<00:03,  2.79it/s][A[A

 69% 22/32 [00:06<00:03,  3.09it/s][A[A

 72% 23/32 [00:06<00:02,  3.48it/s][A[A

 75% 24/32 [00:06<00:02,  3.53it/s][A[A

 78% 25/32 [00:07<00:01,  3.75it/s][A[A

 81% 26/32 [00:07<00:01,  3.81it/s][A[A

 84% 27/32 [00:07<00:01,  4.07it/s][A[A

 88% 28/32 [00:07<00:00,  4.25it/s][A[A

 91% 29/32 [00:08<00:00,  4.24it/s][A[A

 94% 30/32 [00:09<00:01,  1.93it/s][A[A

 97% 31/32 [00:09<00:00,  2.38it/s][A[A

100% 32/32 [00:09<00:00,  2.88it/s][A[A100% 32/32 [00:09<00:00,  3.34it/s]
Meta loss on this task batch = 4.6462e-01, PNorm = 36.5576, GNorm = 0.1184

 89% 17/19 [02:47<00:20, 10.30s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.69it/s][A[A

  6% 2/32 [00:00<00:06,  4.71it/s][A[A

  9% 3/32 [00:00<00:06,  4.55it/s][A[A

 12% 4/32 [00:00<00:06,  4.63it/s][A[A

 16% 5/32 [00:01<00:05,  4.65it/s][A[A

 19% 6/32 [00:01<00:05,  4.77it/s][A[A

 22% 7/32 [00:01<00:05,  4.50it/s][A[A

 25% 8/32 [00:01<00:05,  4.34it/s][A[A

 28% 9/32 [00:01<00:05,  4.51it/s][A[A

 31% 10/32 [00:02<00:04,  4.50it/s][A[A

 34% 11/32 [00:03<00:10,  1.97it/s][A[A

 38% 12/32 [00:03<00:08,  2.40it/s][A[A

 41% 13/32 [00:03<00:07,  2.71it/s][A[A

 44% 14/32 [00:04<00:05,  3.13it/s][A[A

 47% 15/32 [00:04<00:05,  3.39it/s][A[A

 50% 16/32 [00:04<00:04,  3.80it/s][A[A

 53% 17/32 [00:04<00:03,  4.09it/s][A[A

 56% 18/32 [00:04<00:03,  4.41it/s][A[A

 59% 19/32 [00:05<00:02,  4.48it/s][A[A

 62% 20/32 [00:05<00:02,  4.47it/s][A[A

 66% 21/32 [00:05<00:02,  4.41it/s][A[A

 69% 22/32 [00:05<00:02,  4.61it/s][A[A

 72% 23/32 [00:07<00:04,  1.86it/s][A[A

 75% 24/32 [00:07<00:03,  2.20it/s][A[A

 78% 25/32 [00:07<00:02,  2.74it/s][A[A

 81% 26/32 [00:07<00:01,  3.14it/s][A[A

 84% 27/32 [00:07<00:01,  3.32it/s][A[A

 88% 28/32 [00:08<00:01,  3.75it/s][A[A

 91% 29/32 [00:08<00:00,  3.89it/s][A[A

 94% 30/32 [00:08<00:00,  4.29it/s][A[A

 97% 31/32 [00:08<00:00,  4.62it/s][A[A

100% 32/32 [00:08<00:00,  4.54it/s][A[A100% 32/32 [00:08<00:00,  3.59it/s]
Meta loss on this task batch = 5.1526e-01, PNorm = 36.5585, GNorm = 0.1080

 95% 18/19 [02:57<00:10, 10.11s/it][A

  0% 0/23 [00:00<?, ?it/s][A[A

  4% 1/23 [00:01<00:27,  1.25s/it][A[A

  9% 2/23 [00:01<00:19,  1.06it/s][A[A

 13% 3/23 [00:01<00:14,  1.39it/s][A[A

 17% 4/23 [00:01<00:10,  1.80it/s][A[A

 26% 6/23 [00:02<00:07,  2.32it/s][A[A

 30% 7/23 [00:02<00:05,  2.83it/s][A[A

 35% 8/23 [00:02<00:04,  3.11it/s][A[A

 39% 9/23 [00:02<00:03,  3.60it/s][A[A

 43% 10/23 [00:02<00:03,  3.70it/s][A[A

 48% 11/23 [00:03<00:02,  4.17it/s][A[A

 52% 12/23 [00:04<00:05,  1.85it/s][A[A

 57% 13/23 [00:04<00:04,  2.27it/s][A[A

 61% 14/23 [00:04<00:03,  2.89it/s][A[A

 65% 15/23 [00:05<00:02,  2.80it/s][A[A

 70% 16/23 [00:05<00:02,  3.07it/s][A[A

 74% 17/23 [00:05<00:01,  3.33it/s][A[A

 78% 18/23 [00:05<00:01,  3.67it/s][A[A

 83% 19/23 [00:06<00:01,  3.83it/s][A[A

 87% 20/23 [00:06<00:00,  4.18it/s][A[A

 91% 21/23 [00:06<00:00,  4.45it/s][A[A

 96% 22/23 [00:06<00:00,  4.72it/s][A[A

100% 23/23 [00:07<00:00,  1.93it/s][A[A100% 23/23 [00:07<00:00,  2.94it/s]
Meta loss on this task batch = 4.4784e-01, PNorm = 36.5565, GNorm = 0.1109

100% 19/19 [03:05<00:00,  9.60s/it][A100% 19/19 [03:05<00:00,  9.77s/it]
Took 185.6076431274414 seconds to complete one epoch of meta training

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/20 [00:00<?, ?it/s][A[A


  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 32.88it/s]


  5% 1/20 [00:00<00:03,  5.65it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 20.68it/s]


 10% 2/20 [00:00<00:03,  5.04it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 18.24it/s][A[A[A100% 3/3 [00:00<00:00, 20.07it/s]


 15% 3/20 [00:00<00:03,  4.42it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 15.79it/s]


 20% 4/20 [00:00<00:03,  4.58it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 14.91it/s][A[A[A100% 4/4 [00:00<00:00, 17.41it/s]


 25% 5/20 [00:01<00:03,  3.77it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.63it/s][A[A[A


100% 4/4 [00:00<00:00, 15.02it/s][A[A[A100% 4/4 [00:00<00:00, 16.11it/s]


 30% 6/20 [00:01<00:04,  3.25it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.85it/s][A[A[A100% 4/4 [00:00<00:00, 20.68it/s]


 35% 7/20 [00:03<00:08,  1.60it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 17.25it/s][A[A[A100% 4/4 [00:00<00:00, 19.58it/s]


 40% 8/20 [00:03<00:06,  1.82it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.57it/s][A[A[A100% 4/4 [00:00<00:00, 23.20it/s]


 45% 9/20 [00:03<00:05,  2.04it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.83it/s][A[A[A


100% 4/4 [00:00<00:00, 19.63it/s][A[A[A100% 4/4 [00:00<00:00, 19.47it/s]


 50% 10/20 [00:04<00:04,  2.20it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 15.83it/s][A[A[A100% 4/4 [00:00<00:00, 18.18it/s]


 55% 11/20 [00:04<00:03,  2.30it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.20it/s][A[A[A100% 4/4 [00:00<00:00, 22.62it/s]


 60% 12/20 [00:04<00:03,  2.47it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 15.18it/s][A[A[A100% 3/3 [00:00<00:00, 14.28it/s]


 65% 13/20 [00:05<00:02,  2.53it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.22it/s][A[A[A100% 3/3 [00:00<00:00, 13.78it/s]


 70% 14/20 [00:05<00:02,  2.54it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.88it/s][A[A[A


 75% 3/4 [00:01<00:00,  2.68it/s][A[A[A100% 4/4 [00:01<00:00,  3.27it/s]


 75% 15/20 [00:07<00:03,  1.44it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.29it/s][A[A[A100% 3/3 [00:00<00:00, 17.68it/s]


 80% 16/20 [00:07<00:02,  1.72it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 12.08it/s][A[A[A100% 3/3 [00:00<00:00, 14.94it/s]


 85% 17/20 [00:07<00:01,  1.91it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 22.94it/s]


 90% 18/20 [00:07<00:00,  2.29it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 15.66it/s][A[A[A100% 3/3 [00:00<00:00, 19.79it/s]


 95% 19/20 [00:08<00:00,  2.60it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 13.56it/s]


100% 20/20 [00:08<00:00,  2.94it/s][A[A100% 20/20 [00:08<00:00,  2.36it/s]

100% 1/1 [00:08<00:00,  8.48s/it][A100% 1/1 [00:08<00:00,  8.48s/it]
Took 194.09202480316162 seconds to complete one epoch of meta training and validating
Meta validation score auc = 0.604873
 33% 10/30 [32:08<1:04:02, 192.10s/it]Epoch 10

  0% 0/19 [00:00<?, ?it/s][A

  0% 0/32 [00:00<?, ?it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  3% 1/32 [00:00<00:06,  4.84it/s][A[A

  6% 2/32 [00:00<00:06,  4.47it/s][A[A

  9% 3/32 [00:00<00:06,  4.50it/s][A[A

 12% 4/32 [00:00<00:05,  4.96it/s][A[A

 16% 5/32 [00:00<00:04,  5.46it/s][A[A

 19% 6/32 [00:01<00:04,  5.45it/s][A[A

 22% 7/32 [00:01<00:04,  5.54it/s][A[A

 25% 8/32 [00:02<00:11,  2.04it/s][A[A

 28% 9/32 [00:02<00:08,  2.63it/s][A[A

 31% 10/32 [00:02<00:06,  3.29it/s][A[A

 34% 11/32 [00:03<00:06,  3.44it/s][A[A

 38% 12/32 [00:03<00:05,  3.54it/s][A[A

 41% 13/32 [00:03<00:05,  3.56it/s][A[A

 44% 14/32 [00:03<00:04,  3.72it/s][A[A

 47% 15/32 [00:04<00:04,  4.12it/s][A[A

 50% 16/32 [00:04<00:03,  4.26it/s][A[A

 53% 17/32 [00:04<00:03,  4.37it/s][A[A

 56% 18/32 [00:04<00:02,  4.75it/s][A[A

 59% 19/32 [00:04<00:02,  5.14it/s][A[A

 62% 20/32 [00:06<00:06,  1.96it/s][A[A

 66% 21/32 [00:06<00:04,  2.55it/s][A[A

 69% 22/32 [00:06<00:03,  3.00it/s][A[A

 72% 23/32 [00:06<00:02,  3.42it/s][A[A

 75% 24/32 [00:06<00:02,  3.69it/s][A[A

 78% 25/32 [00:06<00:01,  4.05it/s][A[A

 81% 26/32 [00:07<00:01,  4.19it/s][A[A

 84% 27/32 [00:07<00:01,  4.36it/s][A[A

 88% 28/32 [00:07<00:00,  4.72it/s][A[A

 91% 29/32 [00:07<00:00,  4.54it/s][A[A

 94% 30/32 [00:07<00:00,  5.37it/s][A[A

 97% 31/32 [00:08<00:00,  4.88it/s][A[A

100% 32/32 [00:09<00:00,  2.02it/s][A[A100% 32/32 [00:09<00:00,  3.44it/s]
Meta loss on this task batch = 5.2831e-01, PNorm = 36.5563, GNorm = 0.0558

  5% 1/19 [00:10<03:01, 10.07s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  7.68it/s][A[A

  6% 2/32 [00:00<00:04,  7.48it/s][A[A

  9% 3/32 [00:00<00:04,  6.80it/s][A[A

 12% 4/32 [00:00<00:04,  5.71it/s][A[A

 16% 5/32 [00:00<00:04,  5.73it/s][A[A

 19% 6/32 [00:01<00:04,  6.05it/s][A[A

 22% 7/32 [00:01<00:03,  6.44it/s][A[A

 25% 8/32 [00:01<00:03,  6.59it/s][A[A

 28% 9/32 [00:01<00:03,  6.49it/s][A[A

 31% 10/32 [00:01<00:03,  6.65it/s][A[A

 34% 11/32 [00:01<00:03,  6.24it/s][A[A

 38% 12/32 [00:01<00:03,  5.99it/s][A[A

 41% 13/32 [00:02<00:03,  6.27it/s][A[A

 44% 14/32 [00:02<00:03,  5.68it/s][A[A

 47% 15/32 [00:02<00:03,  5.41it/s][A[A

 50% 16/32 [00:03<00:07,  2.08it/s][A[A

 53% 17/32 [00:03<00:06,  2.47it/s][A[A

 56% 18/32 [00:04<00:04,  2.84it/s][A[A

 59% 19/32 [00:04<00:03,  3.28it/s][A[A

 62% 20/32 [00:04<00:03,  3.55it/s][A[A

 66% 21/32 [00:04<00:02,  3.88it/s][A[A

 69% 22/32 [00:04<00:02,  4.45it/s][A[A

 72% 23/32 [00:05<00:01,  4.91it/s][A[A

 75% 24/32 [00:05<00:01,  5.50it/s][A[A

 78% 25/32 [00:05<00:01,  5.37it/s][A[A

 81% 26/32 [00:05<00:01,  5.25it/s][A[A

 84% 27/32 [00:05<00:00,  5.50it/s][A[A

 88% 28/32 [00:05<00:00,  5.07it/s][A[A

 91% 29/32 [00:06<00:00,  5.66it/s][A[A

 94% 30/32 [00:07<00:00,  2.05it/s][A[A

 97% 31/32 [00:07<00:00,  2.47it/s][A[A

100% 32/32 [00:07<00:00,  3.02it/s][A[A100% 32/32 [00:07<00:00,  4.15it/s]
Meta loss on this task batch = 5.1670e-01, PNorm = 36.5566, GNorm = 0.0409

 11% 2/19 [00:18<02:42,  9.57s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.45it/s][A[A

  6% 2/32 [00:00<00:05,  5.54it/s][A[A

  9% 3/32 [00:00<00:05,  5.04it/s][A[A

 12% 4/32 [00:00<00:06,  4.66it/s][A[A

 16% 5/32 [00:01<00:05,  5.02it/s][A[A

 19% 6/32 [00:01<00:05,  4.85it/s][A[A

 22% 7/32 [00:01<00:04,  5.17it/s][A[A

 25% 8/32 [00:01<00:04,  5.53it/s][A[A

 28% 9/32 [00:01<00:03,  5.80it/s][A[A

 31% 10/32 [00:01<00:04,  5.32it/s][A[A

 34% 11/32 [00:02<00:04,  4.89it/s][A[A

 38% 12/32 [00:03<00:10,  1.98it/s][A[A

 41% 13/32 [00:03<00:07,  2.38it/s][A[A

 44% 14/32 [00:03<00:06,  2.76it/s][A[A

 47% 15/32 [00:04<00:05,  3.26it/s][A[A

 50% 16/32 [00:04<00:04,  3.92it/s][A[A

 53% 17/32 [00:04<00:03,  4.06it/s][A[A

 56% 18/32 [00:04<00:03,  4.65it/s][A[A

 59% 19/32 [00:04<00:02,  5.07it/s][A[A

 62% 20/32 [00:04<00:02,  5.59it/s][A[A

 66% 21/32 [00:04<00:01,  6.15it/s][A[A

 69% 22/32 [00:05<00:01,  6.42it/s][A[A

 72% 23/32 [00:05<00:01,  5.54it/s][A[A

 75% 24/32 [00:05<00:01,  5.25it/s][A[A

 78% 25/32 [00:05<00:01,  5.26it/s][A[A

 81% 26/32 [00:05<00:01,  5.31it/s][A[A

 84% 27/32 [00:06<00:01,  4.94it/s][A[A

 88% 28/32 [00:06<00:00,  5.20it/s][A[A

 91% 29/32 [00:07<00:01,  2.00it/s][A[A

 94% 30/32 [00:07<00:00,  2.35it/s][A[A

 97% 31/32 [00:07<00:00,  3.00it/s][A[A

100% 32/32 [00:08<00:00,  3.35it/s][A[A100% 32/32 [00:08<00:00,  3.95it/s]
Meta loss on this task batch = 5.0780e-01, PNorm = 36.5591, GNorm = 0.0347

 16% 3/19 [00:27<02:29,  9.35s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.17it/s][A[A

  6% 2/32 [00:00<00:06,  4.34it/s][A[A

  9% 3/32 [00:00<00:06,  4.68it/s][A[A

 12% 4/32 [00:00<00:06,  4.58it/s][A[A

 16% 5/32 [00:01<00:05,  4.68it/s][A[A

 19% 6/32 [00:01<00:05,  4.96it/s][A[A

 22% 7/32 [00:01<00:04,  5.60it/s][A[A

 25% 8/32 [00:01<00:04,  5.05it/s][A[A

 28% 9/32 [00:01<00:04,  5.67it/s][A[A

 31% 10/32 [00:01<00:04,  5.25it/s][A[A

 34% 11/32 [00:02<00:03,  5.57it/s][A[A

 38% 12/32 [00:02<00:03,  6.02it/s][A[A

 41% 13/32 [00:02<00:03,  5.55it/s][A[A

 44% 14/32 [00:02<00:03,  5.29it/s][A[A

 47% 15/32 [00:03<00:08,  2.07it/s][A[A

 50% 16/32 [00:04<00:06,  2.47it/s][A[A

 53% 17/32 [00:04<00:05,  2.93it/s][A[A

 56% 18/32 [00:04<00:04,  3.28it/s][A[A

 59% 19/32 [00:04<00:03,  3.92it/s][A[A

 62% 20/32 [00:04<00:02,  4.06it/s][A[A

 66% 21/32 [00:04<00:02,  4.46it/s][A[A

 69% 22/32 [00:05<00:02,  4.33it/s][A[A

 72% 23/32 [00:05<00:02,  4.28it/s][A[A

 75% 24/32 [00:05<00:01,  4.37it/s][A[A

 78% 25/32 [00:05<00:01,  5.07it/s][A[A

 81% 26/32 [00:05<00:01,  5.74it/s][A[A

 84% 27/32 [00:06<00:00,  5.34it/s][A[A

 88% 28/32 [00:06<00:00,  5.29it/s][A[A

 91% 29/32 [00:07<00:01,  2.11it/s][A[A

 94% 30/32 [00:07<00:00,  2.48it/s][A[A

 97% 31/32 [00:07<00:00,  3.08it/s][A[A

100% 32/32 [00:08<00:00,  3.51it/s][A[A100% 32/32 [00:08<00:00,  3.97it/s]
Meta loss on this task batch = 5.7403e-01, PNorm = 36.5620, GNorm = 0.0220

 21% 4/19 [00:36<02:17,  9.19s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.45it/s][A[A

  6% 2/32 [00:00<00:05,  5.14it/s][A[A

  9% 3/32 [00:00<00:05,  4.94it/s][A[A

 12% 4/32 [00:00<00:06,  4.59it/s][A[A

 16% 5/32 [00:01<00:05,  4.68it/s][A[A

 19% 6/32 [00:01<00:05,  4.79it/s][A[A

 22% 7/32 [00:01<00:05,  4.71it/s][A[A

 25% 8/32 [00:01<00:04,  5.30it/s][A[A

 28% 9/32 [00:01<00:04,  5.15it/s][A[A

 31% 10/32 [00:01<00:04,  5.41it/s][A[A

 34% 11/32 [00:02<00:04,  5.22it/s][A[A

 38% 12/32 [00:03<00:09,  2.01it/s][A[A

 41% 13/32 [00:03<00:08,  2.36it/s][A[A

 44% 14/32 [00:03<00:06,  2.92it/s][A[A

 47% 15/32 [00:03<00:05,  3.26it/s][A[A

 50% 16/32 [00:04<00:04,  3.79it/s][A[A

 53% 17/32 [00:04<00:03,  4.16it/s][A[A

 56% 18/32 [00:04<00:03,  4.39it/s][A[A

 59% 19/32 [00:04<00:03,  4.27it/s][A[A

 62% 20/32 [00:05<00:02,  4.28it/s][A[A

 66% 21/32 [00:05<00:02,  4.28it/s][A[A

 69% 22/32 [00:05<00:02,  4.65it/s][A[A

 72% 23/32 [00:05<00:01,  4.66it/s][A[A

 75% 24/32 [00:05<00:01,  4.46it/s][A[A

 78% 25/32 [00:07<00:03,  1.92it/s][A[A

 81% 26/32 [00:07<00:02,  2.44it/s][A[A

 84% 27/32 [00:07<00:01,  2.93it/s][A[A

 88% 28/32 [00:07<00:01,  3.49it/s][A[A

 91% 29/32 [00:07<00:00,  3.66it/s][A[A

 94% 30/32 [00:07<00:00,  4.19it/s][A[A

 97% 31/32 [00:08<00:00,  4.54it/s][A[A

100% 32/32 [00:08<00:00,  4.53it/s][A[A100% 32/32 [00:08<00:00,  3.81it/s]
Meta loss on this task batch = 5.2349e-01, PNorm = 36.5656, GNorm = 0.0225

 26% 5/19 [00:45<02:08,  9.17s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  6.30it/s][A[A

  6% 2/32 [00:00<00:05,  5.83it/s][A[A

  9% 3/32 [00:00<00:04,  6.11it/s][A[A

 12% 4/32 [00:00<00:05,  5.48it/s][A[A

 16% 5/32 [00:02<00:14,  1.92it/s][A[A

 19% 6/32 [00:02<00:11,  2.26it/s][A[A

 22% 7/32 [00:02<00:09,  2.75it/s][A[A

 25% 8/32 [00:02<00:07,  3.11it/s][A[A

 28% 9/32 [00:02<00:06,  3.56it/s][A[A

 31% 10/32 [00:03<00:05,  3.70it/s][A[A

 34% 11/32 [00:03<00:05,  3.85it/s][A[A

 38% 12/32 [00:03<00:04,  4.07it/s][A[A

 41% 13/32 [00:03<00:04,  4.66it/s][A[A

 44% 14/32 [00:04<00:09,  1.93it/s][A[A

 47% 15/32 [00:05<00:07,  2.34it/s][A[A

 50% 16/32 [00:05<00:05,  2.69it/s][A[A

 53% 17/32 [00:05<00:04,  3.27it/s][A[A

 56% 18/32 [00:05<00:04,  3.45it/s][A[A

 59% 19/32 [00:06<00:03,  3.68it/s][A[A

 62% 20/32 [00:06<00:02,  4.00it/s][A[A

 66% 21/32 [00:06<00:02,  4.23it/s][A[A

 69% 22/32 [00:06<00:02,  4.28it/s][A[A

 72% 23/32 [00:06<00:02,  4.36it/s][A[A

 75% 24/32 [00:07<00:01,  4.72it/s][A[A

 78% 25/32 [00:07<00:01,  4.60it/s][A[A

 81% 26/32 [00:07<00:01,  5.12it/s][A[A

 84% 27/32 [00:07<00:01,  4.83it/s][A[A

 88% 28/32 [00:08<00:02,  1.86it/s][A[A

 91% 29/32 [00:09<00:01,  2.25it/s][A[A

 94% 30/32 [00:09<00:00,  2.69it/s][A[A

 97% 31/32 [00:09<00:00,  3.12it/s][A[A

100% 32/32 [00:09<00:00,  3.50it/s][A[A100% 32/32 [00:09<00:00,  3.26it/s]
Meta loss on this task batch = 4.4221e-01, PNorm = 36.5701, GNorm = 0.0755

 32% 6/19 [00:55<02:04,  9.60s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.12it/s][A[A

  6% 2/32 [00:00<00:05,  5.10it/s][A[A

  9% 3/32 [00:00<00:05,  5.05it/s][A[A

 12% 4/32 [00:00<00:05,  4.83it/s][A[A

 16% 5/32 [00:00<00:04,  5.41it/s][A[A

 19% 6/32 [00:01<00:04,  5.33it/s][A[A

 22% 7/32 [00:01<00:04,  5.07it/s][A[A

 25% 8/32 [00:01<00:04,  4.93it/s][A[A

 28% 9/32 [00:01<00:04,  5.16it/s][A[A

 31% 10/32 [00:02<00:11,  1.98it/s][A[A

 34% 11/32 [00:03<00:08,  2.38it/s][A[A

 38% 12/32 [00:03<00:07,  2.77it/s][A[A

 41% 13/32 [00:03<00:06,  3.10it/s][A[A

 44% 14/32 [00:03<00:04,  3.68it/s][A[A

 47% 15/32 [00:04<00:04,  3.85it/s][A[A

 50% 16/32 [00:04<00:04,  3.96it/s][A[A

 53% 17/32 [00:04<00:03,  4.06it/s][A[A

 56% 18/32 [00:04<00:03,  4.29it/s][A[A

 59% 19/32 [00:04<00:02,  4.58it/s][A[A

 62% 20/32 [00:05<00:02,  4.48it/s][A[A

 66% 21/32 [00:06<00:05,  1.90it/s][A[A

 69% 22/32 [00:06<00:04,  2.33it/s][A[A

 72% 23/32 [00:06<00:03,  2.67it/s][A[A

 75% 24/32 [00:07<00:02,  3.05it/s][A[A

 78% 25/32 [00:07<00:02,  3.41it/s][A[A

 81% 26/32 [00:07<00:01,  3.63it/s][A[A

 84% 27/32 [00:07<00:01,  3.83it/s][A[A

 88% 28/32 [00:07<00:01,  3.92it/s][A[A

 91% 29/32 [00:08<00:00,  4.15it/s][A[A

 94% 30/32 [00:08<00:00,  4.37it/s][A[A

 97% 31/32 [00:09<00:00,  1.91it/s][A[A

100% 32/32 [00:09<00:00,  2.32it/s][A[A100% 32/32 [00:09<00:00,  3.27it/s]
Meta loss on this task batch = 4.7113e-01, PNorm = 36.5762, GNorm = 0.0614

 37% 7/19 [01:06<01:58,  9.90s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.69it/s][A[A

  6% 2/32 [00:00<00:06,  4.45it/s][A[A

  9% 3/32 [00:00<00:06,  4.28it/s][A[A

 12% 4/32 [00:00<00:06,  4.18it/s][A[A

 16% 5/32 [00:01<00:06,  4.26it/s][A[A

 19% 6/32 [00:01<00:05,  4.38it/s][A[A

 22% 7/32 [00:01<00:05,  4.49it/s][A[A

 25% 8/32 [00:01<00:05,  4.30it/s][A[A

 28% 9/32 [00:02<00:05,  4.31it/s][A[A

 31% 10/32 [00:02<00:05,  4.35it/s][A[A

 34% 11/32 [00:03<00:11,  1.89it/s][A[A

 38% 12/32 [00:03<00:08,  2.30it/s][A[A

 41% 13/32 [00:03<00:06,  2.75it/s][A[A

 44% 14/32 [00:04<00:05,  3.13it/s][A[A

 47% 15/32 [00:04<00:04,  3.48it/s][A[A

 50% 16/32 [00:04<00:04,  3.74it/s][A[A

 53% 17/32 [00:04<00:03,  3.79it/s][A[A

 56% 18/32 [00:05<00:03,  3.96it/s][A[A

 59% 19/32 [00:05<00:03,  3.92it/s][A[A

 62% 20/32 [00:05<00:02,  4.02it/s][A[A

 66% 21/32 [00:05<00:02,  4.18it/s][A[A

 69% 22/32 [00:06<00:02,  4.22it/s][A[A

 72% 23/32 [00:07<00:04,  1.87it/s][A[A

 75% 24/32 [00:07<00:03,  2.23it/s][A[A

 78% 25/32 [00:07<00:02,  2.65it/s][A[A

 81% 26/32 [00:07<00:02,  2.97it/s][A[A

 84% 27/32 [00:08<00:01,  3.23it/s][A[A

 88% 28/32 [00:08<00:01,  3.50it/s][A[A

 91% 29/32 [00:08<00:00,  3.64it/s][A[A

 94% 30/32 [00:08<00:00,  3.74it/s][A[A

 97% 31/32 [00:09<00:00,  3.88it/s][A[A

100% 32/32 [00:09<00:00,  4.05it/s][A[A100% 32/32 [00:09<00:00,  3.40it/s]
Meta loss on this task batch = 3.8143e-01, PNorm = 36.5849, GNorm = 0.0847

 42% 8/19 [01:16<01:50, 10.00s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.09it/s][A[A

  6% 2/32 [00:00<00:07,  4.21it/s][A[A

  9% 3/32 [00:00<00:06,  4.30it/s][A[A

 12% 4/32 [00:01<00:14,  1.87it/s][A[A

 16% 5/32 [00:02<00:12,  2.24it/s][A[A

 19% 6/32 [00:02<00:09,  2.62it/s][A[A

 22% 7/32 [00:02<00:08,  2.99it/s][A[A

 25% 8/32 [00:02<00:07,  3.30it/s][A[A

 28% 9/32 [00:03<00:06,  3.48it/s][A[A

 31% 10/32 [00:03<00:06,  3.62it/s][A[A

 34% 11/32 [00:03<00:05,  3.76it/s][A[A

 38% 12/32 [00:03<00:05,  3.93it/s][A[A

 41% 13/32 [00:04<00:04,  4.03it/s][A[A

 44% 14/32 [00:05<00:09,  1.83it/s][A[A

 47% 15/32 [00:05<00:07,  2.20it/s][A[A

 50% 16/32 [00:05<00:06,  2.57it/s][A[A

 53% 17/32 [00:06<00:05,  2.91it/s][A[A

 56% 18/32 [00:06<00:04,  3.20it/s][A[A

 59% 19/32 [00:06<00:03,  3.40it/s][A[A

 62% 20/32 [00:06<00:03,  3.54it/s][A[A

 66% 21/32 [00:06<00:02,  3.74it/s][A[A

 69% 22/32 [00:07<00:02,  3.76it/s][A[A

 72% 23/32 [00:07<00:02,  3.85it/s][A[A

 75% 24/32 [00:07<00:02,  3.91it/s][A[A

 78% 25/32 [00:08<00:03,  1.83it/s][A[A

 81% 26/32 [00:09<00:02,  2.22it/s][A[A

 84% 27/32 [00:09<00:01,  2.57it/s][A[A

 88% 28/32 [00:09<00:01,  2.86it/s][A[A

 91% 29/32 [00:09<00:00,  3.14it/s][A[A

 94% 30/32 [00:10<00:00,  3.36it/s][A[A

 97% 31/32 [00:10<00:00,  3.51it/s][A[A

100% 32/32 [00:10<00:00,  3.67it/s][A[A100% 32/32 [00:10<00:00,  2.99it/s]
Meta loss on this task batch = 2.2694e-01, PNorm = 36.5974, GNorm = 0.1271

 47% 9/19 [01:28<01:44, 10.46s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:38,  1.24s/it][A[A

  6% 2/32 [00:01<00:28,  1.07it/s][A[A

  9% 3/32 [00:01<00:21,  1.38it/s][A[A

 12% 4/32 [00:01<00:16,  1.73it/s][A[A

 16% 5/32 [00:02<00:12,  2.10it/s][A[A

 19% 6/32 [00:02<00:10,  2.47it/s][A[A

 22% 7/32 [00:02<00:08,  2.78it/s][A[A

 25% 8/32 [00:02<00:07,  3.08it/s][A[A

 28% 9/32 [00:03<00:06,  3.30it/s][A[A

 31% 10/32 [00:03<00:06,  3.49it/s][A[A

 34% 11/32 [00:04<00:11,  1.76it/s][A[A

 38% 12/32 [00:04<00:09,  2.12it/s][A[A

 41% 13/32 [00:05<00:07,  2.49it/s][A[A

 44% 14/32 [00:05<00:06,  2.80it/s][A[A

 47% 15/32 [00:05<00:05,  3.06it/s][A[A

 50% 16/32 [00:05<00:04,  3.30it/s][A[A

 53% 17/32 [00:06<00:04,  3.51it/s][A[A

 56% 18/32 [00:06<00:03,  3.73it/s][A[A

 59% 19/32 [00:06<00:03,  3.90it/s][A[A

 62% 20/32 [00:06<00:03,  3.95it/s][A[A

 66% 21/32 [00:07<00:02,  4.00it/s][A[A

 69% 22/32 [00:08<00:05,  1.83it/s][A[A

 72% 23/32 [00:08<00:04,  2.18it/s][A[A

 75% 24/32 [00:08<00:03,  2.50it/s][A[A

 78% 25/32 [00:09<00:02,  2.80it/s][A[A

 81% 26/32 [00:09<00:01,  3.08it/s][A[A

 84% 27/32 [00:09<00:01,  3.37it/s][A[A

 88% 28/32 [00:09<00:01,  3.55it/s][A[A

 91% 29/32 [00:10<00:00,  3.74it/s][A[A

 94% 30/32 [00:10<00:00,  3.85it/s][A[A

 97% 31/32 [00:11<00:00,  1.83it/s][A[A

100% 32/32 [00:11<00:00,  2.21it/s][A[A100% 32/32 [00:11<00:00,  2.73it/s]
Meta loss on this task batch = 2.9585e-01, PNorm = 36.6122, GNorm = 0.0984

 53% 10/19 [01:40<01:39, 11.10s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.19it/s][A[A

  6% 2/32 [00:00<00:07,  4.17it/s][A[A

  9% 3/32 [00:00<00:06,  4.20it/s][A[A

 12% 4/32 [00:00<00:06,  4.18it/s][A[A

 16% 5/32 [00:01<00:06,  4.24it/s][A[A

 19% 6/32 [00:01<00:06,  4.24it/s][A[A

 22% 7/32 [00:01<00:05,  4.26it/s][A[A

 25% 8/32 [00:02<00:12,  1.87it/s][A[A

 28% 9/32 [00:03<00:10,  2.23it/s][A[A

 31% 10/32 [00:03<00:08,  2.58it/s][A[A

 34% 11/32 [00:03<00:07,  2.94it/s][A[A

 38% 12/32 [00:03<00:06,  3.24it/s][A[A

 41% 13/32 [00:04<00:05,  3.47it/s][A[A

 44% 14/32 [00:04<00:04,  3.64it/s][A[A

 47% 15/32 [00:04<00:04,  3.75it/s][A[A

 50% 16/32 [00:04<00:03,  4.10it/s][A[A

 53% 17/32 [00:05<00:03,  4.06it/s][A[A

 56% 18/32 [00:05<00:03,  4.05it/s][A[A

 59% 19/32 [00:05<00:03,  4.04it/s][A[A

 62% 20/32 [00:05<00:02,  4.06it/s][A[A

 66% 21/32 [00:06<00:02,  4.08it/s][A[A

 69% 22/32 [00:06<00:02,  4.08it/s][A[A

 72% 23/32 [00:06<00:02,  4.16it/s][A[A

 75% 24/32 [00:07<00:04,  1.87it/s][A[A

 78% 25/32 [00:07<00:03,  2.24it/s][A[A

 81% 26/32 [00:08<00:02,  2.58it/s][A[A

 84% 27/32 [00:08<00:01,  2.91it/s][A[A

 88% 28/32 [00:08<00:01,  3.14it/s][A[A

 91% 29/32 [00:08<00:00,  3.36it/s][A[A

 94% 30/32 [00:09<00:00,  3.53it/s][A[A

 97% 31/32 [00:10<00:00,  1.76it/s][A[A

100% 32/32 [00:10<00:00,  2.21it/s][A[A100% 32/32 [00:10<00:00,  3.02it/s]
Meta loss on this task batch = 5.8274e-01, PNorm = 36.6272, GNorm = 0.0196

 58% 11/19 [01:52<01:29, 11.20s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.05it/s][A[A

  6% 2/32 [00:00<00:06,  4.29it/s][A[A

  9% 3/32 [00:00<00:06,  4.27it/s][A[A

 12% 4/32 [00:00<00:06,  4.30it/s][A[A

 16% 5/32 [00:01<00:06,  4.26it/s][A[A

 19% 6/32 [00:01<00:06,  4.26it/s][A[A

 22% 7/32 [00:01<00:05,  4.26it/s][A[A

 25% 8/32 [00:01<00:05,  4.27it/s][A[A

 28% 9/32 [00:02<00:05,  4.22it/s][A[A

 31% 10/32 [00:03<00:11,  1.87it/s][A[A

 34% 11/32 [00:03<00:09,  2.23it/s][A[A

 38% 12/32 [00:03<00:07,  2.59it/s][A[A

 41% 13/32 [00:04<00:06,  2.93it/s][A[A

 44% 14/32 [00:04<00:05,  3.21it/s][A[A

 47% 15/32 [00:04<00:04,  3.46it/s][A[A

 50% 16/32 [00:04<00:04,  3.68it/s][A[A

 53% 17/32 [00:05<00:03,  3.77it/s][A[A

 56% 18/32 [00:06<00:07,  1.81it/s][A[A

 59% 19/32 [00:06<00:05,  2.26it/s][A[A

 62% 20/32 [00:06<00:04,  2.63it/s][A[A

 66% 21/32 [00:06<00:03,  3.00it/s][A[A

 69% 22/32 [00:07<00:03,  3.30it/s][A[A

 72% 23/32 [00:07<00:02,  3.55it/s][A[A

 75% 24/32 [00:07<00:01,  4.01it/s][A[A

 78% 25/32 [00:07<00:01,  4.17it/s][A[A

 81% 26/32 [00:07<00:01,  4.33it/s][A[A

 84% 27/32 [00:08<00:01,  4.29it/s][A[A

 88% 28/32 [00:08<00:00,  4.25it/s][A[A

 91% 29/32 [00:09<00:01,  1.89it/s][A[A

 94% 30/32 [00:09<00:00,  2.38it/s][A[A

 97% 31/32 [00:10<00:00,  2.74it/s][A[A

100% 32/32 [00:10<00:00,  3.06it/s][A[A100% 32/32 [00:10<00:00,  3.11it/s]
Meta loss on this task batch = 5.7577e-01, PNorm = 36.6407, GNorm = 0.0293

 63% 12/19 [02:03<01:18, 11.17s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.58it/s][A[A

  6% 2/32 [00:00<00:06,  4.48it/s][A[A

  9% 3/32 [00:00<00:06,  4.43it/s][A[A

 12% 4/32 [00:00<00:06,  4.41it/s][A[A

 16% 5/32 [00:02<00:14,  1.89it/s][A[A

 19% 6/32 [00:02<00:11,  2.26it/s][A[A

 22% 7/32 [00:02<00:09,  2.62it/s][A[A

 25% 8/32 [00:02<00:08,  2.93it/s][A[A

 28% 9/32 [00:03<00:07,  3.28it/s][A[A

 31% 10/32 [00:03<00:06,  3.53it/s][A[A

 34% 11/32 [00:03<00:05,  3.73it/s][A[A

 38% 12/32 [00:03<00:05,  3.90it/s][A[A

 41% 13/32 [00:05<00:10,  1.83it/s][A[A

 44% 14/32 [00:05<00:08,  2.19it/s][A[A

 47% 15/32 [00:05<00:06,  2.55it/s][A[A

 50% 16/32 [00:05<00:05,  2.88it/s][A[A

 53% 17/32 [00:05<00:04,  3.17it/s][A[A

 56% 18/32 [00:06<00:04,  3.40it/s][A[A

 59% 19/32 [00:07<00:07,  1.73it/s][A[A

 62% 20/32 [00:07<00:05,  2.09it/s][A[A

 66% 21/32 [00:07<00:04,  2.46it/s][A[A

 69% 22/32 [00:08<00:03,  2.79it/s][A[A

 72% 23/32 [00:08<00:02,  3.25it/s][A[A

 75% 24/32 [00:08<00:02,  3.44it/s][A[A

 78% 25/32 [00:08<00:01,  3.66it/s][A[A

 81% 26/32 [00:10<00:03,  1.77it/s][A[A

 84% 27/32 [00:10<00:02,  2.16it/s][A[A

 88% 28/32 [00:10<00:01,  2.52it/s][A[A

 91% 29/32 [00:10<00:01,  2.84it/s][A[A

 94% 30/32 [00:11<00:00,  3.15it/s][A[A

 97% 31/32 [00:11<00:00,  3.60it/s][A[A

100% 32/32 [00:11<00:00,  3.72it/s][A[A100% 32/32 [00:11<00:00,  2.78it/s]
Meta loss on this task batch = 5.9428e-01, PNorm = 36.6539, GNorm = 0.0242

 68% 13/19 [02:15<01:09, 11.51s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.07it/s][A[A

  6% 2/32 [00:01<00:16,  1.85it/s][A[A

  9% 3/32 [00:01<00:13,  2.23it/s][A[A

 12% 4/32 [00:01<00:10,  2.61it/s][A[A

 16% 5/32 [00:02<00:08,  3.01it/s][A[A

 19% 6/32 [00:02<00:07,  3.34it/s][A[A

 22% 7/32 [00:02<00:07,  3.57it/s][A[A

 25% 8/32 [00:02<00:06,  3.83it/s][A[A

 28% 9/32 [00:03<00:05,  3.97it/s][A[A

 31% 10/32 [00:03<00:05,  4.07it/s][A[A

 34% 11/32 [00:03<00:05,  4.13it/s][A[A

 38% 12/32 [00:03<00:04,  4.17it/s][A[A

 41% 13/32 [00:03<00:04,  4.18it/s][A[A

 44% 14/32 [00:04<00:04,  4.18it/s][A[A

 47% 15/32 [00:05<00:09,  1.87it/s][A[A

 50% 16/32 [00:05<00:07,  2.26it/s][A[A

 53% 17/32 [00:05<00:05,  2.62it/s][A[A

 56% 18/32 [00:06<00:04,  2.99it/s][A[A

 59% 19/32 [00:06<00:03,  3.45it/s][A[A

 62% 20/32 [00:06<00:03,  3.65it/s][A[A

 66% 21/32 [00:06<00:02,  3.79it/s][A[A

 69% 22/32 [00:07<00:02,  4.08it/s][A[A

 72% 23/32 [00:07<00:02,  4.12it/s][A[A

 75% 24/32 [00:07<00:01,  4.52it/s][A[A

 78% 25/32 [00:07<00:01,  4.39it/s][A[A

 81% 26/32 [00:08<00:03,  1.93it/s][A[A

 84% 27/32 [00:09<00:02,  2.37it/s][A[A

 88% 28/32 [00:09<00:01,  2.74it/s][A[A

 91% 29/32 [00:09<00:00,  3.08it/s][A[A

 94% 30/32 [00:09<00:00,  3.44it/s][A[A

 97% 31/32 [00:09<00:00,  3.63it/s][A[A

100% 32/32 [00:10<00:00,  3.71it/s][A[A100% 32/32 [00:10<00:00,  3.13it/s]
Meta loss on this task batch = 5.6334e-01, PNorm = 36.6671, GNorm = 0.0180

 74% 14/19 [02:26<00:56, 11.37s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.53it/s][A[A

  6% 2/32 [00:00<00:06,  4.32it/s][A[A

  9% 3/32 [00:00<00:06,  4.28it/s][A[A

 12% 4/32 [00:00<00:06,  4.31it/s][A[A

 16% 5/32 [00:02<00:14,  1.90it/s][A[A

 19% 6/32 [00:02<00:11,  2.22it/s][A[A

 22% 7/32 [00:02<00:09,  2.55it/s][A[A

 25% 8/32 [00:02<00:07,  3.04it/s][A[A

 28% 9/32 [00:03<00:06,  3.30it/s][A[A

 31% 10/32 [00:03<00:06,  3.57it/s][A[A

 34% 11/32 [00:03<00:05,  3.88it/s][A[A

 38% 12/32 [00:03<00:04,  4.37it/s][A[A

 41% 13/32 [00:03<00:04,  4.57it/s][A[A

 44% 14/32 [00:04<00:04,  4.47it/s][A[A

 47% 15/32 [00:04<00:03,  4.77it/s][A[A

 50% 16/32 [00:04<00:03,  4.72it/s][A[A

 53% 17/32 [00:05<00:07,  1.92it/s][A[A

 56% 18/32 [00:05<00:05,  2.33it/s][A[A

 59% 19/32 [00:06<00:04,  2.88it/s][A[A

 62% 20/32 [00:06<00:03,  3.36it/s][A[A

 66% 21/32 [00:06<00:02,  3.83it/s][A[A

 69% 22/32 [00:06<00:02,  4.01it/s][A[A

 72% 23/32 [00:06<00:02,  4.02it/s][A[A

 75% 24/32 [00:07<00:01,  4.17it/s][A[A

 78% 25/32 [00:07<00:01,  4.26it/s][A[A

 81% 26/32 [00:07<00:01,  4.19it/s][A[A

 84% 27/32 [00:07<00:01,  4.51it/s][A[A

 88% 28/32 [00:09<00:02,  1.90it/s][A[A

 91% 29/32 [00:09<00:01,  2.27it/s][A[A

 94% 30/32 [00:09<00:00,  2.62it/s][A[A

 97% 31/32 [00:09<00:00,  2.99it/s][A[A

100% 32/32 [00:10<00:00,  3.31it/s][A[A100% 32/32 [00:10<00:00,  3.20it/s]
Meta loss on this task batch = 4.7415e-01, PNorm = 36.6798, GNorm = 0.0423

 79% 15/19 [02:37<00:44, 11.20s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.08it/s][A[A

  6% 2/32 [00:00<00:07,  4.18it/s][A[A

  9% 3/32 [00:00<00:06,  4.22it/s][A[A

 12% 4/32 [00:00<00:06,  4.23it/s][A[A

 16% 5/32 [00:01<00:06,  4.25it/s][A[A

 19% 6/32 [00:01<00:06,  4.33it/s][A[A

 22% 7/32 [00:02<00:13,  1.90it/s][A[A

 25% 8/32 [00:02<00:10,  2.26it/s][A[A

 28% 9/32 [00:03<00:08,  2.60it/s][A[A

 31% 10/32 [00:03<00:07,  2.92it/s][A[A

 34% 11/32 [00:03<00:06,  3.21it/s][A[A

 38% 12/32 [00:03<00:05,  3.47it/s][A[A

 41% 13/32 [00:04<00:05,  3.68it/s][A[A

 44% 14/32 [00:04<00:04,  3.87it/s][A[A

 47% 15/32 [00:05<00:09,  1.83it/s][A[A

 50% 16/32 [00:05<00:06,  2.32it/s][A[A

 53% 17/32 [00:05<00:05,  2.68it/s][A[A

 56% 18/32 [00:06<00:04,  3.15it/s][A[A

 59% 19/32 [00:06<00:03,  3.40it/s][A[A

 62% 20/32 [00:06<00:03,  3.63it/s][A[A

 66% 21/32 [00:06<00:02,  3.85it/s][A[A

 69% 22/32 [00:06<00:02,  4.13it/s][A[A

 72% 23/32 [00:07<00:02,  4.17it/s][A[A

 75% 24/32 [00:08<00:04,  1.84it/s][A[A

 78% 25/32 [00:08<00:03,  2.27it/s][A[A

 81% 26/32 [00:08<00:02,  2.65it/s][A[A

 84% 27/32 [00:09<00:01,  2.95it/s][A[A

 88% 28/32 [00:09<00:01,  3.30it/s][A[A

 91% 29/32 [00:09<00:00,  3.39it/s][A[A

 94% 30/32 [00:09<00:00,  3.61it/s][A[A

 97% 31/32 [00:10<00:00,  3.82it/s][A[A

100% 32/32 [00:10<00:00,  3.96it/s][A[A100% 32/32 [00:10<00:00,  3.09it/s]
Meta loss on this task batch = 5.4951e-01, PNorm = 36.6890, GNorm = 0.0585

 84% 16/19 [02:48<00:33, 11.19s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.15it/s][A[A

  6% 2/32 [00:00<00:07,  4.07it/s][A[A

  9% 3/32 [00:01<00:15,  1.86it/s][A[A

 12% 4/32 [00:01<00:12,  2.31it/s][A[A

 16% 5/32 [00:02<00:10,  2.64it/s][A[A

 19% 6/32 [00:02<00:07,  3.26it/s][A[A

 22% 7/32 [00:02<00:06,  3.57it/s][A[A

 25% 8/32 [00:02<00:06,  3.67it/s][A[A

 28% 9/32 [00:02<00:05,  3.89it/s][A[A

 31% 10/32 [00:03<00:04,  4.43it/s][A[A

 34% 11/32 [00:03<00:04,  4.49it/s][A[A

 38% 12/32 [00:03<00:04,  4.67it/s][A[A

 41% 13/32 [00:03<00:04,  4.54it/s][A[A

 44% 14/32 [00:04<00:03,  4.54it/s][A[A

 47% 15/32 [00:04<00:03,  4.41it/s][A[A

 50% 16/32 [00:04<00:03,  4.32it/s][A[A

 53% 17/32 [00:04<00:03,  4.60it/s][A[A

 56% 18/32 [00:05<00:07,  1.95it/s][A[A

 59% 19/32 [00:06<00:05,  2.38it/s][A[A

 62% 20/32 [00:06<00:04,  2.79it/s][A[A

 66% 21/32 [00:06<00:03,  3.28it/s][A[A

 69% 22/32 [00:06<00:02,  3.46it/s][A[A

 72% 23/32 [00:06<00:02,  3.56it/s][A[A

 75% 24/32 [00:07<00:02,  3.56it/s][A[A

 78% 25/32 [00:08<00:03,  1.77it/s][A[A

 81% 26/32 [00:08<00:02,  2.13it/s][A[A

 84% 27/32 [00:08<00:01,  2.62it/s][A[A

 88% 28/32 [00:09<00:01,  3.09it/s][A[A

 91% 29/32 [00:09<00:00,  3.36it/s][A[A

 94% 30/32 [00:09<00:00,  3.57it/s][A[A

 97% 31/32 [00:09<00:00,  3.91it/s][A[A

100% 32/32 [00:10<00:00,  3.96it/s][A[A100% 32/32 [00:10<00:00,  3.19it/s]
Meta loss on this task batch = 4.5098e-01, PNorm = 36.6960, GNorm = 0.0527

 89% 17/19 [02:59<00:22, 11.08s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.14it/s][A[A

  6% 2/32 [00:00<00:06,  4.94it/s][A[A

  9% 3/32 [00:00<00:06,  4.82it/s][A[A

 12% 4/32 [00:01<00:14,  1.94it/s][A[A

 16% 5/32 [00:02<00:11,  2.38it/s][A[A

 19% 6/32 [00:02<00:09,  2.85it/s][A[A

 22% 7/32 [00:02<00:08,  3.12it/s][A[A

 25% 8/32 [00:02<00:07,  3.36it/s][A[A

 28% 9/32 [00:02<00:06,  3.76it/s][A[A

 31% 10/32 [00:03<00:05,  3.94it/s][A[A

 34% 11/32 [00:03<00:05,  4.06it/s][A[A

 38% 12/32 [00:03<00:04,  4.38it/s][A[A

 41% 13/32 [00:03<00:04,  4.15it/s][A[A

 44% 14/32 [00:04<00:03,  4.65it/s][A[A

 47% 15/32 [00:04<00:03,  4.35it/s][A[A

 50% 16/32 [00:04<00:03,  4.41it/s][A[A

 53% 17/32 [00:05<00:07,  2.00it/s][A[A

 56% 18/32 [00:05<00:05,  2.41it/s][A[A

 59% 19/32 [00:06<00:04,  2.75it/s][A[A

 62% 20/32 [00:06<00:03,  3.07it/s][A[A

 66% 21/32 [00:06<00:03,  3.36it/s][A[A

 69% 22/32 [00:06<00:02,  3.99it/s][A[A

 72% 23/32 [00:06<00:02,  4.28it/s][A[A

 75% 24/32 [00:07<00:01,  4.02it/s][A[A

 78% 25/32 [00:07<00:01,  4.09it/s][A[A

 81% 26/32 [00:07<00:01,  4.20it/s][A[A

 84% 27/32 [00:08<00:02,  1.80it/s][A[A

 88% 28/32 [00:09<00:01,  2.18it/s][A[A

 91% 29/32 [00:09<00:01,  2.58it/s][A[A

 94% 30/32 [00:09<00:00,  2.92it/s][A[A

 97% 31/32 [00:09<00:00,  3.31it/s][A[A

100% 32/32 [00:10<00:00,  3.70it/s][A[A100% 32/32 [00:10<00:00,  3.19it/s]
Meta loss on this task batch = 5.0952e-01, PNorm = 36.7004, GNorm = 0.0788

 95% 18/19 [03:10<00:11, 11.01s/it][A

  0% 0/23 [00:00<?, ?it/s][A[A

  4% 1/23 [00:00<00:05,  3.99it/s][A[A

  9% 2/23 [00:00<00:04,  4.36it/s][A[A

 13% 3/23 [00:00<00:04,  4.41it/s][A[A

 17% 4/23 [00:00<00:04,  4.59it/s][A[A

 22% 5/23 [00:01<00:03,  4.78it/s][A[A

 26% 6/23 [00:01<00:03,  4.81it/s][A[A

 30% 7/23 [00:01<00:03,  4.56it/s][A[A

 35% 8/23 [00:01<00:03,  4.43it/s][A[A

 39% 9/23 [00:01<00:02,  5.05it/s][A[A

 43% 10/23 [00:03<00:06,  1.91it/s][A[A

 48% 11/23 [00:03<00:04,  2.45it/s][A[A

 52% 12/23 [00:03<00:03,  2.81it/s][A[A

 57% 13/23 [00:03<00:02,  3.44it/s][A[A

 61% 14/23 [00:03<00:02,  3.62it/s][A[A

 65% 15/23 [00:04<00:02,  3.27it/s][A[A

 70% 16/23 [00:04<00:02,  3.43it/s][A[A

 74% 17/23 [00:04<00:01,  3.98it/s][A[A

 78% 18/23 [00:04<00:01,  3.95it/s][A[A

 83% 19/23 [00:05<00:00,  4.05it/s][A[A

 87% 20/23 [00:05<00:00,  4.74it/s][A[A

 91% 21/23 [00:05<00:00,  5.08it/s][A[A

 96% 22/23 [00:05<00:00,  5.67it/s][A[A

100% 23/23 [00:05<00:00,  5.95it/s][A[A100% 23/23 [00:05<00:00,  4.00it/s]
Meta loss on this task batch = 4.4685e-01, PNorm = 36.6999, GNorm = 0.0954

100% 19/19 [03:16<00:00,  9.60s/it][A100% 19/19 [03:16<00:00, 10.35s/it]
Took 196.65751266479492 seconds to complete one epoch of meta training

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/20 [00:00<?, ?it/s][A[A


  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 31.10it/s]


  5% 1/20 [00:00<00:02,  7.81it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 20.18it/s]


 10% 2/20 [00:00<00:02,  6.65it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 17.30it/s][A[A[A


100% 3/3 [00:01<00:00,  2.84it/s][A[A[A100% 3/3 [00:01<00:00,  2.59it/s]


 15% 3/20 [00:01<00:08,  2.04it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 15.61it/s]


 20% 4/20 [00:01<00:06,  2.43it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 14.85it/s][A[A[A100% 4/4 [00:00<00:00, 17.21it/s]


 25% 5/20 [00:02<00:06,  2.42it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.33it/s][A[A[A


100% 4/4 [00:00<00:00, 14.77it/s][A[A[A100% 4/4 [00:00<00:00, 15.91it/s]


 30% 6/20 [00:02<00:05,  2.43it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.19it/s][A[A[A100% 4/4 [00:00<00:00, 20.94it/s]


 35% 7/20 [00:03<00:05,  2.51it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.75it/s][A[A[A


100% 4/4 [00:00<00:00, 19.43it/s][A[A[A100% 4/4 [00:00<00:00, 19.20it/s]


 40% 8/20 [00:03<00:04,  2.58it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.01it/s][A[A[A100% 4/4 [00:00<00:00, 22.60it/s]


 45% 9/20 [00:03<00:04,  2.67it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.37it/s][A[A[A


100% 4/4 [00:00<00:00, 19.28it/s][A[A[A100% 4/4 [00:00<00:00, 19.20it/s]


 50% 10/20 [00:05<00:06,  1.47it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 16.34it/s][A[A[A100% 4/4 [00:00<00:00, 18.80it/s]


 55% 11/20 [00:05<00:05,  1.69it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.50it/s][A[A[A100% 4/4 [00:00<00:00, 21.76it/s]


 60% 12/20 [00:05<00:04,  1.93it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.87it/s][A[A[A100% 3/3 [00:00<00:00, 14.08it/s]


 65% 13/20 [00:06<00:03,  2.10it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.40it/s][A[A[A100% 3/3 [00:00<00:00, 13.79it/s]


 70% 14/20 [00:06<00:02,  2.22it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.96it/s][A[A[A100% 4/4 [00:00<00:00, 18.07it/s]


 75% 15/20 [00:07<00:02,  2.32it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.91it/s][A[A[A100% 3/3 [00:00<00:00, 18.27it/s]


 80% 16/20 [00:07<00:01,  2.49it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 12.15it/s][A[A[A100% 3/3 [00:00<00:00, 14.77it/s]


 85% 17/20 [00:07<00:01,  2.51it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 21.85it/s]


 90% 18/20 [00:08<00:00,  2.75it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:01<00:00,  1.75it/s][A[A[A100% 3/3 [00:01<00:00,  2.57it/s]


 95% 19/20 [00:09<00:00,  1.53it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 13.37it/s]


100% 20/20 [00:09<00:00,  1.88it/s][A[A100% 20/20 [00:09<00:00,  2.08it/s]

100% 1/1 [00:09<00:00,  9.61s/it][A100% 1/1 [00:09<00:00,  9.61s/it]
Took 206.26821756362915 seconds to complete one epoch of meta training and validating
Meta validation score auc = 0.608919
Found better MAML checkpoint after meta validation, saving now
 37% 11/30 [35:34<1:02:10, 196.36s/it]Epoch 11

  0% 0/19 [00:00<?, ?it/s][A

  0% 0/32 [00:00<?, ?it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  3% 1/32 [00:00<00:05,  6.15it/s][A[A

  6% 2/32 [00:00<00:05,  5.67it/s][A[A

  9% 3/32 [00:00<00:05,  5.73it/s][A[A

 12% 4/32 [00:00<00:04,  6.01it/s][A[A

 16% 5/32 [00:00<00:04,  6.33it/s][A[A

 19% 6/32 [00:01<00:04,  6.10it/s][A[A

 22% 7/32 [00:01<00:04,  6.03it/s][A[A

 25% 8/32 [00:01<00:04,  5.70it/s][A[A

 28% 9/32 [00:01<00:03,  6.28it/s][A[A

 31% 10/32 [00:01<00:03,  6.63it/s][A[A

 34% 11/32 [00:01<00:03,  6.02it/s][A[A

 38% 12/32 [00:02<00:03,  5.18it/s][A[A

 41% 13/32 [00:02<00:03,  4.91it/s][A[A

 44% 14/32 [00:02<00:03,  4.84it/s][A[A

 47% 15/32 [00:02<00:03,  5.44it/s][A[A

 50% 16/32 [00:03<00:07,  2.01it/s][A[A

 53% 17/32 [00:04<00:05,  2.53it/s][A[A

 56% 18/32 [00:04<00:04,  3.08it/s][A[A

 59% 19/32 [00:04<00:03,  3.63it/s][A[A

 62% 20/32 [00:04<00:03,  4.00it/s][A[A

 66% 21/32 [00:04<00:02,  4.71it/s][A[A

 69% 22/32 [00:04<00:02,  4.92it/s][A[A

 72% 23/32 [00:05<00:01,  5.39it/s][A[A

 75% 24/32 [00:05<00:01,  5.63it/s][A[A

 78% 25/32 [00:05<00:01,  5.51it/s][A[A

 81% 26/32 [00:05<00:01,  5.28it/s][A[A

 84% 27/32 [00:05<00:00,  5.67it/s][A[A

 88% 28/32 [00:05<00:00,  5.83it/s][A[A

 91% 29/32 [00:06<00:00,  5.81it/s][A[A

 94% 30/32 [00:06<00:00,  6.40it/s][A[A

 97% 31/32 [00:06<00:00,  6.16it/s][A[A

100% 32/32 [00:06<00:00,  5.94it/s][A[A100% 32/32 [00:06<00:00,  4.91it/s]
Meta loss on this task batch = 5.1355e-01, PNorm = 36.7012, GNorm = 0.1373

  5% 1/19 [00:07<02:09,  7.22s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:03,  7.88it/s][A[A

  6% 2/32 [00:00<00:03,  7.57it/s][A[A

  9% 3/32 [00:01<00:13,  2.23it/s][A[A

 12% 4/32 [00:01<00:10,  2.70it/s][A[A

 16% 5/32 [00:01<00:08,  3.14it/s][A[A

 19% 6/32 [00:01<00:06,  3.73it/s][A[A

 22% 7/32 [00:02<00:05,  4.43it/s][A[A

 25% 8/32 [00:02<00:04,  4.98it/s][A[A

 28% 9/32 [00:02<00:04,  5.30it/s][A[A

 31% 10/32 [00:02<00:03,  5.80it/s][A[A

 34% 11/32 [00:02<00:03,  6.16it/s][A[A

 38% 12/32 [00:02<00:03,  5.96it/s][A[A

 41% 13/32 [00:03<00:03,  6.20it/s][A[A

 44% 14/32 [00:03<00:03,  5.90it/s][A[A

 47% 15/32 [00:03<00:02,  5.86it/s][A[A

 50% 16/32 [00:03<00:02,  5.67it/s][A[A

 53% 17/32 [00:03<00:02,  5.82it/s][A[A

 56% 18/32 [00:03<00:02,  5.30it/s][A[A

 59% 19/32 [00:04<00:02,  5.64it/s][A[A

 62% 20/32 [00:04<00:02,  5.60it/s][A[A

 66% 21/32 [00:04<00:01,  5.92it/s][A[A

 69% 22/32 [00:04<00:01,  6.25it/s][A[A

 72% 23/32 [00:05<00:04,  2.20it/s][A[A

 75% 24/32 [00:05<00:02,  2.76it/s][A[A

 78% 25/32 [00:06<00:02,  3.22it/s][A[A

 81% 26/32 [00:06<00:01,  3.59it/s][A[A

 84% 27/32 [00:06<00:01,  4.10it/s][A[A

 88% 28/32 [00:06<00:00,  4.42it/s][A[A

 91% 29/32 [00:06<00:00,  5.11it/s][A[A

 94% 30/32 [00:06<00:00,  5.54it/s][A[A

 97% 31/32 [00:07<00:00,  5.85it/s][A[A

100% 32/32 [00:07<00:00,  6.27it/s][A[A100% 32/32 [00:07<00:00,  4.47it/s]
Meta loss on this task batch = 5.0688e-01, PNorm = 36.7010, GNorm = 0.0626

 11% 2/19 [00:15<02:05,  7.40s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.61it/s][A[A

  6% 2/32 [00:00<00:05,  5.40it/s][A[A

  9% 3/32 [00:00<00:05,  5.41it/s][A[A

 12% 4/32 [00:00<00:04,  5.84it/s][A[A

 16% 5/32 [00:00<00:04,  5.91it/s][A[A

 19% 6/32 [00:01<00:04,  5.76it/s][A[A

 22% 7/32 [00:01<00:04,  5.69it/s][A[A

 25% 8/32 [00:01<00:04,  5.99it/s][A[A

 28% 9/32 [00:01<00:03,  6.22it/s][A[A

 31% 10/32 [00:01<00:03,  5.59it/s][A[A

 34% 11/32 [00:02<00:10,  2.05it/s][A[A

 38% 12/32 [00:03<00:07,  2.52it/s][A[A

 41% 13/32 [00:03<00:06,  3.05it/s][A[A

 44% 14/32 [00:03<00:04,  3.68it/s][A[A

 47% 15/32 [00:03<00:04,  4.01it/s][A[A

 50% 16/32 [00:03<00:03,  4.65it/s][A[A

 53% 17/32 [00:03<00:02,  5.13it/s][A[A

 56% 18/32 [00:04<00:02,  5.59it/s][A[A

 59% 19/32 [00:04<00:02,  5.67it/s][A[A

 62% 20/32 [00:04<00:01,  6.00it/s][A[A

 66% 21/32 [00:04<00:01,  6.44it/s][A[A

 69% 22/32 [00:04<00:01,  6.58it/s][A[A

 72% 23/32 [00:04<00:01,  6.82it/s][A[A

 75% 24/32 [00:04<00:01,  6.26it/s][A[A

 78% 25/32 [00:05<00:01,  6.71it/s][A[A

 81% 26/32 [00:05<00:00,  7.10it/s][A[A

 84% 27/32 [00:05<00:00,  6.57it/s][A[A

 88% 28/32 [00:05<00:00,  6.45it/s][A[A

 91% 29/32 [00:05<00:00,  6.56it/s][A[A

 94% 30/32 [00:05<00:00,  5.55it/s][A[A

 97% 31/32 [00:06<00:00,  6.23it/s][A[A

100% 32/32 [00:06<00:00,  5.90it/s][A[A100% 32/32 [00:06<00:00,  5.11it/s]
Meta loss on this task batch = 5.4624e-01, PNorm = 36.6950, GNorm = 0.1567

 16% 3/19 [00:21<01:56,  7.26s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.16it/s][A[A

  6% 2/32 [00:00<00:05,  5.02it/s][A[A

  9% 3/32 [00:01<00:14,  2.02it/s][A[A

 12% 4/32 [00:01<00:10,  2.61it/s][A[A

 16% 5/32 [00:01<00:09,  3.00it/s][A[A

 19% 6/32 [00:02<00:07,  3.39it/s][A[A

 22% 7/32 [00:02<00:06,  4.07it/s][A[A

 25% 8/32 [00:02<00:05,  4.50it/s][A[A

 28% 9/32 [00:02<00:04,  5.04it/s][A[A

 31% 10/32 [00:02<00:04,  5.35it/s][A[A

 34% 11/32 [00:02<00:03,  5.75it/s][A[A

 38% 12/32 [00:03<00:03,  6.15it/s][A[A

 41% 13/32 [00:03<00:03,  6.30it/s][A[A

 44% 14/32 [00:03<00:02,  6.19it/s][A[A

 47% 15/32 [00:03<00:02,  6.41it/s][A[A

 50% 16/32 [00:03<00:02,  6.22it/s][A[A

 53% 17/32 [00:03<00:02,  6.43it/s][A[A

 56% 18/32 [00:03<00:02,  6.36it/s][A[A

 59% 19/32 [00:04<00:01,  6.56it/s][A[A

 62% 20/32 [00:04<00:01,  6.33it/s][A[A

 66% 21/32 [00:04<00:01,  5.93it/s][A[A

 69% 22/32 [00:05<00:04,  2.03it/s][A[A

 72% 23/32 [00:05<00:03,  2.44it/s][A[A

 75% 24/32 [00:06<00:02,  2.91it/s][A[A

 78% 25/32 [00:06<00:01,  3.55it/s][A[A

 81% 26/32 [00:06<00:01,  4.28it/s][A[A

 84% 27/32 [00:06<00:01,  4.68it/s][A[A

 88% 28/32 [00:06<00:00,  4.93it/s][A[A

 91% 29/32 [00:06<00:00,  5.47it/s][A[A

 94% 30/32 [00:07<00:00,  5.29it/s][A[A

 97% 31/32 [00:07<00:00,  5.71it/s][A[A

100% 32/32 [00:07<00:00,  5.51it/s][A[A100% 32/32 [00:07<00:00,  4.33it/s]
Meta loss on this task batch = 5.8643e-01, PNorm = 36.6898, GNorm = 0.0757

 21% 4/19 [00:30<01:52,  7.51s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.64it/s][A[A

  6% 2/32 [00:00<00:04,  6.18it/s][A[A

  9% 3/32 [00:00<00:04,  6.32it/s][A[A

 12% 4/32 [00:00<00:04,  6.01it/s][A[A

 16% 5/32 [00:00<00:04,  5.54it/s][A[A

 19% 6/32 [00:01<00:04,  5.62it/s][A[A

 22% 7/32 [00:01<00:04,  5.10it/s][A[A

 25% 8/32 [00:02<00:11,  2.07it/s][A[A

 28% 9/32 [00:02<00:08,  2.59it/s][A[A

 31% 10/32 [00:02<00:07,  3.13it/s][A[A

 34% 11/32 [00:02<00:05,  3.66it/s][A[A

 38% 12/32 [00:03<00:05,  3.97it/s][A[A

 41% 13/32 [00:03<00:04,  4.23it/s][A[A

 44% 14/32 [00:03<00:03,  4.81it/s][A[A

 47% 15/32 [00:03<00:03,  5.30it/s][A[A

 50% 16/32 [00:03<00:02,  6.04it/s][A[A

 53% 17/32 [00:03<00:02,  5.44it/s][A[A

 56% 18/32 [00:04<00:02,  5.35it/s][A[A

 59% 19/32 [00:04<00:02,  5.24it/s][A[A

 62% 20/32 [00:04<00:02,  5.57it/s][A[A

 66% 21/32 [00:04<00:01,  5.68it/s][A[A

 69% 22/32 [00:04<00:01,  6.33it/s][A[A

 72% 23/32 [00:05<00:04,  2.13it/s][A[A

 75% 24/32 [00:06<00:03,  2.52it/s][A[A

 78% 25/32 [00:06<00:02,  2.93it/s][A[A

 81% 26/32 [00:06<00:01,  3.55it/s][A[A

 84% 27/32 [00:06<00:01,  4.00it/s][A[A

 88% 28/32 [00:06<00:00,  4.54it/s][A[A

 91% 29/32 [00:07<00:00,  4.83it/s][A[A

 94% 30/32 [00:07<00:00,  5.24it/s][A[A

 97% 31/32 [00:07<00:00,  5.29it/s][A[A

100% 32/32 [00:07<00:00,  5.16it/s][A[A100% 32/32 [00:07<00:00,  4.22it/s]
Meta loss on this task batch = 5.4217e-01, PNorm = 36.6816, GNorm = 0.1193

 26% 5/19 [00:38<01:48,  7.74s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  6.78it/s][A[A

  6% 2/32 [00:00<00:04,  6.43it/s][A[A

  9% 3/32 [00:00<00:04,  6.54it/s][A[A

 12% 4/32 [00:00<00:04,  6.39it/s][A[A

 16% 5/32 [00:00<00:04,  6.41it/s][A[A

 19% 6/32 [00:00<00:04,  5.97it/s][A[A

 22% 7/32 [00:02<00:11,  2.09it/s][A[A

 25% 8/32 [00:02<00:09,  2.56it/s][A[A

 28% 9/32 [00:02<00:07,  2.92it/s][A[A

 31% 10/32 [00:02<00:06,  3.36it/s][A[A

 34% 11/32 [00:02<00:05,  3.77it/s][A[A

 38% 12/32 [00:03<00:04,  4.14it/s][A[A

 41% 13/32 [00:03<00:04,  4.68it/s][A[A

 44% 14/32 [00:03<00:03,  5.10it/s][A[A

 47% 15/32 [00:03<00:03,  4.97it/s][A[A

 50% 16/32 [00:03<00:03,  5.04it/s][A[A

 53% 17/32 [00:04<00:02,  5.53it/s][A[A

 56% 18/32 [00:04<00:02,  5.78it/s][A[A

 59% 19/32 [00:04<00:02,  5.86it/s][A[A

 62% 20/32 [00:04<00:01,  6.13it/s][A[A

 66% 21/32 [00:04<00:01,  6.26it/s][A[A

 69% 22/32 [00:05<00:04,  2.10it/s][A[A

 72% 23/32 [00:06<00:03,  2.56it/s][A[A

 75% 24/32 [00:06<00:02,  3.02it/s][A[A

 78% 25/32 [00:06<00:02,  3.31it/s][A[A

 81% 26/32 [00:06<00:01,  3.92it/s][A[A

 84% 27/32 [00:06<00:01,  4.00it/s][A[A

 88% 28/32 [00:07<00:00,  4.49it/s][A[A

 91% 29/32 [00:07<00:00,  4.97it/s][A[A

 94% 30/32 [00:07<00:00,  5.42it/s][A[A

 97% 31/32 [00:07<00:00,  5.72it/s][A[A

100% 32/32 [00:07<00:00,  6.05it/s][A[A100% 32/32 [00:07<00:00,  4.21it/s]
Meta loss on this task batch = 4.5028e-01, PNorm = 36.6757, GNorm = 0.0573

 32% 6/19 [00:46<01:42,  7.90s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.99it/s][A[A

  6% 2/32 [00:00<00:04,  6.14it/s][A[A

  9% 3/32 [00:00<00:05,  5.60it/s][A[A

 12% 4/32 [00:00<00:04,  5.83it/s][A[A

 16% 5/32 [00:00<00:04,  6.25it/s][A[A

 19% 6/32 [00:00<00:04,  6.47it/s][A[A

 22% 7/32 [00:02<00:11,  2.12it/s][A[A

 25% 8/32 [00:02<00:09,  2.60it/s][A[A

 28% 9/32 [00:02<00:07,  3.07it/s][A[A

 31% 10/32 [00:02<00:06,  3.45it/s][A[A

 34% 11/32 [00:02<00:05,  3.94it/s][A[A

 38% 12/32 [00:03<00:04,  4.50it/s][A[A

 41% 13/32 [00:03<00:03,  4.76it/s][A[A

 44% 14/32 [00:03<00:03,  5.18it/s][A[A

 47% 15/32 [00:03<00:03,  5.01it/s][A[A

 50% 16/32 [00:03<00:03,  4.71it/s][A[A

 53% 17/32 [00:03<00:02,  5.44it/s][A[A

 56% 18/32 [00:04<00:02,  5.24it/s][A[A

 59% 19/32 [00:04<00:02,  5.39it/s][A[A

 62% 20/32 [00:04<00:02,  5.12it/s][A[A

 66% 21/32 [00:04<00:02,  4.99it/s][A[A

 69% 22/32 [00:05<00:05,  1.99it/s][A[A

 72% 23/32 [00:06<00:03,  2.36it/s][A[A

 75% 24/32 [00:06<00:02,  2.83it/s][A[A

 78% 25/32 [00:06<00:02,  3.30it/s][A[A

 81% 26/32 [00:06<00:01,  4.05it/s][A[A

 84% 27/32 [00:06<00:01,  4.45it/s][A[A

 88% 28/32 [00:07<00:00,  4.63it/s][A[A

 91% 29/32 [00:07<00:00,  5.02it/s][A[A

 94% 30/32 [00:07<00:00,  5.03it/s][A[A

 97% 31/32 [00:07<00:00,  5.41it/s][A[A

100% 32/32 [00:07<00:00,  5.21it/s][A[A100% 32/32 [00:07<00:00,  4.09it/s]
Meta loss on this task batch = 4.7876e-01, PNorm = 36.6740, GNorm = 0.0951

 37% 7/19 [00:55<01:37,  8.09s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.04it/s][A[A

  6% 2/32 [00:00<00:07,  4.23it/s][A[A

  9% 3/32 [00:00<00:06,  4.35it/s][A[A

 12% 4/32 [00:01<00:14,  1.88it/s][A[A

 16% 5/32 [00:02<00:11,  2.30it/s][A[A

 19% 6/32 [00:02<00:09,  2.72it/s][A[A

 22% 7/32 [00:02<00:07,  3.26it/s][A[A

 25% 8/32 [00:02<00:06,  3.54it/s][A[A

 28% 9/32 [00:02<00:05,  3.88it/s][A[A

 31% 10/32 [00:03<00:04,  4.42it/s][A[A

 34% 11/32 [00:03<00:04,  4.80it/s][A[A

 38% 12/32 [00:03<00:04,  4.88it/s][A[A

 41% 13/32 [00:03<00:03,  5.12it/s][A[A

 44% 14/32 [00:03<00:03,  5.40it/s][A[A

 47% 15/32 [00:03<00:03,  5.44it/s][A[A

 50% 16/32 [00:04<00:02,  5.51it/s][A[A

 53% 17/32 [00:04<00:02,  5.12it/s][A[A

 56% 18/32 [00:04<00:02,  5.23it/s][A[A

 59% 19/32 [00:04<00:02,  5.05it/s][A[A

 62% 20/32 [00:04<00:02,  5.54it/s][A[A

 66% 21/32 [00:06<00:05,  2.06it/s][A[A

 69% 22/32 [00:06<00:04,  2.41it/s][A[A

 72% 23/32 [00:06<00:03,  2.95it/s][A[A

 75% 24/32 [00:06<00:02,  3.30it/s][A[A

 78% 25/32 [00:06<00:01,  3.57it/s][A[A

 81% 26/32 [00:07<00:01,  3.90it/s][A[A

 84% 27/32 [00:07<00:01,  4.17it/s][A[A

 88% 28/32 [00:07<00:00,  4.39it/s][A[A

 91% 29/32 [00:07<00:00,  4.53it/s][A[A

 94% 30/32 [00:07<00:00,  4.80it/s][A[A

 97% 31/32 [00:08<00:00,  4.79it/s][A[A

100% 32/32 [00:08<00:00,  4.77it/s][A[A100% 32/32 [00:08<00:00,  3.83it/s]
Meta loss on this task batch = 3.9310e-01, PNorm = 36.6771, GNorm = 0.1147

 42% 8/19 [01:04<01:32,  8.39s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:37,  1.22s/it][A[A

  6% 2/32 [00:01<00:27,  1.09it/s][A[A

  9% 3/32 [00:01<00:20,  1.43it/s][A[A

 12% 4/32 [00:01<00:15,  1.85it/s][A[A

 16% 5/32 [00:01<00:11,  2.30it/s][A[A

 19% 6/32 [00:02<00:09,  2.77it/s][A[A

 22% 7/32 [00:02<00:07,  3.21it/s][A[A

 25% 8/32 [00:02<00:06,  3.62it/s][A[A

 28% 9/32 [00:02<00:05,  3.98it/s][A[A

 31% 10/32 [00:02<00:05,  4.23it/s][A[A

 34% 11/32 [00:03<00:04,  4.41it/s][A[A

 38% 12/32 [00:03<00:04,  4.58it/s][A[A

 41% 13/32 [00:03<00:03,  4.76it/s][A[A

 44% 14/32 [00:03<00:03,  4.93it/s][A[A

 47% 15/32 [00:04<00:08,  1.97it/s][A[A

 50% 16/32 [00:05<00:06,  2.40it/s][A[A

 53% 17/32 [00:05<00:05,  2.87it/s][A[A

 56% 18/32 [00:05<00:04,  3.27it/s][A[A

 59% 19/32 [00:05<00:03,  3.65it/s][A[A

 62% 20/32 [00:05<00:03,  4.00it/s][A[A

 66% 21/32 [00:06<00:02,  4.16it/s][A[A

 69% 22/32 [00:06<00:02,  4.40it/s][A[A

 72% 23/32 [00:06<00:01,  4.68it/s][A[A

 75% 24/32 [00:06<00:01,  4.93it/s][A[A

 78% 25/32 [00:06<00:01,  5.13it/s][A[A

 81% 26/32 [00:07<00:01,  5.02it/s][A[A

 84% 27/32 [00:07<00:01,  4.95it/s][A[A

 88% 28/32 [00:07<00:00,  4.96it/s][A[A

 91% 29/32 [00:07<00:00,  5.04it/s][A[A

 94% 30/32 [00:07<00:00,  5.09it/s][A[A

 97% 31/32 [00:08<00:00,  5.17it/s][A[A

100% 32/32 [00:09<00:00,  2.03it/s][A[A100% 32/32 [00:09<00:00,  3.45it/s]
Meta loss on this task batch = 2.4320e-01, PNorm = 36.6863, GNorm = 0.2078

 47% 9/19 [01:14<01:28,  8.89s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.40it/s][A[A

  6% 2/32 [00:00<00:06,  4.64it/s][A[A

  9% 3/32 [00:00<00:06,  4.79it/s][A[A

 12% 4/32 [00:00<00:05,  4.86it/s][A[A

 16% 5/32 [00:00<00:05,  4.99it/s][A[A

 19% 6/32 [00:01<00:05,  5.04it/s][A[A

 22% 7/32 [00:01<00:04,  5.02it/s][A[A

 25% 8/32 [00:01<00:04,  5.19it/s][A[A

 28% 9/32 [00:01<00:04,  5.23it/s][A[A

 31% 10/32 [00:01<00:04,  5.14it/s][A[A

 34% 11/32 [00:02<00:04,  5.14it/s][A[A

 38% 12/32 [00:02<00:03,  5.16it/s][A[A

 41% 13/32 [00:02<00:03,  5.27it/s][A[A

 44% 14/32 [00:02<00:03,  5.23it/s][A[A

 47% 15/32 [00:02<00:03,  5.26it/s][A[A

 50% 16/32 [00:04<00:07,  2.04it/s][A[A

 53% 17/32 [00:04<00:06,  2.50it/s][A[A

 56% 18/32 [00:04<00:04,  2.95it/s][A[A

 59% 19/32 [00:04<00:03,  3.39it/s][A[A

 62% 20/32 [00:04<00:03,  3.67it/s][A[A

 66% 21/32 [00:05<00:02,  4.12it/s][A[A

 69% 22/32 [00:05<00:02,  4.33it/s][A[A

 72% 23/32 [00:05<00:01,  4.60it/s][A[A

 75% 24/32 [00:05<00:01,  4.69it/s][A[A

 78% 25/32 [00:05<00:01,  4.88it/s][A[A

 81% 26/32 [00:06<00:01,  4.96it/s][A[A

 84% 27/32 [00:06<00:00,  5.62it/s][A[A

 88% 28/32 [00:06<00:00,  5.19it/s][A[A

 91% 29/32 [00:06<00:00,  5.34it/s][A[A

 94% 30/32 [00:06<00:00,  4.89it/s][A[A

 97% 31/32 [00:08<00:00,  1.95it/s][A[A

100% 32/32 [00:08<00:00,  2.48it/s][A[A100% 32/32 [00:08<00:00,  3.91it/s]
Meta loss on this task batch = 2.5568e-01, PNorm = 36.7016, GNorm = 0.2290

 53% 10/19 [01:23<01:20,  8.90s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.85it/s][A[A

  6% 2/32 [00:00<00:06,  4.90it/s][A[A

  9% 3/32 [00:00<00:05,  5.10it/s][A[A

 12% 4/32 [00:00<00:05,  5.12it/s][A[A

 16% 5/32 [00:00<00:05,  5.12it/s][A[A

 19% 6/32 [00:01<00:04,  5.86it/s][A[A

 22% 7/32 [00:01<00:04,  5.92it/s][A[A

 25% 8/32 [00:01<00:04,  5.47it/s][A[A

 28% 9/32 [00:01<00:04,  5.51it/s][A[A

 31% 10/32 [00:01<00:04,  5.06it/s][A[A

 34% 11/32 [00:02<00:04,  4.66it/s][A[A

 38% 12/32 [00:03<00:10,  1.94it/s][A[A

 41% 13/32 [00:03<00:08,  2.31it/s][A[A

 44% 14/32 [00:03<00:06,  2.84it/s][A[A

 47% 15/32 [00:03<00:05,  3.28it/s][A[A

 50% 16/32 [00:04<00:04,  3.57it/s][A[A

 53% 17/32 [00:04<00:03,  3.99it/s][A[A

 56% 18/32 [00:04<00:03,  4.44it/s][A[A

 59% 19/32 [00:04<00:02,  4.50it/s][A[A

 62% 20/32 [00:04<00:02,  4.74it/s][A[A

 66% 21/32 [00:05<00:02,  4.81it/s][A[A

 69% 22/32 [00:05<00:02,  4.65it/s][A[A

 72% 23/32 [00:05<00:01,  4.57it/s][A[A

 75% 24/32 [00:05<00:01,  4.60it/s][A[A

 78% 25/32 [00:06<00:03,  1.97it/s][A[A

 81% 26/32 [00:07<00:02,  2.41it/s][A[A

 84% 27/32 [00:07<00:01,  2.82it/s][A[A

 88% 28/32 [00:07<00:01,  3.24it/s][A[A

 91% 29/32 [00:07<00:00,  3.61it/s][A[A

 94% 30/32 [00:08<00:00,  3.84it/s][A[A

 97% 31/32 [00:08<00:00,  4.21it/s][A[A

100% 32/32 [00:08<00:00,  4.36it/s][A[A100% 32/32 [00:08<00:00,  3.80it/s]
Meta loss on this task batch = 6.7689e-01, PNorm = 36.7137, GNorm = 0.1386

 58% 11/19 [01:32<01:11,  8.98s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.74it/s][A[A

  6% 2/32 [00:00<00:05,  5.34it/s][A[A

  9% 3/32 [00:00<00:05,  5.10it/s][A[A

 12% 4/32 [00:00<00:05,  4.76it/s][A[A

 16% 5/32 [00:01<00:05,  4.57it/s][A[A

 19% 6/32 [00:02<00:13,  1.92it/s][A[A

 22% 7/32 [00:02<00:10,  2.41it/s][A[A

 25% 8/32 [00:02<00:08,  2.78it/s][A[A

 28% 9/32 [00:02<00:07,  3.12it/s][A[A

 31% 10/32 [00:03<00:06,  3.55it/s][A[A

 34% 11/32 [00:03<00:05,  4.08it/s][A[A

 38% 12/32 [00:03<00:04,  4.29it/s][A[A

 41% 13/32 [00:03<00:04,  4.31it/s][A[A

 44% 14/32 [00:03<00:04,  4.39it/s][A[A

 47% 15/32 [00:04<00:03,  4.25it/s][A[A

 50% 16/32 [00:04<00:03,  4.64it/s][A[A

 53% 17/32 [00:04<00:03,  4.66it/s][A[A

 56% 18/32 [00:04<00:03,  4.57it/s][A[A

 59% 19/32 [00:05<00:02,  4.58it/s][A[A

 62% 20/32 [00:06<00:06,  1.98it/s][A[A

 66% 21/32 [00:06<00:04,  2.36it/s][A[A

 69% 22/32 [00:06<00:03,  2.86it/s][A[A

 72% 23/32 [00:06<00:02,  3.21it/s][A[A

 75% 24/32 [00:07<00:02,  3.50it/s][A[A

 78% 25/32 [00:07<00:01,  3.79it/s][A[A

 81% 26/32 [00:07<00:01,  3.98it/s][A[A

 84% 27/32 [00:07<00:01,  4.22it/s][A[A

 88% 28/32 [00:07<00:00,  4.40it/s][A[A

 91% 29/32 [00:08<00:00,  4.60it/s][A[A

 94% 30/32 [00:08<00:00,  4.64it/s][A[A

 97% 31/32 [00:08<00:00,  4.74it/s][A[A

100% 32/32 [00:08<00:00,  4.60it/s][A[A100% 32/32 [00:08<00:00,  3.66it/s]
Meta loss on this task batch = 5.5302e-01, PNorm = 36.7265, GNorm = 0.0576

 63% 12/19 [01:41<01:03,  9.14s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:38,  1.25s/it][A[A

  6% 2/32 [00:01<00:28,  1.07it/s][A[A

  9% 3/32 [00:01<00:21,  1.38it/s][A[A

 12% 4/32 [00:01<00:16,  1.75it/s][A[A

 16% 5/32 [00:02<00:12,  2.21it/s][A[A

 19% 6/32 [00:02<00:10,  2.60it/s][A[A

 22% 7/32 [00:02<00:08,  3.04it/s][A[A

 25% 8/32 [00:02<00:06,  3.74it/s][A[A

 28% 9/32 [00:02<00:05,  4.18it/s][A[A

 31% 10/32 [00:03<00:05,  4.26it/s][A[A

 34% 11/32 [00:03<00:04,  4.47it/s][A[A

 38% 12/32 [00:03<00:04,  4.43it/s][A[A

 41% 13/32 [00:04<00:10,  1.87it/s][A[A

 44% 14/32 [00:04<00:07,  2.33it/s][A[A

 47% 15/32 [00:05<00:06,  2.76it/s][A[A

 50% 16/32 [00:05<00:05,  3.18it/s][A[A

 53% 17/32 [00:05<00:04,  3.74it/s][A[A

 56% 18/32 [00:05<00:03,  3.84it/s][A[A

 59% 19/32 [00:05<00:02,  4.38it/s][A[A

 62% 20/32 [00:06<00:02,  4.61it/s][A[A

 66% 21/32 [00:06<00:02,  4.58it/s][A[A

 69% 22/32 [00:06<00:02,  4.68it/s][A[A

 72% 23/32 [00:06<00:01,  4.60it/s][A[A

 75% 24/32 [00:07<00:04,  1.95it/s][A[A

 78% 25/32 [00:08<00:02,  2.41it/s][A[A

 81% 26/32 [00:08<00:02,  2.81it/s][A[A

 84% 27/32 [00:08<00:01,  3.33it/s][A[A

 88% 28/32 [00:08<00:01,  3.76it/s][A[A

 91% 29/32 [00:08<00:00,  4.03it/s][A[A

 94% 30/32 [00:09<00:00,  4.31it/s][A[A

 97% 31/32 [00:09<00:00,  4.48it/s][A[A

100% 32/32 [00:09<00:00,  4.65it/s][A[A100% 32/32 [00:09<00:00,  3.38it/s]
Meta loss on this task batch = 5.8322e-01, PNorm = 36.7394, GNorm = 0.0366

 68% 13/19 [01:52<00:56,  9.45s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  7.26it/s][A[A

  6% 2/32 [00:00<00:04,  7.14it/s][A[A

  9% 3/32 [00:00<00:04,  5.88it/s][A[A

 12% 4/32 [00:00<00:05,  5.38it/s][A[A

 16% 5/32 [00:00<00:05,  5.11it/s][A[A

 19% 6/32 [00:02<00:12,  2.04it/s][A[A

 22% 7/32 [00:02<00:10,  2.43it/s][A[A

 25% 8/32 [00:02<00:08,  2.97it/s][A[A

 28% 9/32 [00:02<00:07,  3.21it/s][A[A

 31% 10/32 [00:02<00:06,  3.65it/s][A[A

 34% 11/32 [00:03<00:05,  4.17it/s][A[A

 38% 12/32 [00:03<00:04,  4.40it/s][A[A

 41% 13/32 [00:03<00:04,  4.57it/s][A[A

 44% 14/32 [00:03<00:03,  4.58it/s][A[A

 47% 15/32 [00:03<00:03,  4.63it/s][A[A

 50% 16/32 [00:04<00:03,  4.93it/s][A[A

 53% 17/32 [00:04<00:03,  4.81it/s][A[A

 56% 18/32 [00:04<00:02,  4.84it/s][A[A

 59% 19/32 [00:04<00:02,  4.86it/s][A[A

 62% 20/32 [00:04<00:02,  4.98it/s][A[A

 66% 21/32 [00:05<00:02,  4.90it/s][A[A

 69% 22/32 [00:06<00:05,  1.94it/s][A[A

 72% 23/32 [00:06<00:03,  2.32it/s][A[A

 75% 24/32 [00:06<00:02,  2.76it/s][A[A

 78% 25/32 [00:07<00:02,  3.12it/s][A[A

 81% 26/32 [00:07<00:01,  3.45it/s][A[A

 84% 27/32 [00:07<00:01,  3.75it/s][A[A

 88% 28/32 [00:07<00:00,  4.04it/s][A[A

 91% 29/32 [00:07<00:00,  4.08it/s][A[A

 94% 30/32 [00:08<00:00,  4.23it/s][A[A

 97% 31/32 [00:08<00:00,  4.34it/s][A[A

100% 32/32 [00:08<00:00,  4.54it/s][A[A100% 32/32 [00:08<00:00,  3.74it/s]
Meta loss on this task batch = 5.6460e-01, PNorm = 36.7521, GNorm = 0.0500

 74% 14/19 [02:01<00:47,  9.41s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.93it/s][A[A

  6% 2/32 [00:00<00:06,  4.90it/s][A[A

  9% 3/32 [00:01<00:14,  1.94it/s][A[A

 12% 4/32 [00:01<00:11,  2.42it/s][A[A

 16% 5/32 [00:02<00:09,  2.88it/s][A[A

 19% 6/32 [00:02<00:08,  3.18it/s][A[A

 22% 7/32 [00:02<00:07,  3.34it/s][A[A

 25% 8/32 [00:02<00:06,  3.78it/s][A[A

 28% 9/32 [00:02<00:05,  3.92it/s][A[A

 31% 10/32 [00:03<00:05,  4.33it/s][A[A

 34% 11/32 [00:03<00:04,  4.64it/s][A[A

 38% 12/32 [00:03<00:04,  4.94it/s][A[A

 41% 13/32 [00:03<00:03,  4.92it/s][A[A

 44% 14/32 [00:03<00:03,  5.16it/s][A[A

 47% 15/32 [00:04<00:03,  5.19it/s][A[A

 50% 16/32 [00:04<00:03,  4.97it/s][A[A

 53% 17/32 [00:04<00:02,  5.19it/s][A[A

 56% 18/32 [00:05<00:06,  2.02it/s][A[A

 59% 19/32 [00:05<00:05,  2.55it/s][A[A

 62% 20/32 [00:06<00:04,  2.91it/s][A[A

 66% 21/32 [00:06<00:03,  3.39it/s][A[A

 69% 22/32 [00:06<00:02,  3.81it/s][A[A

 72% 23/32 [00:06<00:02,  3.80it/s][A[A

 75% 24/32 [00:06<00:01,  4.03it/s][A[A

 78% 25/32 [00:07<00:01,  4.48it/s][A[A

 81% 26/32 [00:07<00:01,  4.20it/s][A[A

 84% 27/32 [00:07<00:01,  4.21it/s][A[A

 88% 28/32 [00:07<00:00,  4.45it/s][A[A

 91% 29/32 [00:08<00:01,  1.88it/s][A[A

 94% 30/32 [00:09<00:00,  2.22it/s][A[A

 97% 31/32 [00:09<00:00,  2.83it/s][A[A

100% 32/32 [00:09<00:00,  3.13it/s][A[A100% 32/32 [00:09<00:00,  3.33it/s]
Meta loss on this task batch = 4.9961e-01, PNorm = 36.7666, GNorm = 0.0466

 79% 15/19 [02:11<00:38,  9.69s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.87it/s][A[A

  6% 2/32 [00:00<00:05,  5.58it/s][A[A

  9% 3/32 [00:00<00:04,  6.19it/s][A[A

 12% 4/32 [00:00<00:04,  6.47it/s][A[A

 16% 5/32 [00:00<00:04,  6.06it/s][A[A

 19% 6/32 [00:01<00:04,  5.43it/s][A[A

 22% 7/32 [00:01<00:04,  5.78it/s][A[A

 25% 8/32 [00:01<00:04,  5.68it/s][A[A

 28% 9/32 [00:01<00:03,  6.21it/s][A[A

 31% 10/32 [00:01<00:03,  5.65it/s][A[A

 34% 11/32 [00:02<00:10,  2.06it/s][A[A

 38% 12/32 [00:03<00:08,  2.47it/s][A[A

 41% 13/32 [00:03<00:06,  2.93it/s][A[A

 44% 14/32 [00:03<00:05,  3.41it/s][A[A

 47% 15/32 [00:03<00:04,  3.67it/s][A[A

 50% 16/32 [00:03<00:03,  4.20it/s][A[A

 53% 17/32 [00:04<00:03,  4.27it/s][A[A

 56% 18/32 [00:04<00:02,  4.75it/s][A[A

 59% 19/32 [00:04<00:02,  4.80it/s][A[A

 62% 20/32 [00:04<00:02,  4.83it/s][A[A

 66% 21/32 [00:04<00:02,  4.87it/s][A[A

 69% 22/32 [00:05<00:02,  4.92it/s][A[A

 72% 23/32 [00:05<00:01,  4.58it/s][A[A

 75% 24/32 [00:06<00:04,  1.89it/s][A[A

 78% 25/32 [00:06<00:02,  2.46it/s][A[A

 81% 26/32 [00:06<00:02,  2.99it/s][A[A

 84% 27/32 [00:07<00:01,  3.19it/s][A[A

 88% 28/32 [00:07<00:01,  3.51it/s][A[A

 91% 29/32 [00:07<00:00,  3.79it/s][A[A

 94% 30/32 [00:07<00:00,  3.82it/s][A[A

 97% 31/32 [00:08<00:00,  3.88it/s][A[A

100% 32/32 [00:08<00:00,  4.16it/s][A[A100% 32/32 [00:08<00:00,  3.89it/s]
Meta loss on this task batch = 5.6619e-01, PNorm = 36.7804, GNorm = 0.0376

 84% 16/19 [02:20<00:28,  9.47s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.50it/s][A[A

  6% 2/32 [00:01<00:17,  1.72it/s][A[A

  9% 3/32 [00:01<00:13,  2.15it/s][A[A

 12% 4/32 [00:01<00:10,  2.57it/s][A[A

 16% 5/32 [00:02<00:08,  3.00it/s][A[A

 19% 6/32 [00:02<00:07,  3.64it/s][A[A

 22% 7/32 [00:02<00:06,  4.12it/s][A[A

 25% 8/32 [00:02<00:05,  4.05it/s][A[A

 28% 9/32 [00:02<00:05,  4.16it/s][A[A

 31% 10/32 [00:03<00:04,  4.98it/s][A[A

 34% 11/32 [00:03<00:04,  4.97it/s][A[A

 38% 12/32 [00:03<00:04,  5.00it/s][A[A

 41% 13/32 [00:03<00:03,  5.23it/s][A[A

 44% 14/32 [00:03<00:03,  5.34it/s][A[A

 47% 15/32 [00:04<00:08,  2.09it/s][A[A

 50% 16/32 [00:05<00:06,  2.59it/s][A[A

 53% 17/32 [00:05<00:04,  3.04it/s][A[A

 56% 18/32 [00:05<00:04,  3.32it/s][A[A

 59% 19/32 [00:05<00:03,  3.50it/s][A[A

 62% 20/32 [00:06<00:03,  3.71it/s][A[A

 66% 21/32 [00:06<00:02,  4.02it/s][A[A

 69% 22/32 [00:06<00:02,  4.33it/s][A[A

 72% 23/32 [00:06<00:01,  4.72it/s][A[A

 75% 24/32 [00:06<00:01,  4.60it/s][A[A

 78% 25/32 [00:07<00:01,  4.81it/s][A[A

 81% 26/32 [00:07<00:01,  4.98it/s][A[A

 84% 27/32 [00:07<00:00,  5.11it/s][A[A

 88% 28/32 [00:07<00:00,  5.14it/s][A[A

 91% 29/32 [00:08<00:01,  2.01it/s][A[A

 94% 30/32 [00:09<00:00,  2.32it/s][A[A

 97% 31/32 [00:09<00:00,  2.78it/s][A[A

100% 32/32 [00:09<00:00,  3.18it/s][A[A100% 32/32 [00:09<00:00,  3.38it/s]
Meta loss on this task batch = 4.5299e-01, PNorm = 36.7936, GNorm = 0.0334

 89% 17/19 [02:30<00:19,  9.70s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.03it/s][A[A

  6% 2/32 [00:00<00:06,  4.94it/s][A[A

  9% 3/32 [00:00<00:06,  4.79it/s][A[A

 12% 4/32 [00:00<00:05,  4.77it/s][A[A

 16% 5/32 [00:01<00:05,  4.69it/s][A[A

 19% 6/32 [00:01<00:05,  4.68it/s][A[A

 22% 7/32 [00:01<00:05,  4.43it/s][A[A

 25% 8/32 [00:01<00:05,  4.25it/s][A[A

 28% 9/32 [00:02<00:05,  4.41it/s][A[A

 31% 10/32 [00:03<00:11,  1.87it/s][A[A

 34% 11/32 [00:03<00:09,  2.32it/s][A[A

 38% 12/32 [00:03<00:07,  2.76it/s][A[A

 41% 13/32 [00:03<00:05,  3.18it/s][A[A

 44% 14/32 [00:04<00:04,  3.76it/s][A[A

 47% 15/32 [00:04<00:04,  3.83it/s][A[A

 50% 16/32 [00:04<00:03,  4.16it/s][A[A

 53% 17/32 [00:04<00:03,  4.81it/s][A[A

 56% 18/32 [00:04<00:02,  4.91it/s][A[A

 59% 19/32 [00:04<00:02,  4.74it/s][A[A

 62% 20/32 [00:05<00:02,  4.59it/s][A[A

 66% 21/32 [00:05<00:02,  4.91it/s][A[A

 69% 22/32 [00:05<00:01,  5.42it/s][A[A

 72% 23/32 [00:05<00:01,  5.01it/s][A[A

 75% 24/32 [00:07<00:04,  1.91it/s][A[A

 78% 25/32 [00:07<00:02,  2.39it/s][A[A

 81% 26/32 [00:07<00:02,  2.78it/s][A[A

 84% 27/32 [00:07<00:01,  3.13it/s][A[A

 88% 28/32 [00:07<00:01,  3.48it/s][A[A

 91% 29/32 [00:08<00:00,  3.92it/s][A[A

 94% 30/32 [00:08<00:00,  4.24it/s][A[A

 97% 31/32 [00:08<00:00,  4.59it/s][A[A

100% 32/32 [00:08<00:00,  4.45it/s][A[A100% 32/32 [00:08<00:00,  3.69it/s]
Meta loss on this task batch = 5.3051e-01, PNorm = 36.8042, GNorm = 0.0720

 95% 18/19 [02:40<00:09,  9.61s/it][A

  0% 0/23 [00:00<?, ?it/s][A[A

  4% 1/23 [00:00<00:05,  4.09it/s][A[A

  9% 2/23 [00:00<00:04,  4.50it/s][A[A

 13% 3/23 [00:00<00:04,  4.53it/s][A[A

 17% 4/23 [00:00<00:03,  5.21it/s][A[A

 22% 5/23 [00:00<00:03,  5.72it/s][A[A

 26% 6/23 [00:02<00:08,  2.05it/s][A[A

 30% 7/23 [00:02<00:06,  2.50it/s][A[A

 35% 8/23 [00:02<00:05,  2.97it/s][A[A

 39% 9/23 [00:02<00:03,  3.61it/s][A[A

 43% 10/23 [00:02<00:03,  3.63it/s][A[A

 48% 11/23 [00:03<00:02,  4.06it/s][A[A

 52% 12/23 [00:03<00:02,  4.45it/s][A[A

 57% 13/23 [00:03<00:02,  5.00it/s][A[A

 61% 14/23 [00:03<00:01,  5.12it/s][A[A

 65% 15/23 [00:03<00:02,  4.00it/s][A[A

 70% 16/23 [00:04<00:01,  3.89it/s][A[A

 74% 17/23 [00:04<00:01,  4.32it/s][A[A

 78% 18/23 [00:05<00:02,  1.88it/s][A[A

 83% 19/23 [00:05<00:01,  2.34it/s][A[A

 87% 20/23 [00:05<00:01,  2.94it/s][A[A

 91% 21/23 [00:06<00:00,  3.43it/s][A[A

 96% 22/23 [00:06<00:00,  4.07it/s][A[A

100% 23/23 [00:06<00:00,  4.53it/s][A[A100% 23/23 [00:06<00:00,  3.57it/s]
Meta loss on this task batch = 4.2569e-01, PNorm = 36.8148, GNorm = 0.0378

100% 19/19 [02:47<00:00,  8.82s/it][A100% 19/19 [02:47<00:00,  8.81s/it]
Took 167.32342982292175 seconds to complete one epoch of meta training

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/20 [00:00<?, ?it/s][A[A


  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 31.58it/s]


  5% 1/20 [00:00<00:02,  8.02it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A


100% 2/2 [00:00<00:00, 19.89it/s][A[A[A100% 2/2 [00:00<00:00, 19.84it/s]


 10% 2/20 [00:00<00:02,  6.13it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 17.85it/s][A[A[A100% 3/3 [00:00<00:00, 19.68it/s]


 15% 3/20 [00:00<00:03,  4.94it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 15.73it/s]


 20% 4/20 [00:00<00:03,  4.93it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 14.75it/s][A[A[A


100% 4/4 [00:00<00:00, 16.01it/s][A[A[A100% 4/4 [00:00<00:00, 16.95it/s]


 25% 5/20 [00:01<00:03,  3.81it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.03it/s][A[A[A


100% 4/4 [00:00<00:00, 14.39it/s][A[A[A100% 4/4 [00:00<00:00, 15.44it/s]


 30% 6/20 [00:01<00:04,  3.23it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:01<00:00,  2.56it/s][A[A[A100% 4/4 [00:01<00:00,  3.31it/s]


 35% 7/20 [00:03<00:08,  1.58it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 18.06it/s][A[A[A


100% 4/4 [00:00<00:00, 17.75it/s][A[A[A100% 4/4 [00:00<00:00, 17.52it/s]


 40% 8/20 [00:03<00:06,  1.77it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.93it/s][A[A[A100% 4/4 [00:00<00:00, 22.42it/s]


 45% 9/20 [00:03<00:05,  1.99it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.30it/s][A[A[A


100% 4/4 [00:00<00:00, 19.21it/s][A[A[A100% 4/4 [00:00<00:00, 19.11it/s]


 50% 10/20 [00:04<00:04,  2.16it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 15.93it/s][A[A[A100% 4/4 [00:00<00:00, 18.36it/s]


 55% 11/20 [00:04<00:03,  2.28it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.23it/s][A[A[A100% 4/4 [00:00<00:00, 21.49it/s]


 60% 12/20 [00:04<00:03,  2.43it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.39it/s][A[A[A100% 3/3 [00:00<00:00, 13.66it/s]


 65% 13/20 [00:05<00:02,  2.47it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 13.87it/s][A[A[A100% 3/3 [00:00<00:00, 13.44it/s]


 70% 14/20 [00:05<00:02,  2.51it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:01<00:01,  1.72it/s][A[A[A100% 4/4 [00:01<00:00,  3.20it/s]


 75% 15/20 [00:07<00:03,  1.41it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 13.12it/s][A[A[A100% 3/3 [00:00<00:00, 16.32it/s]


 80% 16/20 [00:07<00:02,  1.65it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 11.03it/s][A[A[A100% 3/3 [00:00<00:00, 13.64it/s]


 85% 17/20 [00:07<00:01,  1.81it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 22.18it/s]


 90% 18/20 [00:08<00:00,  2.13it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 15.61it/s][A[A[A100% 3/3 [00:00<00:00, 19.90it/s]


 95% 19/20 [00:08<00:00,  2.37it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 13.44it/s]


100% 20/20 [00:08<00:00,  2.73it/s][A[A100% 20/20 [00:08<00:00,  2.28it/s]

100% 1/1 [00:08<00:00,  8.77s/it][A100% 1/1 [00:08<00:00,  8.77s/it]
Took 176.09389066696167 seconds to complete one epoch of meta training and validating
Meta validation score auc = 0.610062
Found better MAML checkpoint after meta validation, saving now
 40% 12/30 [38:30<57:05, 190.30s/it]  Epoch 12

  0% 0/19 [00:00<?, ?it/s][A

  0% 0/32 [00:00<?, ?it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  3% 1/32 [00:00<00:06,  4.64it/s][A[A

  6% 2/32 [00:00<00:07,  4.26it/s][A[A

  9% 3/32 [00:00<00:06,  4.38it/s][A[A

 12% 4/32 [00:00<00:06,  4.57it/s][A[A

 16% 5/32 [00:01<00:05,  4.76it/s][A[A

 19% 6/32 [00:01<00:05,  4.84it/s][A[A

 22% 7/32 [00:02<00:12,  1.99it/s][A[A

 25% 8/32 [00:02<00:09,  2.41it/s][A[A

 28% 9/32 [00:02<00:08,  2.86it/s][A[A

 31% 10/32 [00:03<00:06,  3.28it/s][A[A

 34% 11/32 [00:03<00:06,  3.40it/s][A[A

 38% 12/32 [00:03<00:05,  3.41it/s][A[A

 41% 13/32 [00:03<00:05,  3.44it/s][A[A

 44% 14/32 [00:04<00:04,  3.61it/s][A[A

 47% 15/32 [00:04<00:04,  4.05it/s][A[A

 50% 16/32 [00:04<00:03,  4.19it/s][A[A

 53% 17/32 [00:04<00:03,  4.23it/s][A[A

 56% 18/32 [00:04<00:03,  4.55it/s][A[A

 59% 19/32 [00:05<00:02,  4.67it/s][A[A

 62% 20/32 [00:06<00:06,  1.92it/s][A[A

 66% 21/32 [00:06<00:04,  2.40it/s][A[A

 69% 22/32 [00:06<00:03,  2.81it/s][A[A

 72% 23/32 [00:06<00:02,  3.34it/s][A[A

 75% 24/32 [00:07<00:02,  3.52it/s][A[A

 78% 25/32 [00:07<00:01,  3.86it/s][A[A

 81% 26/32 [00:07<00:01,  3.94it/s][A[A

 84% 27/32 [00:07<00:01,  4.40it/s][A[A

 88% 28/32 [00:08<00:00,  4.69it/s][A[A

 91% 29/32 [00:08<00:00,  4.50it/s][A[A

 94% 30/32 [00:08<00:00,  4.87it/s][A[A

 97% 31/32 [00:08<00:00,  4.94it/s][A[A

100% 32/32 [00:08<00:00,  4.94it/s][A[A100% 32/32 [00:08<00:00,  3.62it/s]
Meta loss on this task batch = 5.3264e-01, PNorm = 36.8263, GNorm = 0.0478

  5% 1/19 [00:09<02:52,  9.60s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.35it/s][A[A

  6% 2/32 [00:00<00:05,  5.08it/s][A[A

  9% 3/32 [00:00<00:06,  4.58it/s][A[A

 12% 4/32 [00:01<00:14,  1.88it/s][A[A

 16% 5/32 [00:02<00:11,  2.26it/s][A[A

 19% 6/32 [00:02<00:09,  2.64it/s][A[A

 22% 7/32 [00:02<00:07,  3.16it/s][A[A

 25% 8/32 [00:02<00:06,  3.66it/s][A[A

 28% 9/32 [00:02<00:05,  3.98it/s][A[A

 31% 10/32 [00:03<00:05,  4.39it/s][A[A

 34% 11/32 [00:03<00:04,  4.61it/s][A[A

 38% 12/32 [00:03<00:04,  4.28it/s][A[A

 41% 13/32 [00:03<00:04,  4.66it/s][A[A

 44% 14/32 [00:03<00:03,  4.63it/s][A[A

 47% 15/32 [00:04<00:03,  4.59it/s][A[A

 50% 16/32 [00:04<00:03,  4.69it/s][A[A

 53% 17/32 [00:04<00:03,  4.52it/s][A[A

 56% 18/32 [00:05<00:07,  1.88it/s][A[A

 59% 19/32 [00:06<00:05,  2.36it/s][A[A

 62% 20/32 [00:06<00:04,  2.66it/s][A[A

 66% 21/32 [00:06<00:03,  3.06it/s][A[A

 69% 22/32 [00:06<00:02,  3.45it/s][A[A

 72% 23/32 [00:06<00:02,  3.96it/s][A[A

 75% 24/32 [00:07<00:01,  4.37it/s][A[A

 78% 25/32 [00:07<00:01,  4.53it/s][A[A

 81% 26/32 [00:07<00:01,  4.54it/s][A[A

 84% 27/32 [00:07<00:01,  4.58it/s][A[A

 88% 28/32 [00:07<00:00,  4.51it/s][A[A

 91% 29/32 [00:08<00:00,  4.84it/s][A[A

 94% 30/32 [00:08<00:00,  4.77it/s][A[A

 97% 31/32 [00:08<00:00,  4.74it/s][A[A

100% 32/32 [00:09<00:00,  1.99it/s][A[A100% 32/32 [00:09<00:00,  3.29it/s]
Meta loss on this task batch = 5.0722e-01, PNorm = 36.8384, GNorm = 0.0314

 11% 2/19 [00:20<02:47,  9.88s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.50it/s][A[A

  6% 2/32 [00:00<00:05,  5.17it/s][A[A

  9% 3/32 [00:00<00:05,  5.06it/s][A[A

 12% 4/32 [00:00<00:05,  4.70it/s][A[A

 16% 5/32 [00:01<00:05,  4.71it/s][A[A

 19% 6/32 [00:01<00:05,  4.57it/s][A[A

 22% 7/32 [00:01<00:05,  4.56it/s][A[A

 25% 8/32 [00:01<00:04,  4.93it/s][A[A

 28% 9/32 [00:01<00:04,  4.94it/s][A[A

 31% 10/32 [00:02<00:04,  4.65it/s][A[A

 34% 11/32 [00:03<00:10,  1.92it/s][A[A

 38% 12/32 [00:03<00:08,  2.34it/s][A[A

 41% 13/32 [00:03<00:06,  2.73it/s][A[A

 44% 14/32 [00:04<00:05,  3.10it/s][A[A

 47% 15/32 [00:04<00:05,  3.33it/s][A[A

 50% 16/32 [00:04<00:04,  3.70it/s][A[A

 53% 17/32 [00:04<00:03,  3.86it/s][A[A

 56% 18/32 [00:04<00:03,  4.37it/s][A[A

 59% 19/32 [00:05<00:02,  4.65it/s][A[A

 62% 20/32 [00:05<00:02,  5.00it/s][A[A

 66% 21/32 [00:05<00:02,  5.08it/s][A[A

 69% 22/32 [00:05<00:01,  5.07it/s][A[A

 72% 23/32 [00:05<00:01,  4.76it/s][A[A

 75% 24/32 [00:07<00:04,  1.95it/s][A[A

 78% 25/32 [00:07<00:02,  2.42it/s][A[A

 81% 26/32 [00:07<00:02,  2.91it/s][A[A

 84% 27/32 [00:07<00:01,  3.18it/s][A[A

 88% 28/32 [00:07<00:01,  3.49it/s][A[A

 91% 29/32 [00:08<00:00,  3.71it/s][A[A

 94% 30/32 [00:08<00:00,  3.74it/s][A[A

 97% 31/32 [00:08<00:00,  4.17it/s][A[A

100% 32/32 [00:08<00:00,  4.27it/s][A[A100% 32/32 [00:08<00:00,  3.65it/s]
Meta loss on this task batch = 4.9935e-01, PNorm = 36.8522, GNorm = 0.0313

 16% 3/19 [00:29<02:36,  9.79s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.05it/s][A[A

  6% 2/32 [00:00<00:07,  4.27it/s][A[A

  9% 3/32 [00:00<00:06,  4.50it/s][A[A

 12% 4/32 [00:00<00:06,  4.47it/s][A[A

 16% 5/32 [00:01<00:05,  4.51it/s][A[A

 19% 6/32 [00:01<00:05,  4.46it/s][A[A

 22% 7/32 [00:02<00:12,  1.94it/s][A[A

 25% 8/32 [00:02<00:10,  2.39it/s][A[A

 28% 9/32 [00:02<00:08,  2.86it/s][A[A

 31% 10/32 [00:03<00:06,  3.42it/s][A[A

 34% 11/32 [00:03<00:05,  3.87it/s][A[A

 38% 12/32 [00:03<00:04,  4.17it/s][A[A

 41% 13/32 [00:03<00:04,  4.21it/s][A[A

 44% 14/32 [00:03<00:04,  4.31it/s][A[A

 47% 15/32 [00:04<00:03,  4.63it/s][A[A

 50% 16/32 [00:04<00:03,  4.64it/s][A[A

 53% 17/32 [00:04<00:02,  5.07it/s][A[A

 56% 18/32 [00:04<00:02,  4.92it/s][A[A

 59% 19/32 [00:04<00:02,  5.27it/s][A[A

 62% 20/32 [00:05<00:02,  4.97it/s][A[A

 66% 21/32 [00:05<00:02,  4.83it/s][A[A

 69% 22/32 [00:06<00:05,  1.93it/s][A[A

 72% 23/32 [00:06<00:03,  2.32it/s][A[A

 75% 24/32 [00:06<00:02,  2.73it/s][A[A

 78% 25/32 [00:07<00:02,  3.23it/s][A[A

 81% 26/32 [00:07<00:01,  3.63it/s][A[A

 84% 27/32 [00:07<00:01,  3.87it/s][A[A

 88% 28/32 [00:07<00:00,  4.14it/s][A[A

 91% 29/32 [00:07<00:00,  4.50it/s][A[A

 94% 30/32 [00:08<00:00,  4.41it/s][A[A

 97% 31/32 [00:08<00:00,  4.52it/s][A[A

100% 32/32 [00:08<00:00,  4.53it/s][A[A100% 32/32 [00:08<00:00,  3.73it/s]
Meta loss on this task batch = 5.7401e-01, PNorm = 36.8652, GNorm = 0.0327

 21% 4/19 [00:39<02:24,  9.66s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.54it/s][A[A

  6% 2/32 [00:00<00:06,  4.70it/s][A[A

  9% 3/32 [00:00<00:05,  5.09it/s][A[A

 12% 4/32 [00:01<00:14,  1.94it/s][A[A

 16% 5/32 [00:02<00:11,  2.35it/s][A[A

 19% 6/32 [00:02<00:09,  2.77it/s][A[A

 22% 7/32 [00:02<00:08,  3.08it/s][A[A

 25% 8/32 [00:02<00:06,  3.58it/s][A[A

 28% 9/32 [00:02<00:05,  3.86it/s][A[A

 31% 10/32 [00:03<00:05,  4.16it/s][A[A

 34% 11/32 [00:03<00:04,  4.30it/s][A[A

 38% 12/32 [00:03<00:04,  4.33it/s][A[A

 41% 13/32 [00:03<00:04,  4.32it/s][A[A

 44% 14/32 [00:03<00:04,  4.45it/s][A[A

 47% 15/32 [00:05<00:08,  1.92it/s][A[A

 50% 16/32 [00:05<00:06,  2.41it/s][A[A

 53% 17/32 [00:05<00:05,  2.68it/s][A[A

 56% 18/32 [00:05<00:04,  3.07it/s][A[A

 59% 19/32 [00:06<00:03,  3.33it/s][A[A

 62% 20/32 [00:06<00:03,  3.52it/s][A[A

 66% 21/32 [00:06<00:02,  3.87it/s][A[A

 69% 22/32 [00:06<00:02,  4.29it/s][A[A

 72% 23/32 [00:06<00:02,  4.41it/s][A[A

 75% 24/32 [00:07<00:01,  4.27it/s][A[A

 78% 25/32 [00:07<00:01,  4.28it/s][A[A

 81% 26/32 [00:08<00:03,  1.91it/s][A[A

 84% 27/32 [00:08<00:02,  2.34it/s][A[A

 88% 28/32 [00:08<00:01,  2.88it/s][A[A

 91% 29/32 [00:09<00:00,  3.29it/s][A[A

 94% 30/32 [00:09<00:00,  3.59it/s][A[A

 97% 31/32 [00:09<00:00,  3.91it/s][A[A

100% 32/32 [00:09<00:00,  4.06it/s][A[A100% 32/32 [00:09<00:00,  3.26it/s]
Meta loss on this task batch = 5.1313e-01, PNorm = 36.8764, GNorm = 0.0463

 26% 5/19 [00:49<02:19,  9.94s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.91it/s][A[A

  6% 2/32 [00:00<00:06,  4.92it/s][A[A

  9% 3/32 [00:00<00:05,  5.25it/s][A[A

 12% 4/32 [00:00<00:05,  5.22it/s][A[A

 16% 5/32 [00:01<00:06,  4.38it/s][A[A

 19% 6/32 [00:02<00:13,  1.88it/s][A[A

 22% 7/32 [00:02<00:10,  2.30it/s][A[A

 25% 8/32 [00:02<00:08,  2.72it/s][A[A

 28% 9/32 [00:02<00:07,  2.98it/s][A[A

 31% 10/32 [00:03<00:06,  3.33it/s][A[A

 34% 11/32 [00:03<00:05,  3.59it/s][A[A

 38% 12/32 [00:03<00:05,  3.88it/s][A[A

 41% 13/32 [00:03<00:04,  4.15it/s][A[A

 44% 14/32 [00:04<00:04,  4.11it/s][A[A

 47% 15/32 [00:04<00:04,  4.23it/s][A[A

 50% 16/32 [00:05<00:08,  1.86it/s][A[A

 53% 17/32 [00:05<00:06,  2.31it/s][A[A

 56% 18/32 [00:05<00:05,  2.71it/s][A[A

 59% 19/32 [00:06<00:04,  3.11it/s][A[A

 62% 20/32 [00:06<00:03,  3.56it/s][A[A

 66% 21/32 [00:06<00:02,  3.91it/s][A[A

 69% 22/32 [00:06<00:02,  4.09it/s][A[A

 72% 23/32 [00:07<00:02,  4.20it/s][A[A

 75% 24/32 [00:07<00:01,  4.36it/s][A[A

 78% 25/32 [00:07<00:01,  4.37it/s][A[A

 81% 26/32 [00:07<00:01,  4.64it/s][A[A

 84% 27/32 [00:07<00:01,  4.48it/s][A[A

 88% 28/32 [00:09<00:02,  1.83it/s][A[A

 91% 29/32 [00:09<00:01,  2.20it/s][A[A

 94% 30/32 [00:09<00:00,  2.65it/s][A[A

 97% 31/32 [00:09<00:00,  3.25it/s][A[A

100% 32/32 [00:09<00:00,  3.66it/s][A[A100% 32/32 [00:09<00:00,  3.22it/s]
Meta loss on this task batch = 4.4285e-01, PNorm = 36.8875, GNorm = 0.0560

 32% 6/19 [01:00<02:12, 10.18s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.35it/s][A[A

  6% 2/32 [00:00<00:05,  5.43it/s][A[A

  9% 3/32 [00:00<00:05,  5.14it/s][A[A

 12% 4/32 [00:00<00:05,  4.79it/s][A[A

 16% 5/32 [00:01<00:05,  4.87it/s][A[A

 19% 6/32 [00:01<00:05,  4.89it/s][A[A

 22% 7/32 [00:01<00:05,  4.76it/s][A[A

 25% 8/32 [00:01<00:04,  4.81it/s][A[A

 28% 9/32 [00:01<00:04,  4.79it/s][A[A

 31% 10/32 [00:03<00:11,  1.96it/s][A[A

 34% 11/32 [00:03<00:08,  2.38it/s][A[A

 38% 12/32 [00:03<00:07,  2.80it/s][A[A

 41% 13/32 [00:03<00:05,  3.24it/s][A[A

 44% 14/32 [00:03<00:04,  3.65it/s][A[A

 47% 15/32 [00:04<00:04,  3.81it/s][A[A

 50% 16/32 [00:04<00:04,  3.90it/s][A[A

 53% 17/32 [00:04<00:03,  4.00it/s][A[A

 56% 18/32 [00:04<00:03,  4.08it/s][A[A

 59% 19/32 [00:06<00:07,  1.85it/s][A[A

 62% 20/32 [00:06<00:05,  2.23it/s][A[A

 66% 21/32 [00:06<00:04,  2.61it/s][A[A

 69% 22/32 [00:06<00:03,  2.97it/s][A[A

 72% 23/32 [00:06<00:02,  3.27it/s][A[A

 75% 24/32 [00:07<00:02,  3.59it/s][A[A

 78% 25/32 [00:07<00:01,  3.91it/s][A[A

 81% 26/32 [00:07<00:01,  4.02it/s][A[A

 84% 27/32 [00:07<00:01,  4.19it/s][A[A

 88% 28/32 [00:08<00:00,  4.26it/s][A[A

 91% 29/32 [00:08<00:00,  4.77it/s][A[A

 94% 30/32 [00:08<00:00,  4.89it/s][A[A

 97% 31/32 [00:09<00:00,  1.99it/s][A[A

100% 32/32 [00:09<00:00,  2.38it/s][A[A100% 32/32 [00:09<00:00,  3.25it/s]
Meta loss on this task batch = 4.6823e-01, PNorm = 36.8995, GNorm = 0.0511

 37% 7/19 [01:11<02:03, 10.32s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.12it/s][A[A

  6% 2/32 [00:00<00:07,  4.06it/s][A[A

  9% 3/32 [00:00<00:07,  4.12it/s][A[A

 12% 4/32 [00:00<00:06,  4.16it/s][A[A

 16% 5/32 [00:01<00:06,  4.30it/s][A[A

 19% 6/32 [00:01<00:06,  4.21it/s][A[A

 22% 7/32 [00:01<00:05,  4.68it/s][A[A

 25% 8/32 [00:01<00:05,  4.42it/s][A[A

 28% 9/32 [00:03<00:12,  1.91it/s][A[A

 31% 10/32 [00:03<00:09,  2.29it/s][A[A

 34% 11/32 [00:03<00:07,  2.70it/s][A[A

 38% 12/32 [00:03<00:06,  3.12it/s][A[A

 41% 13/32 [00:03<00:05,  3.50it/s][A[A

 44% 14/32 [00:04<00:04,  4.01it/s][A[A

 47% 15/32 [00:04<00:04,  4.15it/s][A[A

 50% 16/32 [00:04<00:03,  4.28it/s][A[A

 53% 17/32 [00:04<00:03,  4.12it/s][A[A

 56% 18/32 [00:05<00:03,  4.22it/s][A[A

 59% 19/32 [00:05<00:03,  4.12it/s][A[A

 62% 20/32 [00:05<00:02,  4.26it/s][A[A

 66% 21/32 [00:05<00:02,  4.41it/s][A[A

 69% 22/32 [00:05<00:02,  4.41it/s][A[A

 72% 23/32 [00:07<00:04,  1.99it/s][A[A

 75% 24/32 [00:07<00:03,  2.37it/s][A[A

 78% 25/32 [00:07<00:02,  2.73it/s][A[A

 81% 26/32 [00:07<00:01,  3.06it/s][A[A

 84% 27/32 [00:07<00:01,  3.60it/s][A[A

 88% 28/32 [00:08<00:01,  3.96it/s][A[A

 91% 29/32 [00:08<00:00,  4.34it/s][A[A

 94% 30/32 [00:08<00:00,  4.51it/s][A[A

 97% 31/32 [00:08<00:00,  4.83it/s][A[A

100% 32/32 [00:08<00:00,  4.98it/s][A[A100% 32/32 [00:08<00:00,  3.61it/s]
Meta loss on this task batch = 3.1861e-01, PNorm = 36.9141, GNorm = 0.0627

 42% 8/19 [01:20<01:51, 10.12s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.12it/s][A[A

  6% 2/32 [00:00<00:05,  5.05it/s][A[A

  9% 3/32 [00:00<00:05,  5.02it/s][A[A

 12% 4/32 [00:00<00:05,  5.14it/s][A[A

 16% 5/32 [00:00<00:05,  5.11it/s][A[A

 19% 6/32 [00:01<00:04,  5.23it/s][A[A

 22% 7/32 [00:01<00:04,  5.31it/s][A[A

 25% 8/32 [00:02<00:11,  2.06it/s][A[A

 28% 9/32 [00:02<00:09,  2.53it/s][A[A

 31% 10/32 [00:02<00:07,  2.99it/s][A[A

 34% 11/32 [00:03<00:06,  3.43it/s][A[A

 38% 12/32 [00:03<00:05,  3.90it/s][A[A

 41% 13/32 [00:03<00:04,  4.25it/s][A[A

 44% 14/32 [00:03<00:03,  4.55it/s][A[A

 47% 15/32 [00:03<00:03,  4.90it/s][A[A

 50% 16/32 [00:03<00:03,  5.07it/s][A[A

 53% 17/32 [00:04<00:02,  5.09it/s][A[A

 56% 18/32 [00:04<00:02,  5.05it/s][A[A

 59% 19/32 [00:04<00:02,  5.25it/s][A[A

 62% 20/32 [00:04<00:02,  5.20it/s][A[A

 66% 21/32 [00:04<00:02,  5.16it/s][A[A

 69% 22/32 [00:05<00:01,  5.36it/s][A[A

 72% 23/32 [00:05<00:01,  5.04it/s][A[A

 75% 24/32 [00:06<00:03,  2.04it/s][A[A

 78% 25/32 [00:06<00:02,  2.45it/s][A[A

 81% 26/32 [00:06<00:02,  2.93it/s][A[A

 84% 27/32 [00:07<00:01,  3.39it/s][A[A

 88% 28/32 [00:07<00:01,  3.86it/s][A[A

 91% 29/32 [00:07<00:00,  4.25it/s][A[A

 94% 30/32 [00:07<00:00,  4.53it/s][A[A

 97% 31/32 [00:07<00:00,  4.76it/s][A[A

100% 32/32 [00:08<00:00,  4.89it/s][A[A100% 32/32 [00:08<00:00,  3.99it/s]
Meta loss on this task batch = 4.8934e-02, PNorm = 36.9295, GNorm = 0.0975

 47% 9/19 [01:29<01:37,  9.73s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.68it/s][A[A

  6% 2/32 [00:00<00:05,  5.27it/s][A[A

  9% 3/32 [00:00<00:05,  5.31it/s][A[A

 12% 4/32 [00:00<00:05,  5.40it/s][A[A

 16% 5/32 [00:00<00:04,  5.48it/s][A[A

 19% 6/32 [00:01<00:04,  5.39it/s][A[A

 22% 7/32 [00:02<00:12,  2.07it/s][A[A

 25% 8/32 [00:02<00:09,  2.57it/s][A[A

 28% 9/32 [00:02<00:07,  3.10it/s][A[A

 31% 10/32 [00:02<00:06,  3.52it/s][A[A

 34% 11/32 [00:03<00:05,  3.94it/s][A[A

 38% 12/32 [00:03<00:04,  4.29it/s][A[A

 41% 13/32 [00:03<00:04,  4.55it/s][A[A

 44% 14/32 [00:03<00:03,  4.77it/s][A[A

 47% 15/32 [00:03<00:03,  5.04it/s][A[A

 50% 16/32 [00:03<00:03,  4.85it/s][A[A

 53% 17/32 [00:04<00:03,  4.95it/s][A[A

 56% 18/32 [00:04<00:02,  5.11it/s][A[A

 59% 19/32 [00:04<00:02,  4.96it/s][A[A

 62% 20/32 [00:05<00:05,  2.04it/s][A[A

 66% 21/32 [00:05<00:04,  2.47it/s][A[A

 69% 22/32 [00:06<00:03,  2.93it/s][A[A

 72% 23/32 [00:06<00:02,  3.37it/s][A[A

 75% 24/32 [00:06<00:02,  3.78it/s][A[A

 78% 25/32 [00:06<00:01,  4.21it/s][A[A

 81% 26/32 [00:06<00:01,  4.21it/s][A[A

 84% 27/32 [00:07<00:01,  4.29it/s][A[A

 88% 28/32 [00:07<00:00,  4.30it/s][A[A

 91% 29/32 [00:07<00:00,  4.24it/s][A[A

 94% 30/32 [00:07<00:00,  4.28it/s][A[A

 97% 31/32 [00:08<00:00,  4.33it/s][A[A

100% 32/32 [00:08<00:00,  4.36it/s][A[A100% 32/32 [00:08<00:00,  3.86it/s]
Meta loss on this task batch = 1.8288e-01, PNorm = 36.9447, GNorm = 0.0659

 53% 10/19 [01:38<01:25,  9.54s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:38,  1.24s/it][A[A

  6% 2/32 [00:01<00:28,  1.06it/s][A[A

  9% 3/32 [00:01<00:21,  1.36it/s][A[A

 12% 4/32 [00:01<00:16,  1.71it/s][A[A

 16% 5/32 [00:02<00:12,  2.08it/s][A[A

 19% 6/32 [00:02<00:10,  2.47it/s][A[A

 22% 7/32 [00:02<00:08,  2.97it/s][A[A

 25% 8/32 [00:03<00:14,  1.65it/s][A[A

 28% 9/32 [00:04<00:11,  2.01it/s][A[A

 31% 10/32 [00:04<00:09,  2.37it/s][A[A

 34% 11/32 [00:04<00:07,  2.72it/s][A[A

 38% 12/32 [00:04<00:06,  3.05it/s][A[A

 41% 13/32 [00:06<00:11,  1.67it/s][A[A

 44% 14/32 [00:06<00:08,  2.03it/s][A[A

 47% 15/32 [00:06<00:07,  2.39it/s][A[A

 50% 16/32 [00:06<00:05,  2.74it/s][A[A

 53% 17/32 [00:07<00:04,  3.03it/s][A[A

 56% 18/32 [00:07<00:03,  3.54it/s][A[A

 59% 19/32 [00:07<00:03,  3.73it/s][A[A

 62% 20/32 [00:07<00:03,  3.87it/s][A[A

 66% 21/32 [00:07<00:02,  3.96it/s][A[A

 69% 22/32 [00:08<00:02,  4.02it/s][A[A

 72% 23/32 [00:08<00:02,  4.06it/s][A[A

 75% 24/32 [00:08<00:01,  4.04it/s][A[A

 78% 25/32 [00:08<00:01,  4.07it/s][A[A

 81% 26/32 [00:09<00:01,  4.15it/s][A[A

 84% 27/32 [00:09<00:01,  4.16it/s][A[A

 88% 28/32 [00:10<00:02,  1.86it/s][A[A

 91% 29/32 [00:10<00:01,  2.21it/s][A[A

 94% 30/32 [00:11<00:00,  2.57it/s][A[A

 97% 31/32 [00:11<00:00,  2.91it/s][A[A

100% 32/32 [00:11<00:00,  3.18it/s][A[A100% 32/32 [00:11<00:00,  2.77it/s]
Meta loss on this task batch = 5.6604e-01, PNorm = 36.9580, GNorm = 0.0671

 58% 11/19 [01:51<01:23, 10.40s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.76it/s][A[A

  6% 2/32 [00:00<00:07,  3.83it/s][A[A

  9% 3/32 [00:01<00:15,  1.81it/s][A[A

 12% 4/32 [00:01<00:12,  2.18it/s][A[A

 16% 5/32 [00:02<00:10,  2.54it/s][A[A

 19% 6/32 [00:02<00:09,  2.87it/s][A[A

 22% 7/32 [00:02<00:07,  3.38it/s][A[A

 25% 8/32 [00:02<00:06,  3.63it/s][A[A

 28% 9/32 [00:03<00:06,  3.81it/s][A[A

 31% 10/32 [00:03<00:05,  3.93it/s][A[A

 34% 11/32 [00:03<00:05,  3.99it/s][A[A

 38% 12/32 [00:03<00:04,  4.02it/s][A[A

 41% 13/32 [00:04<00:04,  4.04it/s][A[A

 44% 14/32 [00:05<00:09,  1.85it/s][A[A

 47% 15/32 [00:05<00:07,  2.23it/s][A[A

 50% 16/32 [00:05<00:05,  2.75it/s][A[A

 53% 17/32 [00:05<00:04,  3.05it/s][A[A

 56% 18/32 [00:06<00:04,  3.35it/s][A[A

 59% 19/32 [00:06<00:03,  3.55it/s][A[A

 62% 20/32 [00:06<00:03,  3.67it/s][A[A

 66% 21/32 [00:06<00:02,  3.79it/s][A[A

 69% 22/32 [00:07<00:02,  4.17it/s][A[A

 72% 23/32 [00:07<00:02,  4.17it/s][A[A

 75% 24/32 [00:07<00:01,  4.24it/s][A[A

 78% 25/32 [00:07<00:01,  4.27it/s][A[A

 81% 26/32 [00:08<00:01,  4.28it/s][A[A

 84% 27/32 [00:09<00:02,  1.88it/s][A[A

 88% 28/32 [00:09<00:01,  2.26it/s][A[A

 91% 29/32 [00:09<00:01,  2.67it/s][A[A

 94% 30/32 [00:09<00:00,  3.02it/s][A[A

 97% 31/32 [00:10<00:00,  3.31it/s][A[A

100% 32/32 [00:10<00:00,  3.57it/s][A[A100% 32/32 [00:10<00:00,  3.08it/s]
Meta loss on this task batch = 5.8529e-01, PNorm = 36.9679, GNorm = 0.0920

 63% 12/19 [02:02<01:14, 10.64s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.04it/s][A[A

  6% 2/32 [00:00<00:07,  4.12it/s][A[A

  9% 3/32 [00:00<00:06,  4.16it/s][A[A

 12% 4/32 [00:01<00:14,  1.87it/s][A[A

 16% 5/32 [00:02<00:12,  2.25it/s][A[A

 19% 6/32 [00:02<00:09,  2.62it/s][A[A

 22% 7/32 [00:02<00:08,  3.05it/s][A[A

 25% 8/32 [00:02<00:07,  3.36it/s][A[A

 28% 9/32 [00:03<00:06,  3.70it/s][A[A

 31% 10/32 [00:03<00:05,  3.90it/s][A[A

 34% 11/32 [00:03<00:05,  4.05it/s][A[A

 38% 12/32 [00:03<00:04,  4.17it/s][A[A

 41% 13/32 [00:03<00:04,  4.22it/s][A[A

 44% 14/32 [00:04<00:03,  4.55it/s][A[A

 47% 15/32 [00:04<00:03,  4.47it/s][A[A

 50% 16/32 [00:04<00:03,  4.41it/s][A[A

 53% 17/32 [00:05<00:07,  1.89it/s][A[A

 56% 18/32 [00:06<00:06,  2.27it/s][A[A

 59% 19/32 [00:06<00:04,  2.61it/s][A[A

 62% 20/32 [00:06<00:04,  2.99it/s][A[A

 66% 21/32 [00:06<00:03,  3.25it/s][A[A

 69% 22/32 [00:07<00:02,  3.47it/s][A[A

 72% 23/32 [00:07<00:02,  3.66it/s][A[A

 75% 24/32 [00:08<00:04,  1.79it/s][A[A

 78% 25/32 [00:08<00:03,  2.19it/s][A[A

 81% 26/32 [00:08<00:02,  2.57it/s][A[A

 84% 27/32 [00:09<00:01,  2.97it/s][A[A

 88% 28/32 [00:09<00:01,  3.45it/s][A[A

 91% 29/32 [00:09<00:00,  3.66it/s][A[A

 94% 30/32 [00:09<00:00,  3.83it/s][A[A

 97% 31/32 [00:10<00:00,  4.02it/s][A[A

100% 32/32 [00:10<00:00,  4.10it/s][A[A100% 32/32 [00:10<00:00,  3.12it/s]
Meta loss on this task batch = 5.8131e-01, PNorm = 36.9776, GNorm = 0.0617

 68% 13/19 [02:13<01:04, 10.77s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.11it/s][A[A

  6% 2/32 [00:00<00:07,  4.19it/s][A[A

  9% 3/32 [00:00<00:06,  4.28it/s][A[A

 12% 4/32 [00:00<00:06,  4.30it/s][A[A

 16% 5/32 [00:01<00:06,  4.29it/s][A[A

 19% 6/32 [00:02<00:13,  1.90it/s][A[A

 22% 7/32 [00:02<00:11,  2.27it/s][A[A

 25% 8/32 [00:02<00:08,  2.68it/s][A[A

 28% 9/32 [00:03<00:07,  3.04it/s][A[A

 31% 10/32 [00:03<00:06,  3.31it/s][A[A

 34% 11/32 [00:03<00:05,  3.52it/s][A[A

 38% 12/32 [00:03<00:05,  3.66it/s][A[A

 41% 13/32 [00:04<00:04,  3.80it/s][A[A

 44% 14/32 [00:05<00:09,  1.81it/s][A[A

 47% 15/32 [00:05<00:07,  2.18it/s][A[A

 50% 16/32 [00:05<00:06,  2.52it/s][A[A

 53% 17/32 [00:05<00:05,  2.86it/s][A[A

 56% 18/32 [00:06<00:04,  3.16it/s][A[A

 59% 19/32 [00:06<00:03,  3.47it/s][A[A

 62% 20/32 [00:06<00:03,  3.85it/s][A[A

 66% 21/32 [00:07<00:06,  1.81it/s][A[A

 69% 22/32 [00:08<00:04,  2.18it/s][A[A

 72% 23/32 [00:08<00:03,  2.56it/s][A[A

 75% 24/32 [00:08<00:02,  2.95it/s][A[A

 78% 25/32 [00:08<00:02,  3.24it/s][A[A

 81% 26/32 [00:09<00:01,  3.48it/s][A[A

 84% 27/32 [00:09<00:01,  3.71it/s][A[A

 88% 28/32 [00:09<00:01,  3.88it/s][A[A

 91% 29/32 [00:09<00:00,  4.04it/s][A[A

 94% 30/32 [00:09<00:00,  4.21it/s][A[A

 97% 31/32 [00:10<00:00,  4.21it/s][A[A

100% 32/32 [00:10<00:00,  4.13it/s][A[A100% 32/32 [00:10<00:00,  3.07it/s]
Meta loss on this task batch = 5.9217e-01, PNorm = 36.9827, GNorm = 0.1101

 74% 14/19 [02:24<00:54, 10.92s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:37,  1.22s/it][A[A

  6% 2/32 [00:01<00:27,  1.08it/s][A[A

  9% 3/32 [00:01<00:20,  1.38it/s][A[A

 12% 4/32 [00:01<00:16,  1.74it/s][A[A

 16% 5/32 [00:02<00:12,  2.14it/s][A[A

 19% 6/32 [00:02<00:10,  2.46it/s][A[A

 22% 7/32 [00:02<00:08,  2.78it/s][A[A

 25% 8/32 [00:02<00:07,  3.22it/s][A[A

 28% 9/32 [00:04<00:13,  1.70it/s][A[A

 31% 10/32 [00:04<00:10,  2.15it/s][A[A

 34% 11/32 [00:04<00:08,  2.58it/s][A[A

 38% 12/32 [00:04<00:06,  2.97it/s][A[A

 41% 13/32 [00:04<00:05,  3.34it/s][A[A

 44% 14/32 [00:05<00:04,  3.68it/s][A[A

 47% 15/32 [00:05<00:04,  4.02it/s][A[A

 50% 16/32 [00:05<00:03,  4.17it/s][A[A

 53% 17/32 [00:05<00:03,  4.17it/s][A[A

 56% 18/32 [00:05<00:03,  4.30it/s][A[A

 59% 19/32 [00:06<00:02,  4.52it/s][A[A

 62% 20/32 [00:07<00:06,  1.91it/s][A[A

 66% 21/32 [00:07<00:04,  2.35it/s][A[A

 69% 22/32 [00:07<00:03,  2.75it/s][A[A

 72% 23/32 [00:08<00:02,  3.05it/s][A[A

 75% 24/32 [00:08<00:02,  3.41it/s][A[A

 78% 25/32 [00:08<00:01,  3.69it/s][A[A

 81% 26/32 [00:08<00:01,  3.78it/s][A[A

 84% 27/32 [00:08<00:01,  3.93it/s][A[A

 88% 28/32 [00:09<00:00,  4.03it/s][A[A

 91% 29/32 [00:09<00:00,  4.14it/s][A[A

 94% 30/32 [00:10<00:01,  1.86it/s][A[A

 97% 31/32 [00:10<00:00,  2.24it/s][A[A

100% 32/32 [00:11<00:00,  2.65it/s][A[A100% 32/32 [00:11<00:00,  2.87it/s]
Meta loss on this task batch = 4.9558e-01, PNorm = 36.9890, GNorm = 0.0424

 79% 15/19 [02:36<00:44, 11.23s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.30it/s][A[A

  6% 2/32 [00:00<00:06,  4.35it/s][A[A

  9% 3/32 [00:00<00:06,  4.37it/s][A[A

 12% 4/32 [00:00<00:06,  4.39it/s][A[A

 16% 5/32 [00:01<00:05,  4.64it/s][A[A

 19% 6/32 [00:01<00:05,  4.53it/s][A[A

 22% 7/32 [00:01<00:05,  4.56it/s][A[A

 25% 8/32 [00:01<00:05,  4.48it/s][A[A

 28% 9/32 [00:02<00:05,  4.43it/s][A[A

 31% 10/32 [00:02<00:04,  4.40it/s][A[A

 34% 11/32 [00:02<00:04,  4.42it/s][A[A

 38% 12/32 [00:03<00:10,  1.92it/s][A[A

 41% 13/32 [00:03<00:08,  2.32it/s][A[A

 44% 14/32 [00:04<00:06,  2.70it/s][A[A

 47% 15/32 [00:04<00:05,  3.07it/s][A[A

 50% 16/32 [00:04<00:04,  3.40it/s][A[A

 53% 17/32 [00:04<00:04,  3.63it/s][A[A

 56% 18/32 [00:04<00:03,  4.03it/s][A[A

 59% 19/32 [00:05<00:03,  4.14it/s][A[A

 62% 20/32 [00:05<00:02,  4.25it/s][A[A

 66% 21/32 [00:05<00:02,  4.33it/s][A[A

 69% 22/32 [00:05<00:02,  4.50it/s][A[A

 72% 23/32 [00:07<00:04,  1.90it/s][A[A

 75% 24/32 [00:07<00:03,  2.24it/s][A[A

 78% 25/32 [00:07<00:02,  2.68it/s][A[A

 81% 26/32 [00:07<00:02,  2.99it/s][A[A

 84% 27/32 [00:08<00:01,  3.20it/s][A[A

 88% 28/32 [00:08<00:01,  3.48it/s][A[A

 91% 29/32 [00:08<00:00,  3.52it/s][A[A

 94% 30/32 [00:08<00:00,  3.72it/s][A[A

 97% 31/32 [00:10<00:00,  1.81it/s][A[A

100% 32/32 [00:10<00:00,  2.19it/s][A[A100% 32/32 [00:10<00:00,  3.13it/s]
Meta loss on this task batch = 5.5193e-01, PNorm = 36.9919, GNorm = 0.0427

 84% 16/19 [02:47<00:33, 11.18s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.05it/s][A[A

  6% 2/32 [00:00<00:07,  4.04it/s][A[A

  9% 3/32 [00:00<00:06,  4.21it/s][A[A

 12% 4/32 [00:00<00:06,  4.43it/s][A[A

 16% 5/32 [00:01<00:05,  4.51it/s][A[A

 19% 6/32 [00:01<00:05,  4.63it/s][A[A

 22% 7/32 [00:01<00:05,  4.87it/s][A[A

 25% 8/32 [00:01<00:05,  4.57it/s][A[A

 28% 9/32 [00:01<00:05,  4.58it/s][A[A

 31% 10/32 [00:02<00:04,  5.05it/s][A[A

 34% 11/32 [00:03<00:10,  2.01it/s][A[A

 38% 12/32 [00:03<00:08,  2.40it/s][A[A

 41% 13/32 [00:03<00:06,  2.85it/s][A[A

 44% 14/32 [00:03<00:05,  3.21it/s][A[A

 47% 15/32 [00:04<00:04,  3.44it/s][A[A

 50% 16/32 [00:04<00:04,  3.61it/s][A[A

 53% 17/32 [00:04<00:03,  3.82it/s][A[A

 56% 18/32 [00:04<00:03,  3.95it/s][A[A

 59% 19/32 [00:05<00:03,  3.91it/s][A[A

 62% 20/32 [00:05<00:02,  4.05it/s][A[A

 66% 21/32 [00:05<00:02,  4.21it/s][A[A

 69% 22/32 [00:05<00:02,  4.48it/s][A[A

 72% 23/32 [00:07<00:04,  1.89it/s][A[A

 75% 24/32 [00:07<00:03,  2.19it/s][A[A

 78% 25/32 [00:07<00:02,  2.63it/s][A[A

 81% 26/32 [00:07<00:01,  3.10it/s][A[A

 84% 27/32 [00:07<00:01,  3.50it/s][A[A

 88% 28/32 [00:08<00:01,  3.81it/s][A[A

 91% 29/32 [00:08<00:00,  4.00it/s][A[A

 94% 30/32 [00:08<00:00,  4.06it/s][A[A

 97% 31/32 [00:08<00:00,  4.29it/s][A[A

100% 32/32 [00:09<00:00,  4.22it/s][A[A100% 32/32 [00:09<00:00,  3.54it/s]
Meta loss on this task batch = 4.3023e-01, PNorm = 36.9941, GNorm = 0.0431

 89% 17/19 [02:57<00:21, 10.78s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:37,  1.21s/it][A[A

  6% 2/32 [00:01<00:27,  1.09it/s][A[A

  9% 3/32 [00:01<00:20,  1.41it/s][A[A

 12% 4/32 [00:01<00:15,  1.77it/s][A[A

 16% 5/32 [00:02<00:12,  2.13it/s][A[A

 19% 6/32 [00:02<00:10,  2.54it/s][A[A

 22% 7/32 [00:02<00:08,  2.82it/s][A[A

 25% 8/32 [00:02<00:07,  3.07it/s][A[A

 28% 9/32 [00:04<00:13,  1.68it/s][A[A

 31% 10/32 [00:04<00:10,  2.07it/s][A[A

 34% 11/32 [00:04<00:08,  2.47it/s][A[A

 38% 12/32 [00:04<00:06,  2.90it/s][A[A

 41% 13/32 [00:04<00:05,  3.17it/s][A[A

 44% 14/32 [00:05<00:05,  3.57it/s][A[A

 47% 15/32 [00:05<00:04,  3.67it/s][A[A

 50% 16/32 [00:05<00:04,  3.94it/s][A[A

 53% 17/32 [00:05<00:03,  4.22it/s][A[A

 56% 18/32 [00:06<00:03,  4.36it/s][A[A

 59% 19/32 [00:06<00:03,  4.32it/s][A[A

 62% 20/32 [00:06<00:02,  4.34it/s][A[A

 66% 21/32 [00:07<00:05,  1.97it/s][A[A

 69% 22/32 [00:07<00:04,  2.40it/s][A[A

 72% 23/32 [00:08<00:03,  2.68it/s][A[A

 75% 24/32 [00:08<00:02,  2.86it/s][A[A

 78% 25/32 [00:08<00:02,  3.19it/s][A[A

 81% 26/32 [00:08<00:01,  3.48it/s][A[A

 84% 27/32 [00:09<00:01,  3.56it/s][A[A

 88% 28/32 [00:09<00:01,  3.71it/s][A[A

 91% 29/32 [00:09<00:00,  3.86it/s][A[A

 94% 30/32 [00:10<00:01,  1.81it/s][A[A

 97% 31/32 [00:11<00:00,  2.22it/s][A[A

100% 32/32 [00:11<00:00,  2.52it/s][A[A100% 32/32 [00:11<00:00,  2.81it/s]
Meta loss on this task batch = 5.0826e-01, PNorm = 36.9960, GNorm = 0.0538

 95% 18/19 [03:09<00:11, 11.21s/it][A

  0% 0/23 [00:00<?, ?it/s][A[A

  4% 1/23 [00:00<00:05,  3.99it/s][A[A

  9% 2/23 [00:00<00:05,  4.17it/s][A[A

 13% 3/23 [00:00<00:04,  4.28it/s][A[A

 17% 4/23 [00:00<00:04,  4.45it/s][A[A

 22% 5/23 [00:01<00:03,  5.04it/s][A[A

 26% 6/23 [00:01<00:03,  4.93it/s][A[A

 30% 7/23 [00:01<00:03,  4.70it/s][A[A

 35% 8/23 [00:02<00:07,  1.92it/s][A[A

 39% 9/23 [00:02<00:05,  2.40it/s][A[A

 43% 10/23 [00:03<00:04,  2.69it/s][A[A

 48% 11/23 [00:03<00:03,  3.14it/s][A[A

 52% 12/23 [00:03<00:03,  3.43it/s][A[A

 57% 13/23 [00:03<00:02,  3.76it/s][A[A

 61% 14/23 [00:03<00:02,  4.15it/s][A[A

 65% 15/23 [00:04<00:02,  3.62it/s][A[A

 70% 16/23 [00:04<00:01,  3.66it/s][A[A

 74% 17/23 [00:04<00:01,  3.79it/s][A[A

 78% 18/23 [00:06<00:02,  1.76it/s][A[A

 83% 19/23 [00:06<00:01,  2.15it/s][A[A

 87% 20/23 [00:06<00:01,  2.63it/s][A[A

 91% 21/23 [00:06<00:00,  3.08it/s][A[A

 96% 22/23 [00:06<00:00,  3.54it/s][A[A

100% 23/23 [00:07<00:00,  3.78it/s][A[A100% 23/23 [00:07<00:00,  3.23it/s]
Meta loss on this task batch = 4.3509e-01, PNorm = 36.9978, GNorm = 0.0434

100% 19/19 [03:17<00:00, 10.16s/it][A100% 19/19 [03:17<00:00, 10.39s/it]
Took 197.3714518547058 seconds to complete one epoch of meta training

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/20 [00:00<?, ?it/s][A[A


  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 33.58it/s]


  5% 1/20 [00:00<00:03,  5.73it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 20.71it/s]


 10% 2/20 [00:00<00:03,  5.10it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 18.86it/s][A[A[A100% 3/3 [00:00<00:00, 20.84it/s]


 15% 3/20 [00:00<00:03,  4.51it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 15.99it/s]


 20% 4/20 [00:00<00:03,  4.55it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 25% 1/4 [00:01<00:03,  1.08s/it][A[A[A


 75% 3/4 [00:01<00:00,  1.29it/s][A[A[A100% 4/4 [00:01<00:00,  3.23it/s]


 25% 5/20 [00:02<00:08,  1.76it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.72it/s][A[A[A100% 4/4 [00:00<00:00, 16.43it/s]


 30% 6/20 [00:02<00:07,  1.94it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.73it/s][A[A[A100% 4/4 [00:00<00:00, 21.35it/s]


 35% 7/20 [00:03<00:06,  2.16it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.92it/s][A[A[A


100% 4/4 [00:00<00:00, 19.62it/s][A[A[A100% 4/4 [00:00<00:00, 19.41it/s]


 40% 8/20 [00:03<00:05,  2.31it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.98it/s][A[A[A100% 4/4 [00:00<00:00, 23.74it/s]


 45% 9/20 [00:03<00:04,  2.50it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 17.61it/s][A[A[A100% 4/4 [00:00<00:00, 19.77it/s]


 50% 10/20 [00:04<00:03,  2.60it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 16.90it/s][A[A[A100% 4/4 [00:00<00:00, 19.39it/s]


 55% 11/20 [00:04<00:03,  2.65it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:01<00:00,  2.59it/s][A[A[A100% 4/4 [00:01<00:00,  3.39it/s]


 60% 12/20 [00:05<00:05,  1.49it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.69it/s][A[A[A100% 3/3 [00:00<00:00, 14.00it/s]


 65% 13/20 [00:06<00:04,  1.70it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.30it/s][A[A[A100% 3/3 [00:00<00:00, 13.94it/s]


 70% 14/20 [00:06<00:03,  1.90it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 14.44it/s][A[A[A100% 4/4 [00:00<00:00, 18.78it/s]


 75% 15/20 [00:06<00:02,  2.16it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.71it/s][A[A[A100% 3/3 [00:00<00:00, 18.05it/s]


 80% 16/20 [00:07<00:01,  2.36it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 12.59it/s][A[A[A100% 3/3 [00:00<00:00, 15.36it/s]


 85% 17/20 [00:07<00:01,  2.42it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 22.86it/s]


 90% 18/20 [00:07<00:00,  2.78it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 16.28it/s][A[A[A100% 3/3 [00:00<00:00, 20.63it/s]


 95% 19/20 [00:08<00:00,  2.92it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 13.63it/s]


100% 20/20 [00:08<00:00,  3.20it/s][A[A100% 20/20 [00:08<00:00,  2.39it/s]

100% 1/1 [00:08<00:00,  8.38s/it][A100% 1/1 [00:08<00:00,  8.38s/it]
Took 205.75099420547485 seconds to complete one epoch of meta training and validating
Meta validation score auc = 0.618841
Found better MAML checkpoint after meta validation, saving now
 43% 13/30 [41:57<55:19, 195.25s/it]Epoch 13

  0% 0/19 [00:00<?, ?it/s][A

  0% 0/32 [00:00<?, ?it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  3% 1/32 [00:00<00:05,  6.10it/s][A[A

  6% 2/32 [00:00<00:05,  5.60it/s][A[A

  9% 3/32 [00:00<00:05,  5.61it/s][A[A

 12% 4/32 [00:00<00:04,  5.80it/s][A[A

 16% 5/32 [00:00<00:04,  6.13it/s][A[A

 19% 6/32 [00:01<00:04,  5.92it/s][A[A

 22% 7/32 [00:01<00:04,  5.95it/s][A[A

 25% 8/32 [00:01<00:04,  5.68it/s][A[A

 28% 9/32 [00:01<00:03,  6.25it/s][A[A

 31% 10/32 [00:01<00:03,  6.61it/s][A[A

 34% 11/32 [00:01<00:03,  5.96it/s][A[A

 38% 12/32 [00:02<00:03,  5.15it/s][A[A

 41% 13/32 [00:02<00:03,  4.94it/s][A[A

 44% 14/32 [00:03<00:09,  1.95it/s][A[A

 47% 15/32 [00:03<00:06,  2.51it/s][A[A

 50% 16/32 [00:03<00:05,  2.93it/s][A[A

 53% 17/32 [00:04<00:04,  3.37it/s][A[A

 56% 18/32 [00:04<00:03,  3.92it/s][A[A

 59% 19/32 [00:04<00:02,  4.47it/s][A[A

 62% 20/32 [00:04<00:02,  4.73it/s][A[A

 66% 21/32 [00:04<00:01,  5.55it/s][A[A

 69% 22/32 [00:04<00:01,  5.54it/s][A[A

 72% 23/32 [00:05<00:01,  5.88it/s][A[A

 75% 24/32 [00:05<00:01,  5.71it/s][A[A

 78% 25/32 [00:05<00:01,  5.55it/s][A[A

 81% 26/32 [00:05<00:01,  5.28it/s][A[A

 84% 27/32 [00:05<00:00,  5.61it/s][A[A

 88% 28/32 [00:05<00:00,  5.78it/s][A[A

 91% 29/32 [00:06<00:00,  5.41it/s][A[A

 94% 30/32 [00:06<00:00,  6.12it/s][A[A

 97% 31/32 [00:06<00:00,  6.04it/s][A[A

100% 32/32 [00:06<00:00,  6.03it/s][A[A100% 32/32 [00:06<00:00,  4.85it/s]
Meta loss on this task batch = 5.0651e-01, PNorm = 37.0025, GNorm = 0.1038

  5% 1/19 [00:07<02:11,  7.29s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:03,  8.01it/s][A[A

  6% 2/32 [00:00<00:03,  7.73it/s][A[A

  9% 3/32 [00:01<00:12,  2.26it/s][A[A

 12% 4/32 [00:01<00:10,  2.74it/s][A[A

 16% 5/32 [00:01<00:08,  3.36it/s][A[A

 19% 6/32 [00:01<00:06,  3.97it/s][A[A

 22% 7/32 [00:02<00:05,  4.68it/s][A[A

 25% 8/32 [00:02<00:04,  5.20it/s][A[A

 28% 9/32 [00:02<00:04,  5.49it/s][A[A

 31% 10/32 [00:02<00:03,  5.89it/s][A[A

 34% 11/32 [00:02<00:03,  6.22it/s][A[A

 38% 12/32 [00:02<00:03,  5.94it/s][A[A

 41% 13/32 [00:02<00:03,  6.05it/s][A[A

 44% 14/32 [00:03<00:03,  5.75it/s][A[A

 47% 15/32 [00:03<00:02,  5.86it/s][A[A

 50% 16/32 [00:03<00:02,  6.13it/s][A[A

 53% 17/32 [00:03<00:02,  5.84it/s][A[A

 56% 18/32 [00:03<00:02,  5.36it/s][A[A

 59% 19/32 [00:04<00:02,  5.77it/s][A[A

 62% 20/32 [00:04<00:02,  5.38it/s][A[A

 66% 21/32 [00:04<00:01,  5.77it/s][A[A

 69% 22/32 [00:05<00:04,  2.16it/s][A[A

 72% 23/32 [00:05<00:03,  2.71it/s][A[A

 75% 24/32 [00:05<00:02,  3.32it/s][A[A

 78% 25/32 [00:06<00:01,  3.69it/s][A[A

 81% 26/32 [00:06<00:01,  4.03it/s][A[A

 84% 27/32 [00:06<00:01,  4.48it/s][A[A

 88% 28/32 [00:06<00:00,  4.78it/s][A[A

 91% 29/32 [00:06<00:00,  5.42it/s][A[A

 94% 30/32 [00:06<00:00,  5.44it/s][A[A

 97% 31/32 [00:07<00:00,  5.71it/s][A[A

100% 32/32 [00:07<00:00,  6.12it/s][A[A100% 32/32 [00:07<00:00,  4.48it/s]
Meta loss on this task batch = 5.0037e-01, PNorm = 37.0086, GNorm = 0.0414

 11% 2/19 [00:15<02:06,  7.45s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.55it/s][A[A

  6% 2/32 [00:00<00:05,  5.88it/s][A[A

  9% 3/32 [00:00<00:04,  5.81it/s][A[A

 12% 4/32 [00:00<00:05,  5.55it/s][A[A

 16% 5/32 [00:00<00:04,  5.72it/s][A[A

 19% 6/32 [00:01<00:04,  5.57it/s][A[A

 22% 7/32 [00:01<00:04,  6.13it/s][A[A

 25% 8/32 [00:01<00:03,  6.34it/s][A[A

 28% 9/32 [00:01<00:03,  6.42it/s][A[A

 31% 10/32 [00:02<00:10,  2.09it/s][A[A

 34% 11/32 [00:02<00:08,  2.49it/s][A[A

 38% 12/32 [00:03<00:06,  2.98it/s][A[A

 41% 13/32 [00:03<00:05,  3.41it/s][A[A

 44% 14/32 [00:03<00:04,  3.68it/s][A[A

 47% 15/32 [00:03<00:03,  4.28it/s][A[A

 50% 16/32 [00:03<00:03,  4.89it/s][A[A

 53% 17/32 [00:04<00:03,  4.83it/s][A[A

 56% 18/32 [00:04<00:02,  5.25it/s][A[A

 59% 19/32 [00:04<00:02,  5.53it/s][A[A

 62% 20/32 [00:04<00:02,  5.97it/s][A[A

 66% 21/32 [00:04<00:01,  6.52it/s][A[A

 69% 22/32 [00:04<00:01,  6.67it/s][A[A

 72% 23/32 [00:04<00:01,  6.24it/s][A[A

 75% 24/32 [00:05<00:01,  5.57it/s][A[A

 78% 25/32 [00:05<00:01,  6.09it/s][A[A

 81% 26/32 [00:05<00:00,  6.61it/s][A[A

 84% 27/32 [00:05<00:00,  5.89it/s][A[A

 88% 28/32 [00:05<00:00,  5.94it/s][A[A

 91% 29/32 [00:06<00:01,  2.12it/s][A[A

 94% 30/32 [00:07<00:00,  2.49it/s][A[A

 97% 31/32 [00:07<00:00,  3.18it/s][A[A

100% 32/32 [00:07<00:00,  3.57it/s][A[A100% 32/32 [00:07<00:00,  4.27it/s]
Meta loss on this task batch = 5.0613e-01, PNorm = 37.0160, GNorm = 0.0334

 16% 3/19 [00:23<02:02,  7.68s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.87it/s][A[A

  6% 2/32 [00:00<00:06,  4.83it/s][A[A

  9% 3/32 [00:00<00:05,  5.01it/s][A[A

 12% 4/32 [00:00<00:05,  5.14it/s][A[A

 16% 5/32 [00:00<00:05,  5.03it/s][A[A

 19% 6/32 [00:01<00:04,  5.51it/s][A[A

 22% 7/32 [00:01<00:04,  6.06it/s][A[A

 25% 8/32 [00:01<00:04,  5.99it/s][A[A

 28% 9/32 [00:01<00:03,  6.32it/s][A[A

 31% 10/32 [00:01<00:03,  6.43it/s][A[A

 34% 11/32 [00:01<00:03,  6.59it/s][A[A

 38% 12/32 [00:01<00:02,  6.80it/s][A[A

 41% 13/32 [00:02<00:02,  6.33it/s][A[A

 44% 14/32 [00:02<00:02,  6.23it/s][A[A

 47% 15/32 [00:02<00:02,  6.48it/s][A[A

 50% 16/32 [00:02<00:02,  6.27it/s][A[A

 53% 17/32 [00:03<00:06,  2.20it/s][A[A

 56% 18/32 [00:03<00:05,  2.68it/s][A[A

 59% 19/32 [00:04<00:03,  3.31it/s][A[A

 62% 20/32 [00:04<00:03,  3.82it/s][A[A

 66% 21/32 [00:04<00:02,  4.36it/s][A[A

 69% 22/32 [00:04<00:02,  4.28it/s][A[A

 72% 23/32 [00:04<00:02,  4.33it/s][A[A

 75% 24/32 [00:05<00:01,  4.44it/s][A[A

 78% 25/32 [00:05<00:01,  5.13it/s][A[A

 81% 26/32 [00:05<00:01,  5.80it/s][A[A

 84% 27/32 [00:05<00:00,  5.86it/s][A[A

 88% 28/32 [00:05<00:00,  5.76it/s][A[A

 91% 29/32 [00:05<00:00,  6.24it/s][A[A

 94% 30/32 [00:06<00:00,  5.56it/s][A[A

 97% 31/32 [00:06<00:00,  5.98it/s][A[A

100% 32/32 [00:06<00:00,  5.98it/s][A[A100% 32/32 [00:06<00:00,  5.02it/s]
Meta loss on this task batch = 5.6491e-01, PNorm = 37.0257, GNorm = 0.0720

 21% 4/19 [00:30<01:52,  7.49s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.67it/s][A[A

  6% 2/32 [00:00<00:04,  6.22it/s][A[A

  9% 3/32 [00:01<00:13,  2.17it/s][A[A

 12% 4/32 [00:01<00:10,  2.62it/s][A[A

 16% 5/32 [00:01<00:08,  3.01it/s][A[A

 19% 6/32 [00:02<00:07,  3.42it/s][A[A

 22% 7/32 [00:02<00:06,  3.60it/s][A[A

 25% 8/32 [00:02<00:05,  4.19it/s][A[A

 28% 9/32 [00:02<00:05,  4.43it/s][A[A

 31% 10/32 [00:02<00:04,  4.86it/s][A[A

 34% 11/32 [00:02<00:04,  5.11it/s][A[A

 38% 12/32 [00:03<00:03,  5.14it/s][A[A

 41% 13/32 [00:03<00:03,  4.86it/s][A[A

 44% 14/32 [00:03<00:03,  5.35it/s][A[A

 47% 15/32 [00:03<00:03,  5.04it/s][A[A

 50% 16/32 [00:03<00:02,  5.80it/s][A[A

 53% 17/32 [00:05<00:07,  2.00it/s][A[A

 56% 18/32 [00:05<00:05,  2.44it/s][A[A

 59% 19/32 [00:05<00:04,  2.87it/s][A[A

 62% 20/32 [00:05<00:03,  3.11it/s][A[A

 66% 21/32 [00:06<00:03,  3.56it/s][A[A

 69% 22/32 [00:06<00:02,  4.29it/s][A[A

 72% 23/32 [00:06<00:02,  4.34it/s][A[A

 75% 24/32 [00:06<00:01,  4.20it/s][A[A

 78% 25/32 [00:06<00:01,  4.27it/s][A[A

 81% 26/32 [00:07<00:03,  1.97it/s][A[A

 84% 27/32 [00:08<00:02,  2.44it/s][A[A

 88% 28/32 [00:08<00:01,  3.01it/s][A[A

 91% 29/32 [00:08<00:00,  3.49it/s][A[A

 94% 30/32 [00:08<00:00,  4.03it/s][A[A

 97% 31/32 [00:08<00:00,  4.44it/s][A[A

100% 32/32 [00:09<00:00,  4.59it/s][A[A100% 32/32 [00:09<00:00,  3.54it/s]
Meta loss on this task batch = 4.8620e-01, PNorm = 37.0367, GNorm = 0.0377

 26% 5/19 [00:40<01:54,  8.17s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  6.37it/s][A[A

  6% 2/32 [00:00<00:04,  6.14it/s][A[A

  9% 3/32 [00:00<00:04,  6.26it/s][A[A

 12% 4/32 [00:00<00:04,  6.18it/s][A[A

 16% 5/32 [00:00<00:05,  5.28it/s][A[A

 19% 6/32 [00:01<00:05,  4.97it/s][A[A

 22% 7/32 [00:01<00:04,  5.17it/s][A[A

 25% 8/32 [00:02<00:11,  2.00it/s][A[A

 28% 9/32 [00:02<00:09,  2.33it/s][A[A

 31% 10/32 [00:02<00:07,  2.79it/s][A[A

 34% 11/32 [00:03<00:06,  3.23it/s][A[A

 38% 12/32 [00:03<00:05,  3.67it/s][A[A

 41% 13/32 [00:03<00:04,  4.27it/s][A[A

 44% 14/32 [00:03<00:04,  4.23it/s][A[A

 47% 15/32 [00:03<00:03,  4.27it/s][A[A

 50% 16/32 [00:04<00:03,  4.26it/s][A[A

 53% 17/32 [00:04<00:03,  4.82it/s][A[A

 56% 18/32 [00:04<00:02,  4.97it/s][A[A

 59% 19/32 [00:04<00:02,  5.04it/s][A[A

 62% 20/32 [00:04<00:02,  5.04it/s][A[A

 66% 21/32 [00:06<00:05,  2.00it/s][A[A

 69% 22/32 [00:06<00:04,  2.40it/s][A[A

 72% 23/32 [00:06<00:03,  2.90it/s][A[A

 75% 24/32 [00:06<00:02,  3.27it/s][A[A

 78% 25/32 [00:06<00:01,  3.54it/s][A[A

 81% 26/32 [00:07<00:01,  4.15it/s][A[A

 84% 27/32 [00:07<00:01,  4.13it/s][A[A

 88% 28/32 [00:07<00:00,  4.13it/s][A[A

 91% 29/32 [00:07<00:00,  4.18it/s][A[A

 94% 30/32 [00:08<00:00,  4.39it/s][A[A

 97% 31/32 [00:08<00:00,  4.89it/s][A[A

100% 32/32 [00:08<00:00,  4.86it/s][A[A100% 32/32 [00:08<00:00,  3.81it/s]
Meta loss on this task batch = 4.4639e-01, PNorm = 37.0473, GNorm = 0.0445

 32% 6/19 [00:49<01:50,  8.47s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.87it/s][A[A

  6% 2/32 [00:01<00:14,  2.04it/s][A[A

  9% 3/32 [00:01<00:11,  2.42it/s][A[A

 12% 4/32 [00:01<00:10,  2.75it/s][A[A

 16% 5/32 [00:01<00:07,  3.38it/s][A[A

 19% 6/32 [00:02<00:06,  3.72it/s][A[A

 22% 7/32 [00:02<00:06,  4.04it/s][A[A

 25% 8/32 [00:02<00:05,  4.16it/s][A[A

 28% 9/32 [00:02<00:05,  4.52it/s][A[A

 31% 10/32 [00:02<00:04,  4.55it/s][A[A

 34% 11/32 [00:03<00:04,  4.85it/s][A[A

 38% 12/32 [00:03<00:04,  4.71it/s][A[A

 41% 13/32 [00:03<00:03,  4.89it/s][A[A

 44% 14/32 [00:03<00:03,  5.26it/s][A[A

 47% 15/32 [00:05<00:08,  1.95it/s][A[A

 50% 16/32 [00:05<00:06,  2.32it/s][A[A

 53% 17/32 [00:05<00:05,  2.79it/s][A[A

 56% 18/32 [00:05<00:04,  3.27it/s][A[A

 59% 19/32 [00:05<00:03,  3.76it/s][A[A

 62% 20/32 [00:06<00:03,  3.85it/s][A[A

 66% 21/32 [00:06<00:02,  4.07it/s][A[A

 69% 22/32 [00:06<00:02,  4.20it/s][A[A

 72% 23/32 [00:06<00:02,  4.23it/s][A[A

 75% 24/32 [00:06<00:01,  4.25it/s][A[A

 78% 25/32 [00:07<00:01,  4.41it/s][A[A

 81% 26/32 [00:07<00:01,  4.68it/s][A[A

 84% 27/32 [00:07<00:01,  4.96it/s][A[A

 88% 28/32 [00:07<00:00,  4.67it/s][A[A

 91% 29/32 [00:08<00:01,  1.99it/s][A[A

 94% 30/32 [00:09<00:00,  2.42it/s][A[A

 97% 31/32 [00:09<00:00,  2.98it/s][A[A

100% 32/32 [00:09<00:00,  3.42it/s][A[A100% 32/32 [00:09<00:00,  3.38it/s]
Meta loss on this task batch = 4.6904e-01, PNorm = 37.0603, GNorm = 0.0690

 37% 7/19 [00:59<01:47,  9.00s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.79it/s][A[A

  6% 2/32 [00:00<00:06,  4.81it/s][A[A

  9% 3/32 [00:00<00:06,  4.64it/s][A[A

 12% 4/32 [00:00<00:06,  4.46it/s][A[A

 16% 5/32 [00:01<00:05,  4.59it/s][A[A

 19% 6/32 [00:01<00:05,  4.99it/s][A[A

 22% 7/32 [00:01<00:04,  5.32it/s][A[A

 25% 8/32 [00:01<00:04,  5.01it/s][A[A

 28% 9/32 [00:01<00:04,  4.95it/s][A[A

 31% 10/32 [00:02<00:04,  5.33it/s][A[A

 34% 11/32 [00:02<00:04,  5.20it/s][A[A

 38% 12/32 [00:03<00:09,  2.00it/s][A[A

 41% 13/32 [00:03<00:07,  2.49it/s][A[A

 44% 14/32 [00:03<00:05,  3.03it/s][A[A

 47% 15/32 [00:03<00:05,  3.38it/s][A[A

 50% 16/32 [00:04<00:04,  3.64it/s][A[A

 53% 17/32 [00:04<00:03,  3.92it/s][A[A

 56% 18/32 [00:04<00:03,  4.29it/s][A[A

 59% 19/32 [00:04<00:02,  4.42it/s][A[A

 62% 20/32 [00:04<00:02,  4.66it/s][A[A

 66% 21/32 [00:05<00:02,  4.91it/s][A[A

 69% 22/32 [00:05<00:02,  4.65it/s][A[A

 72% 23/32 [00:05<00:01,  5.02it/s][A[A

 75% 24/32 [00:05<00:01,  4.81it/s][A[A

 78% 25/32 [00:07<00:03,  1.91it/s][A[A

 81% 26/32 [00:07<00:02,  2.24it/s][A[A

 84% 27/32 [00:07<00:01,  2.69it/s][A[A

 88% 28/32 [00:07<00:01,  3.17it/s][A[A

 91% 29/32 [00:07<00:00,  3.62it/s][A[A

 94% 30/32 [00:08<00:00,  4.08it/s][A[A

 97% 31/32 [00:08<00:00,  4.53it/s][A[A

100% 32/32 [00:08<00:00,  4.80it/s][A[A100% 32/32 [00:08<00:00,  3.81it/s]
Meta loss on this task batch = 3.6955e-01, PNorm = 37.0741, GNorm = 0.0319

 42% 8/19 [01:08<01:39,  9.05s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.90it/s][A[A

  6% 2/32 [00:00<00:05,  5.18it/s][A[A

  9% 3/32 [00:00<00:05,  4.96it/s][A[A

 12% 4/32 [00:00<00:05,  5.09it/s][A[A

 16% 5/32 [00:00<00:05,  5.17it/s][A[A

 19% 6/32 [00:01<00:04,  5.31it/s][A[A

 22% 7/32 [00:02<00:12,  2.06it/s][A[A

 25% 8/32 [00:02<00:09,  2.53it/s][A[A

 28% 9/32 [00:02<00:07,  3.06it/s][A[A

 31% 10/32 [00:02<00:06,  3.53it/s][A[A

 34% 11/32 [00:03<00:05,  3.94it/s][A[A

 38% 12/32 [00:03<00:04,  4.30it/s][A[A

 41% 13/32 [00:03<00:04,  4.59it/s][A[A

 44% 14/32 [00:03<00:03,  4.78it/s][A[A

 47% 15/32 [00:03<00:03,  5.04it/s][A[A

 50% 16/32 [00:03<00:03,  5.23it/s][A[A

 53% 17/32 [00:04<00:02,  5.38it/s][A[A

 56% 18/32 [00:04<00:02,  5.62it/s][A[A

 59% 19/32 [00:04<00:02,  5.46it/s][A[A

 62% 20/32 [00:04<00:02,  5.49it/s][A[A

 66% 21/32 [00:04<00:01,  5.56it/s][A[A

 69% 22/32 [00:05<00:01,  5.55it/s][A[A

 72% 23/32 [00:05<00:01,  5.68it/s][A[A

 75% 24/32 [00:05<00:01,  5.70it/s][A[A

 78% 25/32 [00:05<00:01,  5.74it/s][A[A

 81% 26/32 [00:05<00:01,  5.70it/s][A[A

 84% 27/32 [00:05<00:00,  5.74it/s][A[A

 88% 28/32 [00:06<00:00,  5.69it/s][A[A

 91% 29/32 [00:06<00:00,  5.58it/s][A[A

 94% 30/32 [00:06<00:00,  5.53it/s][A[A

 97% 31/32 [00:06<00:00,  5.63it/s][A[A

100% 32/32 [00:06<00:00,  5.62it/s][A[A100% 32/32 [00:06<00:00,  4.73it/s]
Meta loss on this task batch = 2.1200e-01, PNorm = 37.0892, GNorm = 0.0426

 47% 9/19 [01:16<01:25,  8.56s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:37,  1.20s/it][A[A

  6% 2/32 [00:01<00:26,  1.12it/s][A[A

  9% 3/32 [00:01<00:19,  1.48it/s][A[A

 12% 4/32 [00:01<00:14,  1.89it/s][A[A

 16% 5/32 [00:01<00:11,  2.39it/s][A[A

 19% 6/32 [00:02<00:08,  2.93it/s][A[A

 22% 7/32 [00:02<00:07,  3.44it/s][A[A

 25% 8/32 [00:02<00:06,  3.87it/s][A[A

 28% 9/32 [00:02<00:05,  4.22it/s][A[A

 31% 10/32 [00:02<00:04,  4.56it/s][A[A

 34% 11/32 [00:02<00:04,  4.75it/s][A[A

 38% 12/32 [00:03<00:04,  4.95it/s][A[A

 41% 13/32 [00:03<00:03,  5.05it/s][A[A

 44% 14/32 [00:03<00:03,  5.32it/s][A[A

 47% 15/32 [00:03<00:03,  5.52it/s][A[A

 50% 16/32 [00:03<00:02,  5.92it/s][A[A

 53% 17/32 [00:03<00:02,  5.67it/s][A[A

 56% 18/32 [00:04<00:02,  5.43it/s][A[A

 59% 19/32 [00:05<00:06,  2.02it/s][A[A

 62% 20/32 [00:05<00:04,  2.51it/s][A[A

 66% 21/32 [00:05<00:03,  2.89it/s][A[A

 69% 22/32 [00:05<00:02,  3.42it/s][A[A

 72% 23/32 [00:06<00:02,  3.92it/s][A[A

 75% 24/32 [00:06<00:01,  4.35it/s][A[A

 78% 25/32 [00:06<00:01,  4.71it/s][A[A

 81% 26/32 [00:06<00:01,  4.44it/s][A[A

 84% 27/32 [00:06<00:01,  4.69it/s][A[A

 88% 28/32 [00:07<00:00,  4.45it/s][A[A

 91% 29/32 [00:07<00:00,  4.38it/s][A[A

 94% 30/32 [00:07<00:00,  4.25it/s][A[A

 97% 31/32 [00:08<00:00,  1.85it/s][A[A

100% 32/32 [00:09<00:00,  2.28it/s][A[A100% 32/32 [00:09<00:00,  3.50it/s]
Meta loss on this task batch = 2.6687e-01, PNorm = 37.1060, GNorm = 0.0430

 53% 10/19 [01:25<01:20,  8.95s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.79it/s][A[A

  6% 2/32 [00:00<00:06,  4.56it/s][A[A

  9% 3/32 [00:00<00:06,  4.76it/s][A[A

 12% 4/32 [00:00<00:06,  4.53it/s][A[A

 16% 5/32 [00:01<00:06,  4.48it/s][A[A

 19% 6/32 [00:01<00:05,  4.69it/s][A[A

 22% 7/32 [00:01<00:05,  4.50it/s][A[A

 25% 8/32 [00:01<00:05,  4.39it/s][A[A

 28% 9/32 [00:01<00:04,  4.74it/s][A[A

 31% 10/32 [00:02<00:04,  4.55it/s][A[A

 34% 11/32 [00:02<00:04,  4.47it/s][A[A

 38% 12/32 [00:02<00:04,  4.28it/s][A[A

 41% 13/32 [00:02<00:04,  4.16it/s][A[A

 44% 14/32 [00:04<00:09,  1.82it/s][A[A

 47% 15/32 [00:04<00:07,  2.18it/s][A[A

 50% 16/32 [00:04<00:05,  2.70it/s][A[A

 53% 17/32 [00:04<00:04,  3.13it/s][A[A

 56% 18/32 [00:05<00:04,  3.38it/s][A[A

 59% 19/32 [00:05<00:03,  3.55it/s][A[A

 62% 20/32 [00:05<00:03,  3.97it/s][A[A

 66% 21/32 [00:05<00:02,  3.95it/s][A[A

 69% 22/32 [00:07<00:05,  1.81it/s][A[A

 72% 23/32 [00:07<00:04,  2.17it/s][A[A

 75% 24/32 [00:07<00:03,  2.54it/s][A[A

 78% 25/32 [00:07<00:02,  2.99it/s][A[A

 81% 26/32 [00:07<00:01,  3.41it/s][A[A

 84% 27/32 [00:08<00:01,  3.73it/s][A[A

 88% 28/32 [00:08<00:01,  3.81it/s][A[A

 91% 29/32 [00:08<00:00,  3.88it/s][A[A

 94% 30/32 [00:08<00:00,  3.96it/s][A[A

 97% 31/32 [00:09<00:00,  4.28it/s][A[A

100% 32/32 [00:09<00:00,  4.27it/s][A[A100% 32/32 [00:09<00:00,  3.46it/s]
Meta loss on this task batch = 5.8290e-01, PNorm = 37.1213, GNorm = 0.0489

 58% 11/19 [01:36<01:14,  9.28s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  3.90it/s][A[A

  6% 2/32 [00:00<00:07,  3.98it/s][A[A

  9% 3/32 [00:00<00:07,  4.01it/s][A[A

 12% 4/32 [00:00<00:06,  4.01it/s][A[A

 16% 5/32 [00:01<00:06,  4.02it/s][A[A

 19% 6/32 [00:01<00:06,  4.06it/s][A[A

 22% 7/32 [00:01<00:06,  4.05it/s][A[A

 25% 8/32 [00:01<00:05,  4.07it/s][A[A

 28% 9/32 [00:02<00:05,  4.13it/s][A[A

 31% 10/32 [00:02<00:05,  4.13it/s][A[A

 34% 11/32 [00:03<00:11,  1.83it/s][A[A

 38% 12/32 [00:03<00:09,  2.19it/s][A[A

 41% 13/32 [00:04<00:07,  2.53it/s][A[A

 44% 14/32 [00:04<00:06,  2.87it/s][A[A

 47% 15/32 [00:04<00:05,  3.16it/s][A[A

 50% 16/32 [00:04<00:04,  3.39it/s][A[A

 53% 17/32 [00:05<00:04,  3.58it/s][A[A

 56% 18/32 [00:05<00:03,  3.74it/s][A[A

 59% 19/32 [00:05<00:03,  3.80it/s][A[A

 62% 20/32 [00:05<00:02,  4.12it/s][A[A

 66% 21/32 [00:06<00:02,  4.01it/s][A[A

 69% 22/32 [00:06<00:02,  3.98it/s][A[A

 72% 23/32 [00:07<00:04,  1.82it/s][A[A

 75% 24/32 [00:07<00:03,  2.31it/s][A[A

 78% 25/32 [00:08<00:02,  2.64it/s][A[A

 81% 26/32 [00:08<00:01,  3.09it/s][A[A

 84% 27/32 [00:08<00:01,  3.35it/s][A[A

 88% 28/32 [00:08<00:01,  3.51it/s][A[A

 91% 29/32 [00:08<00:00,  3.63it/s][A[A

 94% 30/32 [00:09<00:00,  3.66it/s][A[A

 97% 31/32 [00:10<00:00,  1.76it/s][A[A

100% 32/32 [00:10<00:00,  2.11it/s][A[A100% 32/32 [00:10<00:00,  2.97it/s]
Meta loss on this task batch = 5.5579e-01, PNorm = 37.1366, GNorm = 0.0286

 63% 12/19 [01:47<01:09,  9.97s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.86it/s][A[A

  6% 2/32 [00:00<00:07,  3.92it/s][A[A

  9% 3/32 [00:00<00:07,  3.94it/s][A[A

 12% 4/32 [00:01<00:07,  3.95it/s][A[A

 16% 5/32 [00:01<00:06,  4.22it/s][A[A

 19% 6/32 [00:01<00:06,  4.11it/s][A[A

 22% 7/32 [00:01<00:05,  4.35it/s][A[A

 25% 8/32 [00:02<00:12,  1.93it/s][A[A

 28% 9/32 [00:03<00:09,  2.40it/s][A[A

 31% 10/32 [00:03<00:08,  2.72it/s][A[A

 34% 11/32 [00:03<00:06,  3.02it/s][A[A

 38% 12/32 [00:03<00:06,  3.25it/s][A[A

 41% 13/32 [00:04<00:05,  3.47it/s][A[A

 44% 14/32 [00:04<00:04,  3.65it/s][A[A

 47% 15/32 [00:04<00:04,  3.75it/s][A[A

 50% 16/32 [00:04<00:04,  3.77it/s][A[A

 53% 17/32 [00:04<00:03,  4.02it/s][A[A

 56% 18/32 [00:05<00:03,  3.99it/s][A[A

 59% 19/32 [00:06<00:07,  1.80it/s][A[A

 62% 20/32 [00:06<00:05,  2.14it/s][A[A

 66% 21/32 [00:07<00:04,  2.49it/s][A[A

 69% 22/32 [00:07<00:03,  2.79it/s][A[A

 72% 23/32 [00:07<00:02,  3.07it/s][A[A

 75% 24/32 [00:07<00:02,  3.25it/s][A[A

 78% 25/32 [00:08<00:01,  3.54it/s][A[A

 81% 26/32 [00:09<00:03,  1.75it/s][A[A

 84% 27/32 [00:09<00:02,  2.18it/s][A[A

 88% 28/32 [00:09<00:01,  2.50it/s][A[A

 91% 29/32 [00:09<00:01,  2.81it/s][A[A

 94% 30/32 [00:10<00:00,  3.10it/s][A[A

 97% 31/32 [00:10<00:00,  3.66it/s][A[A

100% 32/32 [00:10<00:00,  3.73it/s][A[A100% 32/32 [00:10<00:00,  3.01it/s]
Meta loss on this task batch = 5.9632e-01, PNorm = 37.1502, GNorm = 0.0773

 68% 13/19 [01:59<01:02, 10.42s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.93it/s][A[A

  6% 2/32 [00:00<00:06,  4.99it/s][A[A

  9% 3/32 [00:00<00:06,  4.64it/s][A[A

 12% 4/32 [00:00<00:06,  4.53it/s][A[A

 16% 5/32 [00:01<00:06,  4.41it/s][A[A

 19% 6/32 [00:01<00:05,  4.66it/s][A[A

 22% 7/32 [00:01<00:05,  4.37it/s][A[A

 25% 8/32 [00:01<00:04,  4.82it/s][A[A

 28% 9/32 [00:01<00:05,  4.46it/s][A[A

 31% 10/32 [00:03<00:11,  1.87it/s][A[A

 34% 11/32 [00:03<00:09,  2.24it/s][A[A

 38% 12/32 [00:03<00:07,  2.72it/s][A[A

 41% 13/32 [00:03<00:06,  2.99it/s][A[A

 44% 14/32 [00:04<00:05,  3.21it/s][A[A

 47% 15/32 [00:04<00:05,  3.38it/s][A[A

 50% 16/32 [00:04<00:04,  3.56it/s][A[A

 53% 17/32 [00:04<00:04,  3.68it/s][A[A

 56% 18/32 [00:06<00:07,  1.82it/s][A[A

 59% 19/32 [00:06<00:05,  2.28it/s][A[A

 62% 20/32 [00:06<00:04,  2.58it/s][A[A

 66% 21/32 [00:06<00:03,  2.88it/s][A[A

 69% 22/32 [00:07<00:03,  3.10it/s][A[A

 72% 23/32 [00:07<00:02,  3.29it/s][A[A

 75% 24/32 [00:07<00:02,  3.92it/s][A[A

 78% 25/32 [00:07<00:01,  3.94it/s][A[A

 81% 26/32 [00:08<00:01,  3.98it/s][A[A

 84% 27/32 [00:08<00:01,  3.99it/s][A[A

 88% 28/32 [00:09<00:02,  1.81it/s][A[A

 91% 29/32 [00:09<00:01,  2.16it/s][A[A

 94% 30/32 [00:09<00:00,  2.59it/s][A[A

 97% 31/32 [00:10<00:00,  3.01it/s][A[A

100% 32/32 [00:10<00:00,  3.43it/s][A[A100% 32/32 [00:10<00:00,  3.08it/s]
Meta loss on this task batch = 5.9459e-01, PNorm = 37.1607, GNorm = 0.1725

 74% 14/19 [02:10<00:53, 10.65s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.72it/s][A[A

  6% 2/32 [00:00<00:06,  4.73it/s][A[A

  9% 3/32 [00:00<00:06,  4.48it/s][A[A

 12% 4/32 [00:00<00:05,  4.77it/s][A[A

 16% 5/32 [00:01<00:05,  4.92it/s][A[A

 19% 6/32 [00:02<00:13,  1.89it/s][A[A

 22% 7/32 [00:02<00:11,  2.23it/s][A[A

 25% 8/32 [00:02<00:08,  2.70it/s][A[A

 28% 9/32 [00:03<00:07,  3.04it/s][A[A

 31% 10/32 [00:03<00:06,  3.51it/s][A[A

 34% 11/32 [00:03<00:05,  3.70it/s][A[A

 38% 12/32 [00:03<00:04,  4.24it/s][A[A

 41% 13/32 [00:03<00:04,  4.62it/s][A[A

 44% 14/32 [00:03<00:04,  4.48it/s][A[A

 47% 15/32 [00:04<00:03,  4.65it/s][A[A

 50% 16/32 [00:04<00:03,  4.60it/s][A[A

 53% 17/32 [00:04<00:03,  4.58it/s][A[A

 56% 18/32 [00:04<00:02,  4.72it/s][A[A

 59% 19/32 [00:05<00:02,  5.03it/s][A[A

 62% 20/32 [00:05<00:02,  5.22it/s][A[A

 66% 21/32 [00:06<00:05,  2.04it/s][A[A

 69% 22/32 [00:06<00:03,  2.50it/s][A[A

 72% 23/32 [00:06<00:03,  2.80it/s][A[A

 75% 24/32 [00:07<00:02,  3.19it/s][A[A

 78% 25/32 [00:07<00:01,  3.61it/s][A[A

 81% 26/32 [00:07<00:01,  3.64it/s][A[A

 84% 27/32 [00:07<00:01,  4.13it/s][A[A

 88% 28/32 [00:07<00:00,  4.06it/s][A[A

 91% 29/32 [00:08<00:00,  4.08it/s][A[A

 94% 30/32 [00:08<00:00,  4.08it/s][A[A

 97% 31/32 [00:08<00:00,  4.35it/s][A[A

100% 32/32 [00:09<00:00,  1.84it/s][A[A100% 32/32 [00:09<00:00,  3.25it/s]
Meta loss on this task batch = 4.9523e-01, PNorm = 37.1699, GNorm = 0.0618

 79% 15/19 [02:20<00:42, 10.63s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.86it/s][A[A

  6% 2/32 [00:00<00:07,  4.19it/s][A[A

  9% 3/32 [00:00<00:06,  4.43it/s][A[A

 12% 4/32 [00:00<00:06,  4.59it/s][A[A

 16% 5/32 [00:01<00:05,  4.83it/s][A[A

 19% 6/32 [00:01<00:05,  4.77it/s][A[A

 22% 7/32 [00:01<00:05,  4.80it/s][A[A

 25% 8/32 [00:01<00:04,  4.83it/s][A[A

 28% 9/32 [00:01<00:04,  4.88it/s][A[A

 31% 10/32 [00:03<00:11,  1.92it/s][A[A

 34% 11/32 [00:03<00:09,  2.26it/s][A[A

 38% 12/32 [00:03<00:07,  2.69it/s][A[A

 41% 13/32 [00:03<00:06,  3.15it/s][A[A

 44% 14/32 [00:03<00:04,  3.61it/s][A[A

 47% 15/32 [00:04<00:04,  3.82it/s][A[A

 50% 16/32 [00:04<00:03,  4.29it/s][A[A

 53% 17/32 [00:04<00:03,  4.20it/s][A[A

 56% 18/32 [00:04<00:03,  4.62it/s][A[A

 59% 19/32 [00:04<00:02,  4.70it/s][A[A

 62% 20/32 [00:05<00:02,  4.72it/s][A[A

 66% 21/32 [00:05<00:02,  4.82it/s][A[A

 69% 22/32 [00:05<00:02,  4.82it/s][A[A

 72% 23/32 [00:06<00:04,  1.91it/s][A[A

 75% 24/32 [00:07<00:03,  2.25it/s][A[A

 78% 25/32 [00:07<00:02,  2.75it/s][A[A

 81% 26/32 [00:07<00:01,  3.14it/s][A[A

 84% 27/32 [00:07<00:01,  3.36it/s][A[A

 88% 28/32 [00:07<00:01,  3.66it/s][A[A

 91% 29/32 [00:08<00:00,  3.91it/s][A[A

 94% 30/32 [00:08<00:00,  3.95it/s][A[A

 97% 31/32 [00:08<00:00,  4.03it/s][A[A

100% 32/32 [00:08<00:00,  4.23it/s][A[A100% 32/32 [00:08<00:00,  3.61it/s]
Meta loss on this task batch = 5.5179e-01, PNorm = 37.1807, GNorm = 0.0603

 84% 16/19 [02:30<00:30, 10.33s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.66it/s][A[A

  6% 2/32 [00:01<00:16,  1.77it/s][A[A

  9% 3/32 [00:01<00:13,  2.11it/s][A[A

 12% 4/32 [00:01<00:11,  2.53it/s][A[A

 16% 5/32 [00:02<00:09,  2.97it/s][A[A

 19% 6/32 [00:02<00:07,  3.59it/s][A[A

 22% 7/32 [00:02<00:06,  4.08it/s][A[A

 25% 8/32 [00:02<00:05,  4.07it/s][A[A

 28% 9/32 [00:02<00:05,  4.22it/s][A[A

 31% 10/32 [00:03<00:04,  4.85it/s][A[A

 34% 11/32 [00:03<00:04,  4.89it/s][A[A

 38% 12/32 [00:03<00:03,  5.07it/s][A[A

 41% 13/32 [00:03<00:03,  5.31it/s][A[A

 44% 14/32 [00:03<00:03,  5.34it/s][A[A

 47% 15/32 [00:04<00:03,  5.17it/s][A[A

 50% 16/32 [00:04<00:03,  4.96it/s][A[A

 53% 17/32 [00:04<00:02,  5.06it/s][A[A

 56% 18/32 [00:05<00:06,  2.02it/s][A[A

 59% 19/32 [00:05<00:05,  2.50it/s][A[A

 62% 20/32 [00:06<00:04,  2.86it/s][A[A

 66% 21/32 [00:06<00:03,  3.42it/s][A[A

 69% 22/32 [00:06<00:02,  3.51it/s][A[A

 72% 23/32 [00:06<00:02,  3.81it/s][A[A

 75% 24/32 [00:06<00:02,  3.76it/s][A[A

 78% 25/32 [00:07<00:01,  4.07it/s][A[A

 81% 26/32 [00:07<00:01,  3.99it/s][A[A

 84% 27/32 [00:07<00:01,  4.35it/s][A[A

 88% 28/32 [00:07<00:00,  4.59it/s][A[A

 91% 29/32 [00:09<00:01,  1.91it/s][A[A

 94% 30/32 [00:09<00:00,  2.24it/s][A[A

 97% 31/32 [00:09<00:00,  2.69it/s][A[A

100% 32/32 [00:09<00:00,  3.11it/s][A[A100% 32/32 [00:09<00:00,  3.30it/s]
Meta loss on this task batch = 4.4940e-01, PNorm = 37.1913, GNorm = 0.0432

 89% 17/19 [02:40<00:20, 10.37s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.09it/s][A[A

  6% 2/32 [00:00<00:06,  4.97it/s][A[A

  9% 3/32 [00:00<00:06,  4.81it/s][A[A

 12% 4/32 [00:00<00:05,  4.80it/s][A[A

 16% 5/32 [00:01<00:05,  5.10it/s][A[A

 19% 6/32 [00:01<00:04,  5.31it/s][A[A

 22% 7/32 [00:01<00:05,  4.84it/s][A[A

 25% 8/32 [00:01<00:05,  4.54it/s][A[A

 28% 9/32 [00:01<00:04,  4.75it/s][A[A

 31% 10/32 [00:02<00:04,  4.57it/s][A[A

 34% 11/32 [00:03<00:10,  1.96it/s][A[A

 38% 12/32 [00:03<00:08,  2.41it/s][A[A

 41% 13/32 [00:03<00:07,  2.65it/s][A[A

 44% 14/32 [00:03<00:05,  3.23it/s][A[A

 47% 15/32 [00:04<00:04,  3.42it/s][A[A

 50% 16/32 [00:04<00:04,  3.79it/s][A[A

 53% 17/32 [00:04<00:03,  4.42it/s][A[A

 56% 18/32 [00:04<00:03,  4.60it/s][A[A

 59% 19/32 [00:04<00:02,  4.55it/s][A[A

 62% 20/32 [00:05<00:02,  4.47it/s][A[A

 66% 21/32 [00:05<00:02,  4.36it/s][A[A

 69% 22/32 [00:05<00:02,  4.92it/s][A[A

 72% 23/32 [00:06<00:04,  1.90it/s][A[A

 75% 24/32 [00:07<00:03,  2.19it/s][A[A

 78% 25/32 [00:07<00:02,  2.69it/s][A[A

 81% 26/32 [00:07<00:01,  3.06it/s][A[A

 84% 27/32 [00:07<00:01,  3.17it/s][A[A

 88% 28/32 [00:08<00:01,  3.41it/s][A[A

 91% 29/32 [00:08<00:00,  3.89it/s][A[A

 94% 30/32 [00:08<00:00,  4.27it/s][A[A

 97% 31/32 [00:08<00:00,  4.59it/s][A[A

100% 32/32 [00:08<00:00,  4.93it/s][A[A100% 32/32 [00:08<00:00,  3.65it/s]
Meta loss on this task batch = 5.3378e-01, PNorm = 37.1987, GNorm = 0.1268

 95% 18/19 [02:50<00:10, 10.11s/it][A

  0% 0/23 [00:00<?, ?it/s][A[A

  4% 1/23 [00:00<00:05,  3.92it/s][A[A

  9% 2/23 [00:00<00:04,  4.30it/s][A[A

 13% 3/23 [00:01<00:10,  1.89it/s][A[A

 17% 4/23 [00:01<00:08,  2.36it/s][A[A

 22% 5/23 [00:01<00:06,  2.95it/s][A[A

 26% 6/23 [00:02<00:05,  3.30it/s][A[A

 30% 7/23 [00:02<00:04,  3.72it/s][A[A

 35% 8/23 [00:02<00:03,  3.87it/s][A[A

 39% 9/23 [00:02<00:03,  4.52it/s][A[A

 43% 10/23 [00:03<00:03,  4.29it/s][A[A

 48% 11/23 [00:03<00:02,  5.05it/s][A[A

 52% 12/23 [00:03<00:02,  4.76it/s][A[A

 57% 13/23 [00:03<00:01,  5.27it/s][A[A

 61% 14/23 [00:03<00:01,  5.31it/s][A[A

 65% 15/23 [00:04<00:02,  3.93it/s][A[A

 70% 16/23 [00:05<00:03,  1.79it/s][A[A

 74% 17/23 [00:05<00:02,  2.26it/s][A[A

 78% 18/23 [00:05<00:01,  2.54it/s][A[A

 83% 19/23 [00:06<00:01,  2.85it/s][A[A

 87% 20/23 [00:06<00:00,  3.47it/s][A[A

 91% 21/23 [00:06<00:00,  3.90it/s][A[A

 96% 22/23 [00:06<00:00,  4.50it/s][A[A

100% 23/23 [00:06<00:00,  4.92it/s][A[A100% 23/23 [00:06<00:00,  3.43it/s]
Meta loss on this task batch = 4.3362e-01, PNorm = 37.2073, GNorm = 0.0688

100% 19/19 [02:57<00:00,  9.25s/it][A100% 19/19 [02:57<00:00,  9.35s/it]
Took 177.69250464439392 seconds to complete one epoch of meta training

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/20 [00:00<?, ?it/s][A[A


  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 30.58it/s]


  5% 1/20 [00:00<00:02,  7.97it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 20.11it/s]


 10% 2/20 [00:00<00:02,  6.79it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 17.86it/s][A[A[A100% 3/3 [00:00<00:00, 19.77it/s]


 15% 3/20 [00:00<00:03,  5.24it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 15.00it/s]


 20% 4/20 [00:00<00:03,  5.06it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 14.41it/s][A[A[A


100% 4/4 [00:00<00:00, 15.66it/s][A[A[A100% 4/4 [00:00<00:00, 16.60it/s]


 25% 5/20 [00:01<00:03,  4.13it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 25% 1/4 [00:01<00:03,  1.07s/it][A[A[A


 75% 3/4 [00:01<00:00,  1.29it/s][A[A[A100% 4/4 [00:01<00:00,  3.17it/s]


 30% 6/20 [00:02<00:08,  1.74it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.14it/s][A[A[A100% 4/4 [00:00<00:00, 19.60it/s]


 35% 7/20 [00:02<00:06,  2.01it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 18.48it/s][A[A[A


100% 4/4 [00:00<00:00, 18.12it/s][A[A[A100% 4/4 [00:00<00:00, 17.85it/s]


 40% 8/20 [00:03<00:05,  2.18it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 17.72it/s][A[A[A100% 4/4 [00:00<00:00, 21.05it/s]


 45% 9/20 [00:03<00:04,  2.45it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 18.29it/s][A[A[A


100% 4/4 [00:00<00:00, 18.35it/s][A[A[A100% 4/4 [00:00<00:00, 18.37it/s]


 50% 10/20 [00:03<00:03,  2.53it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 16.10it/s][A[A[A100% 4/4 [00:00<00:00, 18.46it/s]


 55% 11/20 [00:04<00:03,  2.67it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.03it/s][A[A[A100% 4/4 [00:00<00:00, 21.31it/s]


 60% 12/20 [00:04<00:02,  2.85it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.59it/s][A[A[A100% 3/3 [00:00<00:00, 13.82it/s]


 65% 13/20 [00:04<00:02,  2.75it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.11it/s][A[A[A100% 3/3 [00:00<00:00, 13.67it/s]


 70% 14/20 [00:05<00:02,  2.67it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.98it/s][A[A[A100% 4/4 [00:00<00:00, 18.04it/s]


 75% 15/20 [00:06<00:03,  1.47it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 13.34it/s][A[A[A100% 3/3 [00:00<00:00, 16.33it/s]


 80% 16/20 [00:07<00:02,  1.71it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 11.09it/s][A[A[A100% 3/3 [00:00<00:00, 13.61it/s]


 85% 17/20 [00:07<00:01,  1.85it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 20.28it/s]


 90% 18/20 [00:07<00:00,  2.14it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.59it/s][A[A[A100% 3/3 [00:00<00:00, 18.77it/s]


 95% 19/20 [00:08<00:00,  2.33it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 12.88it/s]


100% 20/20 [00:08<00:00,  2.68it/s][A[A100% 20/20 [00:08<00:00,  2.39it/s]

100% 1/1 [00:08<00:00,  8.36s/it][A100% 1/1 [00:08<00:00,  8.36s/it]
Took 186.05016493797302 seconds to complete one epoch of meta training and validating
Meta validation score auc = 0.604495
 47% 14/30 [45:03<51:19, 192.49s/it]Epoch 14

  0% 0/19 [00:00<?, ?it/s][A

  0% 0/32 [00:00<?, ?it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  3% 1/32 [00:00<00:06,  4.65it/s][A[A

  6% 2/32 [00:00<00:06,  4.38it/s][A[A

  9% 3/32 [00:01<00:15,  1.90it/s][A[A

 12% 4/32 [00:01<00:11,  2.42it/s][A[A

 16% 5/32 [00:01<00:09,  3.00it/s][A[A

 19% 6/32 [00:02<00:07,  3.46it/s][A[A

 22% 7/32 [00:02<00:06,  3.97it/s][A[A

 25% 8/32 [00:02<00:05,  4.26it/s][A[A

 28% 9/32 [00:02<00:04,  4.95it/s][A[A

 31% 10/32 [00:02<00:04,  5.42it/s][A[A

 34% 11/32 [00:03<00:04,  4.72it/s][A[A

 38% 12/32 [00:03<00:04,  4.25it/s][A[A

 41% 13/32 [00:03<00:04,  3.95it/s][A[A

 44% 14/32 [00:03<00:04,  4.18it/s][A[A

 47% 15/32 [00:04<00:03,  4.45it/s][A[A

 50% 16/32 [00:04<00:03,  4.50it/s][A[A

 53% 17/32 [00:05<00:07,  1.94it/s][A[A

 56% 18/32 [00:05<00:05,  2.42it/s][A[A

 59% 19/32 [00:05<00:04,  2.95it/s][A[A

 62% 20/32 [00:06<00:03,  3.25it/s][A[A

 66% 21/32 [00:06<00:02,  3.95it/s][A[A

 69% 22/32 [00:06<00:02,  4.16it/s][A[A

 72% 23/32 [00:06<00:01,  4.58it/s][A[A

 75% 24/32 [00:06<00:01,  4.75it/s][A[A

 78% 25/32 [00:06<00:01,  4.89it/s][A[A

 81% 26/32 [00:07<00:01,  4.75it/s][A[A

 84% 27/32 [00:07<00:00,  5.05it/s][A[A

 88% 28/32 [00:07<00:00,  5.19it/s][A[A

 91% 29/32 [00:07<00:00,  5.22it/s][A[A

 94% 30/32 [00:07<00:00,  5.89it/s][A[A

 97% 31/32 [00:08<00:00,  5.51it/s][A[A

100% 32/32 [00:08<00:00,  5.43it/s][A[A100% 32/32 [00:08<00:00,  3.89it/s]
Meta loss on this task batch = 5.2996e-01, PNorm = 37.2173, GNorm = 0.1023

  5% 1/19 [00:08<02:40,  8.94s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  7.46it/s][A[A

  6% 2/32 [00:01<00:13,  2.29it/s][A[A

  9% 3/32 [00:01<00:10,  2.75it/s][A[A

 12% 4/32 [00:01<00:09,  3.05it/s][A[A

 16% 5/32 [00:01<00:08,  3.33it/s][A[A

 19% 6/32 [00:02<00:06,  3.94it/s][A[A

 22% 7/32 [00:02<00:05,  4.61it/s][A[A

 25% 8/32 [00:02<00:04,  5.13it/s][A[A

 28% 9/32 [00:02<00:04,  5.35it/s][A[A

 31% 10/32 [00:02<00:03,  5.78it/s][A[A

 34% 11/32 [00:02<00:03,  5.52it/s][A[A

 38% 12/32 [00:03<00:03,  5.33it/s][A[A

 41% 13/32 [00:03<00:03,  5.65it/s][A[A

 44% 14/32 [00:03<00:03,  5.15it/s][A[A

 47% 15/32 [00:03<00:03,  4.93it/s][A[A

 50% 16/32 [00:03<00:03,  4.90it/s][A[A

 53% 17/32 [00:04<00:03,  4.95it/s][A[A

 56% 18/32 [00:05<00:07,  1.96it/s][A[A

 59% 19/32 [00:05<00:05,  2.47it/s][A[A

 62% 20/32 [00:05<00:04,  2.96it/s][A[A

 66% 21/32 [00:05<00:03,  3.32it/s][A[A

 69% 22/32 [00:06<00:02,  3.89it/s][A[A

 72% 23/32 [00:06<00:02,  4.40it/s][A[A

 75% 24/32 [00:06<00:01,  4.95it/s][A[A

 78% 25/32 [00:06<00:01,  5.03it/s][A[A

 81% 26/32 [00:06<00:01,  4.90it/s][A[A

 84% 27/32 [00:06<00:00,  5.14it/s][A[A

 88% 28/32 [00:07<00:00,  4.74it/s][A[A

 91% 29/32 [00:07<00:00,  5.33it/s][A[A

 94% 30/32 [00:07<00:00,  5.23it/s][A[A

 97% 31/32 [00:07<00:00,  4.95it/s][A[A

100% 32/32 [00:07<00:00,  5.27it/s][A[A100% 32/32 [00:07<00:00,  4.06it/s]
Meta loss on this task batch = 5.3300e-01, PNorm = 37.2256, GNorm = 0.0661

 11% 2/19 [00:17<02:30,  8.83s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.36it/s][A[A

  6% 2/32 [00:01<00:15,  2.00it/s][A[A

  9% 3/32 [00:01<00:12,  2.40it/s][A[A

 12% 4/32 [00:01<00:09,  2.84it/s][A[A

 16% 5/32 [00:02<00:08,  3.32it/s][A[A

 19% 6/32 [00:02<00:07,  3.60it/s][A[A

 22% 7/32 [00:02<00:06,  3.82it/s][A[A

 25% 8/32 [00:02<00:05,  4.36it/s][A[A

 28% 9/32 [00:02<00:04,  4.79it/s][A[A

 31% 10/32 [00:03<00:04,  4.50it/s][A[A

 34% 11/32 [00:03<00:04,  4.71it/s][A[A

 38% 12/32 [00:03<00:04,  4.64it/s][A[A

 41% 13/32 [00:03<00:03,  4.76it/s][A[A

 44% 14/32 [00:03<00:03,  4.60it/s][A[A

 47% 15/32 [00:05<00:08,  1.90it/s][A[A

 50% 16/32 [00:05<00:06,  2.44it/s][A[A

 53% 17/32 [00:05<00:05,  2.79it/s][A[A

 56% 18/32 [00:05<00:04,  3.42it/s][A[A

 59% 19/32 [00:05<00:03,  3.96it/s][A[A

 62% 20/32 [00:05<00:02,  4.58it/s][A[A

 66% 21/32 [00:06<00:02,  5.21it/s][A[A

 69% 22/32 [00:06<00:01,  5.64it/s][A[A

 72% 23/32 [00:06<00:01,  5.52it/s][A[A

 75% 24/32 [00:06<00:01,  5.54it/s][A[A

 78% 25/32 [00:06<00:01,  5.53it/s][A[A

 81% 26/32 [00:06<00:01,  5.54it/s][A[A

 84% 27/32 [00:07<00:00,  5.56it/s][A[A

 88% 28/32 [00:07<00:00,  5.60it/s][A[A

 91% 29/32 [00:07<00:00,  5.42it/s][A[A

 94% 30/32 [00:07<00:00,  4.72it/s][A[A

 97% 31/32 [00:07<00:00,  5.39it/s][A[A

100% 32/32 [00:08<00:00,  5.05it/s][A[A100% 32/32 [00:08<00:00,  3.94it/s]
Meta loss on this task batch = 5.0689e-01, PNorm = 37.2370, GNorm = 0.0432

 16% 3/19 [00:26<02:21,  8.84s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:38,  1.26s/it][A[A

  6% 2/32 [00:01<00:28,  1.06it/s][A[A

  9% 3/32 [00:01<00:20,  1.40it/s][A[A

 12% 4/32 [00:01<00:15,  1.81it/s][A[A

 16% 5/32 [00:02<00:12,  2.20it/s][A[A

 19% 6/32 [00:02<00:10,  2.57it/s][A[A

 22% 7/32 [00:02<00:07,  3.21it/s][A[A

 25% 8/32 [00:02<00:06,  3.60it/s][A[A

 28% 9/32 [00:02<00:05,  4.25it/s][A[A

 31% 10/32 [00:02<00:04,  4.63it/s][A[A

 34% 11/32 [00:03<00:04,  5.02it/s][A[A

 38% 12/32 [00:03<00:03,  5.48it/s][A[A

 41% 13/32 [00:03<00:03,  5.32it/s][A[A

 44% 14/32 [00:03<00:03,  4.95it/s][A[A

 47% 15/32 [00:03<00:03,  5.33it/s][A[A

 50% 16/32 [00:04<00:03,  5.05it/s][A[A

 53% 17/32 [00:04<00:02,  5.39it/s][A[A

 56% 18/32 [00:04<00:02,  5.54it/s][A[A

 59% 19/32 [00:05<00:06,  2.12it/s][A[A

 62% 20/32 [00:05<00:04,  2.50it/s][A[A

 66% 21/32 [00:05<00:03,  2.87it/s][A[A

 69% 22/32 [00:06<00:02,  3.35it/s][A[A

 72% 23/32 [00:06<00:02,  3.52it/s][A[A

 75% 24/32 [00:06<00:02,  3.76it/s][A[A

 78% 25/32 [00:06<00:01,  4.42it/s][A[A

 81% 26/32 [00:06<00:01,  5.04it/s][A[A

 84% 27/32 [00:07<00:01,  4.86it/s][A[A

 88% 28/32 [00:07<00:00,  4.83it/s][A[A

 91% 29/32 [00:07<00:00,  5.31it/s][A[A

 94% 30/32 [00:07<00:00,  5.25it/s][A[A

 97% 31/32 [00:07<00:00,  5.60it/s][A[A

100% 32/32 [00:08<00:00,  5.14it/s][A[A100% 32/32 [00:08<00:00,  3.97it/s]
Meta loss on this task batch = 5.6563e-01, PNorm = 37.2485, GNorm = 0.0540

 21% 4/19 [00:35<02:12,  8.82s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.51it/s][A[A

  6% 2/32 [00:01<00:14,  2.02it/s][A[A

  9% 3/32 [00:01<00:11,  2.54it/s][A[A

 12% 4/32 [00:01<00:09,  2.82it/s][A[A

 16% 5/32 [00:02<00:08,  3.15it/s][A[A

 19% 6/32 [00:02<00:06,  3.85it/s][A[A

 22% 7/32 [00:02<00:05,  4.28it/s][A[A

 25% 8/32 [00:02<00:05,  4.77it/s][A[A

 28% 9/32 [00:02<00:04,  4.65it/s][A[A

 31% 10/32 [00:02<00:04,  4.99it/s][A[A

 34% 11/32 [00:03<00:04,  5.04it/s][A[A

 38% 12/32 [00:03<00:04,  4.97it/s][A[A

 41% 13/32 [00:03<00:03,  5.01it/s][A[A

 44% 14/32 [00:03<00:03,  5.42it/s][A[A

 47% 15/32 [00:03<00:03,  5.20it/s][A[A

 50% 16/32 [00:03<00:02,  5.41it/s][A[A

 53% 17/32 [00:05<00:07,  1.99it/s][A[A

 56% 18/32 [00:05<00:05,  2.41it/s][A[A

 59% 19/32 [00:05<00:04,  2.71it/s][A[A

 62% 20/32 [00:05<00:03,  3.03it/s][A[A

 66% 21/32 [00:06<00:03,  3.41it/s][A[A

 69% 22/32 [00:06<00:02,  3.87it/s][A[A

 72% 23/32 [00:06<00:01,  4.50it/s][A[A

 75% 24/32 [00:06<00:01,  4.63it/s][A[A

 78% 25/32 [00:06<00:01,  4.61it/s][A[A

 81% 26/32 [00:07<00:01,  5.14it/s][A[A

 84% 27/32 [00:07<00:00,  5.29it/s][A[A

 88% 28/32 [00:08<00:01,  2.10it/s][A[A

 91% 29/32 [00:08<00:01,  2.54it/s][A[A

 94% 30/32 [00:08<00:00,  3.10it/s][A[A

 97% 31/32 [00:08<00:00,  3.54it/s][A[A

100% 32/32 [00:09<00:00,  3.74it/s][A[A100% 32/32 [00:09<00:00,  3.51it/s]
Meta loss on this task batch = 5.2233e-01, PNorm = 37.2591, GNorm = 0.0448

 26% 5/19 [00:44<02:07,  9.12s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  6.38it/s][A[A

  6% 2/32 [00:00<00:05,  5.87it/s][A[A

  9% 3/32 [00:00<00:04,  6.17it/s][A[A

 12% 4/32 [00:00<00:04,  5.80it/s][A[A

 16% 5/32 [00:00<00:04,  5.48it/s][A[A

 19% 6/32 [00:01<00:05,  5.04it/s][A[A

 22% 7/32 [00:01<00:04,  5.16it/s][A[A

 25% 8/32 [00:02<00:11,  2.10it/s][A[A

 28% 9/32 [00:02<00:09,  2.45it/s][A[A

 31% 10/32 [00:02<00:07,  2.79it/s][A[A

 34% 11/32 [00:03<00:06,  3.07it/s][A[A

 38% 12/32 [00:03<00:05,  3.39it/s][A[A

 41% 13/32 [00:03<00:04,  4.02it/s][A[A

 44% 14/32 [00:03<00:04,  4.17it/s][A[A

 47% 15/32 [00:04<00:03,  4.27it/s][A[A

 50% 16/32 [00:04<00:03,  4.26it/s][A[A

 53% 17/32 [00:04<00:03,  4.78it/s][A[A

 56% 18/32 [00:04<00:02,  4.72it/s][A[A

 59% 19/32 [00:04<00:02,  5.01it/s][A[A

 62% 20/32 [00:04<00:02,  5.00it/s][A[A

 66% 21/32 [00:05<00:02,  5.11it/s][A[A

 69% 22/32 [00:05<00:02,  4.87it/s][A[A

 72% 23/32 [00:06<00:04,  1.96it/s][A[A

 75% 24/32 [00:06<00:03,  2.42it/s][A[A

 78% 25/32 [00:07<00:02,  2.78it/s][A[A

 81% 26/32 [00:07<00:01,  3.41it/s][A[A

 84% 27/32 [00:07<00:01,  3.62it/s][A[A

 88% 28/32 [00:07<00:01,  3.92it/s][A[A

 91% 29/32 [00:07<00:00,  4.11it/s][A[A

 94% 30/32 [00:08<00:00,  4.39it/s][A[A

 97% 31/32 [00:08<00:00,  4.87it/s][A[A

100% 32/32 [00:08<00:00,  5.00it/s][A[A100% 32/32 [00:08<00:00,  3.83it/s]
Meta loss on this task batch = 4.5325e-01, PNorm = 37.2693, GNorm = 0.0607

 32% 6/19 [00:54<01:58,  9.12s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.00it/s][A[A

  6% 2/32 [00:00<00:05,  5.16it/s][A[A

  9% 3/32 [00:00<00:05,  5.01it/s][A[A

 12% 4/32 [00:00<00:05,  4.81it/s][A[A

 16% 5/32 [00:01<00:13,  2.04it/s][A[A

 19% 6/32 [00:02<00:10,  2.49it/s][A[A

 22% 7/32 [00:02<00:08,  2.87it/s][A[A

 25% 8/32 [00:02<00:06,  3.47it/s][A[A

 28% 9/32 [00:02<00:05,  3.87it/s][A[A

 31% 10/32 [00:02<00:05,  4.14it/s][A[A

 34% 11/32 [00:03<00:04,  4.25it/s][A[A

 38% 12/32 [00:03<00:04,  4.30it/s][A[A

 41% 13/32 [00:03<00:04,  4.50it/s][A[A

 44% 14/32 [00:03<00:03,  4.98it/s][A[A

 47% 15/32 [00:03<00:03,  4.67it/s][A[A

 50% 16/32 [00:04<00:03,  4.55it/s][A[A

 53% 17/32 [00:04<00:03,  4.91it/s][A[A

 56% 18/32 [00:04<00:02,  4.72it/s][A[A

 59% 19/32 [00:04<00:02,  4.99it/s][A[A

 62% 20/32 [00:06<00:06,  1.95it/s][A[A

 66% 21/32 [00:06<00:04,  2.34it/s][A[A

 69% 22/32 [00:06<00:03,  2.77it/s][A[A

 72% 23/32 [00:06<00:02,  3.08it/s][A[A

 75% 24/32 [00:06<00:02,  3.38it/s][A[A

 78% 25/32 [00:07<00:01,  4.05it/s][A[A

 81% 26/32 [00:07<00:01,  4.47it/s][A[A

 84% 27/32 [00:07<00:01,  4.44it/s][A[A

 88% 28/32 [00:07<00:00,  4.32it/s][A[A

 91% 29/32 [00:07<00:00,  4.71it/s][A[A

 94% 30/32 [00:08<00:00,  4.82it/s][A[A

 97% 31/32 [00:09<00:00,  2.04it/s][A[A

100% 32/32 [00:09<00:00,  2.42it/s][A[A100% 32/32 [00:09<00:00,  3.39it/s]
Meta loss on this task batch = 4.7981e-01, PNorm = 37.2821, GNorm = 0.1335

 37% 7/19 [01:04<01:53,  9.44s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.83it/s][A[A

  6% 2/32 [00:00<00:07,  3.80it/s][A[A

  9% 3/32 [00:00<00:07,  3.82it/s][A[A

 12% 4/32 [00:01<00:07,  3.82it/s][A[A

 16% 5/32 [00:02<00:14,  1.82it/s][A[A

 19% 6/32 [00:02<00:12,  2.15it/s][A[A

 22% 7/32 [00:02<00:09,  2.66it/s][A[A

 25% 8/32 [00:02<00:08,  2.93it/s][A[A

 28% 9/32 [00:03<00:07,  3.21it/s][A[A

 31% 10/32 [00:03<00:06,  3.45it/s][A[A

 34% 11/32 [00:03<00:05,  4.00it/s][A[A

 38% 12/32 [00:03<00:04,  4.21it/s][A[A

 41% 13/32 [00:04<00:04,  4.45it/s][A[A

 44% 14/32 [00:04<00:03,  4.82it/s][A[A

 47% 15/32 [00:04<00:03,  5.39it/s][A[A

 50% 16/32 [00:04<00:03,  5.07it/s][A[A

 53% 17/32 [00:04<00:03,  4.66it/s][A[A

 56% 18/32 [00:05<00:03,  4.63it/s][A[A

 59% 19/32 [00:06<00:06,  1.89it/s][A[A

 62% 20/32 [00:06<00:05,  2.34it/s][A[A

 66% 21/32 [00:06<00:04,  2.73it/s][A[A

 69% 22/32 [00:06<00:03,  3.22it/s][A[A

 72% 23/32 [00:07<00:02,  3.73it/s][A[A

 75% 24/32 [00:07<00:02,  3.79it/s][A[A

 78% 25/32 [00:07<00:01,  3.97it/s][A[A

 81% 26/32 [00:07<00:01,  4.64it/s][A[A

 84% 27/32 [00:07<00:01,  4.44it/s][A[A

 88% 28/32 [00:09<00:02,  1.88it/s][A[A

 91% 29/32 [00:09<00:01,  2.22it/s][A[A

 94% 30/32 [00:09<00:00,  2.51it/s][A[A

 97% 31/32 [00:09<00:00,  2.81it/s][A[A

100% 32/32 [00:10<00:00,  3.10it/s][A[A100% 32/32 [00:10<00:00,  3.15it/s]
Meta loss on this task batch = 3.6483e-01, PNorm = 37.2992, GNorm = 0.1095

 42% 8/19 [01:15<01:48,  9.90s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  3.91it/s][A[A

  6% 2/32 [00:00<00:07,  4.07it/s][A[A

  9% 3/32 [00:00<00:06,  4.16it/s][A[A

 12% 4/32 [00:00<00:06,  4.18it/s][A[A

 16% 5/32 [00:01<00:06,  4.08it/s][A[A

 19% 6/32 [00:01<00:06,  4.17it/s][A[A

 22% 7/32 [00:02<00:13,  1.87it/s][A[A

 25% 8/32 [00:02<00:10,  2.23it/s][A[A

 28% 9/32 [00:03<00:08,  2.62it/s][A[A

 31% 10/32 [00:03<00:07,  2.92it/s][A[A

 34% 11/32 [00:03<00:06,  3.17it/s][A[A

 38% 12/32 [00:03<00:05,  3.39it/s][A[A

 41% 13/32 [00:04<00:05,  3.57it/s][A[A

 44% 14/32 [00:04<00:04,  3.62it/s][A[A

 47% 15/32 [00:04<00:04,  3.80it/s][A[A

 50% 16/32 [00:04<00:04,  3.87it/s][A[A

 53% 17/32 [00:05<00:03,  3.92it/s][A[A

 56% 18/32 [00:05<00:03,  4.04it/s][A[A

 59% 19/32 [00:06<00:07,  1.83it/s][A[A

 62% 20/32 [00:06<00:05,  2.19it/s][A[A

 66% 21/32 [00:07<00:04,  2.52it/s][A[A

 69% 22/32 [00:07<00:03,  2.81it/s][A[A

 72% 23/32 [00:07<00:02,  3.11it/s][A[A

 75% 24/32 [00:07<00:02,  3.36it/s][A[A

 78% 25/32 [00:08<00:01,  3.59it/s][A[A

 81% 26/32 [00:08<00:01,  3.70it/s][A[A

 84% 27/32 [00:08<00:01,  3.77it/s][A[A

 88% 28/32 [00:08<00:01,  3.81it/s][A[A

 91% 29/32 [00:10<00:01,  1.82it/s][A[A

 94% 30/32 [00:10<00:00,  2.18it/s][A[A

 97% 31/32 [00:10<00:00,  2.52it/s][A[A

100% 32/32 [00:10<00:00,  2.89it/s][A[A100% 32/32 [00:10<00:00,  2.97it/s]
Meta loss on this task batch = 2.1828e-01, PNorm = 37.3185, GNorm = 0.0805

 47% 9/19 [01:26<01:44, 10.41s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.72it/s][A[A

  6% 2/32 [00:00<00:07,  3.86it/s][A[A

  9% 3/32 [00:00<00:07,  3.83it/s][A[A

 12% 4/32 [00:01<00:07,  3.75it/s][A[A

 16% 5/32 [00:01<00:07,  3.84it/s][A[A

 19% 6/32 [00:01<00:06,  3.92it/s][A[A

 22% 7/32 [00:01<00:06,  4.06it/s][A[A

 25% 8/32 [00:03<00:13,  1.84it/s][A[A

 28% 9/32 [00:03<00:10,  2.20it/s][A[A

 31% 10/32 [00:03<00:08,  2.52it/s][A[A

 34% 11/32 [00:03<00:07,  2.83it/s][A[A

 38% 12/32 [00:04<00:06,  3.11it/s][A[A

 41% 13/32 [00:04<00:05,  3.37it/s][A[A

 44% 14/32 [00:04<00:05,  3.55it/s][A[A

 47% 15/32 [00:04<00:04,  3.61it/s][A[A

 50% 16/32 [00:04<00:04,  3.81it/s][A[A

 53% 17/32 [00:06<00:08,  1.79it/s][A[A

 56% 18/32 [00:06<00:06,  2.18it/s][A[A

 59% 19/32 [00:06<00:05,  2.57it/s][A[A

 62% 20/32 [00:06<00:04,  2.88it/s][A[A

 66% 21/32 [00:07<00:03,  3.19it/s][A[A

 69% 22/32 [00:07<00:02,  3.36it/s][A[A

 72% 23/32 [00:07<00:02,  3.43it/s][A[A

 75% 24/32 [00:07<00:02,  3.55it/s][A[A

 78% 25/32 [00:09<00:04,  1.74it/s][A[A

 81% 26/32 [00:09<00:02,  2.23it/s][A[A

 84% 27/32 [00:09<00:01,  2.72it/s][A[A

 88% 28/32 [00:09<00:01,  3.08it/s][A[A

 91% 29/32 [00:10<00:00,  3.34it/s][A[A

 94% 30/32 [00:10<00:00,  3.57it/s][A[A

 97% 31/32 [00:10<00:00,  4.04it/s][A[A

100% 32/32 [00:10<00:00,  4.34it/s][A[A100% 32/32 [00:10<00:00,  3.01it/s]
Meta loss on this task batch = 2.9651e-01, PNorm = 37.3368, GNorm = 0.0666

 53% 10/19 [01:38<01:36, 10.72s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.19it/s][A[A

  6% 2/32 [00:00<00:07,  4.23it/s][A[A

  9% 3/32 [00:00<00:06,  4.60it/s][A[A

 12% 4/32 [00:00<00:05,  5.21it/s][A[A

 16% 5/32 [00:01<00:05,  4.96it/s][A[A

 19% 6/32 [00:01<00:05,  5.19it/s][A[A

 22% 7/32 [00:01<00:05,  4.86it/s][A[A

 25% 8/32 [00:01<00:04,  4.81it/s][A[A

 28% 9/32 [00:02<00:11,  2.01it/s][A[A

 31% 10/32 [00:03<00:09,  2.37it/s][A[A

 34% 11/32 [00:03<00:07,  2.69it/s][A[A

 38% 12/32 [00:03<00:06,  3.02it/s][A[A

 41% 13/32 [00:03<00:05,  3.29it/s][A[A

 44% 14/32 [00:04<00:05,  3.54it/s][A[A

 47% 15/32 [00:04<00:04,  4.15it/s][A[A

 50% 16/32 [00:04<00:03,  4.09it/s][A[A

 53% 17/32 [00:04<00:03,  4.42it/s][A[A

 56% 18/32 [00:04<00:03,  4.26it/s][A[A

 59% 19/32 [00:06<00:06,  1.92it/s][A[A

 62% 20/32 [00:06<00:05,  2.38it/s][A[A

 66% 21/32 [00:06<00:03,  2.96it/s][A[A

 69% 22/32 [00:06<00:03,  3.22it/s][A[A

 72% 23/32 [00:06<00:02,  3.50it/s][A[A

 75% 24/32 [00:07<00:02,  3.72it/s][A[A

 78% 25/32 [00:07<00:01,  4.04it/s][A[A

 81% 26/32 [00:07<00:01,  4.01it/s][A[A

 84% 27/32 [00:07<00:01,  4.05it/s][A[A

 88% 28/32 [00:07<00:00,  4.42it/s][A[A

 91% 29/32 [00:09<00:01,  1.92it/s][A[A

 94% 30/32 [00:09<00:00,  2.28it/s][A[A

 97% 31/32 [00:09<00:00,  2.76it/s][A[A

100% 32/32 [00:09<00:00,  3.15it/s][A[A100% 32/32 [00:09<00:00,  3.27it/s]
Meta loss on this task batch = 6.5759e-01, PNorm = 37.3547, GNorm = 0.1039

 58% 11/19 [01:48<01:25, 10.66s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.54it/s][A[A

  6% 2/32 [00:00<00:06,  4.63it/s][A[A

  9% 3/32 [00:00<00:06,  4.58it/s][A[A

 12% 4/32 [00:00<00:05,  4.89it/s][A[A

 16% 5/32 [00:01<00:05,  4.68it/s][A[A

 19% 6/32 [00:01<00:05,  4.75it/s][A[A

 22% 7/32 [00:01<00:05,  4.49it/s][A[A

 25% 8/32 [00:01<00:05,  4.37it/s][A[A

 28% 9/32 [00:02<00:11,  1.95it/s][A[A

 31% 10/32 [00:03<00:09,  2.39it/s][A[A

 34% 11/32 [00:03<00:07,  2.72it/s][A[A

 38% 12/32 [00:03<00:06,  3.21it/s][A[A

 41% 13/32 [00:03<00:05,  3.53it/s][A[A

 44% 14/32 [00:03<00:04,  3.99it/s][A[A

 47% 15/32 [00:04<00:04,  4.06it/s][A[A

 50% 16/32 [00:04<00:03,  4.15it/s][A[A

 53% 17/32 [00:04<00:03,  4.57it/s][A[A

 56% 18/32 [00:04<00:03,  4.42it/s][A[A

 59% 19/32 [00:05<00:02,  4.47it/s][A[A

 62% 20/32 [00:06<00:06,  1.96it/s][A[A

 66% 21/32 [00:06<00:04,  2.36it/s][A[A

 69% 22/32 [00:06<00:03,  2.73it/s][A[A

 72% 23/32 [00:06<00:02,  3.23it/s][A[A

 75% 24/32 [00:07<00:02,  3.51it/s][A[A

 78% 25/32 [00:07<00:01,  3.70it/s][A[A

 81% 26/32 [00:07<00:01,  3.84it/s][A[A

 84% 27/32 [00:07<00:01,  3.97it/s][A[A

 88% 28/32 [00:07<00:00,  4.43it/s][A[A

 91% 29/32 [00:08<00:00,  4.94it/s][A[A

 94% 30/32 [00:08<00:00,  4.95it/s][A[A

 97% 31/32 [00:09<00:00,  2.06it/s][A[A

100% 32/32 [00:09<00:00,  2.43it/s][A[A100% 32/32 [00:09<00:00,  3.30it/s]
Meta loss on this task batch = 5.5435e-01, PNorm = 37.3693, GNorm = 0.1379

 63% 12/19 [01:59<01:14, 10.59s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.45it/s][A[A

  6% 2/32 [00:00<00:06,  4.43it/s][A[A

  9% 3/32 [00:00<00:06,  4.42it/s][A[A

 12% 4/32 [00:00<00:05,  4.76it/s][A[A

 16% 5/32 [00:01<00:05,  5.11it/s][A[A

 19% 6/32 [00:01<00:05,  5.11it/s][A[A

 22% 7/32 [00:01<00:05,  5.00it/s][A[A

 25% 8/32 [00:01<00:04,  5.21it/s][A[A

 28% 9/32 [00:01<00:04,  5.13it/s][A[A

 31% 10/32 [00:02<00:04,  4.83it/s][A[A

 34% 11/32 [00:02<00:03,  5.41it/s][A[A

 38% 12/32 [00:02<00:03,  5.07it/s][A[A

 41% 13/32 [00:02<00:03,  4.81it/s][A[A

 44% 14/32 [00:03<00:09,  1.95it/s][A[A

 47% 15/32 [00:04<00:07,  2.42it/s][A[A

 50% 16/32 [00:04<00:05,  2.98it/s][A[A

 53% 17/32 [00:04<00:04,  3.38it/s][A[A

 56% 18/32 [00:04<00:03,  3.86it/s][A[A

 59% 19/32 [00:04<00:03,  4.03it/s][A[A

 62% 20/32 [00:05<00:02,  4.11it/s][A[A

 66% 21/32 [00:05<00:02,  4.13it/s][A[A

 69% 22/32 [00:05<00:02,  4.17it/s][A[A

 72% 23/32 [00:05<00:02,  4.31it/s][A[A

 75% 24/32 [00:05<00:01,  4.84it/s][A[A

 78% 25/32 [00:06<00:01,  4.98it/s][A[A

 81% 26/32 [00:06<00:01,  4.64it/s][A[A

 84% 27/32 [00:06<00:01,  4.65it/s][A[A

 88% 28/32 [00:07<00:02,  1.94it/s][A[A

 91% 29/32 [00:07<00:01,  2.34it/s][A[A

 94% 30/32 [00:08<00:00,  2.94it/s][A[A

 97% 31/32 [00:08<00:00,  3.24it/s][A[A

100% 32/32 [00:08<00:00,  4.00it/s][A[A100% 32/32 [00:08<00:00,  3.80it/s]
Meta loss on this task batch = 5.9487e-01, PNorm = 37.3815, GNorm = 0.1113

 68% 13/19 [02:08<01:00, 10.17s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.46it/s][A[A

  6% 2/32 [00:00<00:05,  5.37it/s][A[A

  9% 3/32 [00:00<00:05,  4.96it/s][A[A

 12% 4/32 [00:00<00:05,  4.72it/s][A[A

 16% 5/32 [00:01<00:05,  4.69it/s][A[A

 19% 6/32 [00:01<00:05,  4.83it/s][A[A

 22% 7/32 [00:02<00:12,  1.97it/s][A[A

 25% 8/32 [00:02<00:10,  2.37it/s][A[A

 28% 9/32 [00:02<00:07,  2.89it/s][A[A

 31% 10/32 [00:03<00:06,  3.17it/s][A[A

 34% 11/32 [00:03<00:06,  3.47it/s][A[A

 38% 12/32 [00:03<00:05,  3.64it/s][A[A

 41% 13/32 [00:03<00:04,  4.09it/s][A[A

 44% 14/32 [00:03<00:04,  4.15it/s][A[A

 47% 15/32 [00:04<00:03,  4.34it/s][A[A

 50% 16/32 [00:04<00:03,  4.42it/s][A[A

 53% 17/32 [00:04<00:03,  4.58it/s][A[A

 56% 18/32 [00:04<00:03,  4.53it/s][A[A

 59% 19/32 [00:05<00:02,  4.47it/s][A[A

 62% 20/32 [00:05<00:02,  4.40it/s][A[A

 66% 21/32 [00:05<00:02,  4.66it/s][A[A

 69% 22/32 [00:05<00:02,  4.69it/s][A[A

 72% 23/32 [00:06<00:04,  1.92it/s][A[A

 75% 24/32 [00:07<00:03,  2.31it/s][A[A

 78% 25/32 [00:07<00:02,  2.71it/s][A[A

 81% 26/32 [00:07<00:01,  3.10it/s][A[A

 84% 27/32 [00:07<00:01,  3.47it/s][A[A

 88% 28/32 [00:08<00:01,  3.72it/s][A[A

 91% 29/32 [00:08<00:00,  3.86it/s][A[A

 94% 30/32 [00:08<00:00,  4.02it/s][A[A

 97% 31/32 [00:08<00:00,  4.00it/s][A[A

100% 32/32 [00:09<00:00,  1.81it/s][A[A100% 32/32 [00:09<00:00,  3.20it/s]
Meta loss on this task batch = 5.9076e-01, PNorm = 37.3906, GNorm = 0.1027

 74% 14/19 [02:19<00:51, 10.35s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.40it/s][A[A

  6% 2/32 [00:00<00:06,  4.81it/s][A[A

  9% 3/32 [00:00<00:06,  4.56it/s][A[A

 12% 4/32 [00:00<00:06,  4.51it/s][A[A

 16% 5/32 [00:01<00:06,  4.47it/s][A[A

 19% 6/32 [00:01<00:05,  4.48it/s][A[A

 22% 7/32 [00:01<00:05,  4.29it/s][A[A

 25% 8/32 [00:01<00:05,  4.67it/s][A[A

 28% 9/32 [00:02<00:05,  4.37it/s][A[A

 31% 10/32 [00:02<00:04,  4.60it/s][A[A

 34% 11/32 [00:02<00:03,  5.41it/s][A[A

 38% 12/32 [00:03<00:09,  2.09it/s][A[A

 41% 13/32 [00:03<00:07,  2.47it/s][A[A

 44% 14/32 [00:03<00:06,  2.84it/s][A[A

 47% 15/32 [00:04<00:05,  3.33it/s][A[A

 50% 16/32 [00:04<00:04,  3.58it/s][A[A

 53% 17/32 [00:04<00:03,  3.87it/s][A[A

 56% 18/32 [00:04<00:03,  4.04it/s][A[A

 59% 19/32 [00:04<00:02,  4.60it/s][A[A

 62% 20/32 [00:05<00:02,  4.41it/s][A[A

 66% 21/32 [00:05<00:02,  4.74it/s][A[A

 69% 22/32 [00:05<00:02,  4.63it/s][A[A

 72% 23/32 [00:05<00:02,  4.45it/s][A[A

 75% 24/32 [00:06<00:01,  4.46it/s][A[A

 78% 25/32 [00:06<00:01,  4.94it/s][A[A

 81% 26/32 [00:07<00:03,  1.93it/s][A[A

 84% 27/32 [00:07<00:02,  2.29it/s][A[A

 88% 28/32 [00:07<00:01,  2.89it/s][A[A

 91% 29/32 [00:08<00:00,  3.17it/s][A[A

 94% 30/32 [00:08<00:00,  3.42it/s][A[A

 97% 31/32 [00:08<00:00,  3.96it/s][A[A

100% 32/32 [00:08<00:00,  4.00it/s][A[A100% 32/32 [00:08<00:00,  3.65it/s]
Meta loss on this task batch = 4.9483e-01, PNorm = 37.3991, GNorm = 0.0964

 79% 15/19 [02:28<00:40, 10.10s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.36it/s][A[A

  6% 2/32 [00:00<00:05,  5.55it/s][A[A

  9% 3/32 [00:00<00:05,  5.71it/s][A[A

 12% 4/32 [00:00<00:04,  5.63it/s][A[A

 16% 5/32 [00:00<00:04,  5.47it/s][A[A

 19% 6/32 [00:01<00:05,  5.05it/s][A[A

 22% 7/32 [00:02<00:12,  2.03it/s][A[A

 25% 8/32 [00:02<00:09,  2.51it/s][A[A

 28% 9/32 [00:02<00:07,  3.02it/s][A[A

 31% 10/32 [00:02<00:06,  3.31it/s][A[A

 34% 11/32 [00:03<00:05,  3.52it/s][A[A

 38% 12/32 [00:03<00:05,  3.72it/s][A[A

 41% 13/32 [00:03<00:04,  3.91it/s][A[A

 44% 14/32 [00:03<00:04,  4.03it/s][A[A

 47% 15/32 [00:04<00:04,  4.10it/s][A[A

 50% 16/32 [00:04<00:03,  4.62it/s][A[A

 53% 17/32 [00:04<00:03,  4.56it/s][A[A

 56% 18/32 [00:04<00:02,  4.84it/s][A[A

 59% 19/32 [00:05<00:06,  1.96it/s][A[A

 62% 20/32 [00:06<00:05,  2.35it/s][A[A

 66% 21/32 [00:06<00:04,  2.73it/s][A[A

 69% 22/32 [00:06<00:03,  3.18it/s][A[A

 72% 23/32 [00:06<00:02,  3.42it/s][A[A

 75% 24/32 [00:06<00:02,  3.52it/s][A[A

 78% 25/32 [00:07<00:01,  4.10it/s][A[A

 81% 26/32 [00:07<00:01,  4.37it/s][A[A

 84% 27/32 [00:07<00:01,  4.20it/s][A[A

 88% 28/32 [00:08<00:02,  1.87it/s][A[A

 91% 29/32 [00:09<00:01,  2.18it/s][A[A

 94% 30/32 [00:09<00:00,  2.53it/s][A[A

 97% 31/32 [00:09<00:00,  2.88it/s][A[A

100% 32/32 [00:09<00:00,  3.19it/s][A[A100% 32/32 [00:09<00:00,  3.26it/s]
Meta loss on this task batch = 5.7061e-01, PNorm = 37.4058, GNorm = 0.1265

 84% 16/19 [02:39<00:30, 10.25s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.49it/s][A[A

  6% 2/32 [00:00<00:06,  4.91it/s][A[A

  9% 3/32 [00:00<00:05,  5.43it/s][A[A

 12% 4/32 [00:00<00:05,  5.31it/s][A[A

 16% 5/32 [00:01<00:05,  4.99it/s][A[A

 19% 6/32 [00:01<00:04,  5.46it/s][A[A

 22% 7/32 [00:01<00:04,  5.30it/s][A[A

 25% 8/32 [00:02<00:12,  1.96it/s][A[A

 28% 9/32 [00:02<00:09,  2.34it/s][A[A

 31% 10/32 [00:02<00:07,  2.93it/s][A[A

 34% 11/32 [00:03<00:06,  3.27it/s][A[A

 38% 12/32 [00:03<00:05,  3.65it/s][A[A

 41% 13/32 [00:03<00:04,  4.01it/s][A[A

 44% 14/32 [00:03<00:04,  4.24it/s][A[A

 47% 15/32 [00:03<00:03,  4.50it/s][A[A

 50% 16/32 [00:04<00:03,  4.61it/s][A[A

 53% 17/32 [00:04<00:03,  4.85it/s][A[A

 56% 18/32 [00:04<00:03,  4.63it/s][A[A

 59% 19/32 [00:04<00:03,  4.32it/s][A[A

 62% 20/32 [00:06<00:06,  1.91it/s][A[A

 66% 21/32 [00:06<00:04,  2.30it/s][A[A

 69% 22/32 [00:06<00:03,  2.65it/s][A[A

 72% 23/32 [00:06<00:02,  3.08it/s][A[A

 75% 24/32 [00:07<00:02,  3.23it/s][A[A

 78% 25/32 [00:07<00:01,  3.59it/s][A[A

 81% 26/32 [00:07<00:01,  3.75it/s][A[A

 84% 27/32 [00:07<00:01,  4.21it/s][A[A

 88% 28/32 [00:07<00:00,  4.49it/s][A[A

 91% 29/32 [00:08<00:00,  4.74it/s][A[A

 94% 30/32 [00:08<00:00,  4.97it/s][A[A

 97% 31/32 [00:08<00:00,  4.97it/s][A[A

100% 32/32 [00:08<00:00,  4.67it/s][A[A100% 32/32 [00:08<00:00,  3.70it/s]
Meta loss on this task batch = 4.2018e-01, PNorm = 37.4163, GNorm = 0.0686

 89% 17/19 [02:48<00:20, 10.00s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:37,  1.20s/it][A[A

  6% 2/32 [00:01<00:27,  1.10it/s][A[A

  9% 3/32 [00:01<00:20,  1.42it/s][A[A

 12% 4/32 [00:01<00:15,  1.77it/s][A[A

 16% 5/32 [00:02<00:12,  2.13it/s][A[A

 19% 6/32 [00:02<00:10,  2.54it/s][A[A

 22% 7/32 [00:02<00:08,  2.82it/s][A[A

 25% 8/32 [00:02<00:07,  3.02it/s][A[A

 28% 9/32 [00:03<00:06,  3.33it/s][A[A

 31% 10/32 [00:03<00:06,  3.52it/s][A[A

 34% 11/32 [00:03<00:05,  3.75it/s][A[A

 38% 12/32 [00:03<00:04,  4.14it/s][A[A

 41% 13/32 [00:04<00:04,  4.11it/s][A[A

 44% 14/32 [00:05<00:09,  1.94it/s][A[A

 47% 15/32 [00:05<00:07,  2.27it/s][A[A

 50% 16/32 [00:05<00:05,  2.70it/s][A[A

 53% 17/32 [00:05<00:04,  3.34it/s][A[A

 56% 18/32 [00:06<00:03,  3.61it/s][A[A

 59% 19/32 [00:06<00:03,  3.69it/s][A[A

 62% 20/32 [00:06<00:03,  3.81it/s][A[A

 66% 21/32 [00:06<00:02,  3.87it/s][A[A

 69% 22/32 [00:06<00:02,  4.47it/s][A[A

 72% 23/32 [00:07<00:02,  4.35it/s][A[A

 75% 24/32 [00:07<00:02,  3.99it/s][A[A

 78% 25/32 [00:07<00:01,  4.06it/s][A[A

 81% 26/32 [00:08<00:03,  1.85it/s][A[A

 84% 27/32 [00:09<00:02,  2.16it/s][A[A

 88% 28/32 [00:09<00:01,  2.72it/s][A[A

 91% 29/32 [00:09<00:00,  3.24it/s][A[A

 94% 30/32 [00:09<00:00,  3.45it/s][A[A

 97% 31/32 [00:09<00:00,  3.71it/s][A[A

100% 32/32 [00:10<00:00,  3.66it/s][A[A100% 32/32 [00:10<00:00,  3.12it/s]
Meta loss on this task batch = 5.3282e-01, PNorm = 37.4259, GNorm = 0.0729

 95% 18/19 [02:59<00:10, 10.32s/it][A

  0% 0/23 [00:00<?, ?it/s][A[A

  4% 1/23 [00:00<00:05,  3.93it/s][A[A

  9% 2/23 [00:00<00:04,  4.36it/s][A[A

 13% 3/23 [00:00<00:04,  4.39it/s][A[A

 17% 4/23 [00:00<00:03,  4.86it/s][A[A

 22% 5/23 [00:00<00:03,  5.43it/s][A[A

 26% 6/23 [00:01<00:03,  5.21it/s][A[A

 30% 7/23 [00:02<00:08,  1.95it/s][A[A

 35% 8/23 [00:02<00:06,  2.32it/s][A[A

 39% 9/23 [00:02<00:04,  2.94it/s][A[A

 43% 10/23 [00:03<00:04,  3.16it/s][A[A

 48% 11/23 [00:03<00:03,  3.52it/s][A[A

 52% 12/23 [00:03<00:02,  3.69it/s][A[A

 57% 13/23 [00:03<00:02,  4.28it/s][A[A

 61% 14/23 [00:03<00:02,  4.49it/s][A[A

 65% 15/23 [00:04<00:02,  3.64it/s][A[A

 70% 16/23 [00:04<00:01,  3.63it/s][A[A

 74% 17/23 [00:04<00:01,  4.11it/s][A[A

 78% 18/23 [00:05<00:02,  1.93it/s][A[A

 83% 19/23 [00:06<00:01,  2.29it/s][A[A

 87% 20/23 [00:06<00:01,  2.90it/s][A[A

 91% 21/23 [00:06<00:00,  3.42it/s][A[A

 96% 22/23 [00:06<00:00,  4.12it/s][A[A

100% 23/23 [00:06<00:00,  4.58it/s][A[A100% 23/23 [00:06<00:00,  3.45it/s]
Meta loss on this task batch = 4.3615e-01, PNorm = 37.4373, GNorm = 0.1123

100% 19/19 [03:07<00:00,  9.39s/it][A100% 19/19 [03:07<00:00,  9.84s/it]
Took 187.04281878471375 seconds to complete one epoch of meta training

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/20 [00:00<?, ?it/s][A[A


  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 30.94it/s]


  5% 1/20 [00:00<00:02,  8.00it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A


100% 2/2 [00:00<00:00, 19.70it/s][A[A[A100% 2/2 [00:00<00:00, 19.62it/s]


 10% 2/20 [00:00<00:03,  5.96it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 16.73it/s][A[A[A100% 3/3 [00:00<00:00, 18.65it/s]


 15% 3/20 [00:00<00:03,  4.74it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 14.67it/s]


 20% 4/20 [00:00<00:03,  4.52it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 14.58it/s][A[A[A


100% 4/4 [00:00<00:00, 15.83it/s][A[A[A100% 4/4 [00:00<00:00, 16.75it/s]


 25% 5/20 [00:01<00:04,  3.61it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.36it/s][A[A[A


100% 4/4 [00:00<00:00, 14.83it/s][A[A[A100% 4/4 [00:00<00:00, 15.99it/s]


 30% 6/20 [00:01<00:04,  3.17it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.10it/s][A[A[A100% 4/4 [00:00<00:00, 20.76it/s]


 35% 7/20 [00:03<00:08,  1.59it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.37it/s][A[A[A


100% 4/4 [00:00<00:00, 19.05it/s][A[A[A100% 4/4 [00:00<00:00, 18.81it/s]


 40% 8/20 [00:03<00:06,  1.81it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.04it/s][A[A[A100% 4/4 [00:00<00:00, 22.65it/s]


 45% 9/20 [00:03<00:05,  2.04it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.64it/s][A[A[A


100% 4/4 [00:00<00:00, 19.52it/s][A[A[A100% 4/4 [00:00<00:00, 19.41it/s]


 50% 10/20 [00:04<00:04,  2.16it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 16.70it/s][A[A[A100% 4/4 [00:00<00:00, 18.93it/s]


 55% 11/20 [00:04<00:03,  2.30it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 17.49it/s][A[A[A100% 4/4 [00:00<00:00, 20.73it/s]


 60% 12/20 [00:04<00:03,  2.42it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 13.59it/s][A[A[A


100% 3/3 [00:01<00:00,  2.64it/s][A[A[A100% 3/3 [00:01<00:00,  2.42it/s]


 65% 13/20 [00:06<00:05,  1.40it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 13.75it/s][A[A[A100% 3/3 [00:00<00:00, 13.37it/s]


 70% 14/20 [00:06<00:03,  1.61it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.57it/s][A[A[A100% 4/4 [00:00<00:00, 17.75it/s]


 75% 15/20 [00:07<00:02,  1.80it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.36it/s][A[A[A100% 3/3 [00:00<00:00, 17.67it/s]


 80% 16/20 [00:07<00:01,  2.07it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 12.10it/s][A[A[A100% 3/3 [00:00<00:00, 14.82it/s]


 85% 17/20 [00:07<00:01,  2.19it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 21.30it/s]


 90% 18/20 [00:08<00:00,  2.53it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:01<00:00,  1.75it/s][A[A[A100% 3/3 [00:01<00:00,  2.58it/s]


 95% 19/20 [00:09<00:00,  1.52it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 12.71it/s]


100% 20/20 [00:09<00:00,  1.85it/s][A[A100% 20/20 [00:09<00:00,  2.06it/s]

100% 1/1 [00:09<00:00,  9.70s/it][A100% 1/1 [00:09<00:00,  9.70s/it]
Took 196.74316811561584 seconds to complete one epoch of meta training and validating
Meta validation score auc = 0.632963
Found better MAML checkpoint after meta validation, saving now
 50% 15/30 [48:20<48:26, 193.78s/it]Epoch 15

  0% 0/19 [00:00<?, ?it/s][A

  0% 0/32 [00:00<?, ?it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  3% 1/32 [00:00<00:05,  5.64it/s][A[A

  6% 2/32 [00:00<00:05,  5.18it/s][A[A

  9% 3/32 [00:00<00:05,  5.23it/s][A[A

 12% 4/32 [00:00<00:05,  5.21it/s][A[A

 16% 5/32 [00:00<00:05,  5.20it/s][A[A

 19% 6/32 [00:01<00:05,  5.18it/s][A[A

 22% 7/32 [00:01<00:04,  5.17it/s][A[A

 25% 8/32 [00:01<00:04,  5.12it/s][A[A

 28% 9/32 [00:01<00:04,  5.19it/s][A[A

 31% 10/32 [00:01<00:04,  5.22it/s][A[A

 34% 11/32 [00:02<00:04,  5.10it/s][A[A

 38% 12/32 [00:03<00:10,  1.93it/s][A[A

 41% 13/32 [00:03<00:08,  2.29it/s][A[A

 44% 14/32 [00:03<00:06,  2.61it/s][A[A

 47% 15/32 [00:04<00:05,  3.22it/s][A[A

 50% 16/32 [00:04<00:04,  3.50it/s][A[A

 53% 17/32 [00:04<00:03,  3.83it/s][A[A

 56% 18/32 [00:04<00:03,  4.20it/s][A[A

 59% 19/32 [00:04<00:02,  4.38it/s][A[A

 62% 20/32 [00:05<00:02,  4.57it/s][A[A

 66% 21/32 [00:05<00:02,  4.87it/s][A[A

 69% 22/32 [00:05<00:01,  5.01it/s][A[A

 72% 23/32 [00:05<00:01,  5.45it/s][A[A

 75% 24/32 [00:05<00:01,  5.36it/s][A[A

 78% 25/32 [00:05<00:01,  5.30it/s][A[A

 81% 26/32 [00:07<00:03,  2.00it/s][A[A

 84% 27/32 [00:07<00:01,  2.53it/s][A[A

 88% 28/32 [00:07<00:01,  3.03it/s][A[A

 91% 29/32 [00:07<00:00,  3.27it/s][A[A

 94% 30/32 [00:07<00:00,  3.78it/s][A[A

 97% 31/32 [00:08<00:00,  4.20it/s][A[A

100% 32/32 [00:08<00:00,  4.38it/s][A[A100% 32/32 [00:08<00:00,  3.84it/s]
Meta loss on this task batch = 5.0999e-01, PNorm = 37.4546, GNorm = 0.1405

  5% 1/19 [00:09<02:43,  9.09s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.44it/s][A[A

  6% 2/32 [00:00<00:05,  5.15it/s][A[A

  9% 3/32 [00:00<00:06,  4.68it/s][A[A

 12% 4/32 [00:00<00:05,  4.87it/s][A[A

 16% 5/32 [00:01<00:05,  4.91it/s][A[A

 19% 6/32 [00:01<00:05,  4.81it/s][A[A

 22% 7/32 [00:01<00:04,  5.01it/s][A[A

 25% 8/32 [00:01<00:04,  5.24it/s][A[A

 28% 9/32 [00:02<00:11,  2.03it/s][A[A

 31% 10/32 [00:02<00:08,  2.50it/s][A[A

 34% 11/32 [00:03<00:06,  3.09it/s][A[A

 38% 12/32 [00:03<00:06,  3.24it/s][A[A

 41% 13/32 [00:03<00:05,  3.69it/s][A[A

 44% 14/32 [00:03<00:04,  4.02it/s][A[A

 47% 15/32 [00:03<00:03,  4.39it/s][A[A

 50% 16/32 [00:04<00:03,  4.57it/s][A[A

 53% 17/32 [00:04<00:03,  4.72it/s][A[A

 56% 18/32 [00:04<00:02,  4.72it/s][A[A

 59% 19/32 [00:04<00:02,  5.19it/s][A[A

 62% 20/32 [00:04<00:02,  4.77it/s][A[A

 66% 21/32 [00:05<00:02,  5.27it/s][A[A

 69% 22/32 [00:05<00:01,  5.17it/s][A[A

 72% 23/32 [00:05<00:01,  5.35it/s][A[A

 75% 24/32 [00:05<00:01,  5.43it/s][A[A

 78% 25/32 [00:05<00:01,  5.29it/s][A[A

 81% 26/32 [00:07<00:03,  1.99it/s][A[A

 84% 27/32 [00:07<00:02,  2.39it/s][A[A

 88% 28/32 [00:07<00:01,  2.86it/s][A[A

 91% 29/32 [00:07<00:00,  3.35it/s][A[A

 94% 30/32 [00:07<00:00,  3.75it/s][A[A

 97% 31/32 [00:08<00:00,  4.27it/s][A[A

100% 32/32 [00:08<00:00,  4.59it/s][A[A100% 32/32 [00:08<00:00,  3.89it/s]
Meta loss on this task batch = 4.9871e-01, PNorm = 37.4730, GNorm = 0.0655

 11% 2/19 [00:18<02:33,  9.06s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.48it/s][A[A

  6% 2/32 [00:00<00:05,  5.35it/s][A[A

  9% 3/32 [00:00<00:05,  5.38it/s][A[A

 12% 4/32 [00:00<00:05,  5.26it/s][A[A

 16% 5/32 [00:00<00:05,  5.07it/s][A[A

 19% 6/32 [00:02<00:12,  2.01it/s][A[A

 22% 7/32 [00:02<00:10,  2.50it/s][A[A

 25% 8/32 [00:02<00:07,  3.02it/s][A[A

 28% 9/32 [00:02<00:06,  3.38it/s][A[A

 31% 10/32 [00:02<00:05,  3.67it/s][A[A

 34% 11/32 [00:03<00:05,  3.93it/s][A[A

 38% 12/32 [00:03<00:04,  4.27it/s][A[A

 41% 13/32 [00:03<00:04,  4.34it/s][A[A

 44% 14/32 [00:03<00:04,  4.35it/s][A[A

 47% 15/32 [00:04<00:03,  4.46it/s][A[A

 50% 16/32 [00:04<00:03,  4.65it/s][A[A

 53% 17/32 [00:04<00:02,  5.12it/s][A[A

 56% 18/32 [00:04<00:02,  5.37it/s][A[A

 59% 19/32 [00:04<00:02,  5.49it/s][A[A

 62% 20/32 [00:04<00:02,  5.65it/s][A[A

 66% 21/32 [00:05<00:02,  5.48it/s][A[A

 69% 22/32 [00:06<00:04,  2.06it/s][A[A

 72% 23/32 [00:06<00:03,  2.51it/s][A[A

 75% 24/32 [00:06<00:02,  2.97it/s][A[A

 78% 25/32 [00:06<00:01,  3.65it/s][A[A

 81% 26/32 [00:06<00:01,  4.39it/s][A[A

 84% 27/32 [00:07<00:01,  4.27it/s][A[A

 88% 28/32 [00:07<00:00,  4.35it/s][A[A

 91% 29/32 [00:07<00:00,  4.57it/s][A[A

 94% 30/32 [00:07<00:00,  4.42it/s][A[A

 97% 31/32 [00:07<00:00,  4.75it/s][A[A

100% 32/32 [00:08<00:00,  4.80it/s][A[A100% 32/32 [00:08<00:00,  3.92it/s]
Meta loss on this task batch = 5.1413e-01, PNorm = 37.4917, GNorm = 0.0525

 16% 3/19 [00:26<02:24,  9.01s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.80it/s][A[A

  6% 2/32 [00:00<00:06,  4.80it/s][A[A

  9% 3/32 [00:00<00:05,  4.88it/s][A[A

 12% 4/32 [00:01<00:14,  1.98it/s][A[A

 16% 5/32 [00:02<00:11,  2.38it/s][A[A

 19% 6/32 [00:02<00:09,  2.82it/s][A[A

 22% 7/32 [00:02<00:07,  3.26it/s][A[A

 25% 8/32 [00:02<00:06,  3.74it/s][A[A

 28% 9/32 [00:02<00:05,  4.11it/s][A[A

 31% 10/32 [00:02<00:04,  4.57it/s][A[A

 34% 11/32 [00:03<00:04,  4.88it/s][A[A

 38% 12/32 [00:03<00:04,  4.96it/s][A[A

 41% 13/32 [00:03<00:03,  5.01it/s][A[A

 44% 14/32 [00:03<00:03,  5.24it/s][A[A

 47% 15/32 [00:03<00:03,  5.39it/s][A[A

 50% 16/32 [00:04<00:02,  5.54it/s][A[A

 53% 17/32 [00:04<00:02,  5.87it/s][A[A

 56% 18/32 [00:04<00:02,  5.40it/s][A[A

 59% 19/32 [00:04<00:02,  5.67it/s][A[A

 62% 20/32 [00:04<00:02,  5.76it/s][A[A

 66% 21/32 [00:05<00:05,  2.08it/s][A[A

 69% 22/32 [00:06<00:04,  2.47it/s][A[A

 72% 23/32 [00:06<00:03,  2.87it/s][A[A

 75% 24/32 [00:06<00:02,  3.32it/s][A[A

 78% 25/32 [00:06<00:01,  3.77it/s][A[A

 81% 26/32 [00:06<00:01,  4.08it/s][A[A

 84% 27/32 [00:07<00:01,  4.46it/s][A[A

 88% 28/32 [00:07<00:00,  4.68it/s][A[A

 91% 29/32 [00:07<00:00,  4.92it/s][A[A

 94% 30/32 [00:07<00:00,  4.90it/s][A[A

 97% 31/32 [00:07<00:00,  4.88it/s][A[A

100% 32/32 [00:08<00:00,  4.94it/s][A[A100% 32/32 [00:08<00:00,  3.96it/s]
Meta loss on this task batch = 5.7149e-01, PNorm = 37.5110, GNorm = 0.0931

 21% 4/19 [00:35<02:14,  8.96s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.76it/s][A[A

  6% 2/32 [00:00<00:05,  5.56it/s][A[A

  9% 3/32 [00:00<00:04,  5.83it/s][A[A

 12% 4/32 [00:01<00:13,  2.12it/s][A[A

 16% 5/32 [00:01<00:10,  2.54it/s][A[A

 19% 6/32 [00:02<00:08,  2.95it/s][A[A

 22% 7/32 [00:02<00:07,  3.27it/s][A[A

 25% 8/32 [00:02<00:06,  3.75it/s][A[A

 28% 9/32 [00:02<00:05,  4.25it/s][A[A

 31% 10/32 [00:02<00:04,  4.45it/s][A[A

 34% 11/32 [00:03<00:04,  4.75it/s][A[A

 38% 12/32 [00:03<00:04,  4.69it/s][A[A

 41% 13/32 [00:03<00:03,  4.77it/s][A[A

 44% 14/32 [00:03<00:03,  4.79it/s][A[A

 47% 15/32 [00:03<00:03,  4.72it/s][A[A

 50% 16/32 [00:04<00:02,  5.51it/s][A[A

 53% 17/32 [00:04<00:03,  4.94it/s][A[A

 56% 18/32 [00:04<00:02,  4.86it/s][A[A

 59% 19/32 [00:04<00:02,  4.96it/s][A[A

 62% 20/32 [00:05<00:06,  1.94it/s][A[A

 66% 21/32 [00:06<00:04,  2.41it/s][A[A

 69% 22/32 [00:06<00:03,  3.06it/s][A[A

 72% 23/32 [00:06<00:02,  3.43it/s][A[A

 75% 24/32 [00:06<00:02,  3.73it/s][A[A

 78% 25/32 [00:06<00:01,  3.87it/s][A[A

 81% 26/32 [00:07<00:01,  4.15it/s][A[A

 84% 27/32 [00:07<00:01,  4.32it/s][A[A

 88% 28/32 [00:07<00:00,  4.79it/s][A[A

 91% 29/32 [00:07<00:00,  5.06it/s][A[A

 94% 30/32 [00:07<00:00,  4.94it/s][A[A

 97% 31/32 [00:08<00:00,  4.92it/s][A[A

100% 32/32 [00:08<00:00,  4.90it/s][A[A100% 32/32 [00:08<00:00,  3.88it/s]
Meta loss on this task batch = 5.2102e-01, PNorm = 37.5256, GNorm = 0.0960

 26% 5/19 [00:44<02:05,  8.97s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:37,  1.21s/it][A[A

  6% 2/32 [00:01<00:26,  1.11it/s][A[A

  9% 3/32 [00:01<00:19,  1.48it/s][A[A

 12% 4/32 [00:01<00:14,  1.92it/s][A[A

 16% 5/32 [00:01<00:11,  2.29it/s][A[A

 19% 6/32 [00:02<00:09,  2.77it/s][A[A

 22% 7/32 [00:02<00:07,  3.19it/s][A[A

 25% 8/32 [00:02<00:06,  3.55it/s][A[A

 28% 9/32 [00:02<00:06,  3.66it/s][A[A

 31% 10/32 [00:02<00:05,  4.03it/s][A[A

 34% 11/32 [00:03<00:04,  4.31it/s][A[A

 38% 12/32 [00:03<00:04,  4.61it/s][A[A

 41% 13/32 [00:03<00:03,  4.77it/s][A[A

 44% 14/32 [00:04<00:09,  1.90it/s][A[A

 47% 15/32 [00:05<00:07,  2.31it/s][A[A

 50% 16/32 [00:05<00:05,  2.78it/s][A[A

 53% 17/32 [00:05<00:04,  3.24it/s][A[A

 56% 18/32 [00:05<00:03,  3.70it/s][A[A

 59% 19/32 [00:05<00:03,  3.95it/s][A[A

 62% 20/32 [00:05<00:02,  4.55it/s][A[A

 66% 21/32 [00:06<00:02,  4.68it/s][A[A

 69% 22/32 [00:06<00:02,  4.69it/s][A[A

 72% 23/32 [00:06<00:01,  4.95it/s][A[A

 75% 24/32 [00:06<00:01,  4.87it/s][A[A

 78% 25/32 [00:06<00:01,  4.65it/s][A[A

 81% 26/32 [00:07<00:01,  4.83it/s][A[A

 84% 27/32 [00:08<00:02,  1.93it/s][A[A

 88% 28/32 [00:08<00:01,  2.30it/s][A[A

 91% 29/32 [00:08<00:01,  2.64it/s][A[A

 94% 30/32 [00:09<00:00,  3.09it/s][A[A

 97% 31/32 [00:09<00:00,  3.72it/s][A[A

100% 32/32 [00:09<00:00,  4.05it/s][A[A100% 32/32 [00:09<00:00,  3.39it/s]
Meta loss on this task batch = 4.5134e-01, PNorm = 37.5420, GNorm = 0.0690

 32% 6/19 [00:55<02:01,  9.34s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  6.06it/s][A[A

  6% 2/32 [00:00<00:04,  6.26it/s][A[A

  9% 3/32 [00:00<00:05,  5.58it/s][A[A

 12% 4/32 [00:00<00:05,  5.01it/s][A[A

 16% 5/32 [00:00<00:05,  5.08it/s][A[A

 19% 6/32 [00:01<00:05,  5.13it/s][A[A

 22% 7/32 [00:01<00:04,  5.25it/s][A[A

 25% 8/32 [00:01<00:04,  5.06it/s][A[A

 28% 9/32 [00:01<00:04,  5.02it/s][A[A

 31% 10/32 [00:02<00:11,  1.96it/s][A[A

 34% 11/32 [00:03<00:08,  2.45it/s][A[A

 38% 12/32 [00:03<00:06,  3.03it/s][A[A

 41% 13/32 [00:03<00:05,  3.54it/s][A[A

 44% 14/32 [00:03<00:04,  3.91it/s][A[A

 47% 15/32 [00:03<00:03,  4.49it/s][A[A

 50% 16/32 [00:04<00:03,  4.41it/s][A[A

 53% 17/32 [00:04<00:03,  4.67it/s][A[A

 56% 18/32 [00:04<00:02,  4.72it/s][A[A

 59% 19/32 [00:04<00:02,  4.67it/s][A[A

 62% 20/32 [00:04<00:02,  5.12it/s][A[A

 66% 21/32 [00:05<00:02,  5.12it/s][A[A

 69% 22/32 [00:05<00:02,  4.95it/s][A[A

 72% 23/32 [00:05<00:01,  4.77it/s][A[A

 75% 24/32 [00:05<00:01,  4.93it/s][A[A

 78% 25/32 [00:05<00:01,  4.83it/s][A[A

 81% 26/32 [00:07<00:03,  1.99it/s][A[A

 84% 27/32 [00:07<00:02,  2.48it/s][A[A

 88% 28/32 [00:07<00:01,  2.96it/s][A[A

 91% 29/32 [00:07<00:00,  3.52it/s][A[A

 94% 30/32 [00:07<00:00,  3.90it/s][A[A

 97% 31/32 [00:07<00:00,  4.06it/s][A[A

100% 32/32 [00:08<00:00,  4.23it/s][A[A100% 32/32 [00:08<00:00,  3.90it/s]
Meta loss on this task batch = 4.9177e-01, PNorm = 37.5612, GNorm = 0.0843

 37% 7/19 [01:03<01:50,  9.23s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.48it/s][A[A

  6% 2/32 [00:00<00:06,  4.57it/s][A[A

  9% 3/32 [00:00<00:06,  4.57it/s][A[A

 12% 4/32 [00:00<00:06,  4.54it/s][A[A

 16% 5/32 [00:01<00:05,  4.64it/s][A[A

 19% 6/32 [00:01<00:05,  4.65it/s][A[A

 22% 7/32 [00:01<00:04,  5.01it/s][A[A

 25% 8/32 [00:02<00:12,  1.97it/s][A[A

 28% 9/32 [00:02<00:09,  2.41it/s][A[A

 31% 10/32 [00:03<00:07,  2.98it/s][A[A

 34% 11/32 [00:03<00:06,  3.33it/s][A[A

 38% 12/32 [00:03<00:05,  3.74it/s][A[A

 41% 13/32 [00:03<00:04,  4.21it/s][A[A

 44% 14/32 [00:03<00:03,  4.67it/s][A[A

 47% 15/32 [00:03<00:03,  4.70it/s][A[A

 50% 16/32 [00:04<00:03,  4.90it/s][A[A

 53% 17/32 [00:04<00:03,  4.83it/s][A[A

 56% 18/32 [00:04<00:02,  5.03it/s][A[A

 59% 19/32 [00:04<00:02,  4.98it/s][A[A

 62% 20/32 [00:04<00:02,  5.11it/s][A[A

 66% 21/32 [00:06<00:05,  2.02it/s][A[A

 69% 22/32 [00:06<00:04,  2.39it/s][A[A

 72% 23/32 [00:06<00:03,  2.95it/s][A[A

 75% 24/32 [00:06<00:02,  3.31it/s][A[A

 78% 25/32 [00:06<00:01,  3.53it/s][A[A

 81% 26/32 [00:07<00:01,  3.69it/s][A[A

 84% 27/32 [00:07<00:01,  4.07it/s][A[A

 88% 28/32 [00:07<00:00,  4.34it/s][A[A

 91% 29/32 [00:07<00:00,  4.68it/s][A[A

 94% 30/32 [00:07<00:00,  4.88it/s][A[A

 97% 31/32 [00:08<00:00,  4.98it/s][A[A

100% 32/32 [00:08<00:00,  4.98it/s][A[A100% 32/32 [00:08<00:00,  3.83it/s]
Meta loss on this task batch = 3.5683e-01, PNorm = 37.5826, GNorm = 0.0577

 42% 8/19 [01:13<01:41,  9.19s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.86it/s][A[A

  6% 2/32 [00:01<00:15,  1.99it/s][A[A

  9% 3/32 [00:01<00:11,  2.45it/s][A[A

 12% 4/32 [00:01<00:09,  2.95it/s][A[A

 16% 5/32 [00:01<00:07,  3.40it/s][A[A

 19% 6/32 [00:02<00:06,  3.84it/s][A[A

 22% 7/32 [00:02<00:05,  4.19it/s][A[A

 25% 8/32 [00:02<00:05,  4.48it/s][A[A

 28% 9/32 [00:02<00:04,  4.66it/s][A[A

 31% 10/32 [00:02<00:04,  4.69it/s][A[A

 34% 11/32 [00:03<00:04,  4.74it/s][A[A

 38% 12/32 [00:03<00:03,  5.02it/s][A[A

 41% 13/32 [00:03<00:03,  5.02it/s][A[A

 44% 14/32 [00:03<00:03,  5.19it/s][A[A

 47% 15/32 [00:03<00:03,  5.12it/s][A[A

 50% 16/32 [00:04<00:03,  5.28it/s][A[A

 53% 17/32 [00:04<00:02,  5.39it/s][A[A

 56% 18/32 [00:04<00:02,  5.48it/s][A[A

 59% 19/32 [00:05<00:06,  2.03it/s][A[A

 62% 20/32 [00:05<00:04,  2.46it/s][A[A

 66% 21/32 [00:06<00:03,  2.91it/s][A[A

 69% 22/32 [00:06<00:02,  3.34it/s][A[A

 72% 23/32 [00:06<00:02,  3.86it/s][A[A

 75% 24/32 [00:06<00:01,  4.22it/s][A[A

 78% 25/32 [00:06<00:01,  4.46it/s][A[A

 81% 26/32 [00:06<00:01,  4.63it/s][A[A

 84% 27/32 [00:07<00:01,  4.72it/s][A[A

 88% 28/32 [00:07<00:00,  4.79it/s][A[A

 91% 29/32 [00:07<00:00,  4.93it/s][A[A

 94% 30/32 [00:07<00:00,  5.07it/s][A[A

 97% 31/32 [00:07<00:00,  5.17it/s][A[A

100% 32/32 [00:08<00:00,  5.12it/s][A[A100% 32/32 [00:08<00:00,  3.94it/s]
Meta loss on this task batch = 2.1842e-01, PNorm = 37.6062, GNorm = 0.0795

 47% 9/19 [01:21<01:30,  9.10s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:37,  1.21s/it][A[A

  6% 2/32 [00:01<00:27,  1.11it/s][A[A

  9% 3/32 [00:01<00:19,  1.45it/s][A[A

 12% 4/32 [00:01<00:15,  1.86it/s][A[A

 16% 5/32 [00:01<00:11,  2.32it/s][A[A

 19% 6/32 [00:02<00:09,  2.77it/s][A[A

 22% 7/32 [00:02<00:07,  3.24it/s][A[A

 25% 8/32 [00:02<00:06,  3.63it/s][A[A

 28% 9/32 [00:02<00:05,  3.99it/s][A[A

 31% 10/32 [00:02<00:05,  4.19it/s][A[A

 34% 11/32 [00:03<00:04,  4.45it/s][A[A

 38% 12/32 [00:03<00:04,  4.66it/s][A[A

 41% 13/32 [00:03<00:03,  4.86it/s][A[A

 44% 14/32 [00:03<00:03,  5.01it/s][A[A

 47% 15/32 [00:03<00:03,  4.94it/s][A[A

 50% 16/32 [00:05<00:07,  2.01it/s][A[A

 53% 17/32 [00:05<00:06,  2.46it/s][A[A

 56% 18/32 [00:05<00:04,  2.92it/s][A[A

 59% 19/32 [00:05<00:03,  3.38it/s][A[A

 62% 20/32 [00:05<00:03,  3.85it/s][A[A

 66% 21/32 [00:06<00:02,  4.21it/s][A[A

 69% 22/32 [00:06<00:02,  4.40it/s][A[A

 72% 23/32 [00:06<00:01,  4.81it/s][A[A

 75% 24/32 [00:06<00:01,  4.99it/s][A[A

 78% 25/32 [00:06<00:01,  4.98it/s][A[A

 81% 26/32 [00:07<00:01,  4.64it/s][A[A

 84% 27/32 [00:07<00:01,  4.89it/s][A[A

 88% 28/32 [00:07<00:00,  4.73it/s][A[A

 91% 29/32 [00:08<00:01,  1.99it/s][A[A

 94% 30/32 [00:08<00:00,  2.36it/s][A[A

 97% 31/32 [00:09<00:00,  2.79it/s][A[A

100% 32/32 [00:09<00:00,  3.26it/s][A[A100% 32/32 [00:09<00:00,  3.46it/s]
Meta loss on this task batch = 2.2879e-01, PNorm = 37.6341, GNorm = 0.0987

 53% 10/19 [01:31<01:24,  9.37s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.99it/s][A[A

  6% 2/32 [00:00<00:06,  4.79it/s][A[A

  9% 3/32 [00:00<00:06,  4.55it/s][A[A

 12% 4/32 [00:00<00:06,  4.49it/s][A[A

 16% 5/32 [00:01<00:05,  4.52it/s][A[A

 19% 6/32 [00:01<00:05,  4.70it/s][A[A

 22% 7/32 [00:01<00:05,  4.99it/s][A[A

 25% 8/32 [00:01<00:04,  4.90it/s][A[A

 28% 9/32 [00:01<00:05,  4.54it/s][A[A

 31% 10/32 [00:02<00:04,  4.43it/s][A[A

 34% 11/32 [00:02<00:04,  4.38it/s][A[A

 38% 12/32 [00:02<00:04,  4.36it/s][A[A

 41% 13/32 [00:03<00:10,  1.88it/s][A[A

 44% 14/32 [00:04<00:08,  2.25it/s][A[A

 47% 15/32 [00:04<00:06,  2.60it/s][A[A

 50% 16/32 [00:04<00:05,  3.00it/s][A[A

 53% 17/32 [00:04<00:04,  3.27it/s][A[A

 56% 18/32 [00:05<00:03,  3.78it/s][A[A

 59% 19/32 [00:05<00:03,  4.03it/s][A[A

 62% 20/32 [00:05<00:02,  4.08it/s][A[A

 66% 21/32 [00:05<00:02,  4.12it/s][A[A

 69% 22/32 [00:05<00:02,  4.25it/s][A[A

 72% 23/32 [00:06<00:02,  4.32it/s][A[A

 75% 24/32 [00:07<00:04,  1.87it/s][A[A

 78% 25/32 [00:07<00:03,  2.31it/s][A[A

 81% 26/32 [00:07<00:02,  2.78it/s][A[A

 84% 27/32 [00:07<00:01,  3.20it/s][A[A

 88% 28/32 [00:08<00:01,  3.43it/s][A[A

 91% 29/32 [00:08<00:00,  3.76it/s][A[A

 94% 30/32 [00:08<00:00,  3.99it/s][A[A

 97% 31/32 [00:08<00:00,  4.06it/s][A[A

100% 32/32 [00:09<00:00,  4.14it/s][A[A100% 32/32 [00:09<00:00,  3.52it/s]
Meta loss on this task batch = 5.9740e-01, PNorm = 37.6580, GNorm = 0.0791

 58% 11/19 [01:41<01:16,  9.53s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.81it/s][A[A

  6% 2/32 [00:01<00:16,  1.79it/s][A[A

  9% 3/32 [00:01<00:13,  2.16it/s][A[A

 12% 4/32 [00:01<00:10,  2.55it/s][A[A

 16% 5/32 [00:02<00:09,  2.91it/s][A[A

 19% 6/32 [00:02<00:08,  3.22it/s][A[A

 22% 7/32 [00:02<00:06,  3.75it/s][A[A

 25% 8/32 [00:02<00:06,  3.99it/s][A[A

 28% 9/32 [00:03<00:05,  4.18it/s][A[A

 31% 10/32 [00:03<00:05,  4.18it/s][A[A

 34% 11/32 [00:03<00:04,  4.61it/s][A[A

 38% 12/32 [00:03<00:04,  4.43it/s][A[A

 41% 13/32 [00:04<00:10,  1.90it/s][A[A

 44% 14/32 [00:05<00:07,  2.25it/s][A[A

 47% 15/32 [00:05<00:06,  2.60it/s][A[A

 50% 16/32 [00:05<00:05,  3.11it/s][A[A

 53% 17/32 [00:05<00:04,  3.48it/s][A[A

 56% 18/32 [00:06<00:03,  3.72it/s][A[A

 59% 19/32 [00:06<00:03,  3.82it/s][A[A

 62% 20/32 [00:07<00:06,  1.80it/s][A[A

 66% 21/32 [00:07<00:05,  2.17it/s][A[A

 69% 22/32 [00:07<00:03,  2.67it/s][A[A

 72% 23/32 [00:08<00:02,  3.05it/s][A[A

 75% 24/32 [00:08<00:02,  3.41it/s][A[A

 78% 25/32 [00:08<00:01,  3.61it/s][A[A

 81% 26/32 [00:08<00:01,  3.84it/s][A[A

 84% 27/32 [00:09<00:01,  3.90it/s][A[A

 88% 28/32 [00:09<00:00,  4.03it/s][A[A

 91% 29/32 [00:09<00:00,  4.15it/s][A[A

 94% 30/32 [00:10<00:01,  1.86it/s][A[A

 97% 31/32 [00:10<00:00,  2.25it/s][A[A

100% 32/32 [00:11<00:00,  2.62it/s][A[A100% 32/32 [00:11<00:00,  2.86it/s]
Meta loss on this task batch = 5.7424e-01, PNorm = 37.6786, GNorm = 0.0779

 63% 12/19 [01:53<01:11, 10.28s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  3.93it/s][A[A

  6% 2/32 [00:00<00:07,  4.26it/s][A[A

  9% 3/32 [00:00<00:06,  4.70it/s][A[A

 12% 4/32 [00:00<00:05,  4.78it/s][A[A

 16% 5/32 [00:01<00:05,  4.57it/s][A[A

 19% 6/32 [00:01<00:05,  4.60it/s][A[A

 22% 7/32 [00:01<00:05,  4.87it/s][A[A

 25% 8/32 [00:01<00:04,  5.02it/s][A[A

 28% 9/32 [00:01<00:04,  5.21it/s][A[A

 31% 10/32 [00:02<00:04,  5.01it/s][A[A

 34% 11/32 [00:03<00:10,  1.97it/s][A[A

 38% 12/32 [00:03<00:08,  2.46it/s][A[A

 41% 13/32 [00:03<00:06,  2.80it/s][A[A

 44% 14/32 [00:03<00:05,  3.29it/s][A[A

 47% 15/32 [00:04<00:04,  3.52it/s][A[A

 50% 16/32 [00:04<00:04,  3.70it/s][A[A

 53% 17/32 [00:04<00:03,  4.03it/s][A[A

 56% 18/32 [00:04<00:03,  4.02it/s][A[A

 59% 19/32 [00:05<00:03,  3.97it/s][A[A

 62% 20/32 [00:05<00:02,  4.30it/s][A[A

 66% 21/32 [00:05<00:02,  4.27it/s][A[A

 69% 22/32 [00:06<00:05,  1.86it/s][A[A

 72% 23/32 [00:06<00:04,  2.22it/s][A[A

 75% 24/32 [00:07<00:03,  2.57it/s][A[A

 78% 25/32 [00:07<00:02,  2.92it/s][A[A

 81% 26/32 [00:07<00:01,  3.33it/s][A[A

 84% 27/32 [00:07<00:01,  3.76it/s][A[A

 88% 28/32 [00:07<00:00,  4.19it/s][A[A

 91% 29/32 [00:08<00:00,  4.11it/s][A[A

 94% 30/32 [00:08<00:00,  4.13it/s][A[A

 97% 31/32 [00:09<00:00,  1.88it/s][A[A

100% 32/32 [00:09<00:00,  2.27it/s][A[A100% 32/32 [00:09<00:00,  3.23it/s]
Meta loss on this task batch = 6.2009e-01, PNorm = 37.6934, GNorm = 0.1431

 68% 13/19 [02:04<01:02, 10.41s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.26it/s][A[A

  6% 2/32 [00:00<00:05,  5.31it/s][A[A

  9% 3/32 [00:00<00:05,  5.08it/s][A[A

 12% 4/32 [00:00<00:05,  4.88it/s][A[A

 16% 5/32 [00:01<00:05,  4.68it/s][A[A

 19% 6/32 [00:01<00:05,  4.88it/s][A[A

 22% 7/32 [00:01<00:05,  4.60it/s][A[A

 25% 8/32 [00:01<00:04,  5.03it/s][A[A

 28% 9/32 [00:01<00:04,  4.73it/s][A[A

 31% 10/32 [00:02<00:04,  4.86it/s][A[A

 34% 11/32 [00:02<00:04,  4.55it/s][A[A

 38% 12/32 [00:02<00:04,  4.70it/s][A[A

 41% 13/32 [00:03<00:09,  1.91it/s][A[A

 44% 14/32 [00:04<00:07,  2.29it/s][A[A

 47% 15/32 [00:04<00:06,  2.63it/s][A[A

 50% 16/32 [00:04<00:05,  2.92it/s][A[A

 53% 17/32 [00:04<00:04,  3.31it/s][A[A

 56% 18/32 [00:04<00:03,  3.62it/s][A[A

 59% 19/32 [00:05<00:03,  3.92it/s][A[A

 62% 20/32 [00:05<00:02,  4.35it/s][A[A

 66% 21/32 [00:05<00:02,  4.33it/s][A[A

 69% 22/32 [00:05<00:02,  4.27it/s][A[A

 72% 23/32 [00:06<00:02,  4.21it/s][A[A

 75% 24/32 [00:06<00:01,  4.40it/s][A[A

 78% 25/32 [00:07<00:03,  1.90it/s][A[A

 81% 26/32 [00:07<00:02,  2.26it/s][A[A

 84% 27/32 [00:07<00:01,  2.62it/s][A[A

 88% 28/32 [00:08<00:01,  2.94it/s][A[A

 91% 29/32 [00:08<00:00,  3.35it/s][A[A

 94% 30/32 [00:08<00:00,  3.71it/s][A[A

 97% 31/32 [00:08<00:00,  4.01it/s][A[A

100% 32/32 [00:08<00:00,  4.33it/s][A[A100% 32/32 [00:08<00:00,  3.57it/s]
Meta loss on this task batch = 5.9362e-01, PNorm = 37.7046, GNorm = 0.1218

 74% 14/19 [02:14<00:51, 10.22s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.61it/s][A[A

  6% 2/32 [00:00<00:06,  4.82it/s][A[A

  9% 3/32 [00:01<00:15,  1.93it/s][A[A

 12% 4/32 [00:01<00:11,  2.41it/s][A[A

 16% 5/32 [00:02<00:09,  2.89it/s][A[A

 19% 6/32 [00:02<00:08,  3.11it/s][A[A

 22% 7/32 [00:02<00:07,  3.47it/s][A[A

 25% 8/32 [00:02<00:06,  3.83it/s][A[A

 28% 9/32 [00:02<00:05,  3.96it/s][A[A

 31% 10/32 [00:03<00:05,  4.37it/s][A[A

 34% 11/32 [00:03<00:04,  4.46it/s][A[A

 38% 12/32 [00:03<00:04,  4.48it/s][A[A

 41% 13/32 [00:03<00:04,  4.67it/s][A[A

 44% 14/32 [00:03<00:03,  4.91it/s][A[A

 47% 15/32 [00:05<00:08,  1.98it/s][A[A

 50% 16/32 [00:05<00:06,  2.40it/s][A[A

 53% 17/32 [00:05<00:05,  2.81it/s][A[A

 56% 18/32 [00:05<00:04,  3.26it/s][A[A

 59% 19/32 [00:05<00:03,  3.62it/s][A[A

 62% 20/32 [00:06<00:03,  3.86it/s][A[A

 66% 21/32 [00:06<00:02,  4.15it/s][A[A

 69% 22/32 [00:06<00:02,  4.47it/s][A[A

 72% 23/32 [00:06<00:02,  4.25it/s][A[A

 75% 24/32 [00:06<00:01,  4.45it/s][A[A

 78% 25/32 [00:07<00:01,  4.44it/s][A[A

 81% 26/32 [00:07<00:01,  4.35it/s][A[A

 84% 27/32 [00:07<00:01,  4.46it/s][A[A

 88% 28/32 [00:08<00:02,  1.90it/s][A[A

 91% 29/32 [00:09<00:01,  2.33it/s][A[A

 94% 30/32 [00:09<00:00,  2.68it/s][A[A

 97% 31/32 [00:09<00:00,  3.13it/s][A[A

100% 32/32 [00:09<00:00,  3.63it/s][A[A100% 32/32 [00:09<00:00,  3.30it/s]
Meta loss on this task batch = 4.9193e-01, PNorm = 37.7166, GNorm = 0.0336

 79% 15/19 [02:24<00:41, 10.29s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.14it/s][A[A

  6% 2/32 [00:00<00:06,  4.48it/s][A[A

  9% 3/32 [00:00<00:06,  4.71it/s][A[A

 12% 4/32 [00:00<00:05,  4.86it/s][A[A

 16% 5/32 [00:00<00:05,  5.04it/s][A[A

 19% 6/32 [00:01<00:05,  4.84it/s][A[A

 22% 7/32 [00:01<00:05,  5.00it/s][A[A

 25% 8/32 [00:01<00:05,  4.75it/s][A[A

 28% 9/32 [00:02<00:11,  1.99it/s][A[A

 31% 10/32 [00:03<00:09,  2.40it/s][A[A

 34% 11/32 [00:03<00:07,  2.85it/s][A[A

 38% 12/32 [00:03<00:06,  3.27it/s][A[A

 41% 13/32 [00:03<00:05,  3.70it/s][A[A

 44% 14/32 [00:03<00:04,  4.09it/s][A[A

 47% 15/32 [00:04<00:04,  4.24it/s][A[A

 50% 16/32 [00:04<00:03,  4.30it/s][A[A

 53% 17/32 [00:04<00:03,  4.37it/s][A[A

 56% 18/32 [00:04<00:02,  4.79it/s][A[A

 59% 19/32 [00:04<00:02,  4.91it/s][A[A

 62% 20/32 [00:05<00:02,  4.92it/s][A[A

 66% 21/32 [00:05<00:02,  5.05it/s][A[A

 69% 22/32 [00:05<00:01,  5.02it/s][A[A

 72% 23/32 [00:06<00:04,  1.97it/s][A[A

 75% 24/32 [00:06<00:03,  2.33it/s][A[A

 78% 25/32 [00:07<00:02,  2.87it/s][A[A

 81% 26/32 [00:07<00:01,  3.27it/s][A[A

 84% 27/32 [00:07<00:01,  3.46it/s][A[A

 88% 28/32 [00:07<00:01,  3.73it/s][A[A

 91% 29/32 [00:07<00:00,  3.96it/s][A[A

 94% 30/32 [00:08<00:00,  4.00it/s][A[A

 97% 31/32 [00:08<00:00,  4.05it/s][A[A

100% 32/32 [00:09<00:00,  1.87it/s][A[A100% 32/32 [00:09<00:00,  3.33it/s]
Meta loss on this task batch = 5.4475e-01, PNorm = 37.7299, GNorm = 0.0597

 84% 16/19 [02:35<00:30, 10.32s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  3.90it/s][A[A

  6% 2/32 [00:00<00:07,  3.96it/s][A[A

  9% 3/32 [00:00<00:07,  4.13it/s][A[A

 12% 4/32 [00:00<00:06,  4.36it/s][A[A

 16% 5/32 [00:01<00:05,  4.59it/s][A[A

 19% 6/32 [00:01<00:05,  4.60it/s][A[A

 22% 7/32 [00:01<00:05,  4.96it/s][A[A

 25% 8/32 [00:01<00:05,  4.64it/s][A[A

 28% 9/32 [00:01<00:04,  4.61it/s][A[A

 31% 10/32 [00:02<00:04,  5.24it/s][A[A

 34% 11/32 [00:02<00:04,  5.25it/s][A[A

 38% 12/32 [00:03<00:10,  1.99it/s][A[A

 41% 13/32 [00:03<00:07,  2.50it/s][A[A

 44% 14/32 [00:03<00:06,  2.98it/s][A[A

 47% 15/32 [00:04<00:05,  3.38it/s][A[A

 50% 16/32 [00:04<00:04,  3.70it/s][A[A

 53% 17/32 [00:04<00:03,  3.84it/s][A[A

 56% 18/32 [00:04<00:03,  4.06it/s][A[A

 59% 19/32 [00:04<00:03,  4.20it/s][A[A

 62% 20/32 [00:05<00:02,  4.28it/s][A[A

 66% 21/32 [00:05<00:02,  4.51it/s][A[A

 69% 22/32 [00:06<00:05,  1.95it/s][A[A

 72% 23/32 [00:06<00:03,  2.38it/s][A[A

 75% 24/32 [00:06<00:02,  2.80it/s][A[A

 78% 25/32 [00:07<00:02,  3.28it/s][A[A

 81% 26/32 [00:07<00:01,  3.75it/s][A[A

 84% 27/32 [00:07<00:01,  4.04it/s][A[A

 88% 28/32 [00:07<00:00,  4.25it/s][A[A

 91% 29/32 [00:07<00:00,  4.47it/s][A[A

 94% 30/32 [00:08<00:00,  4.33it/s][A[A

 97% 31/32 [00:08<00:00,  4.66it/s][A[A

100% 32/32 [00:08<00:00,  4.88it/s][A[A100% 32/32 [00:08<00:00,  3.75it/s]
Meta loss on this task batch = 4.5988e-01, PNorm = 37.7426, GNorm = 0.0747

 89% 17/19 [02:44<00:20, 10.02s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.77it/s][A[A

  6% 2/32 [00:00<00:06,  4.80it/s][A[A

  9% 3/32 [00:00<00:06,  4.62it/s][A[A

 12% 4/32 [00:01<00:14,  1.95it/s][A[A

 16% 5/32 [00:02<00:11,  2.36it/s][A[A

 19% 6/32 [00:02<00:09,  2.83it/s][A[A

 22% 7/32 [00:02<00:08,  3.12it/s][A[A

 25% 8/32 [00:02<00:07,  3.37it/s][A[A

 28% 9/32 [00:02<00:06,  3.73it/s][A[A

 31% 10/32 [00:03<00:05,  3.94it/s][A[A

 34% 11/32 [00:03<00:04,  4.32it/s][A[A

 38% 12/32 [00:03<00:04,  4.42it/s][A[A

 41% 13/32 [00:03<00:04,  4.20it/s][A[A

 44% 14/32 [00:04<00:04,  4.42it/s][A[A

 47% 15/32 [00:04<00:03,  4.37it/s][A[A

 50% 16/32 [00:04<00:03,  4.58it/s][A[A

 53% 17/32 [00:04<00:03,  4.69it/s][A[A

 56% 18/32 [00:04<00:02,  4.82it/s][A[A

 59% 19/32 [00:05<00:02,  4.65it/s][A[A

 62% 20/32 [00:06<00:06,  1.86it/s][A[A

 66% 21/32 [00:06<00:04,  2.34it/s][A[A

 69% 22/32 [00:06<00:03,  2.79it/s][A[A

 72% 23/32 [00:07<00:02,  3.01it/s][A[A

 75% 24/32 [00:07<00:02,  3.22it/s][A[A

 78% 25/32 [00:07<00:01,  3.80it/s][A[A

 81% 26/32 [00:07<00:01,  4.05it/s][A[A

 84% 27/32 [00:07<00:01,  4.26it/s][A[A

 88% 28/32 [00:08<00:00,  4.22it/s][A[A

 91% 29/32 [00:08<00:00,  4.26it/s][A[A

 94% 30/32 [00:08<00:00,  4.52it/s][A[A

 97% 31/32 [00:08<00:00,  4.82it/s][A[A

100% 32/32 [00:08<00:00,  4.61it/s][A[A100% 32/32 [00:08<00:00,  3.59it/s]
Meta loss on this task batch = 5.0491e-01, PNorm = 37.7544, GNorm = 0.0696

 95% 18/19 [02:54<00:09,  9.93s/it][A

  0% 0/23 [00:00<?, ?it/s][A[A

  4% 1/23 [00:01<00:28,  1.30s/it][A[A

  9% 2/23 [00:01<00:20,  1.03it/s][A[A

 13% 3/23 [00:01<00:14,  1.35it/s][A[A

 17% 4/23 [00:01<00:10,  1.75it/s][A[A

 22% 5/23 [00:02<00:07,  2.26it/s][A[A

 26% 6/23 [00:02<00:06,  2.68it/s][A[A

 30% 7/23 [00:02<00:05,  3.20it/s][A[A

 35% 8/23 [00:02<00:04,  3.63it/s][A[A

 39% 9/23 [00:02<00:03,  4.08it/s][A[A

 43% 10/23 [00:03<00:03,  3.92it/s][A[A

 48% 11/23 [00:03<00:02,  4.29it/s][A[A

 52% 12/23 [00:03<00:02,  4.66it/s][A[A

 57% 13/23 [00:04<00:05,  1.92it/s][A[A

 61% 14/23 [00:04<00:03,  2.40it/s][A[A

 65% 15/23 [00:05<00:03,  2.48it/s][A[A

 70% 16/23 [00:05<00:02,  2.76it/s][A[A

 74% 17/23 [00:05<00:01,  3.07it/s][A[A

 78% 18/23 [00:05<00:01,  3.22it/s][A[A

 83% 19/23 [00:06<00:01,  3.75it/s][A[A

 87% 20/23 [00:06<00:00,  4.07it/s][A[A

 91% 21/23 [00:06<00:00,  4.33it/s][A[A

 96% 22/23 [00:06<00:00,  4.56it/s][A[A

100% 23/23 [00:08<00:00,  1.84it/s][A[A100% 23/23 [00:08<00:00,  2.87it/s]
Meta loss on this task batch = 4.2472e-01, PNorm = 37.7658, GNorm = 0.0727

100% 19/19 [03:02<00:00,  9.55s/it][A100% 19/19 [03:02<00:00,  9.63s/it]
Took 182.9565885066986 seconds to complete one epoch of meta training

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/20 [00:00<?, ?it/s][A[A


  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 31.10it/s]


  5% 1/20 [00:00<00:03,  5.34it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 20.74it/s]


 10% 2/20 [00:00<00:03,  4.88it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 17.59it/s][A[A[A100% 3/3 [00:00<00:00, 19.52it/s]


 15% 3/20 [00:00<00:03,  4.29it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 15.80it/s]


 20% 4/20 [00:00<00:03,  4.44it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 15.58it/s][A[A[A100% 4/4 [00:00<00:00, 17.87it/s]


 25% 5/20 [00:01<00:04,  3.72it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.70it/s][A[A[A100% 4/4 [00:00<00:00, 16.43it/s]


 30% 6/20 [00:01<00:04,  3.28it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 17.98it/s][A[A[A100% 4/4 [00:00<00:00, 19.75it/s]


 35% 7/20 [00:02<00:04,  3.09it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 18.15it/s][A[A[A


100% 4/4 [00:00<00:00, 17.83it/s][A[A[A100% 4/4 [00:00<00:00, 17.60it/s]


 40% 8/20 [00:02<00:04,  2.86it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.77it/s][A[A[A100% 4/4 [00:00<00:00, 23.42it/s]


 45% 9/20 [00:02<00:03,  2.92it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.46it/s][A[A[A


100% 4/4 [00:01<00:00,  4.96it/s][A[A[A100% 4/4 [00:01<00:00,  3.31it/s]


 50% 10/20 [00:04<00:06,  1.54it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.57it/s][A[A[A


 75% 3/4 [00:00<00:00, 15.02it/s][A[A[A100% 4/4 [00:00<00:00, 17.07it/s]


 55% 11/20 [00:04<00:05,  1.72it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.48it/s][A[A[A100% 4/4 [00:00<00:00, 21.77it/s]


 60% 12/20 [00:04<00:04,  1.95it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.39it/s][A[A[A100% 3/3 [00:00<00:00, 13.72it/s]


 65% 13/20 [00:05<00:03,  2.09it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 13.08it/s][A[A[A100% 3/3 [00:00<00:00, 12.95it/s]


 70% 14/20 [00:05<00:02,  2.18it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:01<00:01,  1.64it/s][A[A[A100% 4/4 [00:01<00:00,  3.08it/s]


 75% 15/20 [00:07<00:03,  1.30it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 12.47it/s][A[A[A100% 3/3 [00:00<00:00, 15.46it/s]


 80% 16/20 [00:07<00:02,  1.54it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 11.52it/s][A[A[A100% 3/3 [00:00<00:00, 14.37it/s]


 85% 17/20 [00:08<00:01,  1.73it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A


100% 2/2 [00:00<00:00, 19.41it/s][A[A[A100% 2/2 [00:00<00:00, 19.36it/s]


 90% 18/20 [00:08<00:00,  2.04it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 15.40it/s][A[A[A100% 3/3 [00:00<00:00, 19.70it/s]


 95% 19/20 [00:08<00:00,  2.28it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 13.15it/s]


100% 20/20 [00:08<00:00,  2.65it/s][A[A100% 20/20 [00:08<00:00,  2.26it/s]

100% 1/1 [00:08<00:00,  8.87s/it][A100% 1/1 [00:08<00:00,  8.87s/it]
Took 191.8230791091919 seconds to complete one epoch of meta training and validating
Meta validation score auc = 0.615124
 53% 16/30 [51:32<45:04, 193.20s/it]Epoch 16

  0% 0/19 [00:00<?, ?it/s][A

  0% 0/32 [00:00<?, ?it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  3% 1/32 [00:00<00:06,  4.46it/s][A[A

  6% 2/32 [00:00<00:07,  4.20it/s][A[A

  9% 3/32 [00:00<00:06,  4.40it/s][A[A

 12% 4/32 [00:01<00:14,  1.99it/s][A[A

 16% 5/32 [00:01<00:10,  2.55it/s][A[A

 19% 6/32 [00:02<00:08,  3.04it/s][A[A

 22% 7/32 [00:02<00:06,  3.58it/s][A[A

 25% 8/32 [00:02<00:06,  3.96it/s][A[A

 28% 9/32 [00:02<00:04,  4.69it/s][A[A

 31% 10/32 [00:02<00:04,  5.29it/s][A[A

 34% 11/32 [00:03<00:04,  4.70it/s][A[A

 38% 12/32 [00:03<00:04,  4.33it/s][A[A

 41% 13/32 [00:03<00:04,  4.12it/s][A[A

 44% 14/32 [00:03<00:04,  4.12it/s][A[A

 47% 15/32 [00:04<00:03,  4.45it/s][A[A

 50% 16/32 [00:05<00:08,  1.91it/s][A[A

 53% 17/32 [00:05<00:06,  2.31it/s][A[A

 56% 18/32 [00:05<00:04,  2.85it/s][A[A

 59% 19/32 [00:05<00:03,  3.42it/s][A[A

 62% 20/32 [00:05<00:03,  3.67it/s][A[A

 66% 21/32 [00:06<00:02,  4.44it/s][A[A

 69% 22/32 [00:06<00:02,  4.61it/s][A[A

 72% 23/32 [00:06<00:01,  4.76it/s][A[A

 75% 24/32 [00:06<00:01,  4.67it/s][A[A

 78% 25/32 [00:06<00:01,  4.87it/s][A[A

 81% 26/32 [00:07<00:01,  4.84it/s][A[A

 84% 27/32 [00:07<00:01,  4.83it/s][A[A

 88% 28/32 [00:07<00:00,  5.21it/s][A[A

 91% 29/32 [00:07<00:00,  4.92it/s][A[A

 94% 30/32 [00:08<00:00,  2.09it/s][A[A

 97% 31/32 [00:09<00:00,  2.44it/s][A[A

100% 32/32 [00:09<00:00,  2.95it/s][A[A100% 32/32 [00:09<00:00,  3.46it/s]
Meta loss on this task batch = 5.2786e-01, PNorm = 37.7759, GNorm = 0.1116

  5% 1/19 [00:09<02:59,  9.98s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:03,  7.99it/s][A[A

  6% 2/32 [00:00<00:03,  7.63it/s][A[A

  9% 3/32 [00:00<00:04,  6.88it/s][A[A

 12% 4/32 [00:00<00:04,  5.70it/s][A[A

 16% 5/32 [00:00<00:04,  5.43it/s][A[A

 19% 6/32 [00:01<00:04,  5.85it/s][A[A

 22% 7/32 [00:01<00:04,  6.25it/s][A[A

 25% 8/32 [00:01<00:03,  6.53it/s][A[A

 28% 9/32 [00:01<00:03,  6.51it/s][A[A

 31% 10/32 [00:01<00:03,  6.85it/s][A[A

 34% 11/32 [00:01<00:03,  6.35it/s][A[A

 38% 12/32 [00:01<00:03,  6.09it/s][A[A

 41% 13/32 [00:02<00:03,  6.16it/s][A[A

 44% 14/32 [00:03<00:08,  2.07it/s][A[A

 47% 15/32 [00:03<00:06,  2.50it/s][A[A

 50% 16/32 [00:03<00:05,  2.93it/s][A[A

 53% 17/32 [00:03<00:04,  3.29it/s][A[A

 56% 18/32 [00:04<00:03,  3.55it/s][A[A

 59% 19/32 [00:04<00:03,  3.91it/s][A[A

 62% 20/32 [00:04<00:02,  4.04it/s][A[A

 66% 21/32 [00:04<00:02,  4.27it/s][A[A

 69% 22/32 [00:04<00:02,  4.85it/s][A[A

 72% 23/32 [00:05<00:01,  5.28it/s][A[A

 75% 24/32 [00:05<00:01,  5.78it/s][A[A

 78% 25/32 [00:05<00:01,  5.48it/s][A[A

 81% 26/32 [00:05<00:01,  5.29it/s][A[A

 84% 27/32 [00:05<00:00,  5.56it/s][A[A

 88% 28/32 [00:06<00:00,  5.16it/s][A[A

 91% 29/32 [00:07<00:01,  2.09it/s][A[A

 94% 30/32 [00:07<00:00,  2.49it/s][A[A

 97% 31/32 [00:07<00:00,  2.91it/s][A[A

100% 32/32 [00:07<00:00,  3.55it/s][A[A100% 32/32 [00:07<00:00,  4.12it/s]
Meta loss on this task batch = 5.1369e-01, PNorm = 37.7867, GNorm = 0.0604

 11% 2/19 [00:18<02:41,  9.53s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.90it/s][A[A

  6% 2/32 [00:00<00:05,  5.54it/s][A[A

  9% 3/32 [00:00<00:05,  5.14it/s][A[A

 12% 4/32 [00:00<00:05,  4.76it/s][A[A

 16% 5/32 [00:01<00:05,  5.13it/s][A[A

 19% 6/32 [00:01<00:05,  4.85it/s][A[A

 22% 7/32 [00:01<00:05,  4.71it/s][A[A

 25% 8/32 [00:01<00:04,  5.17it/s][A[A

 28% 9/32 [00:01<00:04,  5.60it/s][A[A

 31% 10/32 [00:02<00:10,  2.02it/s][A[A

 34% 11/32 [00:03<00:08,  2.38it/s][A[A

 38% 12/32 [00:03<00:07,  2.81it/s][A[A

 41% 13/32 [00:03<00:05,  3.17it/s][A[A

 44% 14/32 [00:03<00:05,  3.45it/s][A[A

 47% 15/32 [00:04<00:04,  3.68it/s][A[A

 50% 16/32 [00:04<00:03,  4.37it/s][A[A

 53% 17/32 [00:04<00:03,  4.37it/s][A[A

 56% 18/32 [00:04<00:02,  4.94it/s][A[A

 59% 19/32 [00:04<00:02,  5.27it/s][A[A

 62% 20/32 [00:04<00:02,  5.76it/s][A[A

 66% 21/32 [00:05<00:01,  6.29it/s][A[A

 69% 22/32 [00:05<00:01,  6.52it/s][A[A

 72% 23/32 [00:05<00:01,  5.62it/s][A[A

 75% 24/32 [00:06<00:03,  2.05it/s][A[A

 78% 25/32 [00:06<00:02,  2.51it/s][A[A

 81% 26/32 [00:07<00:02,  2.99it/s][A[A

 84% 27/32 [00:07<00:01,  3.27it/s][A[A

 88% 28/32 [00:07<00:01,  3.78it/s][A[A

 91% 29/32 [00:07<00:00,  3.93it/s][A[A

 94% 30/32 [00:07<00:00,  3.92it/s][A[A

 97% 31/32 [00:08<00:00,  4.69it/s][A[A

100% 32/32 [00:08<00:00,  4.72it/s][A[A100% 32/32 [00:08<00:00,  3.89it/s]
Meta loss on this task batch = 5.1344e-01, PNorm = 37.7996, GNorm = 0.0398

 16% 3/19 [00:27<02:29,  9.36s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.15it/s][A[A

  6% 2/32 [00:00<00:06,  4.30it/s][A[A

  9% 3/32 [00:00<00:06,  4.63it/s][A[A

 12% 4/32 [00:01<00:14,  1.93it/s][A[A

 16% 5/32 [00:02<00:11,  2.35it/s][A[A

 19% 6/32 [00:02<00:09,  2.70it/s][A[A

 22% 7/32 [00:02<00:07,  3.34it/s][A[A

 25% 8/32 [00:02<00:06,  3.51it/s][A[A

 28% 9/32 [00:02<00:05,  4.18it/s][A[A

 31% 10/32 [00:03<00:05,  4.27it/s][A[A

 34% 11/32 [00:03<00:04,  4.82it/s][A[A

 38% 12/32 [00:03<00:03,  5.38it/s][A[A

 41% 13/32 [00:03<00:03,  5.12it/s][A[A

 44% 14/32 [00:03<00:03,  5.00it/s][A[A

 47% 15/32 [00:03<00:03,  5.48it/s][A[A

 50% 16/32 [00:04<00:03,  5.23it/s][A[A

 53% 17/32 [00:04<00:02,  5.18it/s][A[A

 56% 18/32 [00:04<00:02,  4.98it/s][A[A

 59% 19/32 [00:04<00:02,  5.51it/s][A[A

 62% 20/32 [00:04<00:02,  5.20it/s][A[A

 66% 21/32 [00:06<00:05,  2.01it/s][A[A

 69% 22/32 [00:06<00:04,  2.49it/s][A[A

 72% 23/32 [00:06<00:03,  2.81it/s][A[A

 75% 24/32 [00:06<00:02,  3.16it/s][A[A

 78% 25/32 [00:06<00:01,  3.84it/s][A[A

 81% 26/32 [00:06<00:01,  4.57it/s][A[A

 84% 27/32 [00:07<00:01,  4.62it/s][A[A

 88% 28/32 [00:07<00:00,  4.75it/s][A[A

 91% 29/32 [00:07<00:00,  5.38it/s][A[A

 94% 30/32 [00:07<00:00,  5.06it/s][A[A

 97% 31/32 [00:07<00:00,  5.57it/s][A[A

100% 32/32 [00:08<00:00,  5.20it/s][A[A100% 32/32 [00:08<00:00,  3.95it/s]
Meta loss on this task batch = 5.7087e-01, PNorm = 37.8123, GNorm = 0.0546

 21% 4/19 [00:36<02:18,  9.21s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.44it/s][A[A

  6% 2/32 [00:00<00:05,  5.03it/s][A[A

  9% 3/32 [00:01<00:14,  1.96it/s][A[A

 12% 4/32 [00:01<00:12,  2.29it/s][A[A

 16% 5/32 [00:02<00:09,  2.71it/s][A[A

 19% 6/32 [00:02<00:08,  3.15it/s][A[A

 22% 7/32 [00:02<00:06,  3.70it/s][A[A

 25% 8/32 [00:02<00:05,  4.27it/s][A[A

 28% 9/32 [00:02<00:05,  4.40it/s][A[A

 31% 10/32 [00:02<00:04,  4.82it/s][A[A

 34% 11/32 [00:03<00:04,  4.77it/s][A[A

 38% 12/32 [00:03<00:04,  4.84it/s][A[A

 41% 13/32 [00:03<00:04,  4.68it/s][A[A

 44% 14/32 [00:03<00:03,  5.18it/s][A[A

 47% 15/32 [00:03<00:03,  5.06it/s][A[A

 50% 16/32 [00:04<00:02,  5.36it/s][A[A

 53% 17/32 [00:05<00:07,  2.08it/s][A[A

 56% 18/32 [00:05<00:05,  2.55it/s][A[A

 59% 19/32 [00:05<00:04,  2.89it/s][A[A

 62% 20/32 [00:05<00:03,  3.23it/s][A[A

 66% 21/32 [00:06<00:03,  3.53it/s][A[A

 69% 22/32 [00:06<00:02,  4.03it/s][A[A

 72% 23/32 [00:06<00:02,  4.23it/s][A[A

 75% 24/32 [00:06<00:01,  4.19it/s][A[A

 78% 25/32 [00:06<00:01,  4.32it/s][A[A

 81% 26/32 [00:07<00:01,  4.85it/s][A[A

 84% 27/32 [00:07<00:00,  5.03it/s][A[A

 88% 28/32 [00:07<00:00,  5.37it/s][A[A

 91% 29/32 [00:07<00:00,  4.93it/s][A[A

 94% 30/32 [00:07<00:00,  5.26it/s][A[A

 97% 31/32 [00:08<00:00,  5.35it/s][A[A

100% 32/32 [00:08<00:00,  5.06it/s][A[A100% 32/32 [00:08<00:00,  3.87it/s]
Meta loss on this task batch = 5.0760e-01, PNorm = 37.8248, GNorm = 0.0326

 26% 5/19 [00:45<02:08,  9.15s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:35,  1.14s/it][A[A

  6% 2/32 [00:01<00:25,  1.16it/s][A[A

  9% 3/32 [00:01<00:18,  1.55it/s][A[A

 12% 4/32 [00:01<00:14,  1.92it/s][A[A

 16% 5/32 [00:02<00:12,  2.18it/s][A[A

 19% 6/32 [00:02<00:10,  2.55it/s][A[A

 22% 7/32 [00:02<00:08,  3.02it/s][A[A

 25% 8/32 [00:02<00:07,  3.36it/s][A[A

 28% 9/32 [00:02<00:06,  3.80it/s][A[A

 31% 10/32 [00:03<00:05,  3.91it/s][A[A

 34% 11/32 [00:03<00:05,  4.05it/s][A[A

 38% 12/32 [00:04<00:10,  1.88it/s][A[A

 41% 13/32 [00:04<00:07,  2.41it/s][A[A

 44% 14/32 [00:04<00:06,  2.76it/s][A[A

 47% 15/32 [00:05<00:05,  3.12it/s][A[A

 50% 16/32 [00:05<00:04,  3.36it/s][A[A

 53% 17/32 [00:05<00:03,  3.94it/s][A[A

 56% 18/32 [00:05<00:03,  4.00it/s][A[A

 59% 19/32 [00:05<00:03,  4.11it/s][A[A

 62% 20/32 [00:06<00:02,  4.35it/s][A[A

 66% 21/32 [00:06<00:02,  4.53it/s][A[A

 69% 22/32 [00:06<00:02,  4.53it/s][A[A

 72% 23/32 [00:07<00:04,  1.92it/s][A[A

 75% 24/32 [00:07<00:03,  2.41it/s][A[A

 78% 25/32 [00:08<00:02,  2.79it/s][A[A

 81% 26/32 [00:08<00:01,  3.41it/s][A[A

 84% 27/32 [00:08<00:01,  3.61it/s][A[A

 88% 28/32 [00:08<00:01,  3.52it/s][A[A

 91% 29/32 [00:09<00:00,  3.68it/s][A[A

 94% 30/32 [00:09<00:00,  4.00it/s][A[A

 97% 31/32 [00:09<00:00,  4.21it/s][A[A

100% 32/32 [00:10<00:00,  1.91it/s][A[A100% 32/32 [00:10<00:00,  2.98it/s]
Meta loss on this task batch = 4.3674e-01, PNorm = 37.8372, GNorm = 0.0746

 32% 6/19 [00:56<02:08,  9.86s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.34it/s][A[A

  6% 2/32 [00:00<00:05,  5.30it/s][A[A

  9% 3/32 [00:00<00:05,  5.19it/s][A[A

 12% 4/32 [00:00<00:05,  4.87it/s][A[A

 16% 5/32 [00:00<00:04,  5.44it/s][A[A

 19% 6/32 [00:01<00:04,  5.32it/s][A[A

 22% 7/32 [00:01<00:04,  5.01it/s][A[A

 25% 8/32 [00:01<00:04,  4.93it/s][A[A

 28% 9/32 [00:01<00:04,  5.03it/s][A[A

 31% 10/32 [00:02<00:04,  4.82it/s][A[A

 34% 11/32 [00:02<00:04,  4.71it/s][A[A

 38% 12/32 [00:03<00:10,  1.95it/s][A[A

 41% 13/32 [00:03<00:08,  2.34it/s][A[A

 44% 14/32 [00:03<00:06,  2.91it/s][A[A

 47% 15/32 [00:04<00:05,  3.25it/s][A[A

 50% 16/32 [00:04<00:04,  3.52it/s][A[A

 53% 17/32 [00:04<00:04,  3.75it/s][A[A

 56% 18/32 [00:04<00:03,  3.94it/s][A[A

 59% 19/32 [00:04<00:02,  4.35it/s][A[A

 62% 20/32 [00:05<00:02,  4.30it/s][A[A

 66% 21/32 [00:05<00:02,  4.34it/s][A[A

 69% 22/32 [00:05<00:02,  4.52it/s][A[A

 72% 23/32 [00:05<00:02,  4.39it/s][A[A

 75% 24/32 [00:07<00:04,  1.90it/s][A[A

 78% 25/32 [00:07<00:03,  2.31it/s][A[A

 81% 26/32 [00:07<00:02,  2.67it/s][A[A

 84% 27/32 [00:07<00:01,  3.05it/s][A[A

 88% 28/32 [00:07<00:01,  3.32it/s][A[A

 91% 29/32 [00:08<00:00,  3.68it/s][A[A

 94% 30/32 [00:08<00:00,  4.05it/s][A[A

 97% 31/32 [00:08<00:00,  4.25it/s][A[A

100% 32/32 [00:08<00:00,  4.32it/s][A[A100% 32/32 [00:08<00:00,  3.65it/s]
Meta loss on this task batch = 4.7111e-01, PNorm = 37.8525, GNorm = 0.1060

 37% 7/19 [01:06<01:57,  9.77s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  3.98it/s][A[A

  6% 2/32 [00:00<00:07,  3.95it/s][A[A

  9% 3/32 [00:00<00:07,  3.97it/s][A[A

 12% 4/32 [00:01<00:15,  1.84it/s][A[A

 16% 5/32 [00:02<00:12,  2.24it/s][A[A

 19% 6/32 [00:02<00:10,  2.60it/s][A[A

 22% 7/32 [00:02<00:08,  3.00it/s][A[A

 25% 8/32 [00:02<00:07,  3.22it/s][A[A

 28% 9/32 [00:03<00:06,  3.49it/s][A[A

 31% 10/32 [00:03<00:05,  3.70it/s][A[A

 34% 11/32 [00:03<00:05,  3.94it/s][A[A

 38% 12/32 [00:03<00:04,  4.15it/s][A[A

 41% 13/32 [00:04<00:04,  4.33it/s][A[A

 44% 14/32 [00:04<00:04,  4.40it/s][A[A

 47% 15/32 [00:05<00:08,  1.91it/s][A[A

 50% 16/32 [00:05<00:06,  2.31it/s][A[A

 53% 17/32 [00:05<00:05,  2.64it/s][A[A

 56% 18/32 [00:06<00:04,  3.02it/s][A[A

 59% 19/32 [00:06<00:04,  3.23it/s][A[A

 62% 20/32 [00:06<00:03,  3.54it/s][A[A

 66% 21/32 [00:06<00:02,  3.76it/s][A[A

 69% 22/32 [00:07<00:02,  4.17it/s][A[A

 72% 23/32 [00:07<00:02,  4.33it/s][A[A

 75% 24/32 [00:07<00:01,  4.28it/s][A[A

 78% 25/32 [00:08<00:03,  1.90it/s][A[A

 81% 26/32 [00:08<00:02,  2.25it/s][A[A

 84% 27/32 [00:09<00:01,  2.72it/s][A[A

 88% 28/32 [00:09<00:01,  3.20it/s][A[A

 91% 29/32 [00:09<00:00,  3.63it/s][A[A

 94% 30/32 [00:09<00:00,  4.08it/s][A[A

 97% 31/32 [00:09<00:00,  4.40it/s][A[A

100% 32/32 [00:10<00:00,  4.69it/s][A[A100% 32/32 [00:10<00:00,  3.19it/s]
Meta loss on this task batch = 3.2912e-01, PNorm = 37.8719, GNorm = 0.0900

 42% 8/19 [01:17<01:51, 10.09s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.47it/s][A[A

  6% 2/32 [00:00<00:05,  5.34it/s][A[A

  9% 3/32 [00:00<00:05,  5.30it/s][A[A

 12% 4/32 [00:00<00:05,  5.18it/s][A[A

 16% 5/32 [00:01<00:13,  2.05it/s][A[A

 19% 6/32 [00:02<00:10,  2.51it/s][A[A

 22% 7/32 [00:02<00:08,  2.98it/s][A[A

 25% 8/32 [00:02<00:07,  3.39it/s][A[A

 28% 9/32 [00:02<00:06,  3.81it/s][A[A

 31% 10/32 [00:02<00:05,  4.18it/s][A[A

 34% 11/32 [00:03<00:04,  4.49it/s][A[A

 38% 12/32 [00:03<00:04,  4.60it/s][A[A

 41% 13/32 [00:03<00:03,  4.94it/s][A[A

 44% 14/32 [00:03<00:03,  5.06it/s][A[A

 47% 15/32 [00:03<00:03,  5.15it/s][A[A

 50% 16/32 [00:04<00:03,  5.09it/s][A[A

 53% 17/32 [00:04<00:02,  5.13it/s][A[A

 56% 18/32 [00:04<00:02,  5.07it/s][A[A

 59% 19/32 [00:04<00:02,  5.24it/s][A[A

 62% 20/32 [00:04<00:02,  5.31it/s][A[A

 66% 21/32 [00:05<00:05,  2.04it/s][A[A

 69% 22/32 [00:06<00:04,  2.50it/s][A[A

 72% 23/32 [00:06<00:03,  2.86it/s][A[A

 75% 24/32 [00:06<00:02,  3.30it/s][A[A

 78% 25/32 [00:06<00:01,  3.64it/s][A[A

 81% 26/32 [00:06<00:01,  4.04it/s][A[A

 84% 27/32 [00:07<00:01,  4.44it/s][A[A

 88% 28/32 [00:07<00:00,  4.71it/s][A[A

 91% 29/32 [00:07<00:00,  4.92it/s][A[A

 94% 30/32 [00:07<00:00,  4.97it/s][A[A

 97% 31/32 [00:07<00:00,  5.16it/s][A[A

100% 32/32 [00:08<00:00,  5.18it/s][A[A100% 32/32 [00:08<00:00,  3.95it/s]
Meta loss on this task batch = 5.6007e-02, PNorm = 37.8952, GNorm = 0.1357

 47% 9/19 [01:26<01:37,  9.73s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.86it/s][A[A

  6% 2/32 [00:01<00:15,  1.98it/s][A[A

  9% 3/32 [00:01<00:11,  2.44it/s][A[A

 12% 4/32 [00:01<00:09,  2.94it/s][A[A

 16% 5/32 [00:01<00:07,  3.39it/s][A[A

 19% 6/32 [00:02<00:06,  3.85it/s][A[A

 22% 7/32 [00:02<00:06,  4.15it/s][A[A

 25% 8/32 [00:02<00:05,  4.41it/s][A[A

 28% 9/32 [00:02<00:04,  4.66it/s][A[A

 31% 10/32 [00:02<00:04,  4.92it/s][A[A

 34% 11/32 [00:03<00:04,  5.05it/s][A[A

 38% 12/32 [00:03<00:03,  5.10it/s][A[A

 41% 13/32 [00:03<00:03,  5.23it/s][A[A

 44% 14/32 [00:03<00:03,  5.13it/s][A[A

 47% 15/32 [00:03<00:03,  5.24it/s][A[A

 50% 16/32 [00:04<00:03,  4.79it/s][A[A

 53% 17/32 [00:04<00:03,  4.89it/s][A[A

 56% 18/32 [00:05<00:07,  2.00it/s][A[A

 59% 19/32 [00:05<00:05,  2.38it/s][A[A

 62% 20/32 [00:05<00:04,  2.84it/s][A[A

 66% 21/32 [00:06<00:03,  3.32it/s][A[A

 69% 22/32 [00:06<00:02,  3.76it/s][A[A

 72% 23/32 [00:06<00:02,  4.14it/s][A[A

 75% 24/32 [00:06<00:01,  4.57it/s][A[A

 78% 25/32 [00:06<00:01,  4.78it/s][A[A

 81% 26/32 [00:07<00:01,  4.55it/s][A[A

 84% 27/32 [00:07<00:01,  4.49it/s][A[A

 88% 28/32 [00:07<00:00,  4.40it/s][A[A

 91% 29/32 [00:08<00:01,  1.89it/s][A[A

 94% 30/32 [00:08<00:00,  2.26it/s][A[A

 97% 31/32 [00:09<00:00,  2.63it/s][A[A

100% 32/32 [00:09<00:00,  2.98it/s][A[A100% 32/32 [00:09<00:00,  3.38it/s]
Meta loss on this task batch = 1.7993e-01, PNorm = 37.9215, GNorm = 0.0783

 53% 10/19 [01:36<01:29,  9.89s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  3.98it/s][A[A

  6% 2/32 [00:00<00:07,  4.01it/s][A[A

  9% 3/32 [00:00<00:07,  4.07it/s][A[A

 12% 4/32 [00:01<00:15,  1.84it/s][A[A

 16% 5/32 [00:02<00:12,  2.21it/s][A[A

 19% 6/32 [00:02<00:10,  2.59it/s][A[A

 22% 7/32 [00:02<00:08,  2.92it/s][A[A

 25% 8/32 [00:02<00:07,  3.18it/s][A[A

 28% 9/32 [00:03<00:06,  3.42it/s][A[A

 31% 10/32 [00:03<00:06,  3.59it/s][A[A

 34% 11/32 [00:03<00:05,  3.77it/s][A[A

 38% 12/32 [00:03<00:05,  3.87it/s][A[A

 41% 13/32 [00:04<00:04,  3.97it/s][A[A

 44% 14/32 [00:04<00:04,  4.00it/s][A[A

 47% 15/32 [00:05<00:09,  1.82it/s][A[A

 50% 16/32 [00:05<00:07,  2.18it/s][A[A

 53% 17/32 [00:06<00:05,  2.52it/s][A[A

 56% 18/32 [00:06<00:04,  2.83it/s][A[A

 59% 19/32 [00:06<00:04,  3.13it/s][A[A

 62% 20/32 [00:06<00:03,  3.33it/s][A[A

 66% 21/32 [00:07<00:03,  3.55it/s][A[A

 69% 22/32 [00:07<00:02,  3.70it/s][A[A

 72% 23/32 [00:07<00:02,  3.82it/s][A[A

 75% 24/32 [00:08<00:04,  1.84it/s][A[A

 78% 25/32 [00:09<00:03,  2.22it/s][A[A

 81% 26/32 [00:09<00:02,  2.59it/s][A[A

 84% 27/32 [00:09<00:01,  2.94it/s][A[A

 88% 28/32 [00:09<00:01,  3.22it/s][A[A

 91% 29/32 [00:09<00:00,  3.45it/s][A[A

 94% 30/32 [00:10<00:00,  3.66it/s][A[A

 97% 31/32 [00:10<00:00,  3.77it/s][A[A

100% 32/32 [00:10<00:00,  4.16it/s][A[A100% 32/32 [00:10<00:00,  3.01it/s]
Meta loss on this task batch = 5.7086e-01, PNorm = 37.9488, GNorm = 0.0418

 58% 11/19 [01:47<01:22, 10.37s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.08it/s][A[A

  6% 2/32 [00:00<00:06,  4.31it/s][A[A

  9% 3/32 [00:01<00:15,  1.87it/s][A[A

 12% 4/32 [00:01<00:12,  2.24it/s][A[A

 16% 5/32 [00:02<00:10,  2.59it/s][A[A

 19% 6/32 [00:02<00:08,  2.91it/s][A[A

 22% 7/32 [00:02<00:07,  3.26it/s][A[A

 25% 8/32 [00:02<00:06,  3.52it/s][A[A

 28% 9/32 [00:03<00:06,  3.71it/s][A[A

 31% 10/32 [00:03<00:05,  3.85it/s][A[A

 34% 11/32 [00:04<00:11,  1.81it/s][A[A

 38% 12/32 [00:04<00:09,  2.19it/s][A[A

 41% 13/32 [00:05<00:07,  2.55it/s][A[A

 44% 14/32 [00:05<00:06,  2.91it/s][A[A

 47% 15/32 [00:05<00:05,  3.24it/s][A[A

 50% 16/32 [00:05<00:04,  3.53it/s][A[A

 53% 17/32 [00:05<00:04,  3.67it/s][A[A

 56% 18/32 [00:06<00:03,  3.84it/s][A[A

 59% 19/32 [00:06<00:03,  4.29it/s][A[A

 62% 20/32 [00:06<00:02,  4.27it/s][A[A

 66% 21/32 [00:07<00:05,  1.89it/s][A[A

 69% 22/32 [00:08<00:04,  2.27it/s][A[A

 72% 23/32 [00:08<00:03,  2.66it/s][A[A

 75% 24/32 [00:08<00:02,  3.00it/s][A[A

 78% 25/32 [00:08<00:02,  3.31it/s][A[A

 81% 26/32 [00:08<00:01,  3.60it/s][A[A

 84% 27/32 [00:09<00:01,  3.82it/s][A[A

 88% 28/32 [00:09<00:01,  3.99it/s][A[A

 91% 29/32 [00:09<00:00,  4.13it/s][A[A

 94% 30/32 [00:09<00:00,  4.57it/s][A[A

 97% 31/32 [00:10<00:00,  4.55it/s][A[A

100% 32/32 [00:11<00:00,  1.92it/s][A[A100% 32/32 [00:11<00:00,  2.84it/s]
Meta loss on this task batch = 5.6316e-01, PNorm = 37.9755, GNorm = 0.0638

 63% 12/19 [01:59<01:16, 10.88s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.51it/s][A[A

  6% 2/32 [00:00<00:06,  4.38it/s][A[A

  9% 3/32 [00:00<00:06,  4.35it/s][A[A

 12% 4/32 [00:00<00:06,  4.29it/s][A[A

 16% 5/32 [00:01<00:06,  4.31it/s][A[A

 19% 6/32 [00:02<00:13,  1.87it/s][A[A

 22% 7/32 [00:02<00:11,  2.24it/s][A[A

 25% 8/32 [00:02<00:09,  2.59it/s][A[A

 28% 9/32 [00:03<00:07,  2.97it/s][A[A

 31% 10/32 [00:03<00:06,  3.28it/s][A[A

 34% 11/32 [00:03<00:05,  3.54it/s][A[A

 38% 12/32 [00:03<00:05,  3.77it/s][A[A

 41% 13/32 [00:04<00:04,  3.91it/s][A[A

 44% 14/32 [00:04<00:04,  3.95it/s][A[A

 47% 15/32 [00:04<00:04,  4.00it/s][A[A

 50% 16/32 [00:05<00:08,  1.84it/s][A[A

 53% 17/32 [00:05<00:06,  2.21it/s][A[A

 56% 18/32 [00:06<00:05,  2.71it/s][A[A

 59% 19/32 [00:06<00:04,  3.03it/s][A[A

 62% 20/32 [00:06<00:03,  3.29it/s][A[A

 66% 21/32 [00:06<00:03,  3.51it/s][A[A

 69% 22/32 [00:07<00:02,  3.69it/s][A[A

 72% 23/32 [00:07<00:02,  4.09it/s][A[A

 75% 24/32 [00:08<00:04,  1.84it/s][A[A

 78% 25/32 [00:08<00:03,  2.21it/s][A[A

 81% 26/32 [00:09<00:02,  2.57it/s][A[A

 84% 27/32 [00:09<00:01,  2.93it/s][A[A

 88% 28/32 [00:09<00:01,  3.19it/s][A[A

 91% 29/32 [00:09<00:00,  3.44it/s][A[A

 94% 30/32 [00:09<00:00,  3.65it/s][A[A

 97% 31/32 [00:10<00:00,  3.83it/s][A[A

100% 32/32 [00:11<00:00,  1.80it/s][A[A100% 32/32 [00:11<00:00,  2.79it/s]
Meta loss on this task batch = 5.6984e-01, PNorm = 38.0001, GNorm = 0.0408

 68% 13/19 [02:12<01:07, 11.31s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  3.97it/s][A[A

  6% 2/32 [00:00<00:07,  4.05it/s][A[A

  9% 3/32 [00:00<00:07,  4.14it/s][A[A

 12% 4/32 [00:00<00:06,  4.26it/s][A[A

 16% 5/32 [00:01<00:06,  4.37it/s][A[A

 19% 6/32 [00:01<00:05,  4.42it/s][A[A

 22% 7/32 [00:01<00:05,  4.40it/s][A[A

 25% 8/32 [00:01<00:05,  4.45it/s][A[A

 28% 9/32 [00:01<00:04,  4.83it/s][A[A

 31% 10/32 [00:03<00:11,  1.95it/s][A[A

 34% 11/32 [00:03<00:08,  2.34it/s][A[A

 38% 12/32 [00:03<00:07,  2.69it/s][A[A

 41% 13/32 [00:03<00:06,  3.05it/s][A[A

 44% 14/32 [00:04<00:05,  3.34it/s][A[A

 47% 15/32 [00:04<00:04,  3.57it/s][A[A

 50% 16/32 [00:04<00:04,  3.76it/s][A[A

 53% 17/32 [00:04<00:03,  3.89it/s][A[A

 56% 18/32 [00:05<00:03,  3.98it/s][A[A

 59% 19/32 [00:05<00:03,  4.15it/s][A[A

 62% 20/32 [00:05<00:02,  4.13it/s][A[A

 66% 21/32 [00:05<00:02,  4.18it/s][A[A

 69% 22/32 [00:05<00:02,  4.43it/s][A[A

 72% 23/32 [00:06<00:02,  4.45it/s][A[A

 75% 24/32 [00:06<00:01,  4.45it/s][A[A

 78% 25/32 [00:06<00:01,  4.31it/s][A[A

 81% 26/32 [00:06<00:01,  4.45it/s][A[A

 84% 27/32 [00:07<00:01,  4.71it/s][A[A

 88% 28/32 [00:08<00:02,  1.95it/s][A[A

 91% 29/32 [00:08<00:01,  2.32it/s][A[A

 94% 30/32 [00:08<00:00,  2.73it/s][A[A

 97% 31/32 [00:08<00:00,  3.05it/s][A[A

100% 32/32 [00:09<00:00,  3.26it/s][A[A100% 32/32 [00:09<00:00,  3.47it/s]
Meta loss on this task batch = 5.6467e-01, PNorm = 38.0200, GNorm = 0.0906

 74% 14/19 [02:22<00:54, 10.93s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.11it/s][A[A

  6% 2/32 [00:00<00:07,  4.02it/s][A[A

  9% 3/32 [00:01<00:15,  1.83it/s][A[A

 12% 4/32 [00:01<00:12,  2.22it/s][A[A

 16% 5/32 [00:02<00:10,  2.64it/s][A[A

 19% 6/32 [00:02<00:09,  2.87it/s][A[A

 22% 7/32 [00:02<00:07,  3.19it/s][A[A

 25% 8/32 [00:02<00:06,  3.67it/s][A[A

 28% 9/32 [00:03<00:06,  3.75it/s][A[A

 31% 10/32 [00:03<00:05,  3.96it/s][A[A

 34% 11/32 [00:03<00:05,  4.15it/s][A[A

 38% 12/32 [00:03<00:04,  4.55it/s][A[A

 41% 13/32 [00:03<00:04,  4.52it/s][A[A

 44% 14/32 [00:05<00:09,  1.92it/s][A[A

 47% 15/32 [00:05<00:07,  2.40it/s][A[A

 50% 16/32 [00:05<00:05,  2.80it/s][A[A

 53% 17/32 [00:05<00:04,  3.11it/s][A[A

 56% 18/32 [00:06<00:04,  3.40it/s][A[A

 59% 19/32 [00:06<00:03,  3.99it/s][A[A

 62% 20/32 [00:06<00:02,  4.05it/s][A[A

 66% 21/32 [00:06<00:02,  4.42it/s][A[A

 69% 22/32 [00:06<00:02,  4.43it/s][A[A

 72% 23/32 [00:07<00:02,  4.29it/s][A[A

 75% 24/32 [00:07<00:01,  4.42it/s][A[A

 78% 25/32 [00:07<00:01,  4.42it/s][A[A

 81% 26/32 [00:08<00:03,  1.86it/s][A[A

 84% 27/32 [00:09<00:02,  2.24it/s][A[A

 88% 28/32 [00:09<00:01,  2.61it/s][A[A

 91% 29/32 [00:09<00:01,  2.98it/s][A[A

 94% 30/32 [00:09<00:00,  3.27it/s][A[A

 97% 31/32 [00:09<00:00,  3.54it/s][A[A

100% 32/32 [00:10<00:00,  3.79it/s][A[A100% 32/32 [00:10<00:00,  3.15it/s]
Meta loss on this task batch = 4.6872e-01, PNorm = 38.0390, GNorm = 0.0621

 79% 15/19 [02:33<00:43, 10.94s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.15it/s][A[A

  6% 2/32 [00:00<00:07,  4.24it/s][A[A

  9% 3/32 [00:00<00:06,  4.31it/s][A[A

 12% 4/32 [00:00<00:06,  4.31it/s][A[A

 16% 5/32 [00:01<00:06,  4.33it/s][A[A

 19% 6/32 [00:02<00:13,  1.90it/s][A[A

 22% 7/32 [00:02<00:10,  2.30it/s][A[A

 25% 8/32 [00:02<00:09,  2.66it/s][A[A

 28% 9/32 [00:03<00:07,  3.00it/s][A[A

 31% 10/32 [00:03<00:06,  3.28it/s][A[A

 34% 11/32 [00:03<00:05,  3.50it/s][A[A

 38% 12/32 [00:03<00:05,  3.72it/s][A[A

 41% 13/32 [00:03<00:04,  3.90it/s][A[A

 44% 14/32 [00:04<00:04,  4.09it/s][A[A

 47% 15/32 [00:04<00:04,  4.17it/s][A[A

 50% 16/32 [00:05<00:08,  1.95it/s][A[A

 53% 17/32 [00:05<00:06,  2.33it/s][A[A

 56% 18/32 [00:06<00:04,  2.81it/s][A[A

 59% 19/32 [00:06<00:04,  3.13it/s][A[A

 62% 20/32 [00:06<00:03,  3.40it/s][A[A

 66% 21/32 [00:06<00:03,  3.66it/s][A[A

 69% 22/32 [00:06<00:02,  4.00it/s][A[A

 72% 23/32 [00:07<00:02,  4.08it/s][A[A

 75% 24/32 [00:07<00:02,  3.98it/s][A[A

 78% 25/32 [00:07<00:01,  4.28it/s][A[A

 81% 26/32 [00:07<00:01,  4.24it/s][A[A

 84% 27/32 [00:09<00:02,  1.85it/s][A[A

 88% 28/32 [00:09<00:01,  2.25it/s][A[A

 91% 29/32 [00:09<00:01,  2.54it/s][A[A

 94% 30/32 [00:09<00:00,  2.88it/s][A[A

 97% 31/32 [00:10<00:00,  3.21it/s][A[A

100% 32/32 [00:10<00:00,  3.51it/s][A[A100% 32/32 [00:10<00:00,  3.12it/s]
Meta loss on this task batch = 5.5236e-01, PNorm = 38.0520, GNorm = 0.0715

 84% 16/19 [02:44<00:32, 10.98s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.30it/s][A[A

  6% 2/32 [00:00<00:06,  4.81it/s][A[A

  9% 3/32 [00:00<00:06,  4.67it/s][A[A

 12% 4/32 [00:01<00:14,  1.99it/s][A[A

 16% 5/32 [00:02<00:11,  2.35it/s][A[A

 19% 6/32 [00:02<00:08,  2.96it/s][A[A

 22% 7/32 [00:02<00:07,  3.31it/s][A[A

 25% 8/32 [00:02<00:06,  3.47it/s][A[A

 28% 9/32 [00:02<00:06,  3.72it/s][A[A

 31% 10/32 [00:03<00:05,  4.23it/s][A[A

 34% 11/32 [00:03<00:04,  4.34it/s][A[A

 38% 12/32 [00:03<00:04,  4.55it/s][A[A

 41% 13/32 [00:03<00:04,  4.51it/s][A[A

 44% 14/32 [00:03<00:04,  4.49it/s][A[A

 47% 15/32 [00:05<00:08,  1.89it/s][A[A

 50% 16/32 [00:05<00:07,  2.25it/s][A[A

 53% 17/32 [00:05<00:05,  2.73it/s][A[A

 56% 18/32 [00:05<00:04,  3.04it/s][A[A

 59% 19/32 [00:06<00:04,  3.22it/s][A[A

 62% 20/32 [00:06<00:03,  3.49it/s][A[A

 66% 21/32 [00:06<00:02,  3.72it/s][A[A

 69% 22/32 [00:06<00:02,  3.92it/s][A[A

 72% 23/32 [00:07<00:02,  4.02it/s][A[A

 75% 24/32 [00:08<00:04,  1.78it/s][A[A

 78% 25/32 [00:08<00:03,  2.19it/s][A[A

 81% 26/32 [00:08<00:02,  2.58it/s][A[A

 84% 27/32 [00:08<00:01,  3.08it/s][A[A

 88% 28/32 [00:09<00:01,  3.52it/s][A[A

 91% 29/32 [00:09<00:00,  3.74it/s][A[A

 94% 30/32 [00:09<00:00,  4.13it/s][A[A

 97% 31/32 [00:09<00:00,  4.34it/s][A[A

100% 32/32 [00:09<00:00,  4.31it/s][A[A100% 32/32 [00:09<00:00,  3.20it/s]
Meta loss on this task batch = 4.5069e-01, PNorm = 38.0609, GNorm = 0.1025

 89% 17/19 [02:55<00:21, 10.92s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.29it/s][A[A

  6% 2/32 [00:00<00:06,  4.95it/s][A[A

  9% 3/32 [00:01<00:14,  1.96it/s][A[A

 12% 4/32 [00:01<00:11,  2.35it/s][A[A

 16% 5/32 [00:02<00:09,  2.71it/s][A[A

 19% 6/32 [00:02<00:08,  3.09it/s][A[A

 22% 7/32 [00:02<00:07,  3.31it/s][A[A

 25% 8/32 [00:02<00:06,  3.49it/s][A[A

 28% 9/32 [00:03<00:06,  3.74it/s][A[A

 31% 10/32 [00:03<00:05,  3.89it/s][A[A

 34% 11/32 [00:03<00:05,  4.04it/s][A[A

 38% 12/32 [00:03<00:04,  4.42it/s][A[A

 41% 13/32 [00:03<00:04,  4.22it/s][A[A

 44% 14/32 [00:04<00:03,  4.75it/s][A[A

 47% 15/32 [00:05<00:08,  1.89it/s][A[A

 50% 16/32 [00:05<00:06,  2.31it/s][A[A

 53% 17/32 [00:05<00:05,  2.92it/s][A[A

 56% 18/32 [00:05<00:04,  3.31it/s][A[A

 59% 19/32 [00:06<00:03,  3.54it/s][A[A

 62% 20/32 [00:06<00:03,  3.76it/s][A[A

 66% 21/32 [00:06<00:02,  3.88it/s][A[A

 69% 22/32 [00:06<00:02,  4.50it/s][A[A

 72% 23/32 [00:06<00:01,  4.68it/s][A[A

 75% 24/32 [00:07<00:01,  4.20it/s][A[A

 78% 25/32 [00:07<00:01,  4.29it/s][A[A

 81% 26/32 [00:08<00:03,  1.88it/s][A[A

 84% 27/32 [00:08<00:02,  2.19it/s][A[A

 88% 28/32 [00:09<00:01,  2.57it/s][A[A

 91% 29/32 [00:09<00:01,  2.92it/s][A[A

 94% 30/32 [00:09<00:00,  3.19it/s][A[A

 97% 31/32 [00:09<00:00,  3.54it/s][A[A

100% 32/32 [00:10<00:00,  3.61it/s][A[A100% 32/32 [00:10<00:00,  3.15it/s]
Meta loss on this task batch = 5.0041e-01, PNorm = 38.0669, GNorm = 0.0842

 95% 18/19 [03:06<00:10, 10.94s/it][A

  0% 0/23 [00:00<?, ?it/s][A[A

  4% 1/23 [00:00<00:05,  3.99it/s][A[A

  9% 2/23 [00:01<00:11,  1.89it/s][A[A

 13% 3/23 [00:01<00:08,  2.31it/s][A[A

 17% 4/23 [00:01<00:06,  2.76it/s][A[A

 22% 5/23 [00:02<00:05,  3.22it/s][A[A

 26% 6/23 [00:02<00:04,  3.54it/s][A[A

 30% 7/23 [00:02<00:04,  3.70it/s][A[A

 35% 8/23 [00:02<00:03,  3.84it/s][A[A

 39% 9/23 [00:02<00:03,  4.51it/s][A[A

 43% 10/23 [00:03<00:03,  4.30it/s][A[A

 48% 11/23 [00:03<00:02,  4.54it/s][A[A

 52% 12/23 [00:03<00:02,  4.44it/s][A[A

 57% 13/23 [00:03<00:01,  5.01it/s][A[A

 61% 14/23 [00:04<00:04,  1.95it/s][A[A

 65% 15/23 [00:05<00:03,  2.12it/s][A[A

 70% 16/23 [00:05<00:02,  2.45it/s][A[A

 74% 17/23 [00:05<00:02,  2.99it/s][A[A

 78% 18/23 [00:05<00:01,  3.24it/s][A[A

 83% 19/23 [00:06<00:01,  3.51it/s][A[A

 87% 20/23 [00:06<00:00,  4.22it/s][A[A

 91% 21/23 [00:06<00:00,  4.65it/s][A[A

 96% 22/23 [00:06<00:00,  5.35it/s][A[A

100% 23/23 [00:06<00:00,  5.63it/s][A[A100% 23/23 [00:06<00:00,  3.40it/s]
Meta loss on this task batch = 4.2589e-01, PNorm = 38.0716, GNorm = 0.0641

100% 19/19 [03:13<00:00,  9.86s/it][A100% 19/19 [03:13<00:00, 10.18s/it]
Took 193.44769716262817 seconds to complete one epoch of meta training

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/20 [00:00<?, ?it/s][A[A


  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 31.19it/s]


  5% 1/20 [00:00<00:02,  7.83it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 20.64it/s]


 10% 2/20 [00:00<00:03,  5.98it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 18.57it/s][A[A[A100% 3/3 [00:00<00:00, 20.56it/s]


 15% 3/20 [00:00<00:03,  5.10it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 16.18it/s]


 20% 4/20 [00:00<00:03,  4.94it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 14.70it/s][A[A[A100% 4/4 [00:00<00:00, 17.24it/s]


 25% 5/20 [00:02<00:08,  1.79it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.65it/s][A[A[A100% 4/4 [00:00<00:00, 16.40it/s]


 30% 6/20 [00:02<00:07,  1.96it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.13it/s][A[A[A100% 4/4 [00:00<00:00, 20.75it/s]


 35% 7/20 [00:03<00:06,  2.11it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.41it/s][A[A[A


100% 4/4 [00:00<00:00, 19.09it/s][A[A[A100% 4/4 [00:00<00:00, 18.85it/s]


 40% 8/20 [00:03<00:05,  2.23it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.59it/s][A[A[A100% 4/4 [00:00<00:00, 23.27it/s]


 45% 9/20 [00:03<00:04,  2.38it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.29it/s][A[A[A


100% 4/4 [00:00<00:00, 19.30it/s][A[A[A100% 4/4 [00:00<00:00, 19.27it/s]


 50% 10/20 [00:04<00:04,  2.48it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:01<00:00,  2.55it/s][A[A[A100% 4/4 [00:01<00:00,  3.31it/s]


 55% 11/20 [00:05<00:06,  1.44it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.81it/s][A[A[A100% 4/4 [00:00<00:00, 22.24it/s]


 60% 12/20 [00:05<00:04,  1.69it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.20it/s][A[A[A100% 3/3 [00:00<00:00, 13.48it/s]


 65% 13/20 [00:06<00:03,  1.88it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 13.58it/s][A[A[A100% 3/3 [00:00<00:00, 13.19it/s]


 70% 14/20 [00:06<00:02,  2.03it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.51it/s][A[A[A100% 4/4 [00:00<00:00, 17.71it/s]


 75% 15/20 [00:07<00:02,  2.15it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 33% 1/3 [00:01<00:02,  1.07s/it][A[A[A


100% 3/3 [00:01<00:00,  1.31it/s][A[A[A100% 3/3 [00:01<00:00,  2.55it/s]


 80% 16/20 [00:08<00:02,  1.37it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 11.98it/s][A[A[A100% 3/3 [00:00<00:00, 14.78it/s]


 85% 17/20 [00:08<00:01,  1.59it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 22.86it/s]


 90% 18/20 [00:09<00:01,  1.91it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 16.39it/s][A[A[A100% 3/3 [00:00<00:00, 20.74it/s]


 95% 19/20 [00:09<00:00,  2.20it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 13.63it/s]


100% 20/20 [00:09<00:00,  2.56it/s][A[A100% 20/20 [00:09<00:00,  2.08it/s]

100% 1/1 [00:09<00:00,  9.63s/it][A100% 1/1 [00:09<00:00,  9.63s/it]
Took 203.07864618301392 seconds to complete one epoch of meta training and validating
Meta validation score auc = 0.634052
Found better MAML checkpoint after meta validation, saving now
 57% 17/30 [54:55<42:30, 196.17s/it]Epoch 17

  0% 0/19 [00:00<?, ?it/s][A

  0% 0/32 [00:00<?, ?it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  3% 1/32 [00:00<00:05,  6.06it/s][A[A

  6% 2/32 [00:00<00:05,  5.61it/s][A[A

  9% 3/32 [00:00<00:05,  5.62it/s][A[A

 12% 4/32 [00:00<00:04,  5.85it/s][A[A

 16% 5/32 [00:00<00:04,  6.20it/s][A[A

 19% 6/32 [00:01<00:04,  6.01it/s][A[A

 22% 7/32 [00:01<00:04,  6.02it/s][A[A

 25% 8/32 [00:02<00:11,  2.10it/s][A[A

 28% 9/32 [00:02<00:08,  2.70it/s][A[A

 31% 10/32 [00:02<00:06,  3.35it/s][A[A

 34% 11/32 [00:02<00:05,  3.72it/s][A[A

 38% 12/32 [00:03<00:05,  3.80it/s][A[A

 41% 13/32 [00:03<00:04,  4.01it/s][A[A

 44% 14/32 [00:03<00:04,  4.26it/s][A[A

 47% 15/32 [00:03<00:03,  4.90it/s][A[A

 50% 16/32 [00:03<00:03,  4.83it/s][A[A

 53% 17/32 [00:04<00:02,  5.20it/s][A[A

 56% 18/32 [00:04<00:02,  5.35it/s][A[A

 59% 19/32 [00:04<00:02,  5.57it/s][A[A

 62% 20/32 [00:04<00:02,  5.43it/s][A[A

 66% 21/32 [00:04<00:01,  6.00it/s][A[A

 69% 22/32 [00:04<00:01,  5.86it/s][A[A

 72% 23/32 [00:04<00:01,  6.69it/s][A[A

 75% 24/32 [00:05<00:01,  6.66it/s][A[A

 78% 25/32 [00:05<00:01,  6.18it/s][A[A

 81% 26/32 [00:06<00:02,  2.11it/s][A[A

 84% 27/32 [00:06<00:01,  2.76it/s][A[A

 88% 28/32 [00:06<00:01,  3.30it/s][A[A

 91% 29/32 [00:06<00:00,  3.85it/s][A[A

 94% 30/32 [00:07<00:00,  4.59it/s][A[A

 97% 31/32 [00:07<00:00,  5.30it/s][A[A

100% 32/32 [00:07<00:00,  5.49it/s][A[A100% 32/32 [00:07<00:00,  4.37it/s]
Meta loss on this task batch = 5.2320e-01, PNorm = 38.0804, GNorm = 0.1420

  5% 1/19 [00:07<02:23,  8.00s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  7.51it/s][A[A

  6% 2/32 [00:00<00:04,  7.27it/s][A[A

  9% 3/32 [00:00<00:04,  6.57it/s][A[A

 12% 4/32 [00:00<00:04,  6.30it/s][A[A

 16% 5/32 [00:00<00:04,  6.38it/s][A[A

 19% 6/32 [00:00<00:03,  6.55it/s][A[A

 22% 7/32 [00:01<00:03,  6.84it/s][A[A

 25% 8/32 [00:01<00:03,  6.86it/s][A[A

 28% 9/32 [00:01<00:03,  6.68it/s][A[A

 31% 10/32 [00:01<00:03,  6.96it/s][A[A

 34% 11/32 [00:01<00:02,  7.12it/s][A[A

 38% 12/32 [00:02<00:09,  2.20it/s][A[A

 41% 13/32 [00:02<00:06,  2.79it/s][A[A

 44% 14/32 [00:03<00:05,  3.29it/s][A[A

 47% 15/32 [00:03<00:04,  3.82it/s][A[A

 50% 16/32 [00:03<00:03,  4.43it/s][A[A

 53% 17/32 [00:03<00:03,  4.86it/s][A[A

 56% 18/32 [00:03<00:02,  4.70it/s][A[A

 59% 19/32 [00:03<00:02,  5.54it/s][A[A

 62% 20/32 [00:04<00:02,  5.66it/s][A[A

 66% 21/32 [00:04<00:01,  5.88it/s][A[A

 69% 22/32 [00:04<00:01,  6.14it/s][A[A

 72% 23/32 [00:04<00:01,  6.32it/s][A[A

 75% 24/32 [00:04<00:01,  6.55it/s][A[A

 78% 25/32 [00:04<00:01,  6.20it/s][A[A

 81% 26/32 [00:05<00:01,  5.73it/s][A[A

 84% 27/32 [00:05<00:00,  5.80it/s][A[A

 88% 28/32 [00:05<00:00,  5.75it/s][A[A

 91% 29/32 [00:05<00:00,  6.42it/s][A[A

 94% 30/32 [00:05<00:00,  6.54it/s][A[A

 97% 31/32 [00:05<00:00,  6.60it/s][A[A

100% 32/32 [00:05<00:00,  6.66it/s][A[A100% 32/32 [00:05<00:00,  5.36it/s]
Meta loss on this task batch = 5.0781e-01, PNorm = 38.0888, GNorm = 0.0910

 11% 2/19 [00:14<02:08,  7.59s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:36,  1.18s/it][A[A

  6% 2/32 [00:01<00:26,  1.14it/s][A[A

  9% 3/32 [00:01<00:19,  1.51it/s][A[A

 12% 4/32 [00:01<00:14,  1.99it/s][A[A

 16% 5/32 [00:01<00:10,  2.49it/s][A[A

 19% 6/32 [00:01<00:08,  2.97it/s][A[A

 22% 7/32 [00:02<00:06,  3.65it/s][A[A

 25% 8/32 [00:02<00:05,  4.19it/s][A[A

 28% 9/32 [00:02<00:04,  4.69it/s][A[A

 31% 10/32 [00:02<00:04,  4.61it/s][A[A

 34% 11/32 [00:02<00:04,  4.55it/s][A[A

 38% 12/32 [00:03<00:04,  4.82it/s][A[A

 41% 13/32 [00:03<00:03,  5.28it/s][A[A

 44% 14/32 [00:03<00:03,  5.65it/s][A[A

 47% 15/32 [00:03<00:02,  5.97it/s][A[A

 50% 16/32 [00:03<00:02,  6.36it/s][A[A

 53% 17/32 [00:03<00:02,  5.78it/s][A[A

 56% 18/32 [00:03<00:02,  6.18it/s][A[A

 59% 19/32 [00:05<00:06,  2.16it/s][A[A

 62% 20/32 [00:05<00:04,  2.74it/s][A[A

 66% 21/32 [00:05<00:03,  3.42it/s][A[A

 69% 22/32 [00:05<00:02,  4.01it/s][A[A

 72% 23/32 [00:05<00:01,  4.67it/s][A[A

 75% 24/32 [00:05<00:01,  4.61it/s][A[A

 78% 25/32 [00:06<00:01,  5.24it/s][A[A

 81% 26/32 [00:06<00:01,  5.84it/s][A[A

 84% 27/32 [00:06<00:00,  5.98it/s][A[A

 88% 28/32 [00:06<00:00,  6.00it/s][A[A

 91% 29/32 [00:06<00:00,  6.20it/s][A[A

 94% 30/32 [00:06<00:00,  5.35it/s][A[A

 97% 31/32 [00:06<00:00,  6.10it/s][A[A

100% 32/32 [00:07<00:00,  5.80it/s][A[A100% 32/32 [00:07<00:00,  4.46it/s]
Meta loss on this task batch = 5.4080e-01, PNorm = 38.0929, GNorm = 0.1471

 16% 3/19 [00:22<02:02,  7.66s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.12it/s][A[A

  6% 2/32 [00:00<00:06,  4.94it/s][A[A

  9% 3/32 [00:00<00:05,  5.10it/s][A[A

 12% 4/32 [00:00<00:04,  5.77it/s][A[A

 16% 5/32 [00:01<00:13,  2.06it/s][A[A

 19% 6/32 [00:02<00:09,  2.62it/s][A[A

 22% 7/32 [00:02<00:07,  3.25it/s][A[A

 25% 8/32 [00:02<00:06,  3.97it/s][A[A

 28% 9/32 [00:02<00:05,  4.56it/s][A[A

 34% 11/32 [00:02<00:03,  5.25it/s][A[A

 38% 12/32 [00:02<00:03,  5.74it/s][A[A

 41% 13/32 [00:03<00:03,  5.97it/s][A[A

 44% 14/32 [00:03<00:03,  5.96it/s][A[A

 47% 15/32 [00:03<00:02,  6.22it/s][A[A

 50% 16/32 [00:03<00:02,  6.13it/s][A[A

 53% 17/32 [00:03<00:02,  6.82it/s][A[A

 56% 18/32 [00:03<00:01,  7.10it/s][A[A

 59% 19/32 [00:03<00:01,  6.99it/s][A[A

 62% 20/32 [00:04<00:01,  6.39it/s][A[A

 66% 21/32 [00:04<00:01,  6.39it/s][A[A

 69% 22/32 [00:04<00:01,  5.44it/s][A[A

 72% 23/32 [00:04<00:01,  5.21it/s][A[A

 75% 24/32 [00:04<00:01,  5.02it/s][A[A

 78% 25/32 [00:05<00:01,  5.61it/s][A[A

 81% 26/32 [00:06<00:02,  2.16it/s][A[A

 84% 27/32 [00:06<00:01,  2.68it/s][A[A

 88% 28/32 [00:06<00:01,  3.15it/s][A[A

 91% 29/32 [00:06<00:00,  3.79it/s][A[A

 94% 30/32 [00:06<00:00,  4.02it/s][A[A

 97% 31/32 [00:06<00:00,  4.65it/s][A[A

100% 32/32 [00:07<00:00,  4.96it/s][A[A100% 32/32 [00:07<00:00,  4.47it/s]
Meta loss on this task batch = 5.7185e-01, PNorm = 38.0992, GNorm = 0.0895

 21% 4/19 [00:30<01:55,  7.71s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.55it/s][A[A

  6% 2/32 [00:00<00:04,  6.14it/s][A[A

 12% 4/32 [00:00<00:04,  6.47it/s][A[A

 16% 5/32 [00:00<00:04,  5.85it/s][A[A

 19% 6/32 [00:00<00:04,  5.88it/s][A[A

 22% 7/32 [00:01<00:04,  5.35it/s][A[A

 25% 8/32 [00:01<00:04,  5.83it/s][A[A

 28% 9/32 [00:01<00:04,  5.55it/s][A[A

 31% 10/32 [00:01<00:03,  5.74it/s][A[A

 34% 11/32 [00:01<00:03,  5.84it/s][A[A

 38% 12/32 [00:02<00:03,  5.53it/s][A[A

 41% 13/32 [00:03<00:09,  2.00it/s][A[A

 44% 14/32 [00:03<00:07,  2.54it/s][A[A

 47% 15/32 [00:03<00:05,  3.15it/s][A[A

 50% 16/32 [00:03<00:04,  3.91it/s][A[A

 53% 17/32 [00:03<00:03,  4.09it/s][A[A

 56% 18/32 [00:04<00:03,  4.31it/s][A[A

 59% 19/32 [00:04<00:02,  4.53it/s][A[A

 62% 20/32 [00:04<00:02,  4.97it/s][A[A

 66% 21/32 [00:04<00:02,  5.42it/s][A[A

 69% 22/32 [00:04<00:01,  6.04it/s][A[A

 72% 23/32 [00:04<00:01,  5.83it/s][A[A

 75% 24/32 [00:05<00:01,  5.11it/s][A[A

 78% 25/32 [00:05<00:01,  5.00it/s][A[A

 81% 26/32 [00:05<00:01,  5.47it/s][A[A

 84% 27/32 [00:05<00:00,  5.47it/s][A[A

 88% 28/32 [00:05<00:00,  5.64it/s][A[A

 91% 29/32 [00:06<00:00,  5.88it/s][A[A

 94% 30/32 [00:06<00:00,  6.04it/s][A[A

 97% 31/32 [00:06<00:00,  5.81it/s][A[A

100% 32/32 [00:06<00:00,  5.54it/s][A[A100% 32/32 [00:06<00:00,  4.88it/s]
Meta loss on this task batch = 5.0321e-01, PNorm = 38.1069, GNorm = 0.0455

 26% 5/19 [00:37<01:46,  7.57s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  6.38it/s][A[A

  6% 2/32 [00:01<00:13,  2.18it/s][A[A

  9% 3/32 [00:01<00:10,  2.72it/s][A[A

 12% 4/32 [00:01<00:08,  3.33it/s][A[A

 16% 5/32 [00:01<00:06,  3.88it/s][A[A

 19% 6/32 [00:02<00:06,  3.97it/s][A[A

 22% 7/32 [00:02<00:05,  4.32it/s][A[A

 25% 8/32 [00:02<00:05,  4.65it/s][A[A

 28% 9/32 [00:02<00:04,  4.62it/s][A[A

 31% 10/32 [00:02<00:04,  4.91it/s][A[A

 34% 11/32 [00:02<00:04,  5.03it/s][A[A

 38% 12/32 [00:03<00:03,  5.12it/s][A[A

 41% 13/32 [00:03<00:03,  5.57it/s][A[A

 44% 14/32 [00:03<00:03,  5.78it/s][A[A

 47% 15/32 [00:03<00:03,  5.36it/s][A[A

 50% 16/32 [00:03<00:03,  5.00it/s][A[A

 53% 17/32 [00:05<00:07,  2.05it/s][A[A

 56% 18/32 [00:05<00:05,  2.56it/s][A[A

 59% 19/32 [00:05<00:04,  3.17it/s][A[A

 62% 20/32 [00:05<00:03,  3.62it/s][A[A

 66% 21/32 [00:05<00:02,  4.20it/s][A[A

 69% 22/32 [00:05<00:02,  4.36it/s][A[A

 72% 23/32 [00:06<00:01,  4.66it/s][A[A

 75% 24/32 [00:06<00:01,  4.86it/s][A[A

 78% 25/32 [00:06<00:01,  4.76it/s][A[A

 81% 26/32 [00:06<00:01,  5.25it/s][A[A

 84% 27/32 [00:06<00:01,  4.86it/s][A[A

 88% 28/32 [00:07<00:00,  5.27it/s][A[A

 91% 29/32 [00:07<00:00,  5.54it/s][A[A

 94% 30/32 [00:07<00:00,  5.87it/s][A[A

100% 32/32 [00:07<00:00,  6.48it/s][A[A100% 32/32 [00:07<00:00,  4.23it/s]
Meta loss on this task batch = 4.6391e-01, PNorm = 38.1184, GNorm = 0.1110

 32% 6/19 [00:45<01:41,  7.78s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.88it/s][A[A

  6% 2/32 [00:00<00:05,  5.36it/s][A[A

  9% 3/32 [00:00<00:05,  5.13it/s][A[A

 12% 4/32 [00:01<00:13,  2.07it/s][A[A

 16% 5/32 [00:01<00:10,  2.64it/s][A[A

 19% 6/32 [00:02<00:08,  3.24it/s][A[A

 22% 7/32 [00:02<00:06,  3.66it/s][A[A

 25% 8/32 [00:02<00:05,  4.07it/s][A[A

 28% 9/32 [00:02<00:05,  4.43it/s][A[A

 31% 10/32 [00:02<00:04,  4.59it/s][A[A

 34% 11/32 [00:02<00:04,  4.89it/s][A[A

 38% 12/32 [00:03<00:04,  4.82it/s][A[A

 41% 13/32 [00:03<00:03,  5.22it/s][A[A

 44% 14/32 [00:03<00:03,  5.48it/s][A[A

 47% 15/32 [00:03<00:03,  4.93it/s][A[A

 50% 16/32 [00:03<00:03,  4.67it/s][A[A

 53% 17/32 [00:04<00:02,  5.44it/s][A[A

 56% 18/32 [00:04<00:02,  5.46it/s][A[A

 59% 19/32 [00:04<00:02,  5.54it/s][A[A

 62% 20/32 [00:04<00:02,  4.97it/s][A[A

 66% 21/32 [00:05<00:05,  1.99it/s][A[A

 69% 22/32 [00:06<00:04,  2.42it/s][A[A

 72% 23/32 [00:06<00:03,  2.81it/s][A[A

 75% 24/32 [00:06<00:02,  3.15it/s][A[A

 78% 25/32 [00:06<00:01,  3.59it/s][A[A

 81% 26/32 [00:06<00:01,  4.32it/s][A[A

 84% 27/32 [00:07<00:01,  4.60it/s][A[A

 88% 28/32 [00:07<00:00,  4.43it/s][A[A

 91% 29/32 [00:07<00:00,  5.24it/s][A[A

 94% 30/32 [00:07<00:00,  5.19it/s][A[A

100% 32/32 [00:07<00:00,  5.62it/s][A[A100% 32/32 [00:07<00:00,  4.07it/s]
Meta loss on this task batch = 4.8828e-01, PNorm = 38.1352, GNorm = 0.1777

 37% 7/19 [00:54<01:36,  8.01s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:37,  1.20s/it][A[A

  6% 2/32 [00:01<00:27,  1.11it/s][A[A

  9% 3/32 [00:01<00:20,  1.42it/s][A[A

 12% 4/32 [00:01<00:15,  1.77it/s][A[A

 16% 5/32 [00:02<00:12,  2.19it/s][A[A

 19% 6/32 [00:02<00:09,  2.68it/s][A[A

 22% 7/32 [00:02<00:07,  3.40it/s][A[A

 25% 8/32 [00:02<00:06,  3.65it/s][A[A

 28% 9/32 [00:02<00:05,  3.94it/s][A[A

 31% 10/32 [00:02<00:04,  4.46it/s][A[A

 34% 11/32 [00:03<00:04,  5.01it/s][A[A

 38% 12/32 [00:03<00:03,  5.01it/s][A[A

 41% 13/32 [00:03<00:03,  5.24it/s][A[A

 44% 14/32 [00:03<00:02,  6.08it/s][A[A

 47% 15/32 [00:03<00:02,  6.00it/s][A[A

 50% 16/32 [00:03<00:02,  5.47it/s][A[A

 53% 17/32 [00:05<00:07,  2.02it/s][A[A

 56% 18/32 [00:05<00:05,  2.49it/s][A[A

 59% 19/32 [00:05<00:04,  2.90it/s][A[A

 62% 20/32 [00:05<00:03,  3.51it/s][A[A

 66% 21/32 [00:05<00:02,  4.08it/s][A[A

 69% 22/32 [00:06<00:02,  4.12it/s][A[A

 72% 23/32 [00:06<00:01,  4.93it/s][A[A

 75% 24/32 [00:06<00:01,  4.75it/s][A[A

 78% 25/32 [00:06<00:01,  4.69it/s][A[A

 81% 26/32 [00:06<00:01,  4.77it/s][A[A

 84% 27/32 [00:07<00:01,  4.58it/s][A[A

 88% 28/32 [00:07<00:00,  4.50it/s][A[A

 91% 29/32 [00:07<00:00,  4.40it/s][A[A

 94% 30/32 [00:07<00:00,  4.46it/s][A[A

 97% 31/32 [00:09<00:00,  1.93it/s][A[A

100% 32/32 [00:09<00:00,  2.31it/s][A[A100% 32/32 [00:09<00:00,  3.45it/s]
Meta loss on this task batch = 3.8218e-01, PNorm = 38.1589, GNorm = 0.1784

 42% 8/19 [01:04<01:34,  8.61s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  3.95it/s][A[A

  6% 2/32 [00:00<00:07,  4.01it/s][A[A

  9% 3/32 [00:00<00:07,  4.00it/s][A[A

 12% 4/32 [00:00<00:06,  4.12it/s][A[A

 16% 5/32 [00:01<00:06,  4.26it/s][A[A

 19% 6/32 [00:01<00:06,  4.21it/s][A[A

 22% 7/32 [00:01<00:05,  4.26it/s][A[A

 25% 8/32 [00:01<00:05,  4.32it/s][A[A

 28% 9/32 [00:02<00:05,  4.34it/s][A[A

 31% 10/32 [00:03<00:11,  1.88it/s][A[A

 34% 11/32 [00:03<00:09,  2.24it/s][A[A

 38% 12/32 [00:03<00:07,  2.64it/s][A[A

 41% 13/32 [00:04<00:06,  3.00it/s][A[A

 44% 14/32 [00:04<00:05,  3.31it/s][A[A

 47% 15/32 [00:04<00:04,  3.50it/s][A[A

 50% 16/32 [00:04<00:04,  3.66it/s][A[A

 53% 17/32 [00:04<00:03,  3.83it/s][A[A

 56% 18/32 [00:05<00:03,  4.00it/s][A[A

 59% 19/32 [00:05<00:03,  4.10it/s][A[A

 62% 20/32 [00:05<00:02,  4.22it/s][A[A

 66% 21/32 [00:05<00:02,  4.24it/s][A[A

 69% 22/32 [00:07<00:05,  1.86it/s][A[A

 72% 23/32 [00:07<00:03,  2.25it/s][A[A

 75% 24/32 [00:07<00:03,  2.65it/s][A[A

 78% 25/32 [00:07<00:02,  3.03it/s][A[A

 81% 26/32 [00:08<00:01,  3.26it/s][A[A

 84% 27/32 [00:08<00:01,  3.50it/s][A[A

 88% 28/32 [00:08<00:01,  3.78it/s][A[A

 91% 29/32 [00:08<00:00,  3.98it/s][A[A

 94% 30/32 [00:08<00:00,  4.05it/s][A[A

 97% 31/32 [00:09<00:00,  4.14it/s][A[A

100% 32/32 [00:09<00:00,  4.13it/s][A[A100% 32/32 [00:09<00:00,  3.39it/s]
Meta loss on this task batch = 2.2689e-01, PNorm = 38.1883, GNorm = 0.1573

 47% 9/19 [01:14<01:31,  9.11s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.06it/s][A[A

  6% 2/32 [00:01<00:16,  1.87it/s][A[A

  9% 3/32 [00:01<00:12,  2.30it/s][A[A

 12% 4/32 [00:01<00:10,  2.66it/s][A[A

 16% 5/32 [00:02<00:09,  2.97it/s][A[A

 19% 6/32 [00:02<00:07,  3.26it/s][A[A

 22% 7/32 [00:02<00:07,  3.51it/s][A[A

 25% 8/32 [00:02<00:06,  3.74it/s][A[A

 28% 9/32 [00:03<00:05,  3.85it/s][A[A

 31% 10/32 [00:03<00:05,  3.97it/s][A[A

 34% 11/32 [00:03<00:05,  3.99it/s][A[A

 38% 12/32 [00:03<00:04,  4.11it/s][A[A

 41% 13/32 [00:04<00:04,  4.19it/s][A[A

 44% 14/32 [00:04<00:04,  4.23it/s][A[A

 47% 15/32 [00:05<00:09,  1.88it/s][A[A

 50% 16/32 [00:05<00:07,  2.24it/s][A[A

 53% 17/32 [00:05<00:05,  2.61it/s][A[A

 56% 18/32 [00:06<00:04,  2.93it/s][A[A

 59% 19/32 [00:06<00:03,  3.25it/s][A[A

 62% 20/32 [00:06<00:03,  3.56it/s][A[A

 66% 21/32 [00:06<00:02,  3.79it/s][A[A

 69% 22/32 [00:07<00:02,  3.95it/s][A[A

 72% 23/32 [00:07<00:02,  4.01it/s][A[A

 75% 24/32 [00:07<00:01,  4.09it/s][A[A

 78% 25/32 [00:07<00:01,  4.15it/s][A[A

 81% 26/32 [00:07<00:01,  4.45it/s][A[A

 84% 27/32 [00:08<00:00,  5.13it/s][A[A

 88% 28/32 [00:09<00:02,  1.98it/s][A[A

 91% 29/32 [00:09<00:01,  2.38it/s][A[A

 94% 30/32 [00:09<00:00,  2.72it/s][A[A

 97% 31/32 [00:10<00:00,  3.01it/s][A[A

100% 32/32 [00:10<00:00,  3.62it/s][A[A100% 32/32 [00:10<00:00,  3.14it/s]
Meta loss on this task batch = 2.7326e-01, PNorm = 38.2218, GNorm = 0.1369

 53% 10/19 [01:25<01:27,  9.68s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.74it/s][A[A

  6% 2/32 [00:00<00:07,  4.01it/s][A[A

  9% 3/32 [00:00<00:06,  4.74it/s][A[A

 12% 4/32 [00:00<00:05,  4.84it/s][A[A

 16% 5/32 [00:00<00:05,  4.91it/s][A[A

 19% 6/32 [00:01<00:04,  5.67it/s][A[A

 22% 7/32 [00:01<00:04,  5.19it/s][A[A

 25% 8/32 [00:01<00:04,  4.84it/s][A[A

 28% 9/32 [00:01<00:04,  5.53it/s][A[A

 31% 10/32 [00:02<00:10,  2.02it/s][A[A

 34% 11/32 [00:03<00:08,  2.38it/s][A[A

 38% 12/32 [00:03<00:07,  2.80it/s][A[A

 41% 13/32 [00:03<00:06,  3.15it/s][A[A

 44% 14/32 [00:03<00:04,  3.74it/s][A[A

 47% 15/32 [00:03<00:04,  4.04it/s][A[A

 50% 16/32 [00:04<00:03,  4.50it/s][A[A

 53% 17/32 [00:04<00:02,  5.10it/s][A[A

 56% 18/32 [00:04<00:02,  4.88it/s][A[A

 59% 19/32 [00:04<00:02,  4.61it/s][A[A

 62% 20/32 [00:04<00:02,  5.40it/s][A[A

 66% 21/32 [00:05<00:02,  5.20it/s][A[A

 69% 22/32 [00:05<00:02,  4.72it/s][A[A

 72% 23/32 [00:05<00:01,  4.52it/s][A[A

 75% 24/32 [00:05<00:01,  4.51it/s][A[A

 78% 25/32 [00:05<00:01,  4.93it/s][A[A

 81% 26/32 [00:07<00:03,  1.93it/s][A[A

 84% 27/32 [00:07<00:02,  2.28it/s][A[A

 88% 28/32 [00:07<00:01,  2.72it/s][A[A

 91% 29/32 [00:07<00:00,  3.04it/s][A[A

 94% 30/32 [00:08<00:00,  3.33it/s][A[A

 97% 31/32 [00:08<00:00,  4.04it/s][A[A

100% 32/32 [00:08<00:00,  4.18it/s][A[A100% 32/32 [00:08<00:00,  3.79it/s]
Meta loss on this task batch = 6.6693e-01, PNorm = 38.2488, GNorm = 0.1529

 58% 11/19 [01:34<01:16,  9.54s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.40it/s][A[A

  6% 2/32 [00:00<00:05,  5.19it/s][A[A

  9% 3/32 [00:00<00:05,  4.98it/s][A[A

 12% 4/32 [00:01<00:14,  1.95it/s][A[A

 16% 5/32 [00:02<00:11,  2.35it/s][A[A

 19% 6/32 [00:02<00:09,  2.79it/s][A[A

 22% 7/32 [00:02<00:08,  3.04it/s][A[A

 25% 8/32 [00:02<00:07,  3.23it/s][A[A

 28% 9/32 [00:03<00:06,  3.40it/s][A[A

 31% 10/32 [00:03<00:05,  3.84it/s][A[A

 34% 11/32 [00:03<00:05,  3.93it/s][A[A

 38% 12/32 [00:03<00:04,  4.17it/s][A[A

 41% 13/32 [00:03<00:04,  4.11it/s][A[A

 44% 14/32 [00:05<00:09,  1.88it/s][A[A

 47% 15/32 [00:05<00:07,  2.26it/s][A[A

 50% 16/32 [00:05<00:06,  2.61it/s][A[A

 53% 17/32 [00:05<00:05,  2.93it/s][A[A

 56% 18/32 [00:06<00:04,  3.18it/s][A[A

 59% 19/32 [00:06<00:03,  3.47it/s][A[A

 62% 20/32 [00:06<00:02,  4.25it/s][A[A

 66% 21/32 [00:06<00:02,  4.24it/s][A[A

 69% 22/32 [00:06<00:02,  4.15it/s][A[A

 72% 23/32 [00:07<00:02,  4.08it/s][A[A

 75% 24/32 [00:07<00:01,  4.59it/s][A[A

 78% 25/32 [00:07<00:01,  4.64it/s][A[A

 81% 26/32 [00:08<00:03,  1.97it/s][A[A

 84% 27/32 [00:08<00:02,  2.40it/s][A[A

 88% 28/32 [00:09<00:01,  2.80it/s][A[A

 91% 29/32 [00:09<00:00,  3.20it/s][A[A

 94% 30/32 [00:09<00:00,  3.52it/s][A[A

 97% 31/32 [00:09<00:00,  3.79it/s][A[A

100% 32/32 [00:10<00:00,  3.95it/s][A[A100% 32/32 [00:10<00:00,  3.18it/s]
Meta loss on this task batch = 5.3265e-01, PNorm = 38.2755, GNorm = 0.0666

 63% 12/19 [01:45<01:09,  9.93s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.18it/s][A[A

  6% 2/32 [00:00<00:07,  4.14it/s][A[A

  9% 3/32 [00:00<00:06,  4.23it/s][A[A

 12% 4/32 [00:00<00:06,  4.25it/s][A[A

 16% 5/32 [00:01<00:05,  4.95it/s][A[A

 19% 6/32 [00:01<00:05,  4.63it/s][A[A

 22% 7/32 [00:02<00:12,  1.97it/s][A[A

 25% 8/32 [00:02<00:09,  2.56it/s][A[A

 28% 9/32 [00:02<00:07,  3.12it/s][A[A

 31% 10/32 [00:03<00:06,  3.36it/s][A[A

 34% 11/32 [00:03<00:05,  3.74it/s][A[A

 38% 12/32 [00:03<00:05,  3.85it/s][A[A

 41% 13/32 [00:03<00:04,  4.00it/s][A[A

 44% 14/32 [00:03<00:04,  3.93it/s][A[A

 47% 15/32 [00:04<00:04,  4.17it/s][A[A

 50% 16/32 [00:04<00:03,  4.42it/s][A[A

 53% 17/32 [00:04<00:03,  4.91it/s][A[A

 56% 18/32 [00:04<00:03,  4.57it/s][A[A

 59% 19/32 [00:04<00:02,  4.94it/s][A[A

 62% 20/32 [00:06<00:06,  1.91it/s][A[A

 66% 21/32 [00:06<00:04,  2.30it/s][A[A

 69% 22/32 [00:06<00:03,  2.75it/s][A[A

 72% 23/32 [00:06<00:02,  3.12it/s][A[A

 75% 24/32 [00:07<00:02,  3.53it/s][A[A

 78% 25/32 [00:07<00:01,  4.07it/s][A[A

 81% 26/32 [00:07<00:01,  4.12it/s][A[A

 84% 27/32 [00:07<00:01,  4.57it/s][A[A

 88% 28/32 [00:07<00:00,  4.40it/s][A[A

 91% 29/32 [00:08<00:00,  4.45it/s][A[A

 94% 30/32 [00:08<00:00,  4.68it/s][A[A

 97% 31/32 [00:08<00:00,  5.13it/s][A[A

100% 32/32 [00:08<00:00,  5.18it/s][A[A100% 32/32 [00:08<00:00,  3.73it/s]
Meta loss on this task batch = 5.8006e-01, PNorm = 38.3019, GNorm = 0.0576

 68% 13/19 [01:55<00:58,  9.76s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  6.99it/s][A[A

  6% 2/32 [00:00<00:04,  6.98it/s][A[A

  9% 3/32 [00:00<00:05,  5.70it/s][A[A

 12% 4/32 [00:00<00:05,  5.22it/s][A[A

 16% 5/32 [00:00<00:05,  5.04it/s][A[A

 19% 6/32 [00:02<00:12,  2.07it/s][A[A

 22% 7/32 [00:02<00:10,  2.47it/s][A[A

 25% 8/32 [00:02<00:07,  3.00it/s][A[A

 28% 9/32 [00:02<00:07,  3.25it/s][A[A

 31% 10/32 [00:02<00:06,  3.51it/s][A[A

 34% 11/32 [00:03<00:05,  4.03it/s][A[A

 38% 12/32 [00:03<00:05,  3.96it/s][A[A

 41% 13/32 [00:03<00:04,  4.13it/s][A[A

 44% 14/32 [00:03<00:04,  4.22it/s][A[A

 47% 15/32 [00:04<00:03,  4.30it/s][A[A

 50% 16/32 [00:04<00:03,  4.70it/s][A[A

 53% 17/32 [00:04<00:03,  4.52it/s][A[A

 56% 18/32 [00:04<00:02,  4.73it/s][A[A

 59% 19/32 [00:05<00:06,  2.01it/s][A[A

 62% 20/32 [00:06<00:05,  2.35it/s][A[A

 66% 21/32 [00:06<00:03,  2.79it/s][A[A

 69% 22/32 [00:06<00:03,  3.20it/s][A[A

 72% 23/32 [00:06<00:02,  3.47it/s][A[A

 75% 24/32 [00:06<00:01,  4.02it/s][A[A

 78% 25/32 [00:07<00:01,  3.96it/s][A[A

 81% 26/32 [00:07<00:01,  4.05it/s][A[A

 84% 27/32 [00:07<00:01,  4.12it/s][A[A

 88% 28/32 [00:07<00:00,  4.32it/s][A[A

 91% 29/32 [00:08<00:00,  4.26it/s][A[A

 94% 30/32 [00:08<00:00,  4.40it/s][A[A

 97% 31/32 [00:09<00:00,  1.91it/s][A[A

100% 32/32 [00:09<00:00,  2.35it/s][A[A100% 32/32 [00:09<00:00,  3.30it/s]
Meta loss on this task batch = 5.5663e-01, PNorm = 38.3278, GNorm = 0.0729

 74% 14/19 [02:05<00:49,  9.96s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  6.43it/s][A[A

  6% 2/32 [00:00<00:05,  5.82it/s][A[A

  9% 3/32 [00:00<00:05,  5.17it/s][A[A

 12% 4/32 [00:00<00:05,  5.36it/s][A[A

 16% 5/32 [00:00<00:05,  5.33it/s][A[A

 19% 6/32 [00:01<00:05,  4.96it/s][A[A

 22% 7/32 [00:01<00:05,  4.54it/s][A[A

 25% 8/32 [00:01<00:04,  4.83it/s][A[A

 28% 9/32 [00:01<00:04,  4.65it/s][A[A

 31% 10/32 [00:02<00:04,  5.18it/s][A[A

 34% 11/32 [00:02<00:03,  5.34it/s][A[A

 38% 12/32 [00:02<00:03,  5.61it/s][A[A

 41% 13/32 [00:03<00:09,  2.07it/s][A[A

 44% 14/32 [00:03<00:07,  2.42it/s][A[A

 47% 15/32 [00:03<00:05,  2.89it/s][A[A

 50% 16/32 [00:04<00:04,  3.24it/s][A[A

 53% 17/32 [00:04<00:03,  3.75it/s][A[A

 56% 18/32 [00:04<00:03,  4.13it/s][A[A

 59% 19/32 [00:04<00:02,  4.62it/s][A[A

 62% 20/32 [00:04<00:02,  4.88it/s][A[A

 66% 21/32 [00:05<00:02,  5.01it/s][A[A

 69% 22/32 [00:05<00:01,  5.06it/s][A[A

 72% 23/32 [00:05<00:01,  4.51it/s][A[A

 75% 24/32 [00:05<00:01,  4.47it/s][A[A

 78% 25/32 [00:05<00:01,  5.20it/s][A[A

 81% 26/32 [00:06<00:01,  4.62it/s][A[A

 84% 27/32 [00:06<00:01,  4.96it/s][A[A

 88% 28/32 [00:06<00:00,  5.14it/s][A[A

 91% 29/32 [00:06<00:00,  4.78it/s][A[A

 94% 30/32 [00:08<00:01,  1.91it/s][A[A

 97% 31/32 [00:08<00:00,  2.48it/s][A[A

100% 32/32 [00:08<00:00,  2.80it/s][A[A100% 32/32 [00:08<00:00,  3.81it/s]
Meta loss on this task batch = 4.9962e-01, PNorm = 38.3523, GNorm = 0.0542

 79% 15/19 [02:14<00:38,  9.71s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.71it/s][A[A

  6% 2/32 [00:00<00:05,  5.45it/s][A[A

  9% 3/32 [00:00<00:04,  6.06it/s][A[A

 12% 4/32 [00:00<00:04,  6.38it/s][A[A

 16% 5/32 [00:00<00:04,  6.58it/s][A[A

 19% 6/32 [00:00<00:04,  5.79it/s][A[A

 22% 7/32 [00:01<00:04,  6.05it/s][A[A

 25% 8/32 [00:01<00:03,  6.49it/s][A[A

 28% 9/32 [00:01<00:03,  6.77it/s][A[A

 31% 10/32 [00:01<00:03,  5.55it/s][A[A

 34% 11/32 [00:01<00:04,  4.82it/s][A[A

 38% 12/32 [00:02<00:04,  4.75it/s][A[A

 41% 13/32 [00:02<00:03,  4.82it/s][A[A

 44% 14/32 [00:03<00:09,  1.98it/s][A[A

 47% 15/32 [00:03<00:07,  2.38it/s][A[A

 50% 16/32 [00:03<00:05,  2.93it/s][A[A

 53% 17/32 [00:04<00:04,  3.21it/s][A[A

 56% 18/32 [00:04<00:03,  3.78it/s][A[A

 59% 19/32 [00:04<00:03,  4.00it/s][A[A

 62% 20/32 [00:04<00:02,  4.23it/s][A[A

 66% 21/32 [00:04<00:02,  4.51it/s][A[A

 69% 22/32 [00:05<00:02,  4.67it/s][A[A

 72% 23/32 [00:05<00:02,  4.42it/s][A[A

 75% 24/32 [00:05<00:01,  4.18it/s][A[A

 78% 25/32 [00:05<00:01,  4.92it/s][A[A

 81% 26/32 [00:05<00:01,  5.24it/s][A[A

 84% 27/32 [00:06<00:01,  4.70it/s][A[A

 88% 28/32 [00:07<00:02,  1.93it/s][A[A

 91% 29/32 [00:07<00:01,  2.36it/s][A[A

 94% 30/32 [00:07<00:00,  2.70it/s][A[A

 97% 31/32 [00:08<00:00,  3.01it/s][A[A

100% 32/32 [00:08<00:00,  3.37it/s][A[A100% 32/32 [00:08<00:00,  3.86it/s]
Meta loss on this task batch = 5.5460e-01, PNorm = 38.3760, GNorm = 0.0954

 84% 16/19 [02:23<00:28,  9.50s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.67it/s][A[A

  6% 2/32 [00:00<00:08,  3.71it/s][A[A

  9% 3/32 [00:00<00:07,  4.06it/s][A[A

 12% 4/32 [00:00<00:06,  4.23it/s][A[A

 16% 5/32 [00:01<00:05,  4.60it/s][A[A

 19% 6/32 [00:01<00:05,  5.17it/s][A[A

 22% 7/32 [00:01<00:04,  5.64it/s][A[A

 25% 8/32 [00:01<00:04,  4.99it/s][A[A

 28% 9/32 [00:02<00:11,  1.98it/s][A[A

 31% 10/32 [00:02<00:08,  2.59it/s][A[A

 34% 11/32 [00:03<00:06,  3.02it/s][A[A

 38% 12/32 [00:03<00:05,  3.44it/s][A[A

 41% 13/32 [00:03<00:04,  4.09it/s][A[A

 44% 14/32 [00:03<00:04,  4.49it/s][A[A

 47% 15/32 [00:03<00:03,  4.93it/s][A[A

 50% 16/32 [00:04<00:03,  5.13it/s][A[A

 53% 17/32 [00:04<00:02,  5.12it/s][A[A

 56% 18/32 [00:04<00:02,  5.11it/s][A[A

 59% 19/32 [00:04<00:02,  5.21it/s][A[A

 62% 20/32 [00:04<00:02,  4.92it/s][A[A

 66% 21/32 [00:04<00:02,  5.28it/s][A[A

 69% 22/32 [00:05<00:02,  4.78it/s][A[A

 72% 23/32 [00:05<00:01,  5.09it/s][A[A

 75% 24/32 [00:06<00:04,  1.93it/s][A[A

 78% 25/32 [00:06<00:03,  2.33it/s][A[A

 81% 26/32 [00:07<00:02,  2.63it/s][A[A

 84% 27/32 [00:07<00:01,  3.10it/s][A[A

 88% 28/32 [00:07<00:01,  3.47it/s][A[A

 91% 29/32 [00:07<00:00,  3.59it/s][A[A

 94% 30/32 [00:08<00:00,  3.67it/s][A[A

 97% 31/32 [00:08<00:00,  4.06it/s][A[A

100% 32/32 [00:08<00:00,  4.30it/s][A[A100% 32/32 [00:08<00:00,  3.79it/s]
Meta loss on this task batch = 4.5617e-01, PNorm = 38.3977, GNorm = 0.0874

 89% 17/19 [02:32<00:18,  9.40s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.09it/s][A[A

  6% 2/32 [00:00<00:06,  4.97it/s][A[A

  9% 3/32 [00:00<00:06,  4.74it/s][A[A

 12% 4/32 [00:00<00:05,  4.78it/s][A[A

 16% 5/32 [00:01<00:05,  5.12it/s][A[A

 19% 6/32 [00:02<00:12,  2.04it/s][A[A

 22% 7/32 [00:02<00:10,  2.38it/s][A[A

 25% 8/32 [00:02<00:08,  2.70it/s][A[A

 28% 9/32 [00:02<00:07,  3.20it/s][A[A

 31% 10/32 [00:03<00:06,  3.48it/s][A[A

 34% 11/32 [00:03<00:05,  3.95it/s][A[A

 38% 12/32 [00:03<00:04,  4.29it/s][A[A

 41% 13/32 [00:03<00:04,  4.58it/s][A[A

 44% 14/32 [00:03<00:03,  4.98it/s][A[A

 47% 15/32 [00:04<00:03,  4.69it/s][A[A

 50% 16/32 [00:04<00:03,  4.84it/s][A[A

 53% 17/32 [00:04<00:02,  5.37it/s][A[A

 56% 18/32 [00:04<00:02,  5.20it/s][A[A

 59% 19/32 [00:04<00:02,  4.83it/s][A[A

 62% 20/32 [00:06<00:06,  1.93it/s][A[A

 66% 21/32 [00:06<00:04,  2.30it/s][A[A

 69% 22/32 [00:06<00:03,  2.89it/s][A[A

 72% 23/32 [00:06<00:02,  3.18it/s][A[A

 75% 24/32 [00:06<00:02,  3.31it/s][A[A

 78% 25/32 [00:07<00:01,  3.84it/s][A[A

 81% 26/32 [00:07<00:01,  4.01it/s][A[A

 84% 27/32 [00:07<00:01,  3.93it/s][A[A

 88% 28/32 [00:07<00:00,  4.22it/s][A[A

 91% 29/32 [00:07<00:00,  5.08it/s][A[A

 94% 30/32 [00:08<00:00,  5.09it/s][A[A

 97% 31/32 [00:08<00:00,  5.16it/s][A[A

100% 32/32 [00:08<00:00,  5.27it/s][A[A100% 32/32 [00:08<00:00,  3.77it/s]
Meta loss on this task batch = 5.1319e-01, PNorm = 38.4164, GNorm = 0.0738

 95% 18/19 [02:41<00:09,  9.34s/it][A

  0% 0/23 [00:00<?, ?it/s][A[A

  4% 1/23 [00:01<00:27,  1.25s/it][A[A

  9% 2/23 [00:01<00:19,  1.08it/s][A[A

 13% 3/23 [00:01<00:14,  1.40it/s][A[A

 17% 4/23 [00:01<00:10,  1.86it/s][A[A

 26% 6/23 [00:02<00:07,  2.38it/s][A[A

 30% 7/23 [00:02<00:05,  2.89it/s][A[A

 35% 8/23 [00:02<00:04,  3.18it/s][A[A

 39% 9/23 [00:02<00:03,  3.79it/s][A[A

 43% 10/23 [00:02<00:03,  3.69it/s][A[A

 48% 11/23 [00:03<00:02,  4.40it/s][A[A

 52% 12/23 [00:03<00:02,  4.28it/s][A[A

 57% 13/23 [00:03<00:02,  4.76it/s][A[A

 61% 14/23 [00:03<00:01,  5.32it/s][A[A

 65% 15/23 [00:04<00:04,  1.82it/s][A[A

 70% 16/23 [00:05<00:03,  2.15it/s][A[A

 74% 17/23 [00:05<00:02,  2.66it/s][A[A

 78% 18/23 [00:05<00:01,  3.04it/s][A[A

 83% 19/23 [00:05<00:01,  3.30it/s][A[A

 87% 20/23 [00:05<00:00,  4.00it/s][A[A

 91% 21/23 [00:06<00:00,  4.43it/s][A[A

 96% 22/23 [00:06<00:00,  5.08it/s][A[A

100% 23/23 [00:06<00:00,  5.35it/s][A[A100% 23/23 [00:06<00:00,  3.57it/s]
Meta loss on this task batch = 4.2270e-01, PNorm = 38.4354, GNorm = 0.0720

100% 19/19 [02:48<00:00,  8.63s/it][A100% 19/19 [02:48<00:00,  8.89s/it]
Took 168.9836401939392 seconds to complete one epoch of meta training

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/20 [00:00<?, ?it/s][A[A


  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 29.65it/s]


  5% 1/20 [00:00<00:02,  7.95it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A


100% 2/2 [00:00<00:00, 18.57it/s][A[A[A100% 2/2 [00:00<00:00, 18.49it/s]


 10% 2/20 [00:00<00:02,  6.56it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 17.57it/s][A[A[A100% 3/3 [00:00<00:00, 19.51it/s]


 15% 3/20 [00:00<00:03,  5.09it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 14.83it/s]


 20% 4/20 [00:00<00:03,  4.97it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 14.69it/s][A[A[A100% 4/4 [00:00<00:00, 17.15it/s]


 25% 5/20 [00:01<00:03,  3.81it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.19it/s][A[A[A


100% 4/4 [00:00<00:00, 14.67it/s][A[A[A100% 4/4 [00:00<00:00, 15.84it/s]


 30% 6/20 [00:01<00:04,  3.30it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.06it/s][A[A[A100% 4/4 [00:00<00:00, 20.70it/s]


 35% 7/20 [00:03<00:08,  1.62it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 17.81it/s][A[A[A


100% 4/4 [00:00<00:00, 17.61it/s][A[A[A100% 4/4 [00:00<00:00, 17.44it/s]


 40% 8/20 [00:03<00:06,  1.82it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.15it/s][A[A[A100% 4/4 [00:00<00:00, 21.55it/s]


 45% 9/20 [00:03<00:05,  2.01it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 18.21it/s][A[A[A


100% 4/4 [00:00<00:00, 18.08it/s][A[A[A100% 4/4 [00:00<00:00, 17.96it/s]


 50% 10/20 [00:04<00:04,  2.15it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 16.20it/s][A[A[A100% 4/4 [00:00<00:00, 18.46it/s]


 55% 11/20 [00:04<00:04,  2.23it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 17.84it/s][A[A[A100% 4/4 [00:00<00:00, 21.13it/s]


 60% 12/20 [00:04<00:03,  2.37it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 13.96it/s][A[A[A100% 3/3 [00:00<00:00, 13.21it/s]


 65% 13/20 [00:05<00:02,  2.42it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.01it/s][A[A[A100% 3/3 [00:00<00:00, 13.46it/s]


 70% 14/20 [00:05<00:02,  2.46it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 12.71it/s][A[A[A100% 4/4 [00:00<00:00, 16.47it/s]


 75% 15/20 [00:07<00:03,  1.41it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 13.24it/s][A[A[A100% 3/3 [00:00<00:00, 16.34it/s]


 80% 16/20 [00:07<00:02,  1.65it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 11.33it/s][A[A[A100% 3/3 [00:00<00:00, 13.84it/s]


 85% 17/20 [00:07<00:01,  1.81it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 21.47it/s]


 90% 18/20 [00:08<00:00,  2.15it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.64it/s][A[A[A100% 3/3 [00:00<00:00, 18.71it/s]


 95% 19/20 [00:08<00:00,  2.36it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A


100% 1/1 [00:01<00:00,  1.08s/it][A[A[A100% 1/1 [00:01<00:00,  1.08s/it]


100% 20/20 [00:09<00:00,  1.49it/s][A[A100% 20/20 [00:09<00:00,  2.05it/s]

100% 1/1 [00:09<00:00,  9.76s/it][A100% 1/1 [00:09<00:00,  9.76s/it]
Took 178.7436854839325 seconds to complete one epoch of meta training and validating
Meta validation score auc = 0.621177
 60% 18/30 [57:53<38:11, 190.95s/it]Epoch 18

  0% 0/19 [00:00<?, ?it/s][A

  0% 0/32 [00:00<?, ?it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  3% 1/32 [00:00<00:06,  4.49it/s][A[A

  6% 2/32 [00:00<00:07,  4.14it/s][A[A

  9% 3/32 [00:00<00:06,  4.28it/s][A[A

 12% 4/32 [00:00<00:06,  4.51it/s][A[A

 16% 5/32 [00:01<00:05,  4.74it/s][A[A

 19% 6/32 [00:01<00:05,  4.84it/s][A[A

 22% 7/32 [00:01<00:05,  4.96it/s][A[A

 25% 8/32 [00:01<00:04,  5.00it/s][A[A

 28% 9/32 [00:01<00:04,  5.12it/s][A[A

 31% 10/32 [00:02<00:04,  5.20it/s][A[A

 34% 11/32 [00:02<00:04,  4.65it/s][A[A

 38% 12/32 [00:02<00:04,  4.32it/s][A[A

 41% 13/32 [00:03<00:10,  1.85it/s][A[A

 44% 14/32 [00:04<00:08,  2.20it/s][A[A

 47% 15/32 [00:04<00:06,  2.68it/s][A[A

 50% 16/32 [00:04<00:05,  3.05it/s][A[A

 53% 17/32 [00:04<00:04,  3.27it/s][A[A

 56% 18/32 [00:04<00:03,  3.71it/s][A[A

 59% 19/32 [00:05<00:03,  3.98it/s][A[A

 62% 20/32 [00:05<00:03,  3.96it/s][A[A

 66% 21/32 [00:05<00:02,  4.35it/s][A[A

 69% 22/32 [00:05<00:02,  4.56it/s][A[A

 72% 23/32 [00:05<00:01,  4.71it/s][A[A

 75% 24/32 [00:06<00:01,  4.60it/s][A[A

 78% 25/32 [00:06<00:01,  4.75it/s][A[A

 81% 26/32 [00:07<00:03,  1.95it/s][A[A

 84% 27/32 [00:07<00:02,  2.36it/s][A[A

 88% 28/32 [00:08<00:01,  2.85it/s][A[A

 91% 29/32 [00:08<00:00,  3.09it/s][A[A

 94% 30/32 [00:08<00:00,  3.63it/s][A[A

 97% 31/32 [00:08<00:00,  3.77it/s][A[A

100% 32/32 [00:08<00:00,  4.10it/s][A[A100% 32/32 [00:08<00:00,  3.60it/s]
Meta loss on this task batch = 5.0960e-01, PNorm = 38.4565, GNorm = 0.0835

  5% 1/19 [00:09<02:54,  9.67s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.31it/s][A[A

  6% 2/32 [00:00<00:05,  5.12it/s][A[A

  9% 3/32 [00:00<00:06,  4.62it/s][A[A

 12% 4/32 [00:00<00:06,  4.42it/s][A[A

 16% 5/32 [00:01<00:05,  4.51it/s][A[A

 19% 6/32 [00:01<00:05,  4.52it/s][A[A

 22% 7/32 [00:01<00:05,  4.85it/s][A[A

 25% 8/32 [00:02<00:11,  2.01it/s][A[A

 28% 9/32 [00:02<00:09,  2.43it/s][A[A

 31% 10/32 [00:03<00:07,  2.93it/s][A[A

 34% 11/32 [00:03<00:06,  3.35it/s][A[A

 38% 12/32 [00:03<00:05,  3.41it/s][A[A

 41% 13/32 [00:03<00:04,  3.88it/s][A[A

 44% 14/32 [00:03<00:04,  4.09it/s][A[A

 47% 15/32 [00:04<00:04,  4.25it/s][A[A

 50% 16/32 [00:04<00:03,  4.42it/s][A[A

 53% 17/32 [00:04<00:03,  4.41it/s][A[A

 56% 18/32 [00:04<00:03,  4.37it/s][A[A

 59% 19/32 [00:05<00:02,  4.53it/s][A[A

 62% 20/32 [00:05<00:02,  4.40it/s][A[A

 66% 21/32 [00:06<00:05,  1.93it/s][A[A

 69% 22/32 [00:06<00:04,  2.35it/s][A[A

 72% 23/32 [00:06<00:03,  2.85it/s][A[A

 75% 24/32 [00:07<00:02,  3.37it/s][A[A

 78% 25/32 [00:07<00:01,  3.71it/s][A[A

 81% 26/32 [00:07<00:01,  3.90it/s][A[A

 84% 27/32 [00:07<00:01,  4.14it/s][A[A

 88% 28/32 [00:07<00:00,  4.16it/s][A[A

 91% 29/32 [00:08<00:00,  4.53it/s][A[A

 94% 30/32 [00:08<00:00,  4.46it/s][A[A

 97% 31/32 [00:08<00:00,  4.47it/s][A[A

100% 32/32 [00:08<00:00,  4.74it/s][A[A100% 32/32 [00:08<00:00,  3.67it/s]
Meta loss on this task batch = 5.0408e-01, PNorm = 38.4801, GNorm = 0.0750

 11% 2/19 [00:19<02:43,  9.62s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.22it/s][A[A

  6% 2/32 [00:00<00:06,  4.99it/s][A[A

  9% 3/32 [00:00<00:06,  4.74it/s][A[A

 12% 4/32 [00:00<00:06,  4.51it/s][A[A

 16% 5/32 [00:01<00:05,  4.57it/s][A[A

 19% 6/32 [00:02<00:13,  1.91it/s][A[A

 22% 7/32 [00:02<00:10,  2.32it/s][A[A

 25% 8/32 [00:02<00:08,  2.82it/s][A[A

 28% 9/32 [00:02<00:07,  3.20it/s][A[A

 31% 10/32 [00:03<00:06,  3.47it/s][A[A

 34% 11/32 [00:03<00:05,  3.63it/s][A[A

 38% 12/32 [00:03<00:05,  3.92it/s][A[A

 41% 13/32 [00:03<00:04,  4.15it/s][A[A

 44% 14/32 [00:04<00:04,  4.27it/s][A[A

 47% 15/32 [00:04<00:03,  4.27it/s][A[A

 50% 16/32 [00:04<00:03,  4.54it/s][A[A

 53% 17/32 [00:04<00:03,  4.46it/s][A[A

 56% 18/32 [00:04<00:02,  4.87it/s][A[A

 59% 19/32 [00:06<00:06,  2.02it/s][A[A

 62% 20/32 [00:06<00:04,  2.51it/s][A[A

 66% 21/32 [00:06<00:03,  2.94it/s][A[A

 69% 22/32 [00:06<00:02,  3.34it/s][A[A

 72% 23/32 [00:06<00:02,  3.49it/s][A[A

 75% 24/32 [00:07<00:02,  3.75it/s][A[A

 78% 25/32 [00:07<00:01,  4.08it/s][A[A

 81% 26/32 [00:07<00:01,  4.41it/s][A[A

 84% 27/32 [00:07<00:01,  4.35it/s][A[A

 88% 28/32 [00:07<00:00,  4.41it/s][A[A

 91% 29/32 [00:08<00:00,  4.40it/s][A[A

 94% 30/32 [00:08<00:00,  4.27it/s][A[A

 97% 31/32 [00:09<00:00,  1.96it/s][A[A

100% 32/32 [00:09<00:00,  2.38it/s][A[A100% 32/32 [00:09<00:00,  3.27it/s]
Meta loss on this task batch = 4.9538e-01, PNorm = 38.5036, GNorm = 0.0442

 16% 3/19 [00:29<02:38,  9.91s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.00it/s][A[A

  6% 2/32 [00:00<00:07,  4.16it/s][A[A

  9% 3/32 [00:00<00:06,  4.40it/s][A[A

 12% 4/32 [00:00<00:06,  4.32it/s][A[A

 16% 5/32 [00:01<00:06,  4.37it/s][A[A

 19% 6/32 [00:01<00:06,  4.33it/s][A[A

 22% 7/32 [00:01<00:05,  4.49it/s][A[A

 25% 8/32 [00:01<00:05,  4.24it/s][A[A

 28% 9/32 [00:02<00:05,  4.53it/s][A[A

 31% 10/32 [00:03<00:11,  1.92it/s][A[A

 34% 11/32 [00:03<00:08,  2.41it/s][A[A

 38% 12/32 [00:03<00:06,  2.89it/s][A[A

 41% 13/32 [00:03<00:05,  3.26it/s][A[A

 44% 14/32 [00:04<00:05,  3.60it/s][A[A

 47% 15/32 [00:04<00:04,  4.04it/s][A[A

 50% 16/32 [00:04<00:03,  4.20it/s][A[A

 53% 17/32 [00:04<00:03,  4.42it/s][A[A

 56% 18/32 [00:04<00:03,  4.44it/s][A[A

 59% 19/32 [00:04<00:02,  4.85it/s][A[A

 62% 20/32 [00:05<00:02,  4.74it/s][A[A

 66% 21/32 [00:05<00:02,  4.69it/s][A[A

 69% 22/32 [00:05<00:02,  4.47it/s][A[A

 72% 23/32 [00:05<00:02,  4.30it/s][A[A

 75% 24/32 [00:07<00:04,  1.89it/s][A[A

 78% 25/32 [00:07<00:02,  2.37it/s][A[A

 81% 26/32 [00:07<00:02,  2.83it/s][A[A

 84% 27/32 [00:07<00:01,  3.21it/s][A[A

 88% 28/32 [00:07<00:01,  3.62it/s][A[A

 91% 29/32 [00:08<00:00,  4.10it/s][A[A

 94% 30/32 [00:08<00:00,  4.17it/s][A[A

 97% 31/32 [00:08<00:00,  4.35it/s][A[A

100% 32/32 [00:08<00:00,  4.33it/s][A[A100% 32/32 [00:08<00:00,  3.65it/s]
Meta loss on this task batch = 5.7251e-01, PNorm = 38.5258, GNorm = 0.0788

 21% 4/19 [00:39<02:27,  9.81s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.51it/s][A[A

  6% 2/32 [00:00<00:06,  4.67it/s][A[A

  9% 3/32 [00:00<00:06,  4.64it/s][A[A

 12% 4/32 [00:00<00:06,  4.39it/s][A[A

 16% 5/32 [00:01<00:05,  4.55it/s][A[A

 19% 6/32 [00:02<00:13,  1.97it/s][A[A

 22% 7/32 [00:02<00:10,  2.36it/s][A[A

 25% 8/32 [00:02<00:08,  2.87it/s][A[A

 28% 9/32 [00:02<00:07,  3.26it/s][A[A

 31% 10/32 [00:03<00:06,  3.66it/s][A[A

 34% 11/32 [00:03<00:05,  3.94it/s][A[A

 38% 12/32 [00:03<00:04,  4.06it/s][A[A

 41% 13/32 [00:03<00:04,  4.07it/s][A[A

 44% 14/32 [00:03<00:04,  4.23it/s][A[A

 47% 15/32 [00:04<00:03,  4.28it/s][A[A

 50% 16/32 [00:04<00:03,  4.71it/s][A[A

 53% 17/32 [00:04<00:03,  4.30it/s][A[A

 56% 18/32 [00:05<00:07,  1.89it/s][A[A

 59% 19/32 [00:06<00:05,  2.25it/s][A[A

 62% 20/32 [00:06<00:04,  2.61it/s][A[A

 66% 21/32 [00:06<00:03,  2.98it/s][A[A

 69% 22/32 [00:06<00:02,  3.48it/s][A[A

 72% 23/32 [00:06<00:02,  3.78it/s][A[A

 75% 24/32 [00:07<00:02,  3.84it/s][A[A

 78% 25/32 [00:07<00:01,  3.96it/s][A[A

 81% 26/32 [00:07<00:01,  4.19it/s][A[A

 84% 27/32 [00:07<00:01,  4.38it/s][A[A

 88% 28/32 [00:08<00:00,  4.81it/s][A[A

 91% 29/32 [00:08<00:00,  4.65it/s][A[A

 94% 30/32 [00:08<00:00,  4.64it/s][A[A

 97% 31/32 [00:08<00:00,  4.70it/s][A[A

100% 32/32 [00:08<00:00,  4.61it/s][A[A100% 32/32 [00:08<00:00,  3.60it/s]
Meta loss on this task batch = 5.2295e-01, PNorm = 38.5447, GNorm = 0.0718

 26% 5/19 [00:49<02:16,  9.78s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:37,  1.19s/it][A[A

  6% 2/32 [00:01<00:26,  1.12it/s][A[A

  9% 3/32 [00:01<00:19,  1.48it/s][A[A

 12% 4/32 [00:01<00:15,  1.84it/s][A[A

 16% 5/32 [00:02<00:12,  2.11it/s][A[A

 19% 6/32 [00:02<00:10,  2.47it/s][A[A

 22% 7/32 [00:02<00:08,  2.87it/s][A[A

 25% 8/32 [00:02<00:07,  3.25it/s][A[A

 28% 9/32 [00:03<00:06,  3.38it/s][A[A

 31% 10/32 [00:03<00:06,  3.56it/s][A[A

 34% 11/32 [00:03<00:05,  3.80it/s][A[A

 38% 12/32 [00:04<00:10,  1.85it/s][A[A

 41% 13/32 [00:04<00:08,  2.28it/s][A[A

 44% 14/32 [00:05<00:06,  2.62it/s][A[A

 47% 15/32 [00:05<00:05,  3.01it/s][A[A

 50% 16/32 [00:05<00:04,  3.28it/s][A[A

 53% 17/32 [00:05<00:04,  3.64it/s][A[A

 56% 18/32 [00:06<00:03,  3.82it/s][A[A

 59% 19/32 [00:06<00:03,  4.02it/s][A[A

 62% 20/32 [00:06<00:02,  4.35it/s][A[A

 66% 21/32 [00:06<00:02,  4.53it/s][A[A

 69% 22/32 [00:07<00:05,  1.94it/s][A[A

 72% 23/32 [00:08<00:03,  2.33it/s][A[A

 75% 24/32 [00:08<00:02,  2.75it/s][A[A

 78% 25/32 [00:08<00:02,  3.09it/s][A[A

 81% 26/32 [00:08<00:01,  3.53it/s][A[A

 84% 27/32 [00:08<00:01,  3.71it/s][A[A

 88% 28/32 [00:09<00:01,  3.59it/s][A[A

 91% 29/32 [00:09<00:00,  3.68it/s][A[A

 94% 30/32 [00:09<00:00,  3.98it/s][A[A

 97% 31/32 [00:09<00:00,  4.20it/s][A[A

100% 32/32 [00:10<00:00,  4.38it/s][A[A100% 32/32 [00:10<00:00,  3.16it/s]
Meta loss on this task batch = 4.3664e-01, PNorm = 38.5613, GNorm = 0.0705

 32% 6/19 [00:59<02:11, 10.13s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:37,  1.19s/it][A[A

  6% 2/32 [00:01<00:26,  1.12it/s][A[A

  9% 3/32 [00:01<00:20,  1.44it/s][A[A

 12% 4/32 [00:01<00:15,  1.78it/s][A[A

 16% 5/32 [00:02<00:12,  2.20it/s][A[A

 19% 6/32 [00:02<00:09,  2.63it/s][A[A

 22% 7/32 [00:02<00:08,  2.96it/s][A[A

 25% 8/32 [00:02<00:07,  3.34it/s][A[A

 28% 9/32 [00:02<00:06,  3.69it/s][A[A

 31% 10/32 [00:03<00:05,  3.86it/s][A[A

 34% 11/32 [00:03<00:05,  4.08it/s][A[A

 38% 12/32 [00:03<00:04,  4.17it/s][A[A

 41% 13/32 [00:03<00:04,  4.14it/s][A[A

 44% 14/32 [00:05<00:09,  1.90it/s][A[A

 47% 15/32 [00:05<00:07,  2.26it/s][A[A

 50% 16/32 [00:05<00:06,  2.60it/s][A[A

 53% 17/32 [00:05<00:05,  2.94it/s][A[A

 56% 18/32 [00:05<00:04,  3.29it/s][A[A

 59% 19/32 [00:06<00:03,  3.61it/s][A[A

 62% 20/32 [00:06<00:03,  3.81it/s][A[A

 66% 21/32 [00:06<00:02,  3.97it/s][A[A

 69% 22/32 [00:06<00:02,  4.15it/s][A[A

 72% 23/32 [00:07<00:02,  4.19it/s][A[A

 75% 24/32 [00:08<00:04,  1.88it/s][A[A

 78% 25/32 [00:08<00:03,  2.28it/s][A[A

 81% 26/32 [00:08<00:02,  2.66it/s][A[A

 84% 27/32 [00:08<00:01,  3.06it/s][A[A

 88% 28/32 [00:09<00:01,  3.32it/s][A[A

 91% 29/32 [00:09<00:00,  3.63it/s][A[A

 94% 30/32 [00:09<00:00,  4.01it/s][A[A

 97% 31/32 [00:09<00:00,  4.18it/s][A[A

100% 32/32 [00:10<00:00,  4.28it/s][A[A100% 32/32 [00:10<00:00,  3.18it/s]
Meta loss on this task batch = 4.6995e-01, PNorm = 38.5797, GNorm = 0.0983

 37% 7/19 [01:10<02:04, 10.36s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.10it/s][A[A

  6% 2/32 [00:00<00:07,  4.05it/s][A[A

  9% 3/32 [00:00<00:07,  4.09it/s][A[A

 12% 4/32 [00:01<00:15,  1.85it/s][A[A

 16% 5/32 [00:02<00:11,  2.25it/s][A[A

 19% 6/32 [00:02<00:09,  2.60it/s][A[A

 22% 7/32 [00:02<00:08,  3.02it/s][A[A

 25% 8/32 [00:02<00:07,  3.28it/s][A[A

 28% 9/32 [00:03<00:06,  3.54it/s][A[A

 31% 10/32 [00:03<00:05,  3.77it/s][A[A

 34% 11/32 [00:03<00:05,  4.07it/s][A[A

 38% 12/32 [00:03<00:04,  4.30it/s][A[A

 41% 13/32 [00:03<00:04,  4.55it/s][A[A

 44% 14/32 [00:04<00:03,  4.62it/s][A[A

 47% 15/32 [00:04<00:03,  4.70it/s][A[A

 50% 16/32 [00:04<00:03,  4.65it/s][A[A

 53% 17/32 [00:05<00:07,  1.92it/s][A[A

 56% 18/32 [00:06<00:06,  2.31it/s][A[A

 59% 19/32 [00:06<00:04,  2.64it/s][A[A

 62% 20/32 [00:06<00:03,  3.02it/s][A[A

 66% 21/32 [00:06<00:03,  3.38it/s][A[A

 69% 22/32 [00:06<00:02,  3.65it/s][A[A

 72% 23/32 [00:07<00:02,  3.93it/s][A[A

 75% 24/32 [00:07<00:01,  4.00it/s][A[A

 78% 25/32 [00:08<00:03,  1.84it/s][A[A

 81% 26/32 [00:08<00:02,  2.21it/s][A[A

 84% 27/32 [00:09<00:01,  2.58it/s][A[A

 88% 28/32 [00:09<00:01,  2.87it/s][A[A

 91% 29/32 [00:09<00:00,  3.13it/s][A[A

 94% 30/32 [00:09<00:00,  3.38it/s][A[A

 97% 31/32 [00:11<00:00,  1.74it/s][A[A

100% 32/32 [00:11<00:00,  2.12it/s][A[A100% 32/32 [00:11<00:00,  2.83it/s]
Meta loss on this task batch = 3.6179e-01, PNorm = 38.5998, GNorm = 0.0618

 42% 8/19 [01:22<01:59, 10.89s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.13it/s][A[A

  6% 2/32 [00:00<00:07,  4.19it/s][A[A

  9% 3/32 [00:00<00:06,  4.22it/s][A[A

 12% 4/32 [00:00<00:06,  4.27it/s][A[A

 16% 5/32 [00:01<00:06,  4.26it/s][A[A

 19% 6/32 [00:01<00:06,  4.30it/s][A[A

 22% 7/32 [00:01<00:05,  4.26it/s][A[A

 25% 8/32 [00:01<00:05,  4.39it/s][A[A

 28% 9/32 [00:03<00:12,  1.88it/s][A[A

 31% 10/32 [00:03<00:09,  2.27it/s][A[A

 34% 11/32 [00:03<00:07,  2.66it/s][A[A

 38% 12/32 [00:03<00:06,  2.99it/s][A[A

 41% 13/32 [00:04<00:05,  3.25it/s][A[A

 44% 14/32 [00:04<00:05,  3.44it/s][A[A

 47% 15/32 [00:04<00:04,  3.71it/s][A[A

 50% 16/32 [00:04<00:04,  3.89it/s][A[A

 53% 17/32 [00:04<00:03,  4.04it/s][A[A

 56% 18/32 [00:05<00:03,  4.07it/s][A[A

 59% 19/32 [00:05<00:03,  4.10it/s][A[A

 62% 20/32 [00:05<00:02,  4.15it/s][A[A

 66% 21/32 [00:06<00:05,  1.88it/s][A[A

 69% 22/32 [00:07<00:04,  2.27it/s][A[A

 72% 23/32 [00:07<00:03,  2.62it/s][A[A

 75% 24/32 [00:07<00:02,  2.98it/s][A[A

 78% 25/32 [00:07<00:02,  3.34it/s][A[A

 81% 26/32 [00:08<00:01,  3.62it/s][A[A

 84% 27/32 [00:08<00:01,  3.78it/s][A[A

 88% 28/32 [00:08<00:01,  3.90it/s][A[A

 91% 29/32 [00:08<00:00,  4.01it/s][A[A

 94% 30/32 [00:08<00:00,  4.09it/s][A[A

 97% 31/32 [00:10<00:00,  1.84it/s][A[A

100% 32/32 [00:10<00:00,  2.25it/s][A[A100% 32/32 [00:10<00:00,  3.07it/s]
Meta loss on this task batch = 2.1356e-01, PNorm = 38.6202, GNorm = 0.0831

 47% 9/19 [01:34<01:49, 10.99s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.02it/s][A[A

  6% 2/32 [00:00<00:07,  4.15it/s][A[A

  9% 3/32 [00:00<00:06,  4.19it/s][A[A

 12% 4/32 [00:00<00:06,  4.19it/s][A[A

 16% 5/32 [00:01<00:06,  4.36it/s][A[A

 19% 6/32 [00:01<00:05,  4.34it/s][A[A

 22% 7/32 [00:01<00:05,  4.33it/s][A[A

 25% 8/32 [00:01<00:05,  4.43it/s][A[A

 28% 9/32 [00:02<00:05,  4.39it/s][A[A

 31% 10/32 [00:03<00:11,  1.90it/s][A[A

 34% 11/32 [00:03<00:09,  2.28it/s][A[A

 38% 12/32 [00:03<00:07,  2.66it/s][A[A

 41% 13/32 [00:03<00:06,  2.99it/s][A[A

 44% 14/32 [00:04<00:05,  3.33it/s][A[A

 47% 15/32 [00:04<00:04,  3.44it/s][A[A

 50% 16/32 [00:04<00:04,  3.79it/s][A[A

 53% 17/32 [00:04<00:03,  3.94it/s][A[A

 56% 18/32 [00:05<00:03,  4.07it/s][A[A

 59% 19/32 [00:05<00:03,  4.16it/s][A[A

 62% 20/32 [00:05<00:02,  4.20it/s][A[A

 66% 21/32 [00:06<00:05,  1.89it/s][A[A

 69% 22/32 [00:07<00:04,  2.28it/s][A[A

 72% 23/32 [00:07<00:03,  2.59it/s][A[A

 75% 24/32 [00:07<00:02,  2.85it/s][A[A

 78% 25/32 [00:07<00:02,  3.20it/s][A[A

 81% 26/32 [00:08<00:01,  3.47it/s][A[A

 84% 27/32 [00:08<00:01,  3.76it/s][A[A

 88% 28/32 [00:08<00:01,  3.91it/s][A[A

 91% 29/32 [00:08<00:00,  4.00it/s][A[A

 94% 30/32 [00:09<00:01,  1.84it/s][A[A

 97% 31/32 [00:10<00:00,  2.23it/s][A[A

100% 32/32 [00:10<00:00,  2.60it/s][A[A100% 32/32 [00:10<00:00,  3.08it/s]
Meta loss on this task batch = 2.8585e-01, PNorm = 38.6410, GNorm = 0.0675

 53% 10/19 [01:45<01:39, 11.06s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.24it/s][A[A

  6% 2/32 [00:00<00:07,  4.23it/s][A[A

  9% 3/32 [00:00<00:06,  4.29it/s][A[A

 12% 4/32 [00:00<00:06,  4.31it/s][A[A

 16% 5/32 [00:01<00:06,  4.39it/s][A[A

 19% 6/32 [00:01<00:06,  4.32it/s][A[A

 22% 7/32 [00:02<00:13,  1.90it/s][A[A

 25% 8/32 [00:02<00:10,  2.26it/s][A[A

 28% 9/32 [00:03<00:08,  2.62it/s][A[A

 31% 10/32 [00:03<00:07,  2.95it/s][A[A

 34% 11/32 [00:03<00:06,  3.26it/s][A[A

 38% 12/32 [00:03<00:05,  3.49it/s][A[A

 41% 13/32 [00:04<00:05,  3.66it/s][A[A

 44% 14/32 [00:05<00:10,  1.79it/s][A[A

 47% 15/32 [00:05<00:07,  2.15it/s][A[A

 50% 16/32 [00:05<00:06,  2.54it/s][A[A

 53% 17/32 [00:05<00:05,  2.89it/s][A[A

 56% 18/32 [00:06<00:04,  3.16it/s][A[A

 59% 19/32 [00:06<00:03,  3.39it/s][A[A

 62% 20/32 [00:07<00:06,  1.74it/s][A[A

 66% 21/32 [00:07<00:05,  2.11it/s][A[A

 69% 22/32 [00:08<00:04,  2.47it/s][A[A

 72% 23/32 [00:08<00:03,  2.82it/s][A[A

 75% 24/32 [00:08<00:02,  3.10it/s][A[A

 78% 25/32 [00:08<00:02,  3.37it/s][A[A

 81% 26/32 [00:09<00:01,  3.58it/s][A[A

 84% 27/32 [00:10<00:02,  1.79it/s][A[A

 88% 28/32 [00:10<00:01,  2.18it/s][A[A

 91% 29/32 [00:10<00:01,  2.56it/s][A[A

 94% 30/32 [00:11<00:00,  2.92it/s][A[A

 97% 31/32 [00:11<00:00,  3.20it/s][A[A

100% 32/32 [00:11<00:00,  3.49it/s][A[A100% 32/32 [00:11<00:00,  2.78it/s]
Meta loss on this task batch = 5.6961e-01, PNorm = 38.6609, GNorm = 0.0595

 58% 11/19 [01:57<01:31, 11.45s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.82it/s][A[A

  6% 2/32 [00:00<00:07,  3.91it/s][A[A

  9% 3/32 [00:00<00:07,  3.99it/s][A[A

 12% 4/32 [00:00<00:06,  4.09it/s][A[A

 16% 5/32 [00:01<00:06,  4.16it/s][A[A

 19% 6/32 [00:01<00:06,  4.16it/s][A[A

 22% 7/32 [00:02<00:13,  1.86it/s][A[A

 25% 8/32 [00:02<00:10,  2.24it/s][A[A

 28% 9/32 [00:03<00:08,  2.61it/s][A[A

 31% 10/32 [00:03<00:07,  2.94it/s][A[A

 34% 11/32 [00:03<00:06,  3.24it/s][A[A

 38% 12/32 [00:03<00:05,  3.48it/s][A[A

 41% 13/32 [00:04<00:05,  3.69it/s][A[A

 44% 14/32 [00:04<00:04,  3.84it/s][A[A

 47% 15/32 [00:05<00:09,  1.83it/s][A[A

 50% 16/32 [00:05<00:07,  2.21it/s][A[A

 53% 17/32 [00:06<00:05,  2.58it/s][A[A

 56% 18/32 [00:06<00:04,  2.93it/s][A[A

 59% 19/32 [00:06<00:04,  3.24it/s][A[A

 62% 20/32 [00:06<00:03,  3.46it/s][A[A

 66% 21/32 [00:06<00:02,  3.68it/s][A[A

 69% 22/32 [00:07<00:02,  3.82it/s][A[A

 72% 23/32 [00:08<00:04,  1.81it/s][A[A

 75% 24/32 [00:08<00:03,  2.17it/s][A[A

 78% 25/32 [00:08<00:02,  2.54it/s][A[A

 81% 26/32 [00:09<00:02,  2.89it/s][A[A

 84% 27/32 [00:09<00:01,  3.19it/s][A[A

 88% 28/32 [00:09<00:01,  3.44it/s][A[A

 91% 29/32 [00:09<00:00,  3.68it/s][A[A

 94% 30/32 [00:10<00:00,  3.84it/s][A[A

 97% 31/32 [00:10<00:00,  3.95it/s][A[A

100% 32/32 [00:10<00:00,  4.02it/s][A[A100% 32/32 [00:10<00:00,  3.03it/s]
Meta loss on this task batch = 5.6864e-01, PNorm = 38.6803, GNorm = 0.0570

 63% 12/19 [02:09<01:19, 11.43s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.87it/s][A[A

  6% 2/32 [00:00<00:07,  3.95it/s][A[A

  9% 3/32 [00:01<00:15,  1.83it/s][A[A

 12% 4/32 [00:01<00:12,  2.20it/s][A[A

 16% 5/32 [00:02<00:10,  2.56it/s][A[A

 19% 6/32 [00:02<00:08,  2.91it/s][A[A

 22% 7/32 [00:02<00:07,  3.16it/s][A[A

 25% 8/32 [00:02<00:07,  3.40it/s][A[A

 28% 9/32 [00:03<00:06,  3.69it/s][A[A

 31% 10/32 [00:04<00:12,  1.78it/s][A[A

 34% 11/32 [00:04<00:09,  2.15it/s][A[A

 38% 12/32 [00:04<00:07,  2.54it/s][A[A

 41% 13/32 [00:05<00:06,  2.87it/s][A[A

 44% 14/32 [00:05<00:05,  3.17it/s][A[A

 47% 15/32 [00:05<00:04,  3.44it/s][A[A

 50% 16/32 [00:05<00:04,  3.63it/s][A[A

 53% 17/32 [00:06<00:03,  3.76it/s][A[A

 56% 18/32 [00:07<00:07,  1.81it/s][A[A

 59% 19/32 [00:07<00:06,  2.16it/s][A[A

 62% 20/32 [00:07<00:04,  2.53it/s][A[A

 66% 21/32 [00:08<00:03,  2.86it/s][A[A

 69% 22/32 [00:08<00:03,  3.17it/s][A[A

 72% 23/32 [00:08<00:02,  3.43it/s][A[A

 75% 24/32 [00:08<00:02,  3.67it/s][A[A

 78% 25/32 [00:08<00:01,  3.87it/s][A[A

 81% 26/32 [00:09<00:01,  3.96it/s][A[A

 84% 27/32 [00:09<00:01,  4.13it/s][A[A

 88% 28/32 [00:09<00:00,  4.14it/s][A[A

 91% 29/32 [00:09<00:00,  4.17it/s][A[A

 94% 30/32 [00:10<00:00,  4.17it/s][A[A

 97% 31/32 [00:10<00:00,  4.23it/s][A[A

100% 32/32 [00:11<00:00,  1.85it/s][A[A100% 32/32 [00:11<00:00,  2.76it/s]
Meta loss on this task batch = 5.9759e-01, PNorm = 38.6982, GNorm = 0.0733

 68% 13/19 [02:21<01:10, 11.74s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  3.97it/s][A[A

  6% 2/32 [00:00<00:07,  4.04it/s][A[A

  9% 3/32 [00:00<00:06,  4.19it/s][A[A

 12% 4/32 [00:00<00:06,  4.26it/s][A[A

 16% 5/32 [00:01<00:06,  4.25it/s][A[A

 19% 6/32 [00:01<00:06,  4.29it/s][A[A

 22% 7/32 [00:01<00:05,  4.27it/s][A[A

 25% 8/32 [00:01<00:05,  4.38it/s][A[A

 28% 9/32 [00:02<00:05,  4.34it/s][A[A

 31% 10/32 [00:03<00:11,  1.87it/s][A[A

 34% 11/32 [00:03<00:09,  2.22it/s][A[A

 38% 12/32 [00:03<00:07,  2.54it/s][A[A

 41% 13/32 [00:04<00:06,  2.86it/s][A[A

 44% 14/32 [00:04<00:05,  3.13it/s][A[A

 47% 15/32 [00:04<00:05,  3.40it/s][A[A

 50% 16/32 [00:04<00:04,  3.57it/s][A[A

 53% 17/32 [00:05<00:03,  3.76it/s][A[A

 56% 18/32 [00:06<00:07,  1.79it/s][A[A

 59% 19/32 [00:06<00:05,  2.19it/s][A[A

 62% 20/32 [00:06<00:04,  2.54it/s][A[A

 66% 21/32 [00:06<00:03,  2.90it/s][A[A

 69% 22/32 [00:07<00:03,  3.23it/s][A[A

 72% 23/32 [00:07<00:02,  3.46it/s][A[A

 75% 24/32 [00:07<00:02,  3.69it/s][A[A

 78% 25/32 [00:07<00:01,  3.81it/s][A[A

 81% 26/32 [00:08<00:01,  3.87it/s][A[A

 84% 27/32 [00:09<00:02,  1.80it/s][A[A

 88% 28/32 [00:09<00:01,  2.17it/s][A[A

 91% 29/32 [00:09<00:01,  2.54it/s][A[A

 94% 30/32 [00:10<00:00,  2.94it/s][A[A

 97% 31/32 [00:10<00:00,  3.25it/s][A[A

100% 32/32 [00:10<00:00,  3.44it/s][A[A100% 32/32 [00:10<00:00,  3.02it/s]
Meta loss on this task batch = 5.7809e-01, PNorm = 38.7137, GNorm = 0.0833

 74% 14/19 [02:33<00:58, 11.65s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.32it/s][A[A

  6% 2/32 [00:00<00:07,  4.20it/s][A[A

  9% 3/32 [00:00<00:06,  4.21it/s][A[A

 12% 4/32 [00:00<00:06,  4.26it/s][A[A

 16% 5/32 [00:01<00:06,  4.33it/s][A[A

 19% 6/32 [00:01<00:06,  4.12it/s][A[A

 22% 7/32 [00:02<00:13,  1.83it/s][A[A

 25% 8/32 [00:02<00:10,  2.26it/s][A[A

 28% 9/32 [00:03<00:08,  2.59it/s][A[A

 31% 10/32 [00:03<00:07,  2.94it/s][A[A

 34% 11/32 [00:03<00:06,  3.32it/s][A[A

 38% 12/32 [00:03<00:05,  3.60it/s][A[A

 41% 13/32 [00:04<00:04,  3.86it/s][A[A

 44% 14/32 [00:04<00:04,  3.97it/s][A[A

 47% 15/32 [00:04<00:04,  4.22it/s][A[A

 50% 16/32 [00:04<00:03,  4.37it/s][A[A

 53% 17/32 [00:04<00:03,  4.29it/s][A[A

 56% 18/32 [00:06<00:07,  1.86it/s][A[A

 59% 19/32 [00:06<00:05,  2.28it/s][A[A

 62% 20/32 [00:06<00:04,  2.65it/s][A[A

 66% 21/32 [00:06<00:03,  3.09it/s][A[A

 69% 22/32 [00:07<00:02,  3.41it/s][A[A

 72% 23/32 [00:07<00:02,  3.60it/s][A[A

 75% 24/32 [00:07<00:02,  3.86it/s][A[A

 78% 25/32 [00:07<00:01,  4.02it/s][A[A

 81% 26/32 [00:07<00:01,  4.01it/s][A[A

 84% 27/32 [00:08<00:01,  4.07it/s][A[A

 88% 28/32 [00:09<00:02,  1.84it/s][A[A

 91% 29/32 [00:09<00:01,  2.21it/s][A[A

 94% 30/32 [00:09<00:00,  2.58it/s][A[A

 97% 31/32 [00:10<00:00,  2.91it/s][A[A

100% 32/32 [00:10<00:00,  3.18it/s][A[A100% 32/32 [00:10<00:00,  3.08it/s]
Meta loss on this task batch = 4.9294e-01, PNorm = 38.7310, GNorm = 0.1120

 79% 15/19 [02:44<00:46, 11.53s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.19it/s][A[A

  6% 2/32 [00:00<00:07,  4.26it/s][A[A

  9% 3/32 [00:00<00:06,  4.31it/s][A[A

 12% 4/32 [00:00<00:06,  4.33it/s][A[A

 16% 5/32 [00:02<00:14,  1.88it/s][A[A

 19% 6/32 [00:02<00:11,  2.27it/s][A[A

 22% 7/32 [00:02<00:09,  2.64it/s][A[A

 25% 8/32 [00:02<00:08,  3.00it/s][A[A

 28% 9/32 [00:03<00:06,  3.30it/s][A[A

 31% 10/32 [00:03<00:06,  3.54it/s][A[A

 34% 11/32 [00:03<00:05,  3.71it/s][A[A

 38% 12/32 [00:03<00:05,  3.88it/s][A[A

 41% 13/32 [00:04<00:04,  4.04it/s][A[A

 44% 14/32 [00:04<00:04,  4.12it/s][A[A

 47% 15/32 [00:04<00:04,  4.14it/s][A[A

 50% 16/32 [00:04<00:03,  4.19it/s][A[A

 53% 17/32 [00:05<00:08,  1.87it/s][A[A

 56% 18/32 [00:06<00:06,  2.32it/s][A[A

 59% 19/32 [00:06<00:04,  2.69it/s][A[A

 62% 20/32 [00:06<00:03,  3.04it/s][A[A

 66% 21/32 [00:06<00:03,  3.40it/s][A[A

 69% 22/32 [00:07<00:02,  3.72it/s][A[A

 72% 23/32 [00:07<00:02,  3.88it/s][A[A

 75% 24/32 [00:07<00:02,  3.91it/s][A[A

 78% 25/32 [00:07<00:01,  4.22it/s][A[A

 81% 26/32 [00:08<00:03,  1.84it/s][A[A

 84% 27/32 [00:09<00:02,  2.19it/s][A[A

 88% 28/32 [00:09<00:01,  2.60it/s][A[A

 91% 29/32 [00:09<00:01,  2.83it/s][A[A

 94% 30/32 [00:09<00:00,  3.13it/s][A[A

 97% 31/32 [00:10<00:00,  3.41it/s][A[A

100% 32/32 [00:10<00:00,  3.62it/s][A[A100% 32/32 [00:10<00:00,  3.08it/s]
Meta loss on this task batch = 5.3784e-01, PNorm = 38.7464, GNorm = 0.0572

 84% 16/19 [02:55<00:34, 11.44s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.82it/s][A[A

  6% 2/32 [00:01<00:16,  1.78it/s][A[A

  9% 3/32 [00:01<00:13,  2.16it/s][A[A

 12% 4/32 [00:01<00:10,  2.58it/s][A[A

 16% 5/32 [00:02<00:09,  2.89it/s][A[A

 19% 6/32 [00:02<00:07,  3.28it/s][A[A

 22% 7/32 [00:02<00:06,  3.58it/s][A[A

 25% 8/32 [00:02<00:06,  3.66it/s][A[A

 28% 9/32 [00:03<00:05,  3.86it/s][A[A

 31% 10/32 [00:03<00:05,  4.36it/s][A[A

 34% 11/32 [00:03<00:04,  4.45it/s][A[A

 38% 12/32 [00:04<00:10,  1.91it/s][A[A

 41% 13/32 [00:04<00:08,  2.32it/s][A[A

 44% 14/32 [00:05<00:06,  2.72it/s][A[A

 47% 15/32 [00:05<00:05,  3.07it/s][A[A

 50% 16/32 [00:05<00:04,  3.32it/s][A[A

 53% 17/32 [00:05<00:04,  3.55it/s][A[A

 56% 18/32 [00:06<00:03,  3.70it/s][A[A

 59% 19/32 [00:06<00:03,  3.70it/s][A[A

 62% 20/32 [00:06<00:03,  3.88it/s][A[A

 66% 21/32 [00:06<00:02,  4.02it/s][A[A

 69% 22/32 [00:08<00:05,  1.83it/s][A[A

 72% 23/32 [00:08<00:04,  2.19it/s][A[A

 75% 24/32 [00:08<00:03,  2.49it/s][A[A

 78% 25/32 [00:08<00:02,  2.91it/s][A[A

 81% 26/32 [00:09<00:01,  3.23it/s][A[A

 84% 27/32 [00:09<00:01,  3.63it/s][A[A

 88% 28/32 [00:09<00:01,  3.94it/s][A[A

 91% 29/32 [00:09<00:00,  4.04it/s][A[A

 94% 30/32 [00:09<00:00,  4.09it/s][A[A

 97% 31/32 [00:10<00:00,  4.37it/s][A[A

100% 32/32 [00:11<00:00,  1.85it/s][A[A100% 32/32 [00:11<00:00,  2.82it/s]
Meta loss on this task batch = 4.2653e-01, PNorm = 38.7635, GNorm = 0.0654

 89% 17/19 [03:07<00:23, 11.67s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.89it/s][A[A

  6% 2/32 [00:00<00:06,  4.80it/s][A[A

  9% 3/32 [00:00<00:06,  4.64it/s][A[A

 12% 4/32 [00:00<00:06,  4.57it/s][A[A

 16% 5/32 [00:01<00:06,  4.43it/s][A[A

 19% 6/32 [00:01<00:05,  4.51it/s][A[A

 22% 7/32 [00:01<00:05,  4.35it/s][A[A

 25% 8/32 [00:02<00:12,  1.86it/s][A[A

 28% 9/32 [00:03<00:10,  2.27it/s][A[A

 31% 10/32 [00:03<00:08,  2.65it/s][A[A

 34% 11/32 [00:03<00:06,  3.02it/s][A[A

 38% 12/32 [00:03<00:05,  3.42it/s][A[A

 41% 13/32 [00:03<00:05,  3.59it/s][A[A

 44% 14/32 [00:04<00:04,  3.93it/s][A[A

 47% 15/32 [00:04<00:04,  3.94it/s][A[A

 50% 16/32 [00:04<00:03,  4.14it/s][A[A

 53% 17/32 [00:04<00:03,  4.39it/s][A[A

 56% 18/32 [00:05<00:03,  4.47it/s][A[A

 59% 19/32 [00:05<00:02,  4.43it/s][A[A

 62% 20/32 [00:05<00:02,  4.46it/s][A[A

 66% 21/32 [00:06<00:05,  1.92it/s][A[A

 69% 22/32 [00:06<00:04,  2.36it/s][A[A

 72% 23/32 [00:07<00:03,  2.62it/s][A[A

 75% 24/32 [00:07<00:02,  2.83it/s][A[A

 78% 25/32 [00:07<00:02,  3.14it/s][A[A

 81% 26/32 [00:07<00:01,  3.46it/s][A[A

 84% 27/32 [00:08<00:01,  3.52it/s][A[A

 88% 28/32 [00:08<00:01,  3.68it/s][A[A

 91% 29/32 [00:08<00:00,  3.87it/s][A[A

 94% 30/32 [00:09<00:01,  1.80it/s][A[A

 97% 31/32 [00:10<00:00,  2.22it/s][A[A

100% 32/32 [00:10<00:00,  2.54it/s][A[A100% 32/32 [00:10<00:00,  3.08it/s]
Meta loss on this task batch = 5.0259e-01, PNorm = 38.7784, GNorm = 0.0690

 95% 18/19 [03:18<00:11, 11.53s/it][A

  0% 0/23 [00:00<?, ?it/s][A[A

  4% 1/23 [00:00<00:05,  4.14it/s][A[A

  9% 2/23 [00:00<00:04,  4.28it/s][A[A

 13% 3/23 [00:00<00:04,  4.32it/s][A[A

 17% 4/23 [00:00<00:04,  4.53it/s][A[A

 22% 5/23 [00:01<00:03,  4.74it/s][A[A

 26% 6/23 [00:01<00:03,  4.74it/s][A[A

 30% 7/23 [00:01<00:03,  4.57it/s][A[A

 35% 8/23 [00:01<00:03,  4.48it/s][A[A

 39% 9/23 [00:01<00:02,  4.79it/s][A[A

 43% 10/23 [00:03<00:06,  1.91it/s][A[A

 48% 11/23 [00:03<00:05,  2.34it/s][A[A

 52% 12/23 [00:03<00:04,  2.72it/s][A[A

 57% 13/23 [00:03<00:03,  3.18it/s][A[A

 61% 14/23 [00:04<00:02,  3.43it/s][A[A

 65% 15/23 [00:04<00:02,  3.15it/s][A[A

 70% 16/23 [00:04<00:02,  3.33it/s][A[A

 74% 17/23 [00:04<00:01,  3.54it/s][A[A

 78% 18/23 [00:05<00:01,  3.62it/s][A[A

 83% 19/23 [00:06<00:02,  1.77it/s][A[A

 87% 20/23 [00:06<00:01,  2.22it/s][A[A

 91% 21/23 [00:06<00:00,  2.68it/s][A[A

 96% 22/23 [00:06<00:00,  3.16it/s][A[A

100% 23/23 [00:07<00:00,  3.48it/s][A[A100% 23/23 [00:07<00:00,  3.19it/s]
Meta loss on this task batch = 4.3745e-01, PNorm = 38.7872, GNorm = 0.1375

100% 19/19 [03:26<00:00, 10.42s/it][A100% 19/19 [03:26<00:00, 10.89s/it]
Took 206.83255243301392 seconds to complete one epoch of meta training

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/20 [00:00<?, ?it/s][A[A


  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 32.35it/s]


  5% 1/20 [00:00<00:03,  5.75it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 20.81it/s]


 10% 2/20 [00:00<00:03,  5.12it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 18.05it/s][A[A[A100% 3/3 [00:00<00:00, 19.91it/s]


 15% 3/20 [00:00<00:03,  4.46it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 15.71it/s]


 20% 4/20 [00:00<00:03,  4.41it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 15.31it/s][A[A[A100% 4/4 [00:00<00:00, 17.66it/s]


 25% 5/20 [00:01<00:04,  3.68it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.67it/s][A[A[A100% 4/4 [00:00<00:00, 16.46it/s]


 30% 6/20 [00:02<00:08,  1.65it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.71it/s][A[A[A100% 4/4 [00:00<00:00, 21.52it/s]


 35% 7/20 [00:03<00:06,  1.89it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.95it/s][A[A[A


100% 4/4 [00:00<00:00, 19.51it/s][A[A[A100% 4/4 [00:00<00:00, 19.20it/s]


 40% 8/20 [00:03<00:05,  2.09it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.70it/s][A[A[A100% 4/4 [00:00<00:00, 23.39it/s]


 45% 9/20 [00:03<00:04,  2.30it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.01it/s][A[A[A100% 4/4 [00:00<00:00, 20.03it/s]


 50% 10/20 [00:04<00:04,  2.42it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 16.30it/s][A[A[A100% 4/4 [00:00<00:00, 18.75it/s]


 55% 11/20 [00:04<00:03,  2.51it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:01<00:01,  1.83it/s][A[A[A100% 4/4 [00:01<00:00,  3.37it/s]


 60% 12/20 [00:05<00:05,  1.46it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 15.18it/s][A[A[A100% 3/3 [00:00<00:00, 14.41it/s]


 65% 13/20 [00:06<00:04,  1.69it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.84it/s][A[A[A100% 3/3 [00:00<00:00, 14.43it/s]


 70% 14/20 [00:06<00:03,  1.90it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 14.42it/s][A[A[A100% 4/4 [00:00<00:00, 18.76it/s]


 75% 15/20 [00:06<00:02,  2.07it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.63it/s][A[A[A100% 3/3 [00:00<00:00, 17.91it/s]


 80% 16/20 [00:07<00:01,  2.28it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 12.51it/s][A[A[A100% 3/3 [00:00<00:00, 15.30it/s]


 85% 17/20 [00:07<00:01,  2.35it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 23.17it/s]


 90% 18/20 [00:07<00:00,  2.66it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 33% 1/3 [00:01<00:02,  1.09s/it][A[A[A100% 3/3 [00:01<00:00,  2.56it/s]


 95% 19/20 [00:09<00:00,  1.51it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 13.85it/s]


100% 20/20 [00:09<00:00,  1.86it/s][A[A100% 20/20 [00:09<00:00,  2.10it/s]

100% 1/1 [00:09<00:00,  9.53s/it][A100% 1/1 [00:09<00:00,  9.53s/it]
Took 216.36632251739502 seconds to complete one epoch of meta training and validating
Meta validation score auc = 0.644439
Found better MAML checkpoint after meta validation, saving now
 63% 19/30 [1:01:30<36:24, 198.59s/it]Epoch 19

  0% 0/19 [00:00<?, ?it/s][A

  0% 0/32 [00:00<?, ?it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  3% 1/32 [00:00<00:04,  6.21it/s][A[A

  6% 2/32 [00:00<00:05,  5.71it/s][A[A

  9% 3/32 [00:00<00:04,  5.82it/s][A[A

 12% 4/32 [00:00<00:04,  6.09it/s][A[A

 16% 5/32 [00:00<00:04,  6.45it/s][A[A

 19% 6/32 [00:00<00:04,  6.18it/s][A[A

 22% 7/32 [00:01<00:04,  6.18it/s][A[A

 25% 8/32 [00:01<00:04,  5.82it/s][A[A

 28% 9/32 [00:01<00:03,  6.41it/s][A[A

 31% 10/32 [00:01<00:03,  6.76it/s][A[A

 34% 11/32 [00:01<00:03,  5.96it/s][A[A

 38% 12/32 [00:02<00:04,  4.90it/s][A[A

 41% 13/32 [00:02<00:04,  4.66it/s][A[A

 44% 14/32 [00:02<00:03,  4.61it/s][A[A

 47% 15/32 [00:03<00:08,  2.01it/s][A[A

 50% 16/32 [00:03<00:06,  2.44it/s][A[A

 53% 17/32 [00:04<00:05,  2.92it/s][A[A

 56% 18/32 [00:04<00:04,  3.48it/s][A[A

 59% 19/32 [00:04<00:03,  4.03it/s][A[A

 62% 20/32 [00:04<00:02,  4.35it/s][A[A

 66% 21/32 [00:04<00:02,  5.08it/s][A[A

 69% 22/32 [00:04<00:01,  5.18it/s][A[A

 72% 23/32 [00:05<00:01,  6.03it/s][A[A

 75% 24/32 [00:05<00:01,  5.86it/s][A[A

 78% 25/32 [00:05<00:01,  5.74it/s][A[A

 81% 26/32 [00:05<00:01,  5.47it/s][A[A

 84% 27/32 [00:05<00:00,  6.31it/s][A[A

 88% 28/32 [00:05<00:00,  6.26it/s][A[A

 91% 29/32 [00:06<00:00,  5.55it/s][A[A

 94% 30/32 [00:06<00:00,  6.35it/s][A[A

 97% 31/32 [00:06<00:00,  6.95it/s][A[A

100% 32/32 [00:07<00:00,  2.19it/s][A[A100% 32/32 [00:07<00:00,  4.28it/s]
Meta loss on this task batch = 5.2377e-01, PNorm = 38.7987, GNorm = 0.1835

  5% 1/19 [00:08<02:26,  8.16s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:03,  8.19it/s][A[A

  6% 2/32 [00:00<00:03,  7.83it/s][A[A

  9% 3/32 [00:00<00:04,  7.07it/s][A[A

 12% 4/32 [00:00<00:04,  6.61it/s][A[A

 16% 5/32 [00:00<00:04,  6.11it/s][A[A

 19% 6/32 [00:00<00:04,  6.40it/s][A[A

 22% 7/32 [00:01<00:03,  6.75it/s][A[A

 25% 8/32 [00:01<00:03,  6.89it/s][A[A

 28% 9/32 [00:01<00:03,  6.70it/s][A[A

 31% 10/32 [00:01<00:03,  6.87it/s][A[A

 34% 11/32 [00:01<00:03,  6.97it/s][A[A

 38% 12/32 [00:01<00:03,  6.52it/s][A[A

 41% 13/32 [00:01<00:02,  6.72it/s][A[A

 44% 14/32 [00:02<00:02,  6.33it/s][A[A

 47% 15/32 [00:02<00:02,  6.35it/s][A[A

 50% 16/32 [00:02<00:02,  6.08it/s][A[A

 53% 17/32 [00:02<00:02,  5.84it/s][A[A

 56% 18/32 [00:02<00:02,  5.33it/s][A[A

 59% 19/32 [00:02<00:02,  6.10it/s][A[A

 62% 20/32 [00:03<00:02,  5.41it/s][A[A

 66% 21/32 [00:04<00:05,  2.11it/s][A[A

 69% 22/32 [00:04<00:03,  2.68it/s][A[A

 72% 23/32 [00:04<00:02,  3.27it/s][A[A

 75% 24/32 [00:04<00:02,  3.92it/s][A[A

 78% 25/32 [00:04<00:01,  4.22it/s][A[A

 81% 26/32 [00:05<00:01,  4.37it/s][A[A

 84% 27/32 [00:05<00:01,  4.77it/s][A[A

 88% 28/32 [00:05<00:00,  5.04it/s][A[A

 91% 29/32 [00:05<00:00,  5.74it/s][A[A

 94% 30/32 [00:05<00:00,  5.73it/s][A[A

 97% 31/32 [00:05<00:00,  5.97it/s][A[A

100% 32/32 [00:06<00:00,  6.24it/s][A[A100% 32/32 [00:06<00:00,  5.22it/s]
Meta loss on this task batch = 4.8515e-01, PNorm = 38.8140, GNorm = 0.1025

 11% 2/19 [00:14<02:11,  7.75s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.70it/s][A[A

  6% 2/32 [00:00<00:05,  5.49it/s][A[A

  9% 3/32 [00:00<00:05,  5.68it/s][A[A

 12% 4/32 [00:00<00:05,  5.45it/s][A[A

 16% 5/32 [00:00<00:04,  5.62it/s][A[A

 19% 6/32 [00:01<00:04,  5.47it/s][A[A

 22% 7/32 [00:02<00:12,  2.06it/s][A[A

 25% 8/32 [00:02<00:09,  2.61it/s][A[A

 28% 9/32 [00:02<00:07,  3.19it/s][A[A

 31% 10/32 [00:02<00:06,  3.52it/s][A[A

 34% 11/32 [00:03<00:05,  3.82it/s][A[A

 38% 12/32 [00:03<00:04,  4.21it/s][A[A

 41% 13/32 [00:03<00:04,  4.29it/s][A[A

 44% 14/32 [00:03<00:04,  4.35it/s][A[A

 47% 15/32 [00:03<00:03,  4.51it/s][A[A

 50% 16/32 [00:03<00:03,  5.16it/s][A[A

 53% 17/32 [00:04<00:02,  5.59it/s][A[A

 56% 18/32 [00:04<00:02,  6.05it/s][A[A

 59% 19/32 [00:04<00:02,  6.12it/s][A[A

 62% 20/32 [00:04<00:01,  6.45it/s][A[A

 66% 21/32 [00:04<00:01,  6.80it/s][A[A

 69% 22/32 [00:04<00:01,  6.85it/s][A[A

 72% 23/32 [00:05<00:01,  6.22it/s][A[A

 75% 24/32 [00:05<00:01,  5.85it/s][A[A

 78% 25/32 [00:05<00:01,  6.26it/s][A[A

 81% 26/32 [00:06<00:02,  2.21it/s][A[A

 84% 27/32 [00:06<00:01,  2.60it/s][A[A

 88% 28/32 [00:06<00:01,  3.14it/s][A[A

 91% 29/32 [00:07<00:00,  3.60it/s][A[A

 94% 30/32 [00:07<00:00,  3.76it/s][A[A

 97% 31/32 [00:07<00:00,  4.54it/s][A[A

100% 32/32 [00:07<00:00,  4.71it/s][A[A100% 32/32 [00:07<00:00,  4.22it/s]
Meta loss on this task batch = 5.3044e-01, PNorm = 38.8296, GNorm = 0.0943

 16% 3/19 [00:23<02:06,  7.91s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.87it/s][A[A

  6% 2/32 [00:00<00:06,  4.81it/s][A[A

  9% 3/32 [00:00<00:05,  4.98it/s][A[A

 12% 4/32 [00:00<00:05,  5.03it/s][A[A

 16% 5/32 [00:01<00:05,  4.93it/s][A[A

 19% 6/32 [00:01<00:05,  4.92it/s][A[A

 22% 7/32 [00:01<00:04,  5.50it/s][A[A

 25% 8/32 [00:01<00:03,  6.18it/s][A[A

 28% 9/32 [00:01<00:03,  6.56it/s][A[A

 34% 11/32 [00:01<00:03,  7.00it/s][A[A

 38% 12/32 [00:01<00:02,  7.14it/s][A[A

 41% 13/32 [00:03<00:08,  2.20it/s][A[A

 44% 14/32 [00:03<00:06,  2.71it/s][A[A

 47% 15/32 [00:03<00:05,  3.34it/s][A[A

 50% 16/32 [00:03<00:04,  3.82it/s][A[A

 53% 17/32 [00:03<00:03,  4.65it/s][A[A

 56% 18/32 [00:03<00:02,  4.70it/s][A[A

 59% 19/32 [00:04<00:02,  5.26it/s][A[A

 62% 20/32 [00:04<00:02,  5.46it/s][A[A

 66% 21/32 [00:04<00:02,  5.31it/s][A[A

 69% 22/32 [00:04<00:01,  5.37it/s][A[A

 72% 23/32 [00:04<00:01,  4.95it/s][A[A

 75% 24/32 [00:05<00:01,  5.02it/s][A[A

 78% 25/32 [00:05<00:01,  5.55it/s][A[A

 81% 26/32 [00:05<00:00,  6.12it/s][A[A

 84% 27/32 [00:05<00:00,  6.08it/s][A[A

 88% 28/32 [00:05<00:00,  5.90it/s][A[A

 91% 29/32 [00:05<00:00,  6.26it/s][A[A

 94% 30/32 [00:06<00:00,  5.83it/s][A[A

 97% 31/32 [00:06<00:00,  6.19it/s][A[A

100% 32/32 [00:07<00:00,  2.11it/s][A[A100% 32/32 [00:07<00:00,  4.35it/s]
Meta loss on this task batch = 5.5442e-01, PNorm = 38.8506, GNorm = 0.1136

 21% 4/19 [00:31<01:59,  7.95s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.64it/s][A[A

  6% 2/32 [00:00<00:04,  6.08it/s][A[A

 12% 4/32 [00:00<00:04,  6.36it/s][A[A

 16% 5/32 [00:00<00:04,  5.75it/s][A[A

 19% 6/32 [00:00<00:04,  5.82it/s][A[A

 22% 7/32 [00:01<00:04,  5.87it/s][A[A

 25% 8/32 [00:01<00:03,  6.16it/s][A[A

 28% 9/32 [00:01<00:03,  6.21it/s][A[A

 31% 10/32 [00:01<00:03,  6.24it/s][A[A

 34% 11/32 [00:01<00:03,  6.08it/s][A[A

 38% 12/32 [00:01<00:03,  5.70it/s][A[A

 41% 13/32 [00:02<00:03,  5.46it/s][A[A

 44% 14/32 [00:02<00:03,  5.83it/s][A[A

 47% 15/32 [00:02<00:03,  5.41it/s][A[A

 50% 16/32 [00:02<00:02,  6.14it/s][A[A

 53% 17/32 [00:02<00:02,  5.13it/s][A[A

 56% 18/32 [00:04<00:06,  2.02it/s][A[A

 59% 19/32 [00:04<00:05,  2.47it/s][A[A

 62% 20/32 [00:04<00:04,  2.81it/s][A[A

 66% 21/32 [00:04<00:03,  3.42it/s][A[A

 69% 22/32 [00:04<00:02,  4.11it/s][A[A

 72% 23/32 [00:05<00:02,  4.46it/s][A[A

 75% 24/32 [00:05<00:01,  4.52it/s][A[A

 78% 25/32 [00:05<00:01,  4.48it/s][A[A

 81% 26/32 [00:05<00:01,  5.02it/s][A[A

 84% 27/32 [00:05<00:00,  5.17it/s][A[A

 88% 28/32 [00:05<00:00,  5.51it/s][A[A

 91% 29/32 [00:06<00:00,  5.89it/s][A[A

 94% 30/32 [00:06<00:00,  6.06it/s][A[A

 97% 31/32 [00:06<00:00,  5.86it/s][A[A

100% 32/32 [00:06<00:00,  5.53it/s][A[A100% 32/32 [00:06<00:00,  4.84it/s]
Meta loss on this task batch = 5.0028e-01, PNorm = 38.8730, GNorm = 0.0508

 26% 5/19 [00:38<01:48,  7.75s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  6.44it/s][A[A

  6% 2/32 [00:01<00:13,  2.17it/s][A[A

  9% 3/32 [00:01<00:10,  2.72it/s][A[A

 12% 4/32 [00:01<00:08,  3.36it/s][A[A

 16% 5/32 [00:01<00:07,  3.57it/s][A[A

 19% 6/32 [00:02<00:06,  3.97it/s][A[A

 22% 7/32 [00:02<00:05,  4.31it/s][A[A

 25% 8/32 [00:02<00:05,  4.63it/s][A[A

 28% 9/32 [00:02<00:05,  4.34it/s][A[A

 31% 10/32 [00:02<00:04,  4.75it/s][A[A

 34% 11/32 [00:03<00:04,  4.93it/s][A[A

 38% 12/32 [00:03<00:04,  4.97it/s][A[A

 41% 13/32 [00:03<00:03,  5.39it/s][A[A

 44% 14/32 [00:03<00:03,  4.93it/s][A[A

 47% 15/32 [00:03<00:03,  4.77it/s][A[A

 50% 16/32 [00:04<00:03,  4.92it/s][A[A

 53% 17/32 [00:04<00:02,  5.41it/s][A[A

 56% 18/32 [00:04<00:02,  5.44it/s][A[A

 59% 19/32 [00:05<00:06,  2.00it/s][A[A

 62% 20/32 [00:05<00:04,  2.54it/s][A[A

 66% 21/32 [00:05<00:03,  2.99it/s][A[A

 69% 22/32 [00:06<00:02,  3.37it/s][A[A

 72% 23/32 [00:06<00:02,  3.78it/s][A[A

 75% 24/32 [00:06<00:02,  3.93it/s][A[A

 78% 25/32 [00:06<00:01,  3.99it/s][A[A

 81% 26/32 [00:06<00:01,  4.55it/s][A[A

 84% 27/32 [00:07<00:01,  4.47it/s][A[A

 88% 28/32 [00:07<00:00,  4.41it/s][A[A

 91% 29/32 [00:07<00:00,  4.33it/s][A[A

 94% 30/32 [00:07<00:00,  4.53it/s][A[A

100% 32/32 [00:09<00:00,  2.87it/s][A[A100% 32/32 [00:09<00:00,  3.50it/s]
Meta loss on this task batch = 4.6161e-01, PNorm = 38.8955, GNorm = 0.1128

 32% 6/19 [00:48<01:49,  8.39s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.75it/s][A[A

  6% 2/32 [00:00<00:04,  6.13it/s][A[A

  9% 3/32 [00:00<00:05,  5.54it/s][A[A

 12% 4/32 [00:00<00:05,  5.04it/s][A[A

 16% 5/32 [00:00<00:04,  5.60it/s][A[A

 19% 6/32 [00:01<00:04,  5.42it/s][A[A

 22% 7/32 [00:01<00:04,  5.47it/s][A[A

 25% 8/32 [00:01<00:04,  5.50it/s][A[A

 28% 9/32 [00:01<00:04,  5.51it/s][A[A

 31% 10/32 [00:01<00:04,  5.20it/s][A[A

 34% 11/32 [00:02<00:03,  5.37it/s][A[A

 38% 12/32 [00:02<00:03,  5.66it/s][A[A

 41% 13/32 [00:02<00:03,  6.03it/s][A[A

 44% 14/32 [00:02<00:02,  6.25it/s][A[A

 47% 15/32 [00:02<00:03,  5.58it/s][A[A

 50% 16/32 [00:03<00:08,  1.99it/s][A[A

 53% 17/32 [00:04<00:06,  2.46it/s][A[A

 56% 18/32 [00:04<00:04,  2.86it/s][A[A

 59% 19/32 [00:04<00:03,  3.37it/s][A[A

 62% 20/32 [00:04<00:03,  3.63it/s][A[A

 66% 21/32 [00:04<00:02,  3.93it/s][A[A

 69% 22/32 [00:05<00:02,  4.06it/s][A[A

 72% 23/32 [00:05<00:02,  4.10it/s][A[A

 75% 24/32 [00:05<00:01,  4.39it/s][A[A

 78% 25/32 [00:05<00:01,  4.66it/s][A[A

 81% 26/32 [00:05<00:01,  4.80it/s][A[A

 84% 27/32 [00:07<00:02,  2.00it/s][A[A

 88% 28/32 [00:07<00:01,  2.44it/s][A[A

 91% 29/32 [00:07<00:00,  3.12it/s][A[A

 94% 30/32 [00:07<00:00,  3.46it/s][A[A

 97% 31/32 [00:07<00:00,  3.74it/s][A[A

100% 32/32 [00:08<00:00,  3.99it/s][A[A100% 32/32 [00:08<00:00,  3.94it/s]
Meta loss on this task batch = 4.7971e-01, PNorm = 38.9220, GNorm = 0.1494

 37% 7/19 [00:57<01:42,  8.53s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  3.92it/s][A[A

  6% 2/32 [00:00<00:07,  4.12it/s][A[A

  9% 3/32 [00:00<00:06,  4.22it/s][A[A

 12% 4/32 [00:00<00:06,  4.24it/s][A[A

 16% 5/32 [00:01<00:06,  4.40it/s][A[A

 19% 6/32 [00:01<00:05,  4.58it/s][A[A

 22% 7/32 [00:01<00:04,  5.44it/s][A[A

 25% 8/32 [00:01<00:04,  5.15it/s][A[A

 28% 9/32 [00:01<00:04,  5.04it/s][A[A

 31% 10/32 [00:03<00:10,  2.04it/s][A[A

 34% 11/32 [00:03<00:08,  2.45it/s][A[A

 38% 12/32 [00:03<00:06,  2.90it/s][A[A

 41% 13/32 [00:03<00:05,  3.43it/s][A[A

 44% 14/32 [00:03<00:04,  4.21it/s][A[A

 47% 15/32 [00:03<00:03,  4.54it/s][A[A

 50% 16/32 [00:04<00:03,  4.74it/s][A[A

 53% 17/32 [00:04<00:03,  4.71it/s][A[A

 56% 18/32 [00:04<00:02,  4.91it/s][A[A

 59% 19/32 [00:04<00:02,  4.82it/s][A[A

 62% 20/32 [00:04<00:02,  4.93it/s][A[A

 66% 21/32 [00:05<00:02,  5.21it/s][A[A

 69% 22/32 [00:05<00:01,  5.32it/s][A[A

 72% 23/32 [00:06<00:04,  2.14it/s][A[A

 75% 24/32 [00:06<00:03,  2.51it/s][A[A

 78% 25/32 [00:06<00:02,  2.78it/s][A[A

 81% 26/32 [00:07<00:01,  3.22it/s][A[A

 84% 27/32 [00:07<00:01,  3.65it/s][A[A

 88% 28/32 [00:07<00:00,  4.07it/s][A[A

 91% 29/32 [00:07<00:00,  4.35it/s][A[A

 94% 30/32 [00:07<00:00,  4.56it/s][A[A

 97% 31/32 [00:08<00:00,  4.71it/s][A[A

100% 32/32 [00:08<00:00,  4.92it/s][A[A100% 32/32 [00:08<00:00,  3.90it/s]
Meta loss on this task batch = 3.5093e-01, PNorm = 38.9512, GNorm = 0.0944

 42% 8/19 [01:06<01:35,  8.65s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.73it/s][A[A

  6% 2/32 [00:01<00:15,  1.99it/s][A[A

  9% 3/32 [00:01<00:11,  2.46it/s][A[A

 12% 4/32 [00:01<00:09,  2.90it/s][A[A

 16% 5/32 [00:01<00:08,  3.31it/s][A[A

 19% 6/32 [00:02<00:06,  3.73it/s][A[A

 22% 7/32 [00:02<00:06,  4.02it/s][A[A

 25% 8/32 [00:02<00:05,  4.31it/s][A[A

 28% 9/32 [00:02<00:05,  4.55it/s][A[A

 31% 10/32 [00:02<00:04,  4.70it/s][A[A

 34% 11/32 [00:03<00:04,  4.87it/s][A[A

 38% 12/32 [00:03<00:03,  5.04it/s][A[A

 41% 13/32 [00:03<00:03,  5.15it/s][A[A

 44% 14/32 [00:03<00:03,  5.07it/s][A[A

 47% 15/32 [00:03<00:03,  4.91it/s][A[A

 50% 16/32 [00:04<00:03,  5.10it/s][A[A

 53% 17/32 [00:04<00:02,  5.19it/s][A[A

 56% 18/32 [00:04<00:02,  5.19it/s][A[A

 59% 19/32 [00:04<00:02,  5.18it/s][A[A

 62% 20/32 [00:05<00:05,  2.03it/s][A[A

 66% 21/32 [00:06<00:04,  2.50it/s][A[A

 69% 22/32 [00:06<00:03,  2.93it/s][A[A

 72% 23/32 [00:06<00:02,  3.37it/s][A[A

 75% 24/32 [00:06<00:02,  3.71it/s][A[A

 78% 25/32 [00:06<00:01,  3.99it/s][A[A

 81% 26/32 [00:07<00:01,  4.20it/s][A[A

 84% 27/32 [00:07<00:01,  4.41it/s][A[A

 88% 28/32 [00:07<00:00,  4.59it/s][A[A

 91% 29/32 [00:07<00:00,  4.74it/s][A[A

 94% 30/32 [00:07<00:00,  4.91it/s][A[A

 97% 31/32 [00:08<00:00,  4.96it/s][A[A

100% 32/32 [00:08<00:00,  4.97it/s][A[A100% 32/32 [00:08<00:00,  3.88it/s]
Meta loss on this task batch = 2.0065e-01, PNorm = 38.9804, GNorm = 0.0823

 47% 9/19 [01:15<01:27,  8.75s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.76it/s][A[A

  6% 2/32 [00:00<00:06,  4.90it/s][A[A

  9% 3/32 [00:01<00:14,  1.99it/s][A[A

 12% 4/32 [00:01<00:11,  2.44it/s][A[A

 16% 5/32 [00:02<00:09,  2.86it/s][A[A

 19% 6/32 [00:02<00:07,  3.27it/s][A[A

 22% 7/32 [00:02<00:06,  3.61it/s][A[A

 25% 8/32 [00:02<00:06,  3.99it/s][A[A

 28% 9/32 [00:02<00:05,  4.35it/s][A[A

 31% 10/32 [00:02<00:04,  4.61it/s][A[A

 34% 11/32 [00:03<00:04,  4.81it/s][A[A

 38% 12/32 [00:03<00:03,  5.02it/s][A[A

 41% 13/32 [00:03<00:03,  4.92it/s][A[A

 44% 14/32 [00:03<00:03,  4.95it/s][A[A

 47% 15/32 [00:03<00:03,  5.14it/s][A[A

 50% 16/32 [00:04<00:03,  5.21it/s][A[A

 53% 17/32 [00:04<00:02,  5.24it/s][A[A

 56% 18/32 [00:04<00:02,  5.17it/s][A[A

 59% 19/32 [00:05<00:06,  2.06it/s][A[A

 62% 20/32 [00:05<00:04,  2.50it/s][A[A

 66% 21/32 [00:06<00:03,  2.96it/s][A[A

 69% 22/32 [00:06<00:02,  3.38it/s][A[A

 72% 23/32 [00:06<00:02,  3.76it/s][A[A

 75% 24/32 [00:06<00:01,  4.09it/s][A[A

 78% 25/32 [00:06<00:01,  4.30it/s][A[A

 81% 26/32 [00:07<00:01,  4.35it/s][A[A

 84% 27/32 [00:07<00:01,  4.60it/s][A[A

 88% 28/32 [00:07<00:00,  4.40it/s][A[A

 91% 29/32 [00:07<00:00,  4.72it/s][A[A

 94% 30/32 [00:07<00:00,  4.50it/s][A[A

 97% 31/32 [00:09<00:00,  1.93it/s][A[A

100% 32/32 [00:09<00:00,  2.39it/s][A[A100% 32/32 [00:09<00:00,  3.43it/s]
Meta loss on this task batch = 2.1682e-01, PNorm = 39.0099, GNorm = 0.0936

 53% 10/19 [01:25<01:22,  9.15s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.38it/s][A[A

  6% 2/32 [00:00<00:07,  4.19it/s][A[A

  9% 3/32 [00:00<00:07,  4.08it/s][A[A

 12% 4/32 [00:00<00:06,  4.32it/s][A[A

 16% 5/32 [00:01<00:06,  4.22it/s][A[A

 19% 6/32 [00:01<00:05,  4.40it/s][A[A

 22% 7/32 [00:01<00:04,  5.08it/s][A[A

 25% 8/32 [00:01<00:04,  4.94it/s][A[A

 28% 9/32 [00:01<00:04,  4.64it/s][A[A

 31% 10/32 [00:02<00:04,  4.43it/s][A[A

 34% 11/32 [00:03<00:11,  1.88it/s][A[A

 38% 12/32 [00:03<00:08,  2.23it/s][A[A

 41% 13/32 [00:03<00:07,  2.56it/s][A[A

 44% 14/32 [00:04<00:06,  2.89it/s][A[A

 47% 15/32 [00:04<00:05,  3.36it/s][A[A

 50% 16/32 [00:04<00:04,  3.63it/s][A[A

 53% 17/32 [00:04<00:04,  3.64it/s][A[A

 56% 18/32 [00:05<00:03,  4.37it/s][A[A

 59% 19/32 [00:05<00:02,  4.37it/s][A[A

 62% 20/32 [00:05<00:02,  4.21it/s][A[A

 66% 21/32 [00:05<00:02,  4.39it/s][A[A

 69% 22/32 [00:06<00:05,  1.88it/s][A[A

 72% 23/32 [00:07<00:03,  2.28it/s][A[A

 75% 24/32 [00:07<00:03,  2.61it/s][A[A

 78% 25/32 [00:07<00:02,  3.03it/s][A[A

 81% 26/32 [00:07<00:01,  3.48it/s][A[A

 84% 27/32 [00:08<00:01,  3.79it/s][A[A

 88% 28/32 [00:08<00:01,  3.90it/s][A[A

 91% 29/32 [00:08<00:00,  4.19it/s][A[A

 94% 30/32 [00:08<00:00,  4.27it/s][A[A

 97% 31/32 [00:08<00:00,  4.15it/s][A[A

100% 32/32 [00:10<00:00,  1.80it/s][A[A100% 32/32 [00:10<00:00,  3.12it/s]
Meta loss on this task batch = 5.8639e-01, PNorm = 39.0415, GNorm = 0.0722

 58% 11/19 [01:36<01:17,  9.72s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.81it/s][A[A

  6% 2/32 [00:00<00:07,  3.86it/s][A[A

  9% 3/32 [00:00<00:07,  3.89it/s][A[A

 12% 4/32 [00:00<00:06,  4.27it/s][A[A

 16% 5/32 [00:01<00:06,  4.12it/s][A[A

 19% 6/32 [00:01<00:06,  4.12it/s][A[A

 22% 7/32 [00:01<00:05,  4.89it/s][A[A

 25% 8/32 [00:02<00:12,  1.92it/s][A[A

 28% 9/32 [00:03<00:09,  2.32it/s][A[A

 31% 10/32 [00:03<00:08,  2.63it/s][A[A

 34% 11/32 [00:03<00:06,  3.15it/s][A[A

 38% 12/32 [00:03<00:05,  3.45it/s][A[A

 41% 13/32 [00:03<00:05,  3.72it/s][A[A

 44% 14/32 [00:04<00:04,  3.93it/s][A[A

 47% 15/32 [00:04<00:04,  3.92it/s][A[A

 50% 16/32 [00:04<00:03,  4.63it/s][A[A

 53% 17/32 [00:04<00:03,  4.63it/s][A[A

 56% 18/32 [00:05<00:07,  1.92it/s][A[A

 59% 19/32 [00:06<00:05,  2.28it/s][A[A

 62% 20/32 [00:06<00:04,  2.61it/s][A[A

 66% 21/32 [00:06<00:03,  2.89it/s][A[A

 69% 22/32 [00:06<00:02,  3.60it/s][A[A

 72% 23/32 [00:07<00:02,  3.80it/s][A[A

 75% 24/32 [00:07<00:02,  3.98it/s][A[A

 78% 25/32 [00:07<00:01,  3.98it/s][A[A

 81% 26/32 [00:07<00:01,  4.06it/s][A[A

 84% 27/32 [00:08<00:01,  4.04it/s][A[A

 88% 28/32 [00:08<00:00,  4.22it/s][A[A

 91% 29/32 [00:09<00:01,  1.92it/s][A[A

 94% 30/32 [00:09<00:00,  2.28it/s][A[A

 97% 31/32 [00:09<00:00,  2.70it/s][A[A

100% 32/32 [00:10<00:00,  2.95it/s][A[A100% 32/32 [00:10<00:00,  3.15it/s]
Meta loss on this task batch = 5.4285e-01, PNorm = 39.0727, GNorm = 0.1096

 63% 12/19 [01:47<01:10, 10.08s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.53it/s][A[A

  6% 2/32 [00:00<00:07,  3.86it/s][A[A

  9% 3/32 [00:00<00:07,  3.92it/s][A[A

 12% 4/32 [00:00<00:06,  4.08it/s][A[A

 16% 5/32 [00:01<00:06,  4.06it/s][A[A

 19% 6/32 [00:01<00:06,  4.19it/s][A[A

 22% 7/32 [00:01<00:05,  4.68it/s][A[A

 25% 8/32 [00:02<00:12,  1.99it/s][A[A

 28% 9/32 [00:02<00:09,  2.51it/s][A[A

 31% 10/32 [00:03<00:07,  2.89it/s][A[A

 34% 11/32 [00:03<00:06,  3.36it/s][A[A

 38% 12/32 [00:03<00:05,  3.59it/s][A[A

 41% 13/32 [00:03<00:05,  3.77it/s][A[A

 44% 14/32 [00:03<00:03,  4.51it/s][A[A

 47% 15/32 [00:04<00:03,  4.40it/s][A[A

 50% 16/32 [00:04<00:03,  4.50it/s][A[A

 53% 17/32 [00:04<00:03,  4.61it/s][A[A

 56% 18/32 [00:04<00:02,  4.84it/s][A[A

 59% 19/32 [00:05<00:02,  4.48it/s][A[A

 62% 20/32 [00:05<00:02,  4.70it/s][A[A

 66% 21/32 [00:06<00:05,  1.88it/s][A[A

 69% 22/32 [00:06<00:04,  2.24it/s][A[A

 72% 23/32 [00:06<00:03,  2.57it/s][A[A

 75% 24/32 [00:07<00:02,  2.96it/s][A[A

 78% 25/32 [00:07<00:02,  3.23it/s][A[A

 81% 26/32 [00:07<00:01,  3.57it/s][A[A

 84% 27/32 [00:07<00:01,  3.96it/s][A[A

 88% 28/32 [00:07<00:00,  4.72it/s][A[A

 91% 29/32 [00:08<00:00,  4.35it/s][A[A

 94% 30/32 [00:08<00:00,  4.60it/s][A[A

 97% 31/32 [00:09<00:00,  1.92it/s][A[A

100% 32/32 [00:09<00:00,  2.38it/s][A[A100% 32/32 [00:09<00:00,  3.26it/s]
Meta loss on this task batch = 5.6959e-01, PNorm = 39.1028, GNorm = 0.0996

 68% 13/19 [01:57<01:01, 10.24s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.93it/s][A[A

  6% 2/32 [00:00<00:05,  5.04it/s][A[A

  9% 3/32 [00:00<00:06,  4.70it/s][A[A

 12% 4/32 [00:00<00:06,  4.58it/s][A[A

 16% 5/32 [00:01<00:06,  4.38it/s][A[A

 19% 6/32 [00:01<00:05,  4.62it/s][A[A

 22% 7/32 [00:01<00:05,  4.43it/s][A[A

 25% 8/32 [00:01<00:04,  4.89it/s][A[A

 28% 9/32 [00:01<00:04,  5.13it/s][A[A

 31% 10/32 [00:02<00:04,  5.16it/s][A[A

 34% 11/32 [00:03<00:10,  1.96it/s][A[A

 38% 12/32 [00:03<00:08,  2.39it/s][A[A

 41% 13/32 [00:03<00:06,  2.75it/s][A[A

 44% 14/32 [00:04<00:06,  2.99it/s][A[A

 47% 15/32 [00:04<00:05,  3.21it/s][A[A

 50% 16/32 [00:04<00:04,  3.36it/s][A[A

 53% 17/32 [00:04<00:04,  3.64it/s][A[A

 56% 18/32 [00:04<00:03,  3.87it/s][A[A

 59% 19/32 [00:05<00:03,  4.09it/s][A[A

 62% 20/32 [00:05<00:02,  4.72it/s][A[A

 66% 21/32 [00:05<00:02,  4.63it/s][A[A

 69% 22/32 [00:06<00:05,  1.90it/s][A[A

 72% 23/32 [00:07<00:03,  2.27it/s][A[A

 75% 24/32 [00:07<00:02,  2.68it/s][A[A

 78% 25/32 [00:07<00:02,  3.04it/s][A[A

 81% 26/32 [00:07<00:01,  3.30it/s][A[A

 84% 27/32 [00:07<00:01,  3.49it/s][A[A

 88% 28/32 [00:08<00:01,  3.62it/s][A[A

 91% 29/32 [00:08<00:00,  3.77it/s][A[A

 94% 30/32 [00:08<00:00,  3.95it/s][A[A

 97% 31/32 [00:08<00:00,  4.11it/s][A[A

100% 32/32 [00:09<00:00,  4.31it/s][A[A100% 32/32 [00:09<00:00,  3.50it/s]
Meta loss on this task batch = 5.4774e-01, PNorm = 39.1332, GNorm = 0.0901

 74% 14/19 [02:07<00:50, 10.14s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.32it/s][A[A

  6% 2/32 [00:00<00:06,  4.51it/s][A[A

  9% 3/32 [00:00<00:06,  4.46it/s][A[A

 12% 4/32 [00:00<00:05,  4.84it/s][A[A

 16% 5/32 [00:01<00:05,  4.99it/s][A[A

 19% 6/32 [00:01<00:05,  4.50it/s][A[A

 22% 7/32 [00:02<00:13,  1.87it/s][A[A

 25% 8/32 [00:02<00:10,  2.34it/s][A[A

 28% 9/32 [00:02<00:08,  2.67it/s][A[A

 31% 10/32 [00:03<00:06,  3.32it/s][A[A

 34% 11/32 [00:03<00:05,  3.80it/s][A[A

 38% 12/32 [00:03<00:04,  4.12it/s][A[A

 41% 13/32 [00:03<00:04,  4.21it/s][A[A

 44% 14/32 [00:03<00:03,  4.65it/s][A[A

 47% 15/32 [00:04<00:03,  4.84it/s][A[A

 50% 16/32 [00:04<00:03,  4.73it/s][A[A

 53% 17/32 [00:04<00:03,  4.71it/s][A[A

 56% 18/32 [00:04<00:02,  4.85it/s][A[A

 59% 19/32 [00:04<00:02,  5.27it/s][A[A

 62% 20/32 [00:06<00:06,  1.99it/s][A[A

 66% 21/32 [00:06<00:04,  2.46it/s][A[A

 69% 22/32 [00:06<00:03,  2.94it/s][A[A

 72% 23/32 [00:06<00:02,  3.12it/s][A[A

 75% 24/32 [00:06<00:02,  3.46it/s][A[A

 78% 25/32 [00:07<00:01,  3.62it/s][A[A

 81% 26/32 [00:07<00:01,  3.65it/s][A[A

 84% 27/32 [00:07<00:01,  3.84it/s][A[A

 88% 28/32 [00:07<00:00,  4.08it/s][A[A

 91% 29/32 [00:08<00:00,  4.04it/s][A[A

 94% 30/32 [00:09<00:01,  1.80it/s][A[A

 97% 31/32 [00:09<00:00,  2.24it/s][A[A

100% 32/32 [00:09<00:00,  2.62it/s][A[A100% 32/32 [00:09<00:00,  3.26it/s]
Meta loss on this task batch = 4.9858e-01, PNorm = 39.1584, GNorm = 0.1557

 79% 15/19 [02:18<00:41, 10.27s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.20it/s][A[A

  6% 2/32 [00:00<00:06,  4.45it/s][A[A

  9% 3/32 [00:00<00:06,  4.68it/s][A[A

 12% 4/32 [00:00<00:05,  4.79it/s][A[A

 16% 5/32 [00:00<00:05,  5.29it/s][A[A

 19% 6/32 [00:01<00:05,  5.04it/s][A[A

 22% 7/32 [00:01<00:04,  5.10it/s][A[A

 25% 8/32 [00:01<00:05,  4.69it/s][A[A

 28% 9/32 [00:01<00:04,  4.76it/s][A[A

 31% 10/32 [00:02<00:04,  4.62it/s][A[A

 34% 11/32 [00:02<00:04,  4.68it/s][A[A

 38% 12/32 [00:03<00:10,  1.93it/s][A[A

 41% 13/32 [00:03<00:07,  2.38it/s][A[A

 44% 14/32 [00:03<00:06,  2.86it/s][A[A

 47% 15/32 [00:04<00:05,  3.24it/s][A[A

 50% 16/32 [00:04<00:04,  3.80it/s][A[A

 53% 17/32 [00:04<00:03,  3.95it/s][A[A

 56% 18/32 [00:04<00:03,  4.39it/s][A[A

 59% 19/32 [00:04<00:02,  4.49it/s][A[A

 62% 20/32 [00:05<00:02,  4.56it/s][A[A

 66% 21/32 [00:05<00:02,  4.60it/s][A[A

 69% 22/32 [00:05<00:02,  4.75it/s][A[A

 72% 23/32 [00:05<00:01,  4.60it/s][A[A

 75% 24/32 [00:05<00:01,  4.35it/s][A[A

 78% 25/32 [00:07<00:03,  1.93it/s][A[A

 81% 26/32 [00:07<00:02,  2.33it/s][A[A

 84% 27/32 [00:07<00:01,  2.66it/s][A[A

 88% 28/32 [00:07<00:01,  3.03it/s][A[A

 91% 29/32 [00:08<00:00,  3.40it/s][A[A

 94% 30/32 [00:08<00:00,  3.56it/s][A[A

 97% 31/32 [00:08<00:00,  3.74it/s][A[A

100% 32/32 [00:08<00:00,  4.00it/s][A[A100% 32/32 [00:08<00:00,  3.66it/s]
Meta loss on this task batch = 5.4913e-01, PNorm = 39.1809, GNorm = 0.0899

 84% 16/19 [02:27<00:30, 10.04s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.02it/s][A[A

  6% 2/32 [00:00<00:06,  4.62it/s][A[A

  9% 3/32 [00:00<00:06,  4.70it/s][A[A

 12% 4/32 [00:00<00:05,  4.83it/s][A[A

 16% 5/32 [00:01<00:05,  5.22it/s][A[A

 19% 6/32 [00:02<00:12,  2.09it/s][A[A

 22% 7/32 [00:02<00:09,  2.64it/s][A[A

 25% 8/32 [00:02<00:08,  2.90it/s][A[A

 28% 9/32 [00:02<00:07,  3.21it/s][A[A

 31% 10/32 [00:02<00:05,  3.89it/s][A[A

 34% 11/32 [00:03<00:05,  4.18it/s][A[A

 38% 12/32 [00:03<00:04,  4.45it/s][A[A

 41% 13/32 [00:03<00:03,  5.03it/s][A[A

 44% 14/32 [00:03<00:03,  5.13it/s][A[A

 47% 15/32 [00:03<00:03,  5.03it/s][A[A

 50% 16/32 [00:04<00:03,  4.94it/s][A[A

 53% 17/32 [00:04<00:02,  5.06it/s][A[A

 56% 18/32 [00:04<00:02,  4.84it/s][A[A

 59% 19/32 [00:04<00:02,  4.65it/s][A[A

 62% 20/32 [00:04<00:02,  4.61it/s][A[A

 66% 21/32 [00:05<00:02,  4.74it/s][A[A

 69% 22/32 [00:05<00:01,  5.30it/s][A[A

 72% 23/32 [00:06<00:04,  2.00it/s][A[A

 75% 24/32 [00:06<00:03,  2.39it/s][A[A

 78% 25/32 [00:06<00:02,  2.86it/s][A[A

 81% 26/32 [00:07<00:01,  3.48it/s][A[A

 84% 27/32 [00:07<00:01,  3.88it/s][A[A

 88% 28/32 [00:07<00:00,  4.10it/s][A[A

 91% 29/32 [00:07<00:00,  4.27it/s][A[A

 94% 30/32 [00:07<00:00,  4.54it/s][A[A

 97% 31/32 [00:08<00:00,  4.77it/s][A[A

100% 32/32 [00:08<00:00,  4.87it/s][A[A100% 32/32 [00:08<00:00,  3.89it/s]
Meta loss on this task batch = 4.5927e-01, PNorm = 39.1988, GNorm = 0.1348

 89% 17/19 [02:36<00:19,  9.71s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.18it/s][A[A

  6% 2/32 [00:00<00:05,  5.02it/s][A[A

  9% 3/32 [00:00<00:05,  4.87it/s][A[A

 12% 4/32 [00:00<00:05,  4.83it/s][A[A

 16% 5/32 [00:02<00:13,  1.95it/s][A[A

 19% 6/32 [00:02<00:11,  2.36it/s][A[A

 22% 7/32 [00:02<00:09,  2.63it/s][A[A

 25% 8/32 [00:02<00:08,  2.87it/s][A[A

 28% 9/32 [00:03<00:07,  3.22it/s][A[A

 31% 10/32 [00:03<00:06,  3.47it/s][A[A

 34% 11/32 [00:03<00:05,  3.95it/s][A[A

 38% 12/32 [00:03<00:04,  4.31it/s][A[A

 41% 13/32 [00:03<00:04,  4.11it/s][A[A

 44% 14/32 [00:04<00:03,  4.66it/s][A[A

 47% 15/32 [00:05<00:08,  1.90it/s][A[A

 50% 16/32 [00:05<00:06,  2.34it/s][A[A

 53% 17/32 [00:05<00:05,  2.91it/s][A[A

 56% 18/32 [00:05<00:04,  3.35it/s][A[A

 59% 19/32 [00:06<00:03,  3.64it/s][A[A

 62% 20/32 [00:06<00:03,  3.82it/s][A[A

 66% 21/32 [00:06<00:02,  4.60it/s][A[A

 69% 22/32 [00:06<00:01,  5.11it/s][A[A

 72% 23/32 [00:06<00:02,  4.47it/s][A[A

 75% 24/32 [00:07<00:01,  4.09it/s][A[A

 78% 25/32 [00:07<00:01,  4.54it/s][A[A

 81% 26/32 [00:07<00:01,  4.54it/s][A[A

 84% 27/32 [00:07<00:01,  4.62it/s][A[A

 88% 28/32 [00:08<00:02,  1.97it/s][A[A

 91% 29/32 [00:09<00:01,  2.33it/s][A[A

 94% 30/32 [00:09<00:00,  2.80it/s][A[A

 97% 31/32 [00:09<00:00,  3.28it/s][A[A

100% 32/32 [00:09<00:00,  3.51it/s][A[A100% 32/32 [00:09<00:00,  3.27it/s]
Meta loss on this task batch = 5.1298e-01, PNorm = 39.2131, GNorm = 0.1240

 95% 18/19 [02:47<00:09,  9.95s/it][A

  0% 0/23 [00:00<?, ?it/s][A[A

  4% 1/23 [00:00<00:05,  4.14it/s][A[A

  9% 2/23 [00:00<00:04,  4.46it/s][A[A

 13% 3/23 [00:00<00:04,  4.51it/s][A[A

 17% 4/23 [00:00<00:03,  4.76it/s][A[A

 26% 6/23 [00:01<00:03,  5.23it/s][A[A

 30% 7/23 [00:01<00:03,  5.27it/s][A[A

 35% 8/23 [00:02<00:07,  2.04it/s][A[A

 39% 9/23 [00:02<00:05,  2.61it/s][A[A

 43% 10/23 [00:02<00:04,  2.87it/s][A[A

 48% 11/23 [00:03<00:03,  3.41it/s][A[A

 52% 12/23 [00:03<00:02,  3.93it/s][A[A

 57% 13/23 [00:03<00:02,  4.54it/s][A[A

 61% 14/23 [00:03<00:01,  5.17it/s][A[A

 65% 15/23 [00:03<00:02,  4.00it/s][A[A

 70% 16/23 [00:04<00:01,  3.90it/s][A[A

 74% 17/23 [00:04<00:01,  4.39it/s][A[A

 78% 18/23 [00:04<00:01,  4.57it/s][A[A

 83% 19/23 [00:04<00:00,  4.85it/s][A[A

 87% 20/23 [00:04<00:00,  5.40it/s][A[A

 91% 21/23 [00:05<00:00,  5.39it/s][A[A

 96% 22/23 [00:05<00:00,  5.85it/s][A[A

100% 23/23 [00:06<00:00,  2.12it/s][A[A100% 23/23 [00:06<00:00,  3.64it/s]
Meta loss on this task batch = 4.2055e-01, PNorm = 39.2296, GNorm = 0.1068

100% 19/19 [02:54<00:00,  9.02s/it][A100% 19/19 [02:54<00:00,  9.17s/it]
Took 174.18667697906494 seconds to complete one epoch of meta training

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/20 [00:00<?, ?it/s][A[A


  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 30.41it/s]


  5% 1/20 [00:00<00:02,  7.96it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 20.28it/s]


 10% 2/20 [00:00<00:02,  6.04it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 16.81it/s][A[A[A100% 3/3 [00:00<00:00, 18.64it/s]


 15% 3/20 [00:00<00:03,  4.83it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 15.53it/s]


 20% 4/20 [00:00<00:03,  4.85it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 15.10it/s][A[A[A100% 4/4 [00:00<00:00, 17.60it/s]


 25% 5/20 [00:01<00:03,  3.84it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 12.01it/s][A[A[A


100% 4/4 [00:00<00:00, 13.38it/s][A[A[A100% 4/4 [00:00<00:00, 14.47it/s]


 30% 6/20 [00:01<00:04,  3.13it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 17.10it/s][A[A[A100% 4/4 [00:00<00:00, 18.74it/s]


 35% 7/20 [00:02<00:04,  2.95it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 18.91it/s][A[A[A


100% 4/4 [00:00<00:00, 18.65it/s][A[A[A100% 4/4 [00:00<00:00, 18.45it/s]


 40% 8/20 [00:03<00:07,  1.53it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.91it/s][A[A[A100% 4/4 [00:00<00:00, 22.38it/s]


 45% 9/20 [00:03<00:06,  1.78it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 18.42it/s][A[A[A


100% 4/4 [00:00<00:00, 18.28it/s][A[A[A100% 4/4 [00:00<00:00, 18.15it/s]


 50% 10/20 [00:04<00:05,  1.97it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 16.27it/s][A[A[A100% 4/4 [00:00<00:00, 18.54it/s]


 55% 11/20 [00:04<00:04,  2.14it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.43it/s][A[A[A100% 4/4 [00:00<00:00, 21.74it/s]


 60% 12/20 [00:04<00:03,  2.31it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.36it/s][A[A[A100% 3/3 [00:00<00:00, 13.58it/s]


 65% 13/20 [00:05<00:02,  2.36it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:01<00:00,  1.76it/s][A[A[A100% 3/3 [00:01<00:00,  2.47it/s]


 70% 14/20 [00:06<00:04,  1.40it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.72it/s][A[A[A100% 4/4 [00:00<00:00, 17.89it/s]


 75% 15/20 [00:07<00:02,  1.67it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.66it/s][A[A[A100% 3/3 [00:00<00:00, 17.98it/s]


 80% 16/20 [00:07<00:02,  1.95it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 11.93it/s][A[A[A100% 3/3 [00:00<00:00, 14.58it/s]


 85% 17/20 [00:07<00:01,  2.10it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 21.29it/s]


 90% 18/20 [00:08<00:00,  2.45it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 13.99it/s][A[A[A100% 3/3 [00:00<00:00, 17.89it/s]


 95% 19/20 [00:08<00:00,  2.70it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 12.11it/s]


100% 20/20 [00:08<00:00,  2.97it/s][A[A100% 20/20 [00:08<00:00,  2.33it/s]

100% 1/1 [00:08<00:00,  8.58s/it][A100% 1/1 [00:08<00:00,  8.58s/it]
Took 182.76618337631226 seconds to complete one epoch of meta training and validating
Meta validation score auc = 0.632526
 67% 20/30 [1:04:33<32:18, 193.84s/it]Epoch 20

  0% 0/19 [00:00<?, ?it/s][A

  0% 0/32 [00:00<?, ?it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  3% 1/32 [00:00<00:06,  4.54it/s][A[A

  6% 2/32 [00:01<00:15,  1.89it/s][A[A

  9% 3/32 [00:01<00:12,  2.30it/s][A[A

 12% 4/32 [00:01<00:09,  2.86it/s][A[A

 16% 5/32 [00:01<00:07,  3.51it/s][A[A

 19% 6/32 [00:02<00:06,  3.91it/s][A[A

 22% 7/32 [00:02<00:05,  4.36it/s][A[A

 25% 8/32 [00:02<00:05,  4.55it/s][A[A

 28% 9/32 [00:02<00:04,  5.21it/s][A[A

 31% 10/32 [00:02<00:03,  5.73it/s][A[A

 34% 11/32 [00:03<00:04,  4.92it/s][A[A

 38% 12/32 [00:03<00:04,  4.41it/s][A[A

 41% 13/32 [00:03<00:04,  4.12it/s][A[A

 44% 14/32 [00:03<00:04,  4.23it/s][A[A

 47% 15/32 [00:04<00:03,  4.52it/s][A[A

 50% 16/32 [00:05<00:08,  1.95it/s][A[A

 53% 17/32 [00:05<00:06,  2.39it/s][A[A

 56% 18/32 [00:05<00:04,  2.90it/s][A[A

 59% 19/32 [00:05<00:03,  3.44it/s][A[A

 62% 20/32 [00:05<00:03,  3.62it/s][A[A

 66% 21/32 [00:06<00:02,  4.39it/s][A[A

 69% 22/32 [00:06<00:02,  4.56it/s][A[A

 72% 23/32 [00:06<00:01,  4.71it/s][A[A

 75% 24/32 [00:06<00:01,  4.79it/s][A[A

 78% 25/32 [00:06<00:01,  4.89it/s][A[A

 81% 26/32 [00:07<00:01,  4.71it/s][A[A

 84% 27/32 [00:07<00:01,  4.63it/s][A[A

 88% 28/32 [00:07<00:00,  4.86it/s][A[A

 91% 29/32 [00:07<00:00,  5.00it/s][A[A

 94% 30/32 [00:07<00:00,  5.66it/s][A[A

 97% 31/32 [00:08<00:00,  5.03it/s][A[A

100% 32/32 [00:08<00:00,  5.15it/s][A[A100% 32/32 [00:08<00:00,  3.86it/s]
Meta loss on this task batch = 5.2128e-01, PNorm = 39.2486, GNorm = 0.1649

  5% 1/19 [00:08<02:41,  9.00s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  7.44it/s][A[A

  6% 2/32 [00:00<00:04,  7.29it/s][A[A

  9% 3/32 [00:01<00:12,  2.23it/s][A[A

 12% 4/32 [00:01<00:10,  2.58it/s][A[A

 16% 5/32 [00:01<00:08,  3.08it/s][A[A

 19% 6/32 [00:02<00:07,  3.67it/s][A[A

 22% 7/32 [00:02<00:05,  4.31it/s][A[A

 25% 8/32 [00:02<00:05,  4.78it/s][A[A

 28% 9/32 [00:02<00:04,  5.13it/s][A[A

 31% 10/32 [00:02<00:03,  5.59it/s][A[A

 34% 11/32 [00:02<00:03,  5.42it/s][A[A

 38% 12/32 [00:03<00:03,  5.38it/s][A[A

 41% 13/32 [00:03<00:03,  5.67it/s][A[A

 44% 14/32 [00:03<00:03,  5.29it/s][A[A

 47% 15/32 [00:03<00:03,  5.02it/s][A[A

 50% 16/32 [00:03<00:03,  5.21it/s][A[A

 53% 17/32 [00:03<00:02,  5.22it/s][A[A

 56% 18/32 [00:04<00:02,  4.87it/s][A[A

 59% 19/32 [00:04<00:02,  4.92it/s][A[A

 62% 20/32 [00:04<00:02,  5.09it/s][A[A

 66% 21/32 [00:04<00:02,  4.94it/s][A[A

 69% 22/32 [00:04<00:01,  5.42it/s][A[A

 72% 23/32 [00:05<00:01,  5.76it/s][A[A

 75% 24/32 [00:06<00:03,  2.16it/s][A[A

 78% 25/32 [00:06<00:02,  2.60it/s][A[A

 81% 26/32 [00:06<00:02,  3.00it/s][A[A

 84% 27/32 [00:06<00:01,  3.51it/s][A[A

 88% 28/32 [00:07<00:01,  3.63it/s][A[A

 91% 29/32 [00:07<00:00,  4.26it/s][A[A

 94% 30/32 [00:07<00:00,  4.50it/s][A[A

 97% 31/32 [00:07<00:00,  4.60it/s][A[A

100% 32/32 [00:07<00:00,  5.05it/s][A[A100% 32/32 [00:07<00:00,  4.12it/s]
Meta loss on this task batch = 5.0662e-01, PNorm = 39.2692, GNorm = 0.1026

 11% 2/19 [00:17<02:30,  8.84s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.69it/s][A[A

  6% 2/32 [00:00<00:05,  5.76it/s][A[A

  9% 3/32 [00:00<00:05,  5.25it/s][A[A

 12% 4/32 [00:00<00:05,  5.24it/s][A[A

 16% 5/32 [00:00<00:04,  5.45it/s][A[A

 19% 6/32 [00:01<00:05,  5.16it/s][A[A

 22% 7/32 [00:01<00:04,  5.41it/s][A[A

 25% 8/32 [00:01<00:04,  5.72it/s][A[A

 28% 9/32 [00:01<00:03,  5.93it/s][A[A

 31% 10/32 [00:02<00:10,  2.05it/s][A[A

 34% 11/32 [00:03<00:08,  2.41it/s][A[A

 38% 12/32 [00:03<00:07,  2.86it/s][A[A

 41% 13/32 [00:03<00:05,  3.38it/s][A[A

 44% 14/32 [00:03<00:04,  3.70it/s][A[A

 47% 15/32 [00:03<00:04,  4.15it/s][A[A

 50% 16/32 [00:03<00:03,  4.78it/s][A[A

 53% 17/32 [00:04<00:03,  4.58it/s][A[A

 56% 18/32 [00:04<00:02,  5.09it/s][A[A

 59% 19/32 [00:04<00:02,  5.34it/s][A[A

 62% 20/32 [00:04<00:02,  5.78it/s][A[A

 66% 21/32 [00:04<00:01,  6.32it/s][A[A

 69% 22/32 [00:04<00:01,  6.52it/s][A[A

 72% 23/32 [00:05<00:01,  6.17it/s][A[A

 75% 24/32 [00:05<00:01,  5.57it/s][A[A

 78% 25/32 [00:05<00:01,  5.44it/s][A[A

 81% 26/32 [00:05<00:01,  5.35it/s][A[A

 84% 27/32 [00:05<00:00,  5.36it/s][A[A

 88% 28/32 [00:06<00:00,  5.55it/s][A[A

 91% 29/32 [00:07<00:01,  2.05it/s][A[A

 94% 30/32 [00:07<00:00,  2.38it/s][A[A

 97% 31/32 [00:07<00:00,  3.04it/s][A[A

100% 32/32 [00:07<00:00,  3.37it/s][A[A100% 32/32 [00:07<00:00,  4.06it/s]
Meta loss on this task batch = 5.1767e-01, PNorm = 39.2889, GNorm = 0.0872

 16% 3/19 [00:26<02:20,  8.76s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.19it/s][A[A

  6% 2/32 [00:00<00:06,  4.30it/s][A[A

  9% 3/32 [00:00<00:06,  4.61it/s][A[A

 12% 4/32 [00:00<00:05,  4.93it/s][A[A

 16% 5/32 [00:01<00:05,  4.83it/s][A[A

 19% 6/32 [00:01<00:05,  5.02it/s][A[A

 22% 7/32 [00:01<00:04,  5.64it/s][A[A

 25% 8/32 [00:01<00:04,  5.10it/s][A[A

 28% 9/32 [00:01<00:04,  5.51it/s][A[A

 31% 10/32 [00:01<00:04,  5.10it/s][A[A

 34% 11/32 [00:02<00:03,  5.47it/s][A[A

 38% 12/32 [00:02<00:03,  5.82it/s][A[A

 41% 13/32 [00:03<00:09,  2.10it/s][A[A

 44% 14/32 [00:03<00:07,  2.50it/s][A[A

 47% 15/32 [00:03<00:05,  3.09it/s][A[A

 50% 16/32 [00:04<00:04,  3.45it/s][A[A

 53% 17/32 [00:04<00:03,  3.84it/s][A[A

 56% 18/32 [00:04<00:03,  4.33it/s][A[A

 59% 19/32 [00:04<00:02,  4.91it/s][A[A

 62% 20/32 [00:04<00:02,  4.77it/s][A[A

 66% 21/32 [00:04<00:02,  5.00it/s][A[A

 69% 22/32 [00:05<00:02,  4.67it/s][A[A

 72% 23/32 [00:05<00:02,  4.50it/s][A[A

 75% 24/32 [00:05<00:01,  4.47it/s][A[A

 78% 25/32 [00:05<00:01,  5.08it/s][A[A

 81% 26/32 [00:05<00:01,  5.71it/s][A[A

 84% 27/32 [00:07<00:02,  2.04it/s][A[A

 88% 28/32 [00:07<00:01,  2.48it/s][A[A

 91% 29/32 [00:07<00:00,  3.10it/s][A[A

 94% 30/32 [00:07<00:00,  3.37it/s][A[A

 97% 31/32 [00:07<00:00,  4.00it/s][A[A

100% 32/32 [00:08<00:00,  4.34it/s][A[A100% 32/32 [00:08<00:00,  4.00it/s]
Meta loss on this task batch = 5.5993e-01, PNorm = 39.3089, GNorm = 0.0731

 21% 4/19 [00:34<02:11,  8.75s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.42it/s][A[A

  6% 2/32 [00:00<00:05,  5.09it/s][A[A

  9% 3/32 [00:00<00:05,  4.86it/s][A[A

 12% 4/32 [00:00<00:06,  4.51it/s][A[A

 16% 5/32 [00:01<00:05,  4.53it/s][A[A

 19% 6/32 [00:01<00:05,  4.91it/s][A[A

 22% 7/32 [00:02<00:12,  1.95it/s][A[A

 25% 8/32 [00:02<00:09,  2.48it/s][A[A

 28% 9/32 [00:02<00:07,  2.89it/s][A[A

 31% 10/32 [00:02<00:06,  3.43it/s][A[A

 34% 11/32 [00:03<00:05,  3.82it/s][A[A

 38% 12/32 [00:03<00:04,  4.11it/s][A[A

 41% 13/32 [00:03<00:04,  4.15it/s][A[A

 44% 14/32 [00:03<00:03,  4.76it/s][A[A

 47% 15/32 [00:03<00:03,  4.79it/s][A[A

 50% 16/32 [00:04<00:03,  5.11it/s][A[A

 53% 17/32 [00:04<00:03,  4.84it/s][A[A

 56% 18/32 [00:04<00:02,  4.91it/s][A[A

 59% 19/32 [00:04<00:02,  4.56it/s][A[A

 62% 20/32 [00:06<00:06,  1.92it/s][A[A

 66% 21/32 [00:06<00:04,  2.30it/s][A[A

 69% 22/32 [00:06<00:03,  2.81it/s][A[A

 72% 23/32 [00:06<00:02,  3.29it/s][A[A

 75% 24/32 [00:06<00:02,  3.41it/s][A[A

 78% 25/32 [00:07<00:01,  3.67it/s][A[A

 81% 26/32 [00:07<00:01,  4.24it/s][A[A

 84% 27/32 [00:07<00:01,  4.53it/s][A[A

 88% 28/32 [00:07<00:00,  4.97it/s][A[A

 91% 29/32 [00:07<00:00,  4.70it/s][A[A

 94% 30/32 [00:07<00:00,  5.13it/s][A[A

 97% 31/32 [00:08<00:00,  5.22it/s][A[A

100% 32/32 [00:08<00:00,  4.90it/s][A[A100% 32/32 [00:08<00:00,  3.81it/s]
Meta loss on this task batch = 5.1044e-01, PNorm = 39.3299, GNorm = 0.0645

 26% 5/19 [00:43<02:04,  8.87s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  6.72it/s][A[A

  6% 2/32 [00:00<00:04,  6.07it/s][A[A

  9% 3/32 [00:00<00:04,  6.26it/s][A[A

 12% 4/32 [00:00<00:05,  5.60it/s][A[A

 16% 5/32 [00:00<00:05,  5.17it/s][A[A

 19% 6/32 [00:02<00:13,  1.96it/s][A[A

 22% 7/32 [00:02<00:10,  2.41it/s][A[A

 25% 8/32 [00:02<00:08,  2.90it/s][A[A

 28% 9/32 [00:02<00:07,  3.22it/s][A[A

 31% 10/32 [00:03<00:06,  3.43it/s][A[A

 34% 11/32 [00:03<00:05,  3.63it/s][A[A

 38% 12/32 [00:03<00:05,  3.88it/s][A[A

 41% 13/32 [00:03<00:04,  4.43it/s][A[A

 44% 14/32 [00:03<00:04,  4.39it/s][A[A

 47% 15/32 [00:04<00:03,  4.38it/s][A[A

 50% 16/32 [00:04<00:03,  4.27it/s][A[A

 53% 17/32 [00:04<00:03,  4.86it/s][A[A

 56% 18/32 [00:04<00:02,  4.77it/s][A[A

 59% 19/32 [00:04<00:02,  5.07it/s][A[A

 62% 20/32 [00:06<00:05,  2.03it/s][A[A

 66% 21/32 [00:06<00:04,  2.49it/s][A[A

 69% 22/32 [00:06<00:03,  2.88it/s][A[A

 72% 23/32 [00:06<00:02,  3.23it/s][A[A

 75% 24/32 [00:06<00:02,  3.68it/s][A[A

 78% 25/32 [00:07<00:01,  3.85it/s][A[A

 81% 26/32 [00:07<00:01,  4.45it/s][A[A

 84% 27/32 [00:07<00:01,  4.33it/s][A[A

 88% 28/32 [00:07<00:00,  4.33it/s][A[A

 91% 29/32 [00:07<00:00,  4.34it/s][A[A

 94% 30/32 [00:08<00:00,  4.54it/s][A[A

 97% 31/32 [00:08<00:00,  4.67it/s][A[A

100% 32/32 [00:08<00:00,  4.80it/s][A[A100% 32/32 [00:08<00:00,  3.74it/s]
Meta loss on this task batch = 4.4589e-01, PNorm = 39.3510, GNorm = 0.1065

 32% 6/19 [00:53<01:57,  9.01s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:37,  1.20s/it][A[A

  6% 2/32 [00:01<00:26,  1.11it/s][A[A

  9% 3/32 [00:01<00:20,  1.44it/s][A[A

 12% 4/32 [00:01<00:15,  1.80it/s][A[A

 16% 5/32 [00:01<00:11,  2.32it/s][A[A

 19% 6/32 [00:02<00:09,  2.77it/s][A[A

 22% 7/32 [00:02<00:08,  3.09it/s][A[A

 25% 8/32 [00:02<00:06,  3.59it/s][A[A

 28% 9/32 [00:02<00:05,  3.99it/s][A[A

 31% 10/32 [00:03<00:05,  4.16it/s][A[A

 34% 11/32 [00:03<00:05,  4.18it/s][A[A

 38% 12/32 [00:03<00:04,  4.25it/s][A[A

 41% 13/32 [00:03<00:04,  4.21it/s][A[A

 44% 14/32 [00:03<00:03,  4.66it/s][A[A

 47% 15/32 [00:05<00:08,  1.90it/s][A[A

 50% 16/32 [00:05<00:07,  2.26it/s][A[A

 53% 17/32 [00:05<00:05,  2.78it/s][A[A

 56% 18/32 [00:05<00:04,  3.18it/s][A[A

 59% 19/32 [00:05<00:03,  3.64it/s][A[A

 62% 20/32 [00:06<00:03,  3.73it/s][A[A

 66% 21/32 [00:06<00:02,  3.90it/s][A[A

 69% 22/32 [00:06<00:02,  4.11it/s][A[A

 72% 23/32 [00:06<00:02,  4.05it/s][A[A

 75% 24/32 [00:07<00:01,  4.13it/s][A[A

 78% 25/32 [00:07<00:01,  4.50it/s][A[A

 81% 26/32 [00:08<00:02,  2.00it/s][A[A

 84% 27/32 [00:08<00:02,  2.41it/s][A[A

 88% 28/32 [00:08<00:01,  2.74it/s][A[A

 91% 29/32 [00:09<00:00,  3.12it/s][A[A

 94% 30/32 [00:09<00:00,  3.54it/s][A[A

 97% 31/32 [00:09<00:00,  4.07it/s][A[A

100% 32/32 [00:09<00:00,  4.28it/s][A[A100% 32/32 [00:09<00:00,  3.31it/s]
Meta loss on this task batch = 4.7602e-01, PNorm = 39.3759, GNorm = 0.1558

 37% 7/19 [01:03<01:53,  9.44s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.60it/s][A[A

  6% 2/32 [00:00<00:06,  4.34it/s][A[A

  9% 3/32 [00:00<00:06,  4.18it/s][A[A

 12% 4/32 [00:01<00:15,  1.83it/s][A[A

 16% 5/32 [00:02<00:12,  2.23it/s][A[A

 19% 6/32 [00:02<00:09,  2.66it/s][A[A

 22% 7/32 [00:02<00:08,  3.06it/s][A[A

 25% 8/32 [00:02<00:07,  3.26it/s][A[A

 28% 9/32 [00:03<00:06,  3.54it/s][A[A

 31% 10/32 [00:03<00:05,  3.76it/s][A[A

 34% 11/32 [00:03<00:04,  4.28it/s][A[A

 38% 12/32 [00:03<00:04,  4.44it/s][A[A

 41% 13/32 [00:03<00:04,  4.55it/s][A[A

 44% 14/32 [00:04<00:03,  4.59it/s][A[A

 47% 15/32 [00:04<00:03,  4.88it/s][A[A

 50% 16/32 [00:05<00:08,  1.97it/s][A[A

 53% 17/32 [00:05<00:06,  2.32it/s][A[A

 56% 18/32 [00:06<00:05,  2.70it/s][A[A

 59% 19/32 [00:06<00:04,  2.98it/s][A[A

 62% 20/32 [00:06<00:03,  3.48it/s][A[A

 66% 21/32 [00:06<00:02,  3.71it/s][A[A

 69% 22/32 [00:06<00:02,  3.88it/s][A[A

 72% 23/32 [00:07<00:02,  4.14it/s][A[A

 75% 24/32 [00:07<00:01,  4.14it/s][A[A

 78% 25/32 [00:08<00:03,  1.88it/s][A[A

 81% 26/32 [00:08<00:02,  2.32it/s][A[A

 84% 27/32 [00:08<00:01,  2.92it/s][A[A

 88% 28/32 [00:09<00:01,  3.56it/s][A[A

 91% 29/32 [00:09<00:00,  4.25it/s][A[A

 94% 30/32 [00:09<00:00,  4.90it/s][A[A

 97% 31/32 [00:09<00:00,  5.53it/s][A[A

100% 32/32 [00:09<00:00,  5.84it/s][A[A100% 32/32 [00:09<00:00,  3.35it/s]
Meta loss on this task batch = 3.1537e-01, PNorm = 39.4056, GNorm = 0.1065

 42% 8/19 [01:13<01:46,  9.70s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  6.64it/s][A[A

  6% 2/32 [00:00<00:04,  6.86it/s][A[A

  9% 3/32 [00:00<00:04,  6.29it/s][A[A

 12% 4/32 [00:00<00:04,  6.61it/s][A[A

 16% 5/32 [00:00<00:03,  6.87it/s][A[A

 19% 6/32 [00:00<00:03,  7.05it/s][A[A

 22% 7/32 [00:00<00:03,  7.30it/s][A[A

 25% 8/32 [00:01<00:03,  7.36it/s][A[A

 28% 9/32 [00:01<00:03,  7.29it/s][A[A

 31% 10/32 [00:01<00:03,  7.21it/s][A[A

 34% 11/32 [00:01<00:02,  7.19it/s][A[A

 38% 12/32 [00:01<00:02,  7.26it/s][A[A

 41% 13/32 [00:01<00:02,  6.95it/s][A[A

 44% 14/32 [00:02<00:07,  2.28it/s][A[A

 47% 15/32 [00:03<00:05,  2.88it/s][A[A

 50% 16/32 [00:03<00:04,  3.54it/s][A[A

 53% 17/32 [00:03<00:03,  4.14it/s][A[A

 56% 18/32 [00:03<00:02,  4.91it/s][A[A

 59% 19/32 [00:03<00:02,  5.46it/s][A[A

 62% 20/32 [00:03<00:02,  5.78it/s][A[A

 66% 21/32 [00:03<00:01,  6.26it/s][A[A

 69% 22/32 [00:04<00:01,  6.43it/s][A[A

 72% 23/32 [00:04<00:01,  6.13it/s][A[A

 75% 24/32 [00:04<00:01,  6.38it/s][A[A

 78% 25/32 [00:04<00:01,  6.10it/s][A[A

 81% 26/32 [00:04<00:00,  6.47it/s][A[A

 84% 27/32 [00:04<00:00,  6.70it/s][A[A

 88% 28/32 [00:04<00:00,  6.83it/s][A[A

 91% 29/32 [00:05<00:00,  6.83it/s][A[A

 94% 30/32 [00:05<00:00,  6.96it/s][A[A

 97% 31/32 [00:05<00:00,  6.96it/s][A[A

100% 32/32 [00:05<00:00,  6.99it/s][A[A100% 32/32 [00:05<00:00,  5.77it/s]
Meta loss on this task batch = 4.2139e-02, PNorm = 39.4371, GNorm = 0.1000

 47% 9/19 [01:20<01:26,  8.64s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  6.92it/s][A[A

  6% 2/32 [00:00<00:04,  6.48it/s][A[A

  9% 3/32 [00:01<00:13,  2.20it/s][A[A

 12% 4/32 [00:01<00:10,  2.78it/s][A[A

 16% 5/32 [00:01<00:07,  3.47it/s][A[A

 19% 6/32 [00:01<00:06,  4.13it/s][A[A

 22% 7/32 [00:02<00:05,  4.74it/s][A[A

 25% 8/32 [00:02<00:04,  5.28it/s][A[A

 28% 9/32 [00:02<00:04,  5.74it/s][A[A

 31% 10/32 [00:02<00:03,  6.07it/s][A[A

 34% 11/32 [00:02<00:03,  6.22it/s][A[A

 38% 12/32 [00:02<00:03,  6.39it/s][A[A

 41% 13/32 [00:02<00:02,  6.75it/s][A[A

 44% 14/32 [00:03<00:02,  6.79it/s][A[A

 47% 15/32 [00:03<00:02,  6.81it/s][A[A

 50% 16/32 [00:03<00:02,  6.61it/s][A[A

 53% 17/32 [00:03<00:02,  6.63it/s][A[A

 56% 18/32 [00:03<00:02,  6.76it/s][A[A

 59% 19/32 [00:03<00:02,  6.36it/s][A[A

 62% 20/32 [00:03<00:01,  6.80it/s][A[A

 66% 21/32 [00:04<00:01,  6.14it/s][A[A

 69% 22/32 [00:04<00:01,  6.47it/s][A[A

 72% 23/32 [00:04<00:01,  6.88it/s][A[A

 75% 24/32 [00:04<00:01,  7.04it/s][A[A

 78% 25/32 [00:04<00:00,  7.39it/s][A[A

 81% 26/32 [00:04<00:00,  6.59it/s][A[A

 84% 27/32 [00:05<00:02,  2.19it/s][A[A

 88% 28/32 [00:06<00:01,  2.57it/s][A[A

 91% 29/32 [00:06<00:01,  2.90it/s][A[A

 94% 30/32 [00:06<00:00,  3.22it/s][A[A

 97% 31/32 [00:06<00:00,  3.51it/s][A[A

100% 32/32 [00:07<00:00,  3.91it/s][A[A100% 32/32 [00:07<00:00,  4.51it/s]
Meta loss on this task batch = 1.6595e-01, PNorm = 39.4699, GNorm = 0.0716

 53% 10/19 [01:27<01:15,  8.38s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.06it/s][A[A

  6% 2/32 [00:00<00:06,  4.85it/s][A[A

  9% 3/32 [00:00<00:05,  5.08it/s][A[A

 12% 4/32 [00:00<00:05,  5.11it/s][A[A

 16% 5/32 [00:01<00:05,  4.86it/s][A[A

 19% 6/32 [00:01<00:05,  5.15it/s][A[A

 22% 7/32 [00:01<00:05,  4.85it/s][A[A

 25% 8/32 [00:01<00:05,  4.62it/s][A[A

 28% 9/32 [00:01<00:04,  4.95it/s][A[A

 31% 10/32 [00:02<00:04,  4.67it/s][A[A

 34% 11/32 [00:02<00:04,  4.50it/s][A[A

 38% 12/32 [00:02<00:04,  4.46it/s][A[A

 41% 13/32 [00:03<00:10,  1.89it/s][A[A

 44% 14/32 [00:04<00:07,  2.28it/s][A[A

 47% 15/32 [00:04<00:06,  2.73it/s][A[A

 50% 16/32 [00:04<00:05,  3.19it/s][A[A

 53% 17/32 [00:04<00:04,  3.64it/s][A[A

 56% 18/32 [00:04<00:03,  3.77it/s][A[A

 59% 19/32 [00:05<00:03,  3.90it/s][A[A

 62% 20/32 [00:05<00:02,  4.31it/s][A[A

 66% 21/32 [00:05<00:02,  4.53it/s][A[A

 69% 22/32 [00:05<00:02,  4.41it/s][A[A

 72% 23/32 [00:05<00:02,  4.32it/s][A[A

 75% 24/32 [00:06<00:01,  4.44it/s][A[A

 78% 25/32 [00:07<00:03,  1.93it/s][A[A

 81% 26/32 [00:07<00:02,  2.39it/s][A[A

 84% 27/32 [00:07<00:01,  2.79it/s][A[A

 88% 28/32 [00:07<00:01,  3.22it/s][A[A

 91% 29/32 [00:08<00:00,  3.44it/s][A[A

 94% 30/32 [00:08<00:00,  3.68it/s][A[A

 97% 31/32 [00:08<00:00,  4.11it/s][A[A

100% 32/32 [00:08<00:00,  4.29it/s][A[A100% 32/32 [00:08<00:00,  3.64it/s]
Meta loss on this task batch = 6.7648e-01, PNorm = 39.4896, GNorm = 0.3684

 58% 11/19 [01:37<01:09,  8.74s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.19it/s][A[A

  6% 2/32 [00:00<00:06,  4.30it/s][A[A

  9% 3/32 [00:00<00:06,  4.30it/s][A[A

 12% 4/32 [00:00<00:06,  4.20it/s][A[A

 16% 5/32 [00:02<00:14,  1.85it/s][A[A

 19% 6/32 [00:02<00:11,  2.28it/s][A[A

 22% 7/32 [00:02<00:09,  2.65it/s][A[A

 25% 8/32 [00:02<00:08,  2.96it/s][A[A

 28% 9/32 [00:03<00:07,  3.24it/s][A[A

 31% 10/32 [00:03<00:05,  3.69it/s][A[A

 34% 11/32 [00:03<00:05,  3.78it/s][A[A

 38% 12/32 [00:03<00:04,  4.05it/s][A[A

 41% 13/32 [00:03<00:04,  4.03it/s][A[A

 44% 14/32 [00:05<00:09,  1.88it/s][A[A

 47% 15/32 [00:05<00:07,  2.26it/s][A[A

 50% 16/32 [00:05<00:06,  2.65it/s][A[A

 53% 17/32 [00:05<00:05,  2.94it/s][A[A

 56% 18/32 [00:06<00:04,  3.22it/s][A[A

 59% 19/32 [00:06<00:03,  3.54it/s][A[A

 62% 20/32 [00:06<00:03,  3.99it/s][A[A

 66% 21/32 [00:06<00:02,  4.17it/s][A[A

 69% 22/32 [00:07<00:02,  4.14it/s][A[A

 72% 23/32 [00:07<00:02,  4.18it/s][A[A

 75% 24/32 [00:07<00:01,  4.54it/s][A[A

 78% 25/32 [00:07<00:01,  4.44it/s][A[A

 81% 26/32 [00:07<00:01,  4.50it/s][A[A

 84% 27/32 [00:09<00:02,  1.91it/s][A[A

 88% 28/32 [00:09<00:01,  2.36it/s][A[A

 91% 29/32 [00:09<00:01,  2.82it/s][A[A

 94% 30/32 [00:09<00:00,  3.22it/s][A[A

 97% 31/32 [00:09<00:00,  3.61it/s][A[A

100% 32/32 [00:10<00:00,  3.77it/s][A[A100% 32/32 [00:10<00:00,  3.16it/s]
Meta loss on this task batch = 5.2348e-01, PNorm = 39.5119, GNorm = 0.0838

 63% 12/19 [01:48<01:05,  9.39s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.33it/s][A[A

  6% 2/32 [00:00<00:07,  4.27it/s][A[A

  9% 3/32 [00:01<00:15,  1.88it/s][A[A

 12% 4/32 [00:01<00:12,  2.26it/s][A[A

 16% 5/32 [00:02<00:09,  2.75it/s][A[A

 19% 6/32 [00:02<00:08,  3.02it/s][A[A

 22% 7/32 [00:02<00:07,  3.24it/s][A[A

 25% 8/32 [00:02<00:06,  3.75it/s][A[A

 28% 9/32 [00:03<00:05,  3.91it/s][A[A

 31% 10/32 [00:03<00:05,  3.99it/s][A[A

 34% 11/32 [00:03<00:04,  4.26it/s][A[A

 38% 12/32 [00:03<00:04,  4.27it/s][A[A

 41% 13/32 [00:03<00:04,  4.25it/s][A[A

 44% 14/32 [00:04<00:04,  4.20it/s][A[A

 47% 15/32 [00:04<00:03,  4.45it/s][A[A

 50% 16/32 [00:04<00:03,  4.55it/s][A[A

 53% 17/32 [00:05<00:07,  1.93it/s][A[A

 56% 18/32 [00:06<00:06,  2.31it/s][A[A

 59% 19/32 [00:06<00:04,  2.70it/s][A[A

 62% 20/32 [00:06<00:04,  2.99it/s][A[A

 66% 21/32 [00:06<00:03,  3.27it/s][A[A

 69% 22/32 [00:06<00:02,  3.49it/s][A[A

 72% 23/32 [00:07<00:02,  3.80it/s][A[A

 75% 24/32 [00:07<00:01,  4.16it/s][A[A

 78% 25/32 [00:07<00:01,  4.54it/s][A[A

 81% 26/32 [00:07<00:01,  4.46it/s][A[A

 84% 27/32 [00:07<00:01,  4.58it/s][A[A

 88% 28/32 [00:08<00:00,  4.43it/s][A[A

 91% 29/32 [00:08<00:00,  4.42it/s][A[A

 94% 30/32 [00:09<00:01,  1.95it/s][A[A

 97% 31/32 [00:09<00:00,  2.40it/s][A[A

100% 32/32 [00:10<00:00,  2.88it/s][A[A100% 32/32 [00:10<00:00,  3.19it/s]
Meta loss on this task batch = 5.5317e-01, PNorm = 39.5346, GNorm = 0.0601

 68% 13/19 [01:59<00:58,  9.81s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.49it/s][A[A

  6% 2/32 [00:00<00:05,  5.40it/s][A[A

  9% 3/32 [00:00<00:05,  5.09it/s][A[A

 12% 4/32 [00:00<00:05,  4.84it/s][A[A

 16% 5/32 [00:01<00:05,  4.86it/s][A[A

 19% 6/32 [00:01<00:05,  5.04it/s][A[A

 22% 7/32 [00:01<00:05,  4.92it/s][A[A

 25% 8/32 [00:01<00:04,  4.83it/s][A[A

 28% 9/32 [00:01<00:04,  4.70it/s][A[A

 31% 10/32 [00:02<00:04,  4.44it/s][A[A

 34% 11/32 [00:02<00:04,  4.36it/s][A[A

 38% 12/32 [00:02<00:04,  4.59it/s][A[A

 41% 13/32 [00:02<00:04,  4.59it/s][A[A

 44% 14/32 [00:04<00:09,  1.91it/s][A[A

 47% 15/32 [00:04<00:07,  2.34it/s][A[A

 50% 16/32 [00:04<00:05,  2.71it/s][A[A

 53% 17/32 [00:04<00:04,  3.01it/s][A[A

 56% 18/32 [00:04<00:04,  3.35it/s][A[A

 59% 19/32 [00:05<00:03,  3.77it/s][A[A

 62% 20/32 [00:05<00:03,  3.84it/s][A[A

 66% 21/32 [00:05<00:02,  4.06it/s][A[A

 69% 22/32 [00:05<00:02,  4.09it/s][A[A

 72% 23/32 [00:06<00:02,  4.07it/s][A[A

 75% 24/32 [00:06<00:01,  4.44it/s][A[A

 78% 25/32 [00:06<00:01,  4.29it/s][A[A

 81% 26/32 [00:06<00:01,  4.31it/s][A[A

 84% 27/32 [00:07<00:02,  1.90it/s][A[A

 88% 28/32 [00:08<00:01,  2.28it/s][A[A

 91% 29/32 [00:08<00:01,  2.65it/s][A[A

 94% 30/32 [00:08<00:00,  3.01it/s][A[A

 97% 31/32 [00:08<00:00,  3.19it/s][A[A

100% 32/32 [00:09<00:00,  3.28it/s][A[A100% 32/32 [00:09<00:00,  3.49it/s]
Meta loss on this task batch = 5.7305e-01, PNorm = 39.5554, GNorm = 0.0550

 74% 14/19 [02:09<00:49,  9.86s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.30it/s][A[A

  6% 2/32 [00:01<00:15,  1.95it/s][A[A

  9% 3/32 [00:01<00:12,  2.27it/s][A[A

 12% 4/32 [00:01<00:10,  2.62it/s][A[A

 16% 5/32 [00:02<00:08,  3.01it/s][A[A

 19% 6/32 [00:02<00:07,  3.31it/s][A[A

 22% 7/32 [00:02<00:07,  3.47it/s][A[A

 25% 8/32 [00:02<00:06,  3.87it/s][A[A

 28% 9/32 [00:03<00:06,  3.80it/s][A[A

 31% 10/32 [00:03<00:05,  3.89it/s][A[A

 34% 11/32 [00:03<00:04,  4.29it/s][A[A

 38% 12/32 [00:03<00:04,  4.79it/s][A[A

 41% 13/32 [00:03<00:03,  4.84it/s][A[A

 44% 14/32 [00:05<00:09,  1.94it/s][A[A

 47% 15/32 [00:05<00:07,  2.42it/s][A[A

 50% 16/32 [00:05<00:05,  2.80it/s][A[A

 53% 17/32 [00:05<00:04,  3.20it/s][A[A

 56% 18/32 [00:05<00:04,  3.48it/s][A[A

 59% 19/32 [00:06<00:03,  4.07it/s][A[A

 62% 20/32 [00:06<00:02,  4.39it/s][A[A

 66% 21/32 [00:06<00:02,  4.68it/s][A[A

 69% 22/32 [00:06<00:02,  4.52it/s][A[A

 72% 23/32 [00:07<00:02,  4.02it/s][A[A

 75% 24/32 [00:07<00:01,  4.10it/s][A[A

 78% 25/32 [00:07<00:01,  4.52it/s][A[A

 81% 26/32 [00:07<00:01,  4.21it/s][A[A

 84% 27/32 [00:07<00:01,  4.45it/s][A[A

 88% 28/32 [00:08<00:00,  4.66it/s][A[A

 91% 29/32 [00:08<00:00,  4.51it/s][A[A

 94% 30/32 [00:09<00:01,  1.89it/s][A[A

 97% 31/32 [00:09<00:00,  2.37it/s][A[A

100% 32/32 [00:10<00:00,  2.73it/s][A[A100% 32/32 [00:10<00:00,  3.20it/s]
Meta loss on this task batch = 5.0344e-01, PNorm = 39.5724, GNorm = 0.1079

 79% 15/19 [02:19<00:40, 10.14s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.65it/s][A[A

  6% 2/32 [00:00<00:06,  4.99it/s][A[A

  9% 3/32 [00:00<00:05,  5.17it/s][A[A

 12% 4/32 [00:00<00:05,  5.10it/s][A[A

 16% 5/32 [00:01<00:05,  4.62it/s][A[A

 19% 6/32 [00:01<00:05,  4.48it/s][A[A

 22% 7/32 [00:01<00:05,  4.62it/s][A[A

 25% 8/32 [00:01<00:04,  4.82it/s][A[A

 28% 9/32 [00:01<00:04,  4.96it/s][A[A

 31% 10/32 [00:03<00:11,  1.93it/s][A[A

 34% 11/32 [00:03<00:09,  2.29it/s][A[A

 38% 12/32 [00:03<00:07,  2.65it/s][A[A

 41% 13/32 [00:03<00:06,  2.98it/s][A[A

 44% 14/32 [00:04<00:05,  3.29it/s][A[A

 47% 15/32 [00:04<00:04,  3.56it/s][A[A

 50% 16/32 [00:04<00:03,  4.10it/s][A[A

 53% 17/32 [00:04<00:03,  4.06it/s][A[A

 56% 18/32 [00:04<00:03,  4.37it/s][A[A

 59% 19/32 [00:05<00:03,  4.26it/s][A[A

 62% 20/32 [00:06<00:06,  1.87it/s][A[A

 66% 21/32 [00:06<00:04,  2.26it/s][A[A

 69% 22/32 [00:06<00:03,  2.71it/s][A[A

 72% 23/32 [00:07<00:02,  3.04it/s][A[A

 75% 24/32 [00:07<00:02,  3.23it/s][A[A

 78% 25/32 [00:07<00:01,  3.83it/s][A[A

 81% 26/32 [00:07<00:01,  4.13it/s][A[A

 84% 27/32 [00:07<00:01,  4.01it/s][A[A

 88% 28/32 [00:08<00:00,  4.08it/s][A[A

 91% 29/32 [00:08<00:00,  3.82it/s][A[A

 94% 30/32 [00:08<00:00,  3.94it/s][A[A

 97% 31/32 [00:09<00:00,  1.80it/s][A[A

100% 32/32 [00:10<00:00,  2.16it/s][A[A100% 32/32 [00:10<00:00,  3.15it/s]
Meta loss on this task batch = 5.8270e-01, PNorm = 39.5836, GNorm = 0.1286

 84% 16/19 [02:30<00:31, 10.38s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.80it/s][A[A

  6% 2/32 [00:00<00:07,  3.80it/s][A[A

  9% 3/32 [00:00<00:06,  4.15it/s][A[A

 12% 4/32 [00:00<00:06,  4.41it/s][A[A

 16% 5/32 [00:01<00:06,  4.21it/s][A[A

 19% 6/32 [00:01<00:05,  4.80it/s][A[A

 22% 7/32 [00:02<00:12,  1.96it/s][A[A

 25% 8/32 [00:02<00:10,  2.28it/s][A[A

 28% 9/32 [00:03<00:08,  2.63it/s][A[A

 31% 10/32 [00:03<00:06,  3.21it/s][A[A

 34% 11/32 [00:03<00:06,  3.48it/s][A[A

 38% 12/32 [00:03<00:05,  3.81it/s][A[A

 41% 13/32 [00:03<00:04,  3.96it/s][A[A

 44% 14/32 [00:04<00:04,  4.11it/s][A[A

 47% 15/32 [00:04<00:03,  4.33it/s][A[A

 50% 16/32 [00:04<00:03,  4.52it/s][A[A

 53% 17/32 [00:04<00:03,  4.77it/s][A[A

 56% 18/32 [00:04<00:02,  4.78it/s][A[A

 59% 19/32 [00:05<00:02,  4.75it/s][A[A

 62% 20/32 [00:05<00:02,  4.63it/s][A[A

 66% 21/32 [00:06<00:05,  1.99it/s][A[A

 69% 22/32 [00:06<00:04,  2.34it/s][A[A

 72% 23/32 [00:06<00:03,  2.80it/s][A[A

 75% 24/32 [00:07<00:02,  3.01it/s][A[A

 78% 25/32 [00:07<00:02,  3.46it/s][A[A

 81% 26/32 [00:07<00:01,  3.60it/s][A[A

 84% 27/32 [00:07<00:01,  4.01it/s][A[A

 88% 28/32 [00:08<00:00,  4.31it/s][A[A

 91% 29/32 [00:08<00:00,  4.24it/s][A[A

 94% 30/32 [00:08<00:00,  4.11it/s][A[A

 97% 31/32 [00:08<00:00,  4.24it/s][A[A

100% 32/32 [00:09<00:00,  1.85it/s][A[A100% 32/32 [00:09<00:00,  3.20it/s]
Meta loss on this task batch = 4.2649e-01, PNorm = 39.5966, GNorm = 0.0577

 89% 17/19 [02:41<00:20, 10.50s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.15it/s][A[A

  6% 2/32 [00:00<00:06,  4.93it/s][A[A

  9% 3/32 [00:00<00:06,  4.81it/s][A[A

 12% 4/32 [00:00<00:06,  4.64it/s][A[A

 16% 5/32 [00:01<00:05,  4.82it/s][A[A

 19% 6/32 [00:01<00:05,  4.93it/s][A[A

 22% 7/32 [00:01<00:05,  4.53it/s][A[A

 25% 8/32 [00:01<00:05,  4.23it/s][A[A

 28% 9/32 [00:01<00:05,  4.45it/s][A[A

 31% 10/32 [00:02<00:05,  4.37it/s][A[A

 34% 11/32 [00:03<00:11,  1.89it/s][A[A

 38% 12/32 [00:03<00:08,  2.34it/s][A[A

 41% 13/32 [00:03<00:07,  2.68it/s][A[A

 44% 14/32 [00:04<00:05,  3.29it/s][A[A

 47% 15/32 [00:04<00:04,  3.43it/s][A[A

 50% 16/32 [00:04<00:04,  3.71it/s][A[A

 53% 17/32 [00:04<00:03,  4.39it/s][A[A

 56% 18/32 [00:04<00:03,  4.46it/s][A[A

 59% 19/32 [00:05<00:03,  4.31it/s][A[A

 62% 20/32 [00:05<00:02,  4.24it/s][A[A

 66% 21/32 [00:05<00:02,  4.25it/s][A[A

 69% 22/32 [00:05<00:02,  4.89it/s][A[A

 72% 23/32 [00:06<00:04,  1.95it/s][A[A

 75% 24/32 [00:07<00:03,  2.23it/s][A[A

 78% 25/32 [00:07<00:02,  2.62it/s][A[A

 81% 26/32 [00:07<00:02,  2.99it/s][A[A

 84% 27/32 [00:07<00:01,  3.15it/s][A[A

 88% 28/32 [00:08<00:01,  3.55it/s][A[A

 91% 29/32 [00:08<00:00,  4.02it/s][A[A

 94% 30/32 [00:08<00:00,  4.03it/s][A[A

 97% 31/32 [00:08<00:00,  4.21it/s][A[A

100% 32/32 [00:09<00:00,  4.42it/s][A[A100% 32/32 [00:09<00:00,  3.55it/s]
Meta loss on this task batch = 5.0390e-01, PNorm = 39.6102, GNorm = 0.0560

 95% 18/19 [02:51<00:10, 10.29s/it][A

  0% 0/23 [00:00<?, ?it/s][A[A

  4% 1/23 [00:00<00:05,  3.86it/s][A[A

  9% 2/23 [00:01<00:11,  1.87it/s][A[A

 13% 3/23 [00:01<00:08,  2.26it/s][A[A

 17% 4/23 [00:01<00:06,  2.78it/s][A[A

 22% 5/23 [00:02<00:05,  3.18it/s][A[A

 26% 6/23 [00:02<00:04,  3.47it/s][A[A

 30% 7/23 [00:02<00:04,  3.58it/s][A[A

 35% 8/23 [00:02<00:04,  3.69it/s][A[A

 39% 9/23 [00:02<00:03,  4.36it/s][A[A

 43% 10/23 [00:03<00:03,  4.20it/s][A[A

 48% 11/23 [00:03<00:02,  4.72it/s][A[A

 52% 12/23 [00:03<00:02,  4.52it/s][A[A

 57% 13/23 [00:04<00:04,  2.01it/s][A[A

 61% 14/23 [00:04<00:03,  2.37it/s][A[A

 65% 15/23 [00:05<00:03,  2.45it/s][A[A

 70% 16/23 [00:05<00:02,  2.74it/s][A[A

 74% 17/23 [00:05<00:01,  3.27it/s][A[A

 78% 18/23 [00:05<00:01,  3.56it/s][A[A

 83% 19/23 [00:06<00:01,  3.66it/s][A[A

 87% 20/23 [00:06<00:00,  4.34it/s][A[A

 91% 21/23 [00:06<00:00,  4.69it/s][A[A

 96% 22/23 [00:06<00:00,  5.27it/s][A[A

100% 23/23 [00:06<00:00,  5.53it/s][A[A100% 23/23 [00:06<00:00,  3.37it/s]
Meta loss on this task batch = 4.2687e-01, PNorm = 39.6265, GNorm = 0.0801

100% 19/19 [02:58<00:00,  9.42s/it][A100% 19/19 [02:58<00:00,  9.42s/it]
Took 178.89586472511292 seconds to complete one epoch of meta training

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/20 [00:00<?, ?it/s][A[A


  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 32.65it/s]


  5% 1/20 [00:00<00:02,  8.28it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A


 50% 1/2 [00:01<00:01,  1.07s/it][A[A[A100% 2/2 [00:01<00:00,  1.81it/s]


 10% 2/20 [00:01<00:07,  2.25it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 18.00it/s][A[A[A100% 3/3 [00:00<00:00, 20.09it/s]


 15% 3/20 [00:01<00:06,  2.51it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 15.05it/s]


 20% 4/20 [00:01<00:05,  2.89it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 15.17it/s][A[A[A100% 4/4 [00:00<00:00, 17.60it/s]


 25% 5/20 [00:02<00:05,  2.91it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.66it/s][A[A[A100% 4/4 [00:00<00:00, 16.33it/s]


 30% 6/20 [00:02<00:04,  2.95it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 17.94it/s][A[A[A100% 4/4 [00:00<00:00, 19.80it/s]


 35% 7/20 [00:02<00:04,  3.01it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 18.63it/s][A[A[A


100% 4/4 [00:00<00:00, 18.31it/s][A[A[A100% 4/4 [00:00<00:00, 18.07it/s]


 40% 8/20 [00:03<00:04,  2.98it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.93it/s][A[A[A100% 4/4 [00:00<00:00, 22.50it/s]


 45% 9/20 [00:03<00:03,  3.13it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 18.60it/s][A[A[A


100% 4/4 [00:00<00:00, 18.52it/s][A[A[A100% 4/4 [00:00<00:00, 18.44it/s]


 50% 10/20 [00:03<00:03,  3.07it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:01<00:00,  2.52it/s][A[A[A100% 4/4 [00:01<00:00,  3.28it/s]


 55% 11/20 [00:05<00:05,  1.59it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.72it/s][A[A[A100% 4/4 [00:00<00:00, 21.99it/s]


 60% 12/20 [00:05<00:04,  1.89it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.57it/s][A[A[A100% 3/3 [00:00<00:00, 13.74it/s]


 65% 13/20 [00:05<00:03,  2.04it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.45it/s][A[A[A100% 3/3 [00:00<00:00, 13.72it/s]


 70% 14/20 [00:06<00:02,  2.17it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.97it/s][A[A[A100% 4/4 [00:00<00:00, 18.46it/s]


 75% 15/20 [00:06<00:02,  2.28it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.71it/s][A[A[A100% 3/3 [00:00<00:00, 18.09it/s]


 80% 16/20 [00:06<00:01,  2.46it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 11.82it/s][A[A[A100% 3/3 [00:00<00:00, 14.47it/s]


 85% 17/20 [00:07<00:01,  2.45it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 21.57it/s]


 90% 18/20 [00:07<00:00,  2.69it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:01<00:00,  1.75it/s][A[A[A100% 3/3 [00:01<00:00,  2.57it/s]


 95% 19/20 [00:08<00:00,  1.51it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 13.27it/s]


100% 20/20 [00:09<00:00,  1.86it/s][A[A100% 20/20 [00:09<00:00,  2.17it/s]

100% 1/1 [00:09<00:00,  9.21s/it][A100% 1/1 [00:09<00:00,  9.21s/it]
Took 188.11342573165894 seconds to complete one epoch of meta training and validating
Meta validation score auc = 0.641813
 70% 21/30 [1:07:41<28:49, 192.13s/it]Epoch 21

  0% 0/19 [00:00<?, ?it/s][A

  0% 0/32 [00:00<?, ?it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  3% 1/32 [00:00<00:05,  5.79it/s][A[A

  6% 2/32 [00:00<00:05,  5.33it/s][A[A

  9% 3/32 [00:00<00:05,  5.48it/s][A[A

 12% 4/32 [00:00<00:05,  5.41it/s][A[A

 16% 5/32 [00:00<00:04,  5.42it/s][A[A

 19% 6/32 [00:01<00:04,  5.36it/s][A[A

 22% 7/32 [00:01<00:04,  5.29it/s][A[A

 25% 8/32 [00:01<00:04,  5.17it/s][A[A

 28% 9/32 [00:01<00:04,  5.21it/s][A[A

 31% 10/32 [00:01<00:04,  5.33it/s][A[A

 34% 11/32 [00:02<00:04,  5.18it/s][A[A

 38% 12/32 [00:02<00:04,  4.68it/s][A[A

 41% 13/32 [00:03<00:10,  1.89it/s][A[A

 44% 14/32 [00:03<00:07,  2.26it/s][A[A

 47% 15/32 [00:03<00:05,  2.87it/s][A[A

 50% 16/32 [00:04<00:04,  3.25it/s][A[A

 53% 17/32 [00:04<00:04,  3.70it/s][A[A

 56% 18/32 [00:04<00:03,  4.16it/s][A[A

 59% 19/32 [00:04<00:02,  4.38it/s][A[A

 62% 20/32 [00:04<00:02,  4.65it/s][A[A

 66% 21/32 [00:05<00:02,  4.94it/s][A[A

 69% 22/32 [00:05<00:01,  5.07it/s][A[A

 72% 23/32 [00:05<00:01,  5.52it/s][A[A

 75% 24/32 [00:05<00:01,  5.48it/s][A[A

 78% 25/32 [00:05<00:01,  5.36it/s][A[A

 81% 26/32 [00:06<00:01,  5.03it/s][A[A

 84% 27/32 [00:06<00:00,  5.44it/s][A[A

 88% 28/32 [00:06<00:00,  5.60it/s][A[A

 91% 29/32 [00:06<00:00,  5.01it/s][A[A

 94% 30/32 [00:06<00:00,  5.31it/s][A[A

 97% 31/32 [00:07<00:00,  2.05it/s][A[A

100% 32/32 [00:08<00:00,  2.49it/s][A[A100% 32/32 [00:08<00:00,  3.92it/s]
Meta loss on this task batch = 5.0062e-01, PNorm = 39.6476, GNorm = 0.1073

  5% 1/19 [00:08<02:40,  8.91s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.45it/s][A[A

  6% 2/32 [00:00<00:05,  5.17it/s][A[A

  9% 3/32 [00:00<00:06,  4.66it/s][A[A

 12% 4/32 [00:00<00:05,  4.80it/s][A[A

 16% 5/32 [00:01<00:05,  4.85it/s][A[A

 19% 6/32 [00:01<00:05,  4.81it/s][A[A

 22% 7/32 [00:01<00:04,  5.11it/s][A[A

 25% 8/32 [00:01<00:04,  5.49it/s][A[A

 28% 9/32 [00:01<00:04,  5.34it/s][A[A

 31% 10/32 [00:01<00:04,  5.46it/s][A[A

 34% 11/32 [00:02<00:03,  5.85it/s][A[A

 38% 12/32 [00:03<00:10,  1.99it/s][A[A

 41% 13/32 [00:03<00:07,  2.47it/s][A[A

 44% 14/32 [00:03<00:06,  2.94it/s][A[A

 47% 15/32 [00:03<00:04,  3.49it/s][A[A

 50% 16/32 [00:04<00:04,  3.89it/s][A[A

 53% 17/32 [00:04<00:03,  4.26it/s][A[A

 56% 18/32 [00:04<00:03,  4.31it/s][A[A

 59% 19/32 [00:04<00:02,  4.85it/s][A[A

 62% 20/32 [00:04<00:02,  4.51it/s][A[A

 66% 21/32 [00:05<00:02,  5.06it/s][A[A

 69% 22/32 [00:05<00:01,  5.06it/s][A[A

 72% 23/32 [00:05<00:01,  5.36it/s][A[A

 75% 24/32 [00:05<00:01,  5.53it/s][A[A

 78% 25/32 [00:05<00:01,  5.40it/s][A[A

 81% 26/32 [00:06<00:01,  5.02it/s][A[A

 84% 27/32 [00:06<00:01,  4.88it/s][A[A

 88% 28/32 [00:07<00:02,  1.98it/s][A[A

 91% 29/32 [00:07<00:01,  2.47it/s][A[A

 94% 30/32 [00:07<00:00,  2.97it/s][A[A

 97% 31/32 [00:07<00:00,  3.54it/s][A[A

100% 32/32 [00:08<00:00,  4.02it/s][A[A100% 32/32 [00:08<00:00,  3.95it/s]
Meta loss on this task batch = 4.8347e-01, PNorm = 39.6718, GNorm = 0.0691

 11% 2/19 [00:17<02:31,  8.89s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.38it/s][A[A

  6% 2/32 [00:00<00:05,  5.28it/s][A[A

  9% 3/32 [00:00<00:05,  5.45it/s][A[A

 12% 4/32 [00:00<00:05,  5.35it/s][A[A

 16% 5/32 [00:00<00:05,  5.16it/s][A[A

 19% 6/32 [00:01<00:05,  5.12it/s][A[A

 22% 7/32 [00:01<00:04,  5.13it/s][A[A

 25% 8/32 [00:01<00:04,  5.30it/s][A[A

 28% 9/32 [00:01<00:04,  5.11it/s][A[A

 31% 10/32 [00:01<00:04,  4.78it/s][A[A

 34% 11/32 [00:03<00:10,  1.93it/s][A[A

 38% 12/32 [00:03<00:08,  2.40it/s][A[A

 41% 13/32 [00:03<00:06,  2.82it/s][A[A

 44% 14/32 [00:03<00:05,  3.18it/s][A[A

 47% 15/32 [00:04<00:04,  3.55it/s][A[A

 50% 16/32 [00:04<00:04,  3.89it/s][A[A

 53% 17/32 [00:04<00:03,  4.09it/s][A[A

 56% 18/32 [00:04<00:03,  4.64it/s][A[A

 59% 19/32 [00:04<00:02,  5.00it/s][A[A

 62% 20/32 [00:04<00:02,  5.24it/s][A[A

 66% 21/32 [00:05<00:02,  5.12it/s][A[A

 69% 22/32 [00:05<00:01,  5.04it/s][A[A

 72% 23/32 [00:05<00:01,  5.04it/s][A[A

 75% 24/32 [00:05<00:01,  4.84it/s][A[A

 78% 25/32 [00:06<00:03,  2.06it/s][A[A

 81% 26/32 [00:07<00:02,  2.66it/s][A[A

 84% 27/32 [00:07<00:01,  2.98it/s][A[A

 88% 28/32 [00:07<00:01,  3.35it/s][A[A

 91% 29/32 [00:07<00:00,  3.78it/s][A[A

 94% 30/32 [00:07<00:00,  3.86it/s][A[A

 97% 31/32 [00:08<00:00,  4.31it/s][A[A

100% 32/32 [00:08<00:00,  4.50it/s][A[A100% 32/32 [00:08<00:00,  3.87it/s]
Meta loss on this task batch = 5.2846e-01, PNorm = 39.6833, GNorm = 0.1684

 16% 3/19 [00:26<02:22,  8.93s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.67it/s][A[A

  6% 2/32 [00:00<00:06,  4.62it/s][A[A

  9% 3/32 [00:00<00:06,  4.72it/s][A[A

 12% 4/32 [00:00<00:05,  4.83it/s][A[A

 16% 5/32 [00:01<00:05,  4.68it/s][A[A

 19% 6/32 [00:02<00:13,  1.95it/s][A[A

 22% 7/32 [00:02<00:10,  2.39it/s][A[A

 25% 8/32 [00:02<00:08,  2.89it/s][A[A

 28% 9/32 [00:02<00:06,  3.39it/s][A[A

 31% 10/32 [00:02<00:05,  3.94it/s][A[A

 34% 11/32 [00:03<00:04,  4.33it/s][A[A

 38% 12/32 [00:03<00:04,  4.61it/s][A[A

 41% 13/32 [00:03<00:03,  4.85it/s][A[A

 44% 14/32 [00:03<00:03,  5.19it/s][A[A

 47% 15/32 [00:03<00:03,  5.25it/s][A[A

 50% 16/32 [00:04<00:02,  5.37it/s][A[A

 53% 17/32 [00:04<00:02,  5.69it/s][A[A

 56% 18/32 [00:04<00:02,  5.17it/s][A[A

 59% 19/32 [00:04<00:02,  5.48it/s][A[A

 62% 20/32 [00:04<00:02,  5.46it/s][A[A

 66% 21/32 [00:04<00:02,  5.32it/s][A[A

 69% 22/32 [00:05<00:02,  4.94it/s][A[A

 72% 23/32 [00:05<00:01,  4.82it/s][A[A

 75% 24/32 [00:05<00:01,  4.75it/s][A[A

 78% 25/32 [00:05<00:01,  5.06it/s][A[A

 81% 26/32 [00:06<00:01,  5.16it/s][A[A

 84% 27/32 [00:07<00:02,  2.06it/s][A[A

 88% 28/32 [00:07<00:01,  2.55it/s][A[A

 91% 29/32 [00:07<00:00,  3.08it/s][A[A

 94% 30/32 [00:07<00:00,  3.41it/s][A[A

 97% 31/32 [00:07<00:00,  3.73it/s][A[A

100% 32/32 [00:08<00:00,  4.00it/s][A[A100% 32/32 [00:08<00:00,  3.93it/s]
Meta loss on this task batch = 5.6480e-01, PNorm = 39.6969, GNorm = 0.1068

 21% 4/19 [00:35<02:13,  8.92s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.63it/s][A[A

  6% 2/32 [00:00<00:05,  5.43it/s][A[A

  9% 3/32 [00:00<00:05,  5.73it/s][A[A

 12% 4/32 [00:00<00:04,  5.61it/s][A[A

 16% 5/32 [00:00<00:05,  5.32it/s][A[A

 19% 6/32 [00:01<00:04,  5.21it/s][A[A

 22% 7/32 [00:01<00:05,  4.92it/s][A[A

 25% 8/32 [00:01<00:04,  5.14it/s][A[A

 28% 9/32 [00:01<00:04,  5.12it/s][A[A

 31% 10/32 [00:01<00:04,  5.10it/s][A[A

 34% 11/32 [00:02<00:04,  5.24it/s][A[A

 38% 12/32 [00:03<00:09,  2.01it/s][A[A

 41% 13/32 [00:03<00:07,  2.38it/s][A[A

 44% 14/32 [00:03<00:06,  2.79it/s][A[A

 47% 15/32 [00:03<00:05,  3.13it/s][A[A

 50% 16/32 [00:04<00:04,  3.86it/s][A[A

 53% 17/32 [00:04<00:03,  4.16it/s][A[A

 56% 18/32 [00:04<00:03,  4.29it/s][A[A

 59% 19/32 [00:04<00:02,  4.51it/s][A[A

 62% 20/32 [00:04<00:02,  4.40it/s][A[A

 66% 21/32 [00:05<00:02,  4.72it/s][A[A

 69% 22/32 [00:05<00:01,  5.44it/s][A[A

 72% 23/32 [00:05<00:01,  5.15it/s][A[A

 75% 24/32 [00:05<00:01,  4.81it/s][A[A

 78% 25/32 [00:05<00:01,  4.78it/s][A[A

 81% 26/32 [00:06<00:01,  4.84it/s][A[A

 84% 27/32 [00:07<00:02,  1.99it/s][A[A

 88% 28/32 [00:07<00:01,  2.51it/s][A[A

 91% 29/32 [00:07<00:00,  3.03it/s][A[A

 94% 30/32 [00:07<00:00,  3.36it/s][A[A

 97% 31/32 [00:08<00:00,  3.68it/s][A[A

100% 32/32 [00:08<00:00,  3.95it/s][A[A100% 32/32 [00:08<00:00,  3.86it/s]
Meta loss on this task batch = 4.8723e-01, PNorm = 39.7124, GNorm = 0.0753

 26% 5/19 [00:44<02:05,  8.96s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.84it/s][A[A

  6% 2/32 [00:00<00:05,  5.17it/s][A[A

  9% 3/32 [00:00<00:05,  5.47it/s][A[A

 12% 4/32 [00:00<00:04,  5.61it/s][A[A

 16% 5/32 [00:00<00:05,  4.92it/s][A[A

 19% 6/32 [00:01<00:05,  4.75it/s][A[A

 22% 7/32 [00:01<00:05,  4.79it/s][A[A

 25% 8/32 [00:01<00:05,  4.74it/s][A[A

 28% 9/32 [00:02<00:11,  1.99it/s][A[A

 31% 10/32 [00:02<00:09,  2.44it/s][A[A

 34% 11/32 [00:03<00:07,  2.91it/s][A[A

 38% 12/32 [00:03<00:05,  3.35it/s][A[A

 41% 13/32 [00:03<00:05,  3.68it/s][A[A

 44% 14/32 [00:03<00:04,  3.76it/s][A[A

 47% 15/32 [00:04<00:04,  3.95it/s][A[A

 50% 16/32 [00:04<00:03,  4.08it/s][A[A

 53% 17/32 [00:04<00:03,  4.33it/s][A[A

 56% 18/32 [00:04<00:03,  4.66it/s][A[A

 59% 19/32 [00:04<00:02,  4.64it/s][A[A

 62% 20/32 [00:05<00:02,  4.84it/s][A[A

 66% 21/32 [00:05<00:02,  4.88it/s][A[A

 69% 22/32 [00:05<00:02,  4.83it/s][A[A

 72% 23/32 [00:06<00:04,  2.02it/s][A[A

 75% 24/32 [00:06<00:03,  2.51it/s][A[A

 78% 25/32 [00:07<00:02,  2.85it/s][A[A

 81% 26/32 [00:07<00:01,  3.29it/s][A[A

 84% 27/32 [00:07<00:01,  3.47it/s][A[A

 88% 28/32 [00:07<00:01,  3.54it/s][A[A

 91% 29/32 [00:08<00:00,  3.66it/s][A[A

 94% 30/32 [00:08<00:00,  3.99it/s][A[A

 97% 31/32 [00:08<00:00,  4.55it/s][A[A

100% 32/32 [00:08<00:00,  4.69it/s][A[A100% 32/32 [00:08<00:00,  3.74it/s]
Meta loss on this task batch = 4.5912e-01, PNorm = 39.7299, GNorm = 0.0936

 32% 6/19 [00:54<01:57,  9.07s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.09it/s][A[A

  6% 2/32 [00:01<00:14,  2.08it/s][A[A

  9% 3/32 [00:01<00:11,  2.49it/s][A[A

 12% 4/32 [00:01<00:09,  2.82it/s][A[A

 16% 5/32 [00:02<00:08,  3.22it/s][A[A

 19% 6/32 [00:02<00:07,  3.57it/s][A[A

 22% 7/32 [00:02<00:06,  3.94it/s][A[A

 25% 8/32 [00:02<00:05,  4.10it/s][A[A

 28% 9/32 [00:02<00:05,  4.29it/s][A[A

 31% 10/32 [00:03<00:05,  4.33it/s][A[A

 34% 11/32 [00:03<00:04,  4.68it/s][A[A

 38% 12/32 [00:03<00:04,  4.71it/s][A[A

 41% 13/32 [00:03<00:03,  4.93it/s][A[A

 44% 14/32 [00:03<00:03,  4.97it/s][A[A

 47% 15/32 [00:04<00:03,  4.72it/s][A[A

 50% 16/32 [00:04<00:03,  4.56it/s][A[A

 53% 17/32 [00:05<00:07,  1.97it/s][A[A

 56% 18/32 [00:05<00:05,  2.39it/s][A[A

 59% 19/32 [00:05<00:04,  2.75it/s][A[A

 62% 20/32 [00:06<00:03,  3.05it/s][A[A

 66% 21/32 [00:06<00:03,  3.41it/s][A[A

 69% 22/32 [00:06<00:02,  3.78it/s][A[A

 72% 23/32 [00:06<00:02,  3.95it/s][A[A

 75% 24/32 [00:07<00:01,  4.11it/s][A[A

 78% 25/32 [00:07<00:01,  4.25it/s][A[A

 81% 26/32 [00:07<00:01,  4.54it/s][A[A

 84% 27/32 [00:07<00:01,  4.87it/s][A[A

 88% 28/32 [00:07<00:00,  4.65it/s][A[A

 91% 29/32 [00:08<00:00,  5.03it/s][A[A

 94% 30/32 [00:08<00:00,  4.99it/s][A[A

 97% 31/32 [00:08<00:00,  4.93it/s][A[A

100% 32/32 [00:09<00:00,  1.99it/s][A[A100% 32/32 [00:09<00:00,  3.33it/s]
Meta loss on this task batch = 4.7542e-01, PNorm = 39.7520, GNorm = 0.1180

 37% 7/19 [01:04<01:53,  9.47s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.28it/s][A[A

  6% 2/32 [00:00<00:06,  4.41it/s][A[A

  9% 3/32 [00:00<00:06,  4.44it/s][A[A

 12% 4/32 [00:00<00:06,  4.48it/s][A[A

 16% 5/32 [00:01<00:05,  4.55it/s][A[A

 19% 6/32 [00:01<00:05,  4.46it/s][A[A

 22% 7/32 [00:01<00:05,  4.93it/s][A[A

 25% 8/32 [00:01<00:04,  4.82it/s][A[A

 28% 9/32 [00:01<00:04,  4.90it/s][A[A

 31% 10/32 [00:02<00:04,  5.30it/s][A[A

 34% 11/32 [00:02<00:04,  4.90it/s][A[A

 38% 12/32 [00:03<00:10,  1.99it/s][A[A

 41% 13/32 [00:03<00:07,  2.50it/s][A[A

 44% 14/32 [00:03<00:05,  3.04it/s][A[A

 47% 15/32 [00:04<00:05,  3.40it/s][A[A

 50% 16/32 [00:04<00:04,  3.67it/s][A[A

 53% 17/32 [00:04<00:03,  3.89it/s][A[A

 56% 18/32 [00:04<00:03,  4.28it/s][A[A

 59% 19/32 [00:04<00:02,  4.41it/s][A[A

 62% 20/32 [00:05<00:02,  4.68it/s][A[A

 66% 21/32 [00:05<00:02,  4.94it/s][A[A

 69% 22/32 [00:05<00:02,  4.59it/s][A[A

 72% 23/32 [00:05<00:01,  4.94it/s][A[A

 75% 24/32 [00:05<00:01,  4.79it/s][A[A

 78% 25/32 [00:06<00:01,  4.86it/s][A[A

 81% 26/32 [00:07<00:03,  1.93it/s][A[A

 84% 27/32 [00:07<00:02,  2.28it/s][A[A

 88% 28/32 [00:07<00:01,  2.67it/s][A[A

 91% 29/32 [00:07<00:00,  3.09it/s][A[A

 94% 30/32 [00:08<00:00,  3.47it/s][A[A

 97% 31/32 [00:08<00:00,  3.75it/s][A[A

100% 32/32 [00:08<00:00,  3.87it/s][A[A100% 32/32 [00:08<00:00,  3.70it/s]
Meta loss on this task batch = 3.5743e-01, PNorm = 39.7771, GNorm = 0.0838

 42% 8/19 [01:13<01:44,  9.46s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.49it/s][A[A

  6% 2/32 [00:00<00:06,  4.42it/s][A[A

  9% 3/32 [00:00<00:06,  4.47it/s][A[A

 12% 4/32 [00:00<00:06,  4.34it/s][A[A

 16% 5/32 [00:01<00:06,  4.40it/s][A[A

 19% 6/32 [00:02<00:13,  1.89it/s][A[A

 22% 7/32 [00:02<00:10,  2.30it/s][A[A

 25% 8/32 [00:02<00:08,  2.69it/s][A[A

 28% 9/32 [00:03<00:07,  3.05it/s][A[A

 31% 10/32 [00:03<00:06,  3.36it/s][A[A

 34% 11/32 [00:03<00:05,  3.63it/s][A[A

 38% 12/32 [00:03<00:05,  3.83it/s][A[A

 41% 13/32 [00:03<00:04,  3.97it/s][A[A

 44% 14/32 [00:04<00:04,  4.07it/s][A[A

 47% 15/32 [00:04<00:04,  4.09it/s][A[A

 50% 16/32 [00:04<00:03,  4.07it/s][A[A

 53% 17/32 [00:04<00:03,  4.20it/s][A[A

 56% 18/32 [00:06<00:07,  1.88it/s][A[A

 59% 19/32 [00:06<00:05,  2.29it/s][A[A

 62% 20/32 [00:06<00:04,  2.69it/s][A[A

 66% 21/32 [00:06<00:03,  3.00it/s][A[A

 69% 22/32 [00:06<00:02,  3.37it/s][A[A

 72% 23/32 [00:07<00:02,  3.72it/s][A[A

 75% 24/32 [00:07<00:02,  3.87it/s][A[A

 78% 25/32 [00:07<00:01,  4.00it/s][A[A

 81% 26/32 [00:07<00:01,  4.08it/s][A[A

 84% 27/32 [00:08<00:01,  4.27it/s][A[A

 88% 28/32 [00:08<00:00,  4.29it/s][A[A

 91% 29/32 [00:08<00:00,  4.26it/s][A[A

 94% 30/32 [00:09<00:01,  1.89it/s][A[A

 97% 31/32 [00:10<00:00,  2.26it/s][A[A

100% 32/32 [00:10<00:00,  2.63it/s][A[A100% 32/32 [00:10<00:00,  3.12it/s]
Meta loss on this task batch = 2.0346e-01, PNorm = 39.8041, GNorm = 0.0727

 47% 9/19 [01:25<01:39,  9.95s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.26it/s][A[A

  6% 2/32 [00:00<00:07,  4.25it/s][A[A

  9% 3/32 [00:00<00:06,  4.31it/s][A[A

 12% 4/32 [00:00<00:06,  4.40it/s][A[A

 16% 5/32 [00:01<00:06,  4.46it/s][A[A

 19% 6/32 [00:01<00:05,  4.38it/s][A[A

 22% 7/32 [00:01<00:05,  4.44it/s][A[A

 25% 8/32 [00:01<00:05,  4.45it/s][A[A

 28% 9/32 [00:03<00:12,  1.90it/s][A[A

 31% 10/32 [00:03<00:09,  2.28it/s][A[A

 34% 11/32 [00:03<00:07,  2.68it/s][A[A

 38% 12/32 [00:03<00:06,  3.02it/s][A[A

 41% 13/32 [00:03<00:05,  3.34it/s][A[A

 44% 14/32 [00:04<00:04,  3.60it/s][A[A

 47% 15/32 [00:04<00:04,  3.79it/s][A[A

 50% 16/32 [00:04<00:04,  3.95it/s][A[A

 53% 17/32 [00:04<00:03,  4.03it/s][A[A

 56% 18/32 [00:05<00:03,  4.10it/s][A[A

 59% 19/32 [00:06<00:06,  1.87it/s][A[A

 62% 20/32 [00:06<00:05,  2.28it/s][A[A

 66% 21/32 [00:06<00:04,  2.68it/s][A[A

 69% 22/32 [00:06<00:03,  3.04it/s][A[A

 72% 23/32 [00:07<00:02,  3.43it/s][A[A

 75% 24/32 [00:07<00:02,  3.73it/s][A[A

 78% 25/32 [00:07<00:01,  3.98it/s][A[A

 81% 26/32 [00:07<00:01,  4.05it/s][A[A

 84% 27/32 [00:08<00:01,  4.38it/s][A[A

 88% 28/32 [00:08<00:00,  4.39it/s][A[A

 91% 29/32 [00:08<00:00,  4.42it/s][A[A

 94% 30/32 [00:08<00:00,  4.38it/s][A[A

 97% 31/32 [00:08<00:00,  4.32it/s][A[A

100% 32/32 [00:09<00:00,  4.61it/s][A[A100% 32/32 [00:09<00:00,  3.50it/s]
Meta loss on this task batch = 2.6319e-01, PNorm = 39.8318, GNorm = 0.0683

 53% 10/19 [01:34<01:29,  9.95s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.07it/s][A[A

  6% 2/32 [00:01<00:16,  1.85it/s][A[A

  9% 3/32 [00:01<00:13,  2.21it/s][A[A

 12% 4/32 [00:01<00:10,  2.60it/s][A[A

 16% 5/32 [00:02<00:09,  2.97it/s][A[A

 19% 6/32 [00:02<00:07,  3.39it/s][A[A

 22% 7/32 [00:02<00:07,  3.56it/s][A[A

 25% 8/32 [00:02<00:06,  3.74it/s][A[A

 28% 9/32 [00:03<00:06,  3.81it/s][A[A

 31% 10/32 [00:03<00:05,  3.92it/s][A[A

 34% 11/32 [00:03<00:05,  4.00it/s][A[A

 38% 12/32 [00:03<00:04,  4.14it/s][A[A

 41% 13/32 [00:04<00:04,  4.18it/s][A[A

 44% 14/32 [00:04<00:04,  4.19it/s][A[A

 47% 15/32 [00:04<00:04,  4.16it/s][A[A

 50% 16/32 [00:05<00:08,  1.88it/s][A[A

 53% 17/32 [00:05<00:06,  2.25it/s][A[A

 56% 18/32 [00:06<00:05,  2.67it/s][A[A

 59% 19/32 [00:06<00:04,  3.03it/s][A[A

 62% 20/32 [00:06<00:03,  3.30it/s][A[A

 66% 21/32 [00:06<00:03,  3.50it/s][A[A

 69% 22/32 [00:07<00:02,  3.68it/s][A[A

 72% 23/32 [00:07<00:02,  3.80it/s][A[A

 75% 24/32 [00:07<00:02,  3.92it/s][A[A

 78% 25/32 [00:07<00:01,  4.16it/s][A[A

 81% 26/32 [00:09<00:03,  1.85it/s][A[A

 84% 27/32 [00:09<00:02,  2.23it/s][A[A

 88% 28/32 [00:09<00:01,  2.59it/s][A[A

 91% 29/32 [00:09<00:01,  2.95it/s][A[A

 94% 30/32 [00:09<00:00,  3.29it/s][A[A

 97% 31/32 [00:10<00:00,  3.52it/s][A[A

100% 32/32 [00:10<00:00,  3.99it/s][A[A100% 32/32 [00:10<00:00,  3.08it/s]
Meta loss on this task batch = 5.8094e-01, PNorm = 39.8525, GNorm = 0.2524

 58% 11/19 [01:46<01:22, 10.33s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.77it/s][A[A

  6% 2/32 [00:00<00:07,  4.04it/s][A[A

  9% 3/32 [00:00<00:07,  4.07it/s][A[A

 12% 4/32 [00:01<00:15,  1.85it/s][A[A

 16% 5/32 [00:02<00:12,  2.23it/s][A[A

 19% 6/32 [00:02<00:10,  2.60it/s][A[A

 22% 7/32 [00:02<00:08,  2.94it/s][A[A

 25% 8/32 [00:02<00:07,  3.28it/s][A[A

 28% 9/32 [00:03<00:06,  3.53it/s][A[A

 31% 10/32 [00:03<00:05,  3.71it/s][A[A

 34% 11/32 [00:03<00:05,  3.90it/s][A[A

 38% 12/32 [00:03<00:05,  3.97it/s][A[A

 41% 13/32 [00:04<00:04,  4.05it/s][A[A

 44% 14/32 [00:04<00:04,  4.05it/s][A[A

 47% 15/32 [00:04<00:04,  4.04it/s][A[A

 50% 16/32 [00:04<00:03,  4.08it/s][A[A

 53% 17/32 [00:06<00:08,  1.83it/s][A[A

 56% 18/32 [00:06<00:06,  2.19it/s][A[A

 59% 19/32 [00:06<00:04,  2.68it/s][A[A

 62% 20/32 [00:06<00:04,  2.98it/s][A[A

 66% 21/32 [00:06<00:03,  3.29it/s][A[A

 69% 22/32 [00:07<00:02,  3.53it/s][A[A

 72% 23/32 [00:07<00:02,  3.68it/s][A[A

 75% 24/32 [00:08<00:04,  1.81it/s][A[A

 78% 25/32 [00:08<00:03,  2.18it/s][A[A

 81% 26/32 [00:09<00:02,  2.58it/s][A[A

 84% 27/32 [00:09<00:01,  2.89it/s][A[A

 88% 28/32 [00:09<00:01,  3.16it/s][A[A

 91% 29/32 [00:09<00:00,  3.44it/s][A[A

 94% 30/32 [00:09<00:00,  3.95it/s][A[A

 97% 31/32 [00:10<00:00,  4.07it/s][A[A

100% 32/32 [00:10<00:00,  4.13it/s][A[A100% 32/32 [00:10<00:00,  3.07it/s]
Meta loss on this task batch = 5.8408e-01, PNorm = 39.8709, GNorm = 0.1843

 63% 12/19 [01:57<01:14, 10.61s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.52it/s][A[A

  6% 2/32 [00:00<00:06,  4.46it/s][A[A

  9% 3/32 [00:01<00:15,  1.90it/s][A[A

 12% 4/32 [00:01<00:12,  2.26it/s][A[A

 16% 5/32 [00:02<00:10,  2.63it/s][A[A

 19% 6/32 [00:02<00:08,  2.95it/s][A[A

 22% 7/32 [00:02<00:07,  3.42it/s][A[A

 25% 8/32 [00:02<00:06,  3.81it/s][A[A

 28% 9/32 [00:02<00:05,  4.22it/s][A[A

 31% 10/32 [00:03<00:05,  4.21it/s][A[A

 34% 11/32 [00:03<00:05,  4.19it/s][A[A

 38% 12/32 [00:04<00:10,  1.86it/s][A[A

 41% 13/32 [00:04<00:08,  2.24it/s][A[A

 44% 14/32 [00:05<00:06,  2.62it/s][A[A

 47% 15/32 [00:05<00:05,  2.96it/s][A[A

 50% 16/32 [00:05<00:04,  3.29it/s][A[A

 53% 17/32 [00:05<00:03,  3.76it/s][A[A

 56% 18/32 [00:06<00:03,  3.93it/s][A[A

 59% 19/32 [00:06<00:03,  3.97it/s][A[A

 62% 20/32 [00:06<00:03,  3.99it/s][A[A

 66% 21/32 [00:06<00:02,  4.06it/s][A[A

 69% 22/32 [00:06<00:02,  4.09it/s][A[A

 72% 23/32 [00:07<00:02,  4.40it/s][A[A

 75% 24/32 [00:07<00:01,  4.32it/s][A[A

 78% 25/32 [00:07<00:01,  4.33it/s][A[A

 81% 26/32 [00:07<00:01,  4.38it/s][A[A

 84% 27/32 [00:08<00:01,  4.65it/s][A[A

 88% 28/32 [00:08<00:00,  4.55it/s][A[A

 91% 29/32 [00:08<00:00,  4.43it/s][A[A

 94% 30/32 [00:08<00:00,  4.39it/s][A[A

 97% 31/32 [00:08<00:00,  4.58it/s][A[A

100% 32/32 [00:09<00:00,  4.43it/s][A[A100% 32/32 [00:09<00:00,  3.49it/s]
Meta loss on this task batch = 5.7715e-01, PNorm = 39.8894, GNorm = 0.1039

 68% 13/19 [02:07<01:02, 10.42s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:37,  1.22s/it][A[A

  6% 2/32 [00:01<00:27,  1.10it/s][A[A

  9% 3/32 [00:01<00:20,  1.42it/s][A[A

 12% 4/32 [00:01<00:15,  1.78it/s][A[A

 16% 5/32 [00:02<00:12,  2.18it/s][A[A

 19% 6/32 [00:02<00:09,  2.64it/s][A[A

 22% 7/32 [00:02<00:08,  2.98it/s][A[A

 25% 8/32 [00:02<00:06,  3.55it/s][A[A

 28% 9/32 [00:02<00:06,  3.72it/s][A[A

 31% 10/32 [00:03<00:05,  3.87it/s][A[A

 34% 11/32 [00:03<00:05,  3.97it/s][A[A

 38% 12/32 [00:04<00:10,  1.83it/s][A[A

 41% 13/32 [00:04<00:08,  2.21it/s][A[A

 44% 14/32 [00:05<00:07,  2.57it/s][A[A

 47% 15/32 [00:05<00:05,  2.89it/s][A[A

 50% 16/32 [00:05<00:05,  3.14it/s][A[A

 53% 17/32 [00:05<00:04,  3.39it/s][A[A

 56% 18/32 [00:06<00:03,  3.71it/s][A[A

 59% 19/32 [00:06<00:03,  3.99it/s][A[A

 62% 20/32 [00:06<00:02,  4.04it/s][A[A

 66% 21/32 [00:06<00:02,  4.08it/s][A[A

 69% 22/32 [00:06<00:02,  4.33it/s][A[A

 72% 23/32 [00:07<00:02,  4.35it/s][A[A

 75% 24/32 [00:07<00:01,  4.55it/s][A[A

 78% 25/32 [00:07<00:01,  4.48it/s][A[A

 81% 26/32 [00:08<00:03,  1.93it/s][A[A

 84% 27/32 [00:08<00:02,  2.37it/s][A[A

 88% 28/32 [00:09<00:01,  2.69it/s][A[A

 91% 29/32 [00:09<00:01,  3.00it/s][A[A

 94% 30/32 [00:09<00:00,  3.39it/s][A[A

 97% 31/32 [00:09<00:00,  3.74it/s][A[A

100% 32/32 [00:10<00:00,  4.05it/s][A[A100% 32/32 [00:10<00:00,  3.17it/s]
Meta loss on this task batch = 5.6070e-01, PNorm = 39.9090, GNorm = 0.0727

 74% 14/19 [02:18<00:52, 10.56s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.30it/s][A[A

  6% 2/32 [00:00<00:06,  4.52it/s][A[A

  9% 3/32 [00:01<00:15,  1.93it/s][A[A

 12% 4/32 [00:01<00:11,  2.42it/s][A[A

 16% 5/32 [00:01<00:09,  2.89it/s][A[A

 19% 6/32 [00:02<00:08,  3.12it/s][A[A

 22% 7/32 [00:02<00:07,  3.36it/s][A[A

 25% 8/32 [00:02<00:06,  3.71it/s][A[A

 28% 9/32 [00:02<00:05,  3.89it/s][A[A

 31% 10/32 [00:03<00:05,  4.29it/s][A[A

 34% 11/32 [00:03<00:04,  4.38it/s][A[A

 38% 12/32 [00:03<00:04,  4.39it/s][A[A

 41% 13/32 [00:03<00:04,  4.53it/s][A[A

 44% 14/32 [00:03<00:04,  4.43it/s][A[A

 47% 15/32 [00:05<00:08,  1.94it/s][A[A

 50% 16/32 [00:05<00:06,  2.37it/s][A[A

 53% 17/32 [00:05<00:05,  2.83it/s][A[A

 56% 18/32 [00:05<00:04,  3.31it/s][A[A

 59% 19/32 [00:05<00:03,  3.68it/s][A[A

 62% 20/32 [00:06<00:03,  3.88it/s][A[A

 66% 21/32 [00:06<00:02,  4.15it/s][A[A

 69% 22/32 [00:06<00:02,  4.47it/s][A[A

 72% 23/32 [00:06<00:02,  4.30it/s][A[A

 75% 24/32 [00:07<00:01,  4.50it/s][A[A

 78% 25/32 [00:08<00:03,  1.92it/s][A[A

 81% 26/32 [00:08<00:02,  2.25it/s][A[A

 84% 27/32 [00:08<00:01,  2.64it/s][A[A

 88% 28/32 [00:08<00:01,  2.97it/s][A[A

 91% 29/32 [00:09<00:00,  3.29it/s][A[A

 94% 30/32 [00:09<00:00,  3.53it/s][A[A

 97% 31/32 [00:09<00:00,  3.96it/s][A[A

100% 32/32 [00:09<00:00,  4.05it/s][A[A100% 32/32 [00:09<00:00,  3.25it/s]
Meta loss on this task batch = 5.0734e-01, PNorm = 39.9286, GNorm = 0.0813

 79% 15/19 [02:28<00:42, 10.58s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.15it/s][A[A

  6% 2/32 [00:00<00:06,  4.49it/s][A[A

  9% 3/32 [00:00<00:06,  4.69it/s][A[A

 12% 4/32 [00:00<00:05,  4.85it/s][A[A

 16% 5/32 [00:00<00:05,  5.04it/s][A[A

 19% 6/32 [00:01<00:05,  4.90it/s][A[A

 22% 7/32 [00:02<00:12,  1.98it/s][A[A

 25% 8/32 [00:02<00:10,  2.34it/s][A[A

 28% 9/32 [00:02<00:08,  2.79it/s][A[A

 31% 10/32 [00:03<00:07,  3.11it/s][A[A

 34% 11/32 [00:03<00:06,  3.35it/s][A[A

 38% 12/32 [00:03<00:05,  3.72it/s][A[A

 41% 13/32 [00:03<00:04,  4.12it/s][A[A

 44% 14/32 [00:03<00:04,  4.46it/s][A[A

 47% 15/32 [00:04<00:03,  4.57it/s][A[A

 50% 16/32 [00:04<00:03,  4.55it/s][A[A

 53% 17/32 [00:05<00:07,  1.90it/s][A[A

 56% 18/32 [00:05<00:05,  2.40it/s][A[A

 59% 19/32 [00:05<00:04,  2.85it/s][A[A

 62% 20/32 [00:06<00:03,  3.29it/s][A[A

 66% 21/32 [00:06<00:02,  3.72it/s][A[A

 69% 22/32 [00:06<00:02,  4.04it/s][A[A

 72% 23/32 [00:06<00:02,  4.07it/s][A[A

 75% 24/32 [00:06<00:01,  4.05it/s][A[A

 78% 25/32 [00:07<00:01,  4.51it/s][A[A

 81% 26/32 [00:07<00:01,  4.58it/s][A[A

 84% 27/32 [00:07<00:01,  4.40it/s][A[A

 88% 28/32 [00:07<00:00,  4.45it/s][A[A

 91% 29/32 [00:09<00:01,  1.92it/s][A[A

 94% 30/32 [00:09<00:00,  2.29it/s][A[A

 97% 31/32 [00:09<00:00,  2.65it/s][A[A

100% 32/32 [00:09<00:00,  3.07it/s][A[A100% 32/32 [00:09<00:00,  3.29it/s]
Meta loss on this task batch = 5.4354e-01, PNorm = 39.9484, GNorm = 0.0896

 84% 16/19 [02:39<00:31, 10.56s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.78it/s][A[A

  6% 2/32 [00:00<00:07,  3.90it/s][A[A

  9% 3/32 [00:00<00:07,  4.05it/s][A[A

 12% 4/32 [00:00<00:06,  4.27it/s][A[A

 16% 5/32 [00:01<00:06,  4.49it/s][A[A

 19% 6/32 [00:01<00:05,  4.55it/s][A[A

 22% 7/32 [00:01<00:05,  4.95it/s][A[A

 25% 8/32 [00:01<00:05,  4.67it/s][A[A

 28% 9/32 [00:01<00:04,  4.66it/s][A[A

 31% 10/32 [00:02<00:04,  5.31it/s][A[A

 34% 11/32 [00:02<00:04,  5.24it/s][A[A

 38% 12/32 [00:02<00:04,  5.00it/s][A[A

 41% 13/32 [00:03<00:09,  2.04it/s][A[A

 44% 14/32 [00:03<00:07,  2.52it/s][A[A

 47% 15/32 [00:04<00:05,  2.95it/s][A[A

 50% 16/32 [00:04<00:04,  3.36it/s][A[A

 53% 17/32 [00:04<00:04,  3.59it/s][A[A

 56% 18/32 [00:04<00:03,  3.83it/s][A[A

 59% 19/32 [00:04<00:03,  4.00it/s][A[A

 62% 20/32 [00:05<00:02,  4.22it/s][A[A

 66% 21/32 [00:05<00:02,  4.47it/s][A[A

 69% 22/32 [00:05<00:02,  4.33it/s][A[A

 72% 23/32 [00:05<00:02,  4.50it/s][A[A

 75% 24/32 [00:06<00:01,  4.19it/s][A[A

 78% 25/32 [00:06<00:01,  4.29it/s][A[A

 81% 26/32 [00:07<00:03,  1.85it/s][A[A

 84% 27/32 [00:07<00:02,  2.28it/s][A[A

 88% 28/32 [00:07<00:01,  2.70it/s][A[A

 91% 29/32 [00:08<00:00,  3.02it/s][A[A

 94% 30/32 [00:08<00:00,  3.24it/s][A[A

 97% 31/32 [00:08<00:00,  3.72it/s][A[A

100% 32/32 [00:08<00:00,  4.12it/s][A[A100% 32/32 [00:08<00:00,  3.63it/s]
Meta loss on this task batch = 4.3905e-01, PNorm = 39.9691, GNorm = 0.0589

 89% 17/19 [02:49<00:20, 10.27s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.67it/s][A[A

  6% 2/32 [00:00<00:06,  4.73it/s][A[A

  9% 3/32 [00:00<00:06,  4.53it/s][A[A

 12% 4/32 [00:01<00:14,  1.92it/s][A[A

 16% 5/32 [00:02<00:11,  2.32it/s][A[A

 19% 6/32 [00:02<00:09,  2.79it/s][A[A

 22% 7/32 [00:02<00:08,  3.10it/s][A[A

 25% 8/32 [00:02<00:07,  3.33it/s][A[A

 28% 9/32 [00:02<00:06,  3.74it/s][A[A

 31% 10/32 [00:03<00:05,  3.91it/s][A[A

 34% 11/32 [00:03<00:04,  4.36it/s][A[A

 38% 12/32 [00:03<00:04,  4.54it/s][A[A

 41% 13/32 [00:03<00:04,  4.34it/s][A[A

 44% 14/32 [00:05<00:09,  1.93it/s][A[A

 47% 15/32 [00:05<00:07,  2.31it/s][A[A

 50% 16/32 [00:05<00:05,  2.77it/s][A[A

 53% 17/32 [00:05<00:04,  3.18it/s][A[A

 56% 18/32 [00:05<00:03,  3.60it/s][A[A

 59% 19/32 [00:06<00:03,  3.83it/s][A[A

 62% 20/32 [00:06<00:03,  3.97it/s][A[A

 66% 21/32 [00:06<00:02,  4.03it/s][A[A

 69% 22/32 [00:06<00:02,  4.30it/s][A[A

 72% 23/32 [00:06<00:02,  4.47it/s][A[A

 75% 24/32 [00:07<00:01,  4.21it/s][A[A

 78% 25/32 [00:08<00:03,  1.94it/s][A[A

 81% 26/32 [00:08<00:02,  2.37it/s][A[A

 84% 27/32 [00:08<00:01,  2.69it/s][A[A

 88% 28/32 [00:09<00:01,  3.00it/s][A[A

 91% 29/32 [00:09<00:00,  3.31it/s][A[A

 94% 30/32 [00:09<00:00,  3.78it/s][A[A

 97% 31/32 [00:09<00:00,  4.25it/s][A[A

100% 32/32 [00:09<00:00,  4.29it/s][A[A100% 32/32 [00:09<00:00,  3.24it/s]
Meta loss on this task batch = 5.1369e-01, PNorm = 39.9867, GNorm = 0.0792

 95% 18/19 [02:59<00:10, 10.39s/it][A

  0% 0/23 [00:00<?, ?it/s][A[A

  4% 1/23 [00:00<00:05,  4.06it/s][A[A

  9% 2/23 [00:00<00:05,  4.19it/s][A[A

 13% 3/23 [00:00<00:04,  4.36it/s][A[A

 17% 4/23 [00:00<00:04,  4.75it/s][A[A

 22% 5/23 [00:00<00:03,  5.40it/s][A[A

 26% 6/23 [00:01<00:03,  5.32it/s][A[A

 30% 7/23 [00:02<00:07,  2.07it/s][A[A

 35% 8/23 [00:02<00:06,  2.44it/s][A[A

 39% 9/23 [00:02<00:04,  2.96it/s][A[A

 43% 10/23 [00:03<00:04,  3.17it/s][A[A

 48% 11/23 [00:03<00:03,  3.70it/s][A[A

 52% 12/23 [00:03<00:02,  3.90it/s][A[A

 57% 13/23 [00:03<00:02,  4.21it/s][A[A

 61% 14/23 [00:03<00:01,  4.59it/s][A[A

 65% 15/23 [00:04<00:02,  3.75it/s][A[A

 70% 16/23 [00:05<00:03,  1.75it/s][A[A

 74% 17/23 [00:05<00:02,  2.12it/s][A[A

 78% 18/23 [00:05<00:02,  2.46it/s][A[A

 83% 19/23 [00:06<00:01,  2.85it/s][A[A

 87% 20/23 [00:06<00:00,  3.32it/s][A[A

 91% 21/23 [00:06<00:00,  3.72it/s][A[A

 96% 22/23 [00:06<00:00,  4.10it/s][A[A

100% 23/23 [00:06<00:00,  4.21it/s][A[A100% 23/23 [00:06<00:00,  3.32it/s]
Meta loss on this task batch = 4.3028e-01, PNorm = 40.0050, GNorm = 0.0791

100% 19/19 [03:07<00:00,  9.53s/it][A100% 19/19 [03:07<00:00,  9.85s/it]
Took 187.24739503860474 seconds to complete one epoch of meta training

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/20 [00:00<?, ?it/s][A[A


  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 31.44it/s]


  5% 1/20 [00:00<00:03,  5.61it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 21.45it/s]


 10% 2/20 [00:00<00:03,  5.01it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 18.52it/s][A[A[A100% 3/3 [00:00<00:00, 20.53it/s]


 15% 3/20 [00:00<00:03,  4.54it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A


100% 1/1 [00:01<00:00,  1.07s/it][A[A[A100% 1/1 [00:01<00:00,  1.07s/it]


 20% 4/20 [00:01<00:08,  1.92it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 14.84it/s][A[A[A100% 4/4 [00:00<00:00, 17.22it/s]


 25% 5/20 [00:02<00:07,  2.09it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.57it/s][A[A[A100% 4/4 [00:00<00:00, 16.25it/s]


 30% 6/20 [00:02<00:06,  2.19it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.64it/s][A[A[A100% 4/4 [00:00<00:00, 21.25it/s]


 35% 7/20 [00:03<00:05,  2.34it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.81it/s][A[A[A


100% 4/4 [00:00<00:00, 19.50it/s][A[A[A100% 4/4 [00:00<00:00, 19.28it/s]


 40% 8/20 [00:03<00:04,  2.45it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.48it/s][A[A[A100% 4/4 [00:00<00:00, 23.18it/s]


 45% 9/20 [00:03<00:04,  2.57it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 17.74it/s][A[A[A100% 4/4 [00:00<00:00, 19.79it/s]


 50% 10/20 [00:04<00:03,  2.65it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:01<00:00,  2.54it/s][A[A[A100% 4/4 [00:01<00:00,  3.31it/s]


 55% 11/20 [00:05<00:06,  1.49it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.26it/s][A[A[A100% 4/4 [00:00<00:00, 21.50it/s]


 60% 12/20 [00:05<00:04,  1.72it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.90it/s][A[A[A100% 3/3 [00:00<00:00, 14.17it/s]


 65% 13/20 [00:06<00:03,  1.91it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.36it/s][A[A[A100% 3/3 [00:00<00:00, 14.01it/s]


 70% 14/20 [00:06<00:02,  2.08it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 14.56it/s][A[A[A100% 4/4 [00:00<00:00, 18.97it/s]


 75% 15/20 [00:07<00:02,  2.21it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.82it/s][A[A[A100% 3/3 [00:00<00:00, 18.17it/s]


 80% 16/20 [00:07<00:01,  2.41it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 12.45it/s][A[A[A100% 3/3 [00:00<00:00, 15.21it/s]


 85% 17/20 [00:07<00:01,  2.46it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 22.76it/s]


 90% 18/20 [00:07<00:00,  2.72it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 15.45it/s][A[A[A100% 3/3 [00:00<00:00, 19.80it/s]


 95% 19/20 [00:08<00:00,  2.86it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A


100% 1/1 [00:01<00:00,  1.06s/it][A[A[A100% 1/1 [00:01<00:00,  1.06s/it]


100% 20/20 [00:09<00:00,  1.63it/s][A[A100% 20/20 [00:09<00:00,  2.10it/s]

100% 1/1 [00:09<00:00,  9.53s/it][A100% 1/1 [00:09<00:00,  9.53s/it]
Took 196.78076314926147 seconds to complete one epoch of meta training and validating
Meta validation score auc = 0.645301
Found better MAML checkpoint after meta validation, saving now
 73% 22/30 [1:10:58<25:48, 193.54s/it]Epoch 22

  0% 0/19 [00:00<?, ?it/s][A

  0% 0/32 [00:00<?, ?it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  3% 1/32 [00:00<00:06,  4.49it/s][A[A

  6% 2/32 [00:00<00:07,  4.19it/s][A[A

  9% 3/32 [00:00<00:06,  4.31it/s][A[A

 12% 4/32 [00:00<00:05,  4.80it/s][A[A

 16% 5/32 [00:01<00:05,  5.33it/s][A[A

 19% 6/32 [00:01<00:04,  5.39it/s][A[A

 22% 7/32 [00:01<00:04,  5.59it/s][A[A

 25% 8/32 [00:01<00:04,  5.49it/s][A[A

 28% 9/32 [00:01<00:03,  6.12it/s][A[A

 31% 10/32 [00:01<00:03,  6.52it/s][A[A

 34% 11/32 [00:02<00:03,  5.45it/s][A[A

 38% 12/32 [00:03<00:10,  1.98it/s][A[A

 41% 13/32 [00:03<00:08,  2.31it/s][A[A

 44% 14/32 [00:03<00:06,  2.65it/s][A[A

 47% 15/32 [00:03<00:05,  3.16it/s][A[A

 50% 16/32 [00:04<00:04,  3.50it/s][A[A

 53% 17/32 [00:04<00:04,  3.69it/s][A[A

 56% 18/32 [00:04<00:03,  4.14it/s][A[A

 59% 19/32 [00:04<00:02,  4.61it/s][A[A

 62% 20/32 [00:04<00:02,  4.60it/s][A[A

 66% 21/32 [00:05<00:02,  5.34it/s][A[A

 69% 22/32 [00:05<00:01,  5.29it/s][A[A

 72% 23/32 [00:05<00:01,  5.61it/s][A[A

 75% 24/32 [00:05<00:01,  5.24it/s][A[A

 78% 25/32 [00:05<00:01,  5.24it/s][A[A

 81% 26/32 [00:07<00:02,  2.02it/s][A[A

 84% 27/32 [00:07<00:01,  2.55it/s][A[A

 88% 28/32 [00:07<00:01,  3.11it/s][A[A

 91% 29/32 [00:07<00:00,  3.51it/s][A[A

 94% 30/32 [00:07<00:00,  4.28it/s][A[A

 97% 31/32 [00:07<00:00,  4.57it/s][A[A

100% 32/32 [00:08<00:00,  4.92it/s][A[A100% 32/32 [00:08<00:00,  3.98it/s]
Meta loss on this task batch = 5.2772e-01, PNorm = 40.0226, GNorm = 0.1351

  5% 1/19 [00:08<02:37,  8.75s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:03,  8.73it/s][A[A

  6% 2/32 [00:00<00:03,  8.25it/s][A[A

  9% 3/32 [00:00<00:03,  7.30it/s][A[A

 12% 4/32 [00:00<00:04,  6.24it/s][A[A

 16% 5/32 [00:00<00:04,  5.74it/s][A[A

 19% 6/32 [00:00<00:04,  6.08it/s][A[A

 22% 7/32 [00:01<00:03,  6.59it/s][A[A

 25% 8/32 [00:01<00:03,  6.78it/s][A[A

 28% 9/32 [00:01<00:03,  6.75it/s][A[A

 31% 10/32 [00:01<00:03,  7.02it/s][A[A

 34% 11/32 [00:01<00:03,  6.49it/s][A[A

 38% 12/32 [00:01<00:03,  6.22it/s][A[A

 41% 13/32 [00:02<00:02,  6.41it/s][A[A

 44% 14/32 [00:03<00:08,  2.11it/s][A[A

 47% 15/32 [00:03<00:06,  2.54it/s][A[A

 50% 16/32 [00:03<00:05,  2.99it/s][A[A

 53% 17/32 [00:03<00:04,  3.33it/s][A[A

 56% 18/32 [00:04<00:03,  3.65it/s][A[A

 59% 19/32 [00:04<00:03,  4.24it/s][A[A

 62% 20/32 [00:04<00:02,  4.42it/s][A[A

 66% 21/32 [00:04<00:02,  4.58it/s][A[A

 69% 22/32 [00:04<00:01,  5.18it/s][A[A

 72% 23/32 [00:04<00:01,  5.65it/s][A[A

 75% 24/32 [00:05<00:01,  6.09it/s][A[A

 78% 25/32 [00:05<00:01,  5.71it/s][A[A

 81% 26/32 [00:05<00:01,  5.41it/s][A[A

 84% 27/32 [00:05<00:00,  5.55it/s][A[A

 88% 28/32 [00:05<00:00,  5.17it/s][A[A

 91% 29/32 [00:05<00:00,  5.78it/s][A[A

 94% 30/32 [00:07<00:00,  2.08it/s][A[A

 97% 31/32 [00:07<00:00,  2.52it/s][A[A

100% 32/32 [00:07<00:00,  3.15it/s][A[A100% 32/32 [00:07<00:00,  4.26it/s]
Meta loss on this task batch = 4.9724e-01, PNorm = 40.0443, GNorm = 0.0726

 11% 2/19 [00:16<02:25,  8.59s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.92it/s][A[A

  6% 2/32 [00:00<00:05,  5.49it/s][A[A

  9% 3/32 [00:00<00:05,  5.17it/s][A[A

 12% 4/32 [00:00<00:05,  4.77it/s][A[A

 16% 5/32 [00:01<00:05,  5.15it/s][A[A

 19% 6/32 [00:01<00:05,  4.87it/s][A[A

 22% 7/32 [00:01<00:05,  4.80it/s][A[A

 25% 8/32 [00:01<00:04,  5.25it/s][A[A

 28% 9/32 [00:01<00:04,  5.59it/s][A[A

 31% 10/32 [00:01<00:04,  5.11it/s][A[A

 34% 11/32 [00:03<00:10,  2.05it/s][A[A

 38% 12/32 [00:03<00:08,  2.49it/s][A[A

 41% 13/32 [00:03<00:06,  2.94it/s][A[A

 44% 14/32 [00:03<00:05,  3.28it/s][A[A

 47% 15/32 [00:04<00:04,  3.55it/s][A[A

 50% 16/32 [00:04<00:03,  4.23it/s][A[A

 53% 17/32 [00:04<00:03,  4.32it/s][A[A

 56% 18/32 [00:04<00:02,  4.84it/s][A[A

 59% 19/32 [00:04<00:02,  5.14it/s][A[A

 62% 20/32 [00:04<00:02,  5.70it/s][A[A

 66% 21/32 [00:04<00:01,  6.24it/s][A[A

 69% 22/32 [00:05<00:01,  6.44it/s][A[A

 72% 23/32 [00:05<00:01,  5.60it/s][A[A

 75% 24/32 [00:05<00:01,  5.66it/s][A[A

 78% 25/32 [00:06<00:03,  2.09it/s][A[A

 81% 26/32 [00:06<00:02,  2.58it/s][A[A

 84% 27/32 [00:07<00:01,  2.98it/s][A[A

 88% 28/32 [00:07<00:01,  3.53it/s][A[A

 91% 29/32 [00:07<00:00,  3.78it/s][A[A

 94% 30/32 [00:07<00:00,  3.87it/s][A[A

 97% 31/32 [00:07<00:00,  4.64it/s][A[A

100% 32/32 [00:07<00:00,  4.69it/s][A[A100% 32/32 [00:07<00:00,  4.00it/s]
Meta loss on this task batch = 5.0759e-01, PNorm = 40.0683, GNorm = 0.0563

 16% 3/19 [00:25<02:18,  8.63s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.22it/s][A[A

  6% 2/32 [00:00<00:06,  4.38it/s][A[A

  9% 3/32 [00:00<00:06,  4.67it/s][A[A

 12% 4/32 [00:00<00:06,  4.55it/s][A[A

 16% 5/32 [00:01<00:05,  4.65it/s][A[A

 19% 6/32 [00:01<00:05,  4.52it/s][A[A

 22% 7/32 [00:01<00:04,  5.19it/s][A[A

 25% 8/32 [00:01<00:04,  5.22it/s][A[A

 28% 9/32 [00:02<00:10,  2.11it/s][A[A

 31% 10/32 [00:02<00:08,  2.65it/s][A[A

 34% 11/32 [00:03<00:06,  3.26it/s][A[A

 38% 12/32 [00:03<00:05,  3.93it/s][A[A

 41% 13/32 [00:03<00:04,  4.11it/s][A[A

 44% 14/32 [00:03<00:04,  4.32it/s][A[A

 47% 15/32 [00:03<00:03,  4.94it/s][A[A

 50% 16/32 [00:03<00:03,  4.89it/s][A[A

 53% 17/32 [00:04<00:02,  5.30it/s][A[A

 56% 18/32 [00:04<00:02,  5.40it/s][A[A

 59% 19/32 [00:04<00:02,  5.83it/s][A[A

 62% 20/32 [00:04<00:02,  5.37it/s][A[A

 66% 21/32 [00:04<00:02,  5.11it/s][A[A

 69% 22/32 [00:05<00:02,  4.83it/s][A[A

 72% 23/32 [00:05<00:01,  4.61it/s][A[A

 75% 24/32 [00:06<00:04,  1.94it/s][A[A

 78% 25/32 [00:06<00:02,  2.51it/s][A[A

 81% 26/32 [00:06<00:01,  3.17it/s][A[A

 84% 27/32 [00:06<00:01,  3.52it/s][A[A

 88% 28/32 [00:07<00:01,  3.88it/s][A[A

 91% 29/32 [00:07<00:00,  4.54it/s][A[A

 94% 30/32 [00:07<00:00,  4.81it/s][A[A

 97% 31/32 [00:07<00:00,  5.37it/s][A[A

100% 32/32 [00:07<00:00,  5.20it/s][A[A100% 32/32 [00:07<00:00,  4.08it/s]
Meta loss on this task batch = 5.6736e-01, PNorm = 40.0933, GNorm = 0.0550

 21% 4/19 [00:34<02:09,  8.61s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.42it/s][A[A

  6% 2/32 [00:00<00:05,  5.08it/s][A[A

  9% 3/32 [00:00<00:05,  5.42it/s][A[A

 12% 4/32 [00:00<00:05,  4.80it/s][A[A

 16% 5/32 [00:00<00:05,  4.77it/s][A[A

 19% 6/32 [00:01<00:05,  4.82it/s][A[A

 22% 7/32 [00:02<00:12,  1.94it/s][A[A

 25% 8/32 [00:02<00:09,  2.48it/s][A[A

 28% 9/32 [00:02<00:07,  2.91it/s][A[A

 31% 10/32 [00:02<00:06,  3.48it/s][A[A

 34% 11/32 [00:03<00:05,  3.84it/s][A[A

 38% 12/32 [00:03<00:04,  4.09it/s][A[A

 41% 13/32 [00:03<00:04,  4.44it/s][A[A

 44% 14/32 [00:03<00:03,  4.99it/s][A[A

 47% 15/32 [00:03<00:03,  4.86it/s][A[A

 50% 16/32 [00:04<00:03,  5.23it/s][A[A

 53% 17/32 [00:04<00:03,  4.72it/s][A[A

 56% 18/32 [00:04<00:02,  4.85it/s][A[A

 59% 19/32 [00:04<00:02,  4.61it/s][A[A

 62% 20/32 [00:05<00:06,  1.91it/s][A[A

 66% 21/32 [00:06<00:04,  2.34it/s][A[A

 69% 22/32 [00:06<00:03,  2.82it/s][A[A

 72% 23/32 [00:06<00:02,  3.19it/s][A[A

 75% 24/32 [00:06<00:02,  3.64it/s][A[A

 78% 25/32 [00:06<00:01,  3.88it/s][A[A

 81% 26/32 [00:07<00:01,  4.51it/s][A[A

 84% 27/32 [00:07<00:01,  4.81it/s][A[A

 88% 28/32 [00:07<00:00,  5.25it/s][A[A

 91% 29/32 [00:07<00:00,  5.19it/s][A[A

 94% 30/32 [00:07<00:00,  5.52it/s][A[A

 97% 31/32 [00:07<00:00,  5.47it/s][A[A

100% 32/32 [00:08<00:00,  5.06it/s][A[A100% 32/32 [00:08<00:00,  3.90it/s]
Meta loss on this task batch = 5.1913e-01, PNorm = 40.1188, GNorm = 0.0694

 26% 5/19 [00:43<02:01,  8.70s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  6.56it/s][A[A

  6% 2/32 [00:01<00:13,  2.15it/s][A[A

  9% 3/32 [00:01<00:10,  2.72it/s][A[A

 12% 4/32 [00:01<00:08,  3.18it/s][A[A

 16% 5/32 [00:01<00:08,  3.27it/s][A[A

 19% 6/32 [00:02<00:07,  3.49it/s][A[A

 22% 7/32 [00:02<00:06,  3.92it/s][A[A

 25% 8/32 [00:02<00:05,  4.08it/s][A[A

 28% 9/32 [00:02<00:05,  4.00it/s][A[A

 31% 10/32 [00:03<00:05,  4.08it/s][A[A

 34% 11/32 [00:04<00:11,  1.85it/s][A[A

 38% 12/32 [00:04<00:08,  2.25it/s][A[A

 41% 13/32 [00:04<00:06,  2.82it/s][A[A

 44% 14/32 [00:04<00:05,  3.12it/s][A[A

 47% 15/32 [00:05<00:04,  3.44it/s][A[A

 50% 16/32 [00:05<00:04,  3.62it/s][A[A

 53% 17/32 [00:05<00:03,  4.25it/s][A[A

 56% 18/32 [00:05<00:03,  4.27it/s][A[A

 59% 19/32 [00:05<00:02,  4.54it/s][A[A

 62% 20/32 [00:06<00:02,  4.76it/s][A[A

 66% 21/32 [00:06<00:02,  4.85it/s][A[A

 69% 22/32 [00:06<00:02,  4.76it/s][A[A

 72% 23/32 [00:06<00:01,  4.69it/s][A[A

 75% 24/32 [00:07<00:04,  1.96it/s][A[A

 78% 25/32 [00:08<00:02,  2.36it/s][A[A

 81% 26/32 [00:08<00:02,  2.95it/s][A[A

 84% 27/32 [00:08<00:01,  3.22it/s][A[A

 88% 28/32 [00:08<00:01,  3.31it/s][A[A

 91% 29/32 [00:09<00:00,  3.55it/s][A[A

 94% 30/32 [00:09<00:00,  3.88it/s][A[A

 97% 31/32 [00:09<00:00,  4.48it/s][A[A

100% 32/32 [00:09<00:00,  4.63it/s][A[A100% 32/32 [00:09<00:00,  3.31it/s]
Meta loss on this task batch = 4.3474e-01, PNorm = 40.1456, GNorm = 0.1222

 32% 6/19 [00:53<01:59,  9.22s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.14it/s][A[A

  6% 2/32 [00:00<00:05,  5.27it/s][A[A

  9% 3/32 [00:00<00:05,  4.99it/s][A[A

 12% 4/32 [00:01<00:14,  1.96it/s][A[A

 16% 5/32 [00:01<00:10,  2.50it/s][A[A

 19% 6/32 [00:02<00:08,  2.91it/s][A[A

 22% 7/32 [00:02<00:07,  3.23it/s][A[A

 25% 8/32 [00:02<00:06,  3.54it/s][A[A

 28% 9/32 [00:02<00:05,  3.96it/s][A[A

 31% 10/32 [00:03<00:05,  4.10it/s][A[A

 34% 11/32 [00:03<00:04,  4.25it/s][A[A

 38% 12/32 [00:03<00:04,  4.36it/s][A[A

 41% 13/32 [00:03<00:04,  4.58it/s][A[A

 44% 14/32 [00:03<00:03,  5.08it/s][A[A

 47% 15/32 [00:04<00:03,  4.82it/s][A[A

 50% 16/32 [00:05<00:08,  1.94it/s][A[A

 53% 17/32 [00:05<00:06,  2.32it/s][A[A

 56% 18/32 [00:05<00:05,  2.71it/s][A[A

 59% 19/32 [00:05<00:04,  3.23it/s][A[A

 62% 20/32 [00:06<00:03,  3.50it/s][A[A

 66% 21/32 [00:06<00:02,  3.72it/s][A[A

 69% 22/32 [00:06<00:02,  3.90it/s][A[A

 72% 23/32 [00:06<00:02,  3.98it/s][A[A

 75% 24/32 [00:07<00:01,  4.12it/s][A[A

 78% 25/32 [00:07<00:01,  4.27it/s][A[A

 81% 26/32 [00:07<00:01,  4.30it/s][A[A

 84% 27/32 [00:07<00:01,  4.40it/s][A[A

 88% 28/32 [00:07<00:00,  4.41it/s][A[A

 91% 29/32 [00:09<00:01,  1.99it/s][A[A

 94% 30/32 [00:09<00:00,  2.44it/s][A[A

 97% 31/32 [00:09<00:00,  3.01it/s][A[A

100% 32/32 [00:09<00:00,  3.32it/s][A[A100% 32/32 [00:09<00:00,  3.31it/s]
Meta loss on this task batch = 4.7611e-01, PNorm = 40.1726, GNorm = 0.1034

 37% 7/19 [01:04<01:55,  9.59s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  3.92it/s][A[A

  6% 2/32 [00:00<00:07,  3.89it/s][A[A

  9% 3/32 [00:00<00:07,  3.95it/s][A[A

 12% 4/32 [00:01<00:06,  4.00it/s][A[A

 16% 5/32 [00:01<00:06,  4.12it/s][A[A

 19% 6/32 [00:01<00:06,  4.08it/s][A[A

 22% 7/32 [00:01<00:05,  4.52it/s][A[A

 25% 8/32 [00:01<00:05,  4.30it/s][A[A

 28% 9/32 [00:02<00:05,  4.28it/s][A[A

 31% 10/32 [00:02<00:05,  4.22it/s][A[A

 34% 11/32 [00:03<00:11,  1.90it/s][A[A

 38% 12/32 [00:03<00:08,  2.32it/s][A[A

 41% 13/32 [00:03<00:06,  2.75it/s][A[A

 44% 14/32 [00:04<00:05,  3.27it/s][A[A

 47% 15/32 [00:04<00:04,  3.58it/s][A[A

 50% 16/32 [00:04<00:04,  3.78it/s][A[A

 53% 17/32 [00:04<00:03,  3.75it/s][A[A

 56% 18/32 [00:05<00:03,  3.95it/s][A[A

 59% 19/32 [00:05<00:03,  3.87it/s][A[A

 62% 20/32 [00:05<00:02,  4.01it/s][A[A

 66% 21/32 [00:05<00:02,  4.18it/s][A[A

 69% 22/32 [00:06<00:02,  4.21it/s][A[A

 72% 23/32 [00:06<00:01,  4.63it/s][A[A

 75% 24/32 [00:06<00:01,  4.41it/s][A[A

 78% 25/32 [00:07<00:03,  1.86it/s][A[A

 81% 26/32 [00:07<00:02,  2.21it/s][A[A

 84% 27/32 [00:08<00:01,  2.61it/s][A[A

 88% 28/32 [00:08<00:01,  2.95it/s][A[A

 91% 29/32 [00:08<00:00,  3.28it/s][A[A

 94% 30/32 [00:08<00:00,  3.50it/s][A[A

 97% 31/32 [00:09<00:00,  3.71it/s][A[A

100% 32/32 [00:09<00:00,  3.88it/s][A[A100% 32/32 [00:09<00:00,  3.41it/s]
Meta loss on this task batch = 3.6699e-01, PNorm = 40.2017, GNorm = 0.0775

 42% 8/19 [01:14<01:47,  9.77s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.26it/s][A[A

  6% 2/32 [00:00<00:06,  4.35it/s][A[A

  9% 3/32 [00:00<00:06,  4.37it/s][A[A

 12% 4/32 [00:01<00:14,  1.89it/s][A[A

 16% 5/32 [00:02<00:11,  2.26it/s][A[A

 19% 6/32 [00:02<00:09,  2.63it/s][A[A

 22% 7/32 [00:02<00:08,  2.94it/s][A[A

 25% 8/32 [00:02<00:07,  3.24it/s][A[A

 28% 9/32 [00:03<00:06,  3.46it/s][A[A

 31% 10/32 [00:03<00:05,  3.69it/s][A[A

 34% 11/32 [00:03<00:05,  3.88it/s][A[A

 38% 12/32 [00:03<00:05,  3.98it/s][A[A

 41% 13/32 [00:04<00:04,  4.05it/s][A[A

 44% 14/32 [00:04<00:04,  4.01it/s][A[A

 47% 15/32 [00:04<00:04,  4.15it/s][A[A

 50% 16/32 [00:04<00:03,  4.18it/s][A[A

 53% 17/32 [00:05<00:08,  1.87it/s][A[A

 56% 18/32 [00:06<00:06,  2.24it/s][A[A

 59% 19/32 [00:06<00:05,  2.59it/s][A[A

 62% 20/32 [00:06<00:04,  2.96it/s][A[A

 66% 21/32 [00:06<00:03,  3.32it/s][A[A

 69% 22/32 [00:07<00:02,  3.56it/s][A[A

 72% 23/32 [00:07<00:02,  3.71it/s][A[A

 75% 24/32 [00:07<00:02,  3.93it/s][A[A

 78% 25/32 [00:07<00:01,  4.15it/s][A[A

 81% 26/32 [00:08<00:01,  4.24it/s][A[A

 84% 27/32 [00:09<00:02,  1.88it/s][A[A

 88% 28/32 [00:09<00:01,  2.26it/s][A[A

 91% 29/32 [00:09<00:01,  2.69it/s][A[A

 94% 30/32 [00:09<00:00,  3.04it/s][A[A

 97% 31/32 [00:10<00:00,  3.32it/s][A[A

100% 32/32 [00:10<00:00,  3.61it/s][A[A100% 32/32 [00:10<00:00,  3.08it/s]
Meta loss on this task batch = 2.2250e-01, PNorm = 40.2314, GNorm = 0.0971

 47% 9/19 [01:25<01:41, 10.19s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.10it/s][A[A

  6% 2/32 [00:00<00:07,  4.20it/s][A[A

  9% 3/32 [00:00<00:06,  4.27it/s][A[A

 12% 4/32 [00:00<00:06,  4.33it/s][A[A

 16% 5/32 [00:01<00:06,  4.37it/s][A[A

 19% 6/32 [00:02<00:13,  1.89it/s][A[A

 22% 7/32 [00:02<00:11,  2.26it/s][A[A

 25% 8/32 [00:02<00:09,  2.64it/s][A[A

 28% 9/32 [00:03<00:07,  2.99it/s][A[A

 31% 10/32 [00:03<00:06,  3.31it/s][A[A

 34% 11/32 [00:03<00:05,  3.51it/s][A[A

 38% 12/32 [00:03<00:05,  3.68it/s][A[A

 41% 13/32 [00:04<00:04,  3.85it/s][A[A

 44% 14/32 [00:04<00:04,  3.86it/s][A[A

 47% 15/32 [00:05<00:09,  1.79it/s][A[A

 50% 16/32 [00:05<00:07,  2.20it/s][A[A

 53% 17/32 [00:05<00:05,  2.59it/s][A[A

 56% 18/32 [00:06<00:04,  2.96it/s][A[A

 59% 19/32 [00:06<00:03,  3.32it/s][A[A

 62% 20/32 [00:06<00:03,  3.54it/s][A[A

 66% 21/32 [00:06<00:02,  3.77it/s][A[A

 69% 22/32 [00:07<00:02,  3.86it/s][A[A

 72% 23/32 [00:07<00:02,  3.87it/s][A[A

 75% 24/32 [00:08<00:04,  1.80it/s][A[A

 78% 25/32 [00:08<00:03,  2.16it/s][A[A

 81% 26/32 [00:09<00:02,  2.51it/s][A[A

 84% 27/32 [00:09<00:01,  2.88it/s][A[A

 88% 28/32 [00:09<00:01,  3.21it/s][A[A

 91% 29/32 [00:09<00:00,  3.48it/s][A[A

 94% 30/32 [00:10<00:00,  3.70it/s][A[A

 97% 31/32 [00:10<00:00,  4.12it/s][A[A

100% 32/32 [00:10<00:00,  4.20it/s][A[A100% 32/32 [00:10<00:00,  3.06it/s]
Meta loss on this task batch = 2.8811e-01, PNorm = 40.2590, GNorm = 0.0825

 53% 10/19 [01:36<01:34, 10.52s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.32it/s][A[A

  6% 2/32 [00:01<00:15,  1.88it/s][A[A

  9% 3/32 [00:01<00:12,  2.34it/s][A[A

 12% 4/32 [00:01<00:10,  2.71it/s][A[A

 16% 5/32 [00:02<00:08,  3.05it/s][A[A

 19% 6/32 [00:02<00:07,  3.34it/s][A[A

 22% 7/32 [00:02<00:06,  3.58it/s][A[A

 25% 8/32 [00:02<00:06,  3.89it/s][A[A

 28% 9/32 [00:02<00:05,  4.30it/s][A[A

 31% 10/32 [00:03<00:05,  4.29it/s][A[A

 34% 11/32 [00:03<00:04,  4.30it/s][A[A

 38% 12/32 [00:03<00:04,  4.28it/s][A[A

 41% 13/32 [00:03<00:04,  4.27it/s][A[A

 44% 14/32 [00:04<00:04,  4.31it/s][A[A

 47% 15/32 [00:04<00:03,  4.30it/s][A[A

 50% 16/32 [00:04<00:03,  4.30it/s][A[A

 53% 17/32 [00:05<00:07,  1.93it/s][A[A

 56% 18/32 [00:06<00:06,  2.31it/s][A[A

 59% 19/32 [00:06<00:04,  2.81it/s][A[A

 62% 20/32 [00:06<00:03,  3.36it/s][A[A

 66% 21/32 [00:06<00:03,  3.58it/s][A[A

 69% 22/32 [00:06<00:02,  3.79it/s][A[A

 72% 23/32 [00:07<00:02,  3.99it/s][A[A

 75% 24/32 [00:07<00:01,  4.07it/s][A[A

 78% 25/32 [00:07<00:01,  4.13it/s][A[A

 81% 26/32 [00:07<00:01,  4.15it/s][A[A

 84% 27/32 [00:07<00:01,  4.20it/s][A[A

 88% 28/32 [00:08<00:00,  4.19it/s][A[A

 91% 29/32 [00:09<00:01,  1.91it/s][A[A

 94% 30/32 [00:09<00:00,  2.28it/s][A[A

 97% 31/32 [00:09<00:00,  2.77it/s][A[A

100% 32/32 [00:10<00:00,  3.07it/s][A[A100% 32/32 [00:10<00:00,  3.18it/s]
Meta loss on this task batch = 5.6579e-01, PNorm = 40.2832, GNorm = 0.0982

 58% 11/19 [01:47<01:24, 10.62s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.07it/s][A[A

  6% 2/32 [00:00<00:07,  4.10it/s][A[A

  9% 3/32 [00:00<00:07,  4.13it/s][A[A

 12% 4/32 [00:00<00:06,  4.20it/s][A[A

 16% 5/32 [00:01<00:06,  4.19it/s][A[A

 19% 6/32 [00:02<00:13,  1.86it/s][A[A

 22% 7/32 [00:02<00:11,  2.23it/s][A[A

 25% 8/32 [00:02<00:09,  2.60it/s][A[A

 28% 9/32 [00:03<00:07,  3.12it/s][A[A

 31% 10/32 [00:03<00:06,  3.41it/s][A[A

 34% 11/32 [00:03<00:05,  3.59it/s][A[A

 38% 12/32 [00:03<00:05,  3.73it/s][A[A

 41% 13/32 [00:04<00:10,  1.82it/s][A[A

 44% 14/32 [00:05<00:08,  2.20it/s][A[A

 47% 15/32 [00:05<00:06,  2.56it/s][A[A

 50% 16/32 [00:05<00:05,  2.92it/s][A[A

 53% 17/32 [00:05<00:04,  3.45it/s][A[A

 56% 18/32 [00:06<00:03,  3.62it/s][A[A

 59% 19/32 [00:06<00:03,  3.76it/s][A[A

 62% 20/32 [00:06<00:02,  4.18it/s][A[A

 66% 21/32 [00:06<00:02,  4.13it/s][A[A

 69% 22/32 [00:08<00:05,  1.86it/s][A[A

 72% 23/32 [00:08<00:03,  2.32it/s][A[A

 75% 24/32 [00:08<00:02,  2.71it/s][A[A

 78% 25/32 [00:08<00:02,  3.04it/s][A[A

 81% 26/32 [00:08<00:01,  3.36it/s][A[A

 84% 27/32 [00:09<00:01,  3.59it/s][A[A

 88% 28/32 [00:09<00:01,  3.79it/s][A[A

 91% 29/32 [00:09<00:00,  3.99it/s][A[A

 94% 30/32 [00:09<00:00,  4.07it/s][A[A

 97% 31/32 [00:10<00:00,  4.13it/s][A[A

100% 32/32 [00:10<00:00,  4.18it/s][A[A100% 32/32 [00:10<00:00,  3.12it/s]
Meta loss on this task batch = 5.4507e-01, PNorm = 40.3068, GNorm = 0.1294

 63% 12/19 [01:58<01:15, 10.75s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.06it/s][A[A

  6% 2/32 [00:01<00:16,  1.86it/s][A[A

  9% 3/32 [00:01<00:13,  2.23it/s][A[A

 12% 4/32 [00:01<00:10,  2.75it/s][A[A

 16% 5/32 [00:02<00:08,  3.24it/s][A[A

 19% 6/32 [00:02<00:07,  3.59it/s][A[A

 22% 7/32 [00:02<00:06,  3.83it/s][A[A

 25% 8/32 [00:02<00:06,  3.96it/s][A[A

 28% 9/32 [00:02<00:05,  4.22it/s][A[A

 31% 10/32 [00:03<00:05,  4.22it/s][A[A

 34% 11/32 [00:03<00:04,  4.24it/s][A[A

 38% 12/32 [00:03<00:04,  4.25it/s][A[A

 41% 13/32 [00:03<00:04,  4.24it/s][A[A

 44% 14/32 [00:04<00:04,  4.25it/s][A[A

 47% 15/32 [00:04<00:04,  4.24it/s][A[A

 50% 16/32 [00:04<00:03,  4.27it/s][A[A

 53% 17/32 [00:04<00:03,  4.24it/s][A[A

 56% 18/32 [00:06<00:07,  1.87it/s][A[A

 59% 19/32 [00:06<00:05,  2.24it/s][A[A

 62% 20/32 [00:06<00:04,  2.61it/s][A[A

 66% 21/32 [00:06<00:03,  2.93it/s][A[A

 69% 22/32 [00:06<00:03,  3.23it/s][A[A

 72% 23/32 [00:07<00:02,  3.46it/s][A[A

 75% 24/32 [00:07<00:02,  3.62it/s][A[A

 78% 25/32 [00:07<00:01,  3.92it/s][A[A

 81% 26/32 [00:07<00:01,  3.97it/s][A[A

 84% 27/32 [00:09<00:02,  1.86it/s][A[A

 88% 28/32 [00:09<00:01,  2.25it/s][A[A

 91% 29/32 [00:09<00:01,  2.62it/s][A[A

 94% 30/32 [00:09<00:00,  2.97it/s][A[A

 97% 31/32 [00:10<00:00,  3.31it/s][A[A

100% 32/32 [00:10<00:00,  3.54it/s][A[A100% 32/32 [00:10<00:00,  3.11it/s]
Meta loss on this task batch = 5.8223e-01, PNorm = 40.3288, GNorm = 0.0611

 68% 13/19 [02:09<01:05, 10.85s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.07it/s][A[A

  6% 2/32 [00:00<00:07,  4.11it/s][A[A

  9% 3/32 [00:00<00:06,  4.18it/s][A[A

 12% 4/32 [00:00<00:06,  4.24it/s][A[A

 16% 5/32 [00:02<00:14,  1.84it/s][A[A

 19% 6/32 [00:02<00:11,  2.22it/s][A[A

 22% 7/32 [00:02<00:09,  2.59it/s][A[A

 25% 8/32 [00:02<00:08,  2.99it/s][A[A

 28% 9/32 [00:03<00:07,  3.28it/s][A[A

 31% 10/32 [00:03<00:06,  3.50it/s][A[A

 34% 11/32 [00:03<00:05,  3.69it/s][A[A

 38% 12/32 [00:03<00:05,  3.76it/s][A[A

 41% 13/32 [00:04<00:04,  3.85it/s][A[A

 44% 14/32 [00:04<00:04,  3.89it/s][A[A

 47% 15/32 [00:05<00:09,  1.79it/s][A[A

 50% 16/32 [00:05<00:07,  2.15it/s][A[A

 53% 17/32 [00:06<00:05,  2.60it/s][A[A

 56% 18/32 [00:06<00:04,  2.95it/s][A[A

 59% 19/32 [00:06<00:04,  3.24it/s][A[A

 62% 20/32 [00:06<00:03,  3.47it/s][A[A

 66% 21/32 [00:07<00:03,  3.64it/s][A[A

 69% 22/32 [00:07<00:02,  3.77it/s][A[A

 72% 23/32 [00:08<00:05,  1.79it/s][A[A

 75% 24/32 [00:08<00:03,  2.17it/s][A[A

 78% 25/32 [00:08<00:02,  2.56it/s][A[A

 81% 26/32 [00:09<00:02,  2.87it/s][A[A

 84% 27/32 [00:09<00:01,  3.14it/s][A[A

 88% 28/32 [00:09<00:01,  3.39it/s][A[A

 91% 29/32 [00:09<00:00,  3.62it/s][A[A

 94% 30/32 [00:10<00:00,  3.88it/s][A[A

 97% 31/32 [00:10<00:00,  3.97it/s][A[A

100% 32/32 [00:10<00:00,  3.92it/s][A[A100% 32/32 [00:10<00:00,  3.01it/s]
Meta loss on this task batch = 5.5924e-01, PNorm = 40.3501, GNorm = 0.0855

 74% 14/19 [02:21<00:55, 11.04s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:37,  1.20s/it][A[A

  6% 2/32 [00:01<00:27,  1.08it/s][A[A

  9% 3/32 [00:01<00:20,  1.39it/s][A[A

 12% 4/32 [00:01<00:16,  1.74it/s][A[A

 16% 5/32 [00:02<00:12,  2.14it/s][A[A

 19% 6/32 [00:02<00:10,  2.43it/s][A[A

 22% 7/32 [00:02<00:09,  2.74it/s][A[A

 25% 8/32 [00:02<00:07,  3.24it/s][A[A

 28% 9/32 [00:03<00:06,  3.43it/s][A[A

 31% 10/32 [00:04<00:12,  1.78it/s][A[A

 34% 11/32 [00:04<00:09,  2.17it/s][A[A

 38% 12/32 [00:04<00:07,  2.67it/s][A[A

 41% 13/32 [00:04<00:06,  3.04it/s][A[A

 44% 14/32 [00:05<00:05,  3.30it/s][A[A

 47% 15/32 [00:05<00:04,  3.78it/s][A[A

 50% 16/32 [00:05<00:04,  3.96it/s][A[A

 53% 17/32 [00:05<00:03,  4.02it/s][A[A

 56% 18/32 [00:06<00:03,  4.13it/s][A[A

 59% 19/32 [00:06<00:02,  4.64it/s][A[A

 62% 20/32 [00:06<00:02,  4.50it/s][A[A

 66% 21/32 [00:06<00:02,  4.83it/s][A[A

 69% 22/32 [00:07<00:05,  1.95it/s][A[A

 72% 23/32 [00:08<00:03,  2.27it/s][A[A

 75% 24/32 [00:08<00:02,  2.67it/s][A[A

 78% 25/32 [00:08<00:02,  3.18it/s][A[A

 81% 26/32 [00:08<00:01,  3.42it/s][A[A

 84% 27/32 [00:08<00:01,  3.65it/s][A[A

 88% 28/32 [00:09<00:01,  3.78it/s][A[A

 91% 29/32 [00:09<00:00,  3.85it/s][A[A

 94% 30/32 [00:09<00:00,  3.94it/s][A[A

 97% 31/32 [00:09<00:00,  4.02it/s][A[A

100% 32/32 [00:10<00:00,  3.99it/s][A[A100% 32/32 [00:10<00:00,  3.13it/s]
Meta loss on this task batch = 4.5840e-01, PNorm = 40.3738, GNorm = 0.0791

 79% 15/19 [02:32<00:44, 11.03s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:38,  1.25s/it][A[A

  6% 2/32 [00:01<00:28,  1.06it/s][A[A

  9% 3/32 [00:01<00:21,  1.36it/s][A[A

 12% 4/32 [00:01<00:16,  1.70it/s][A[A

 16% 5/32 [00:02<00:12,  2.12it/s][A[A

 19% 6/32 [00:02<00:10,  2.48it/s][A[A

 22% 7/32 [00:02<00:08,  2.84it/s][A[A

 25% 8/32 [00:02<00:07,  3.27it/s][A[A

 28% 9/32 [00:03<00:06,  3.51it/s][A[A

 31% 10/32 [00:03<00:06,  3.66it/s][A[A

 34% 11/32 [00:04<00:11,  1.78it/s][A[A

 38% 12/32 [00:04<00:09,  2.14it/s][A[A

 41% 13/32 [00:05<00:07,  2.50it/s][A[A

 44% 14/32 [00:05<00:06,  2.83it/s][A[A

 47% 15/32 [00:05<00:05,  3.15it/s][A[A

 50% 16/32 [00:05<00:04,  3.71it/s][A[A

 53% 17/32 [00:05<00:03,  3.86it/s][A[A

 56% 18/32 [00:06<00:03,  4.20it/s][A[A

 59% 19/32 [00:06<00:03,  4.22it/s][A[A

 62% 20/32 [00:06<00:02,  4.26it/s][A[A

 66% 21/32 [00:06<00:02,  4.28it/s][A[A

 69% 22/32 [00:08<00:05,  1.91it/s][A[A

 72% 23/32 [00:08<00:03,  2.27it/s][A[A

 75% 24/32 [00:08<00:03,  2.56it/s][A[A

 78% 25/32 [00:08<00:02,  2.98it/s][A[A

 81% 26/32 [00:08<00:01,  3.25it/s][A[A

 84% 27/32 [00:09<00:01,  3.35it/s][A[A

 88% 28/32 [00:09<00:01,  3.58it/s][A[A

 91% 29/32 [00:10<00:01,  1.71it/s][A[A

 94% 30/32 [00:11<00:00,  2.07it/s][A[A

 97% 31/32 [00:11<00:00,  2.43it/s][A[A

100% 32/32 [00:11<00:00,  2.78it/s][A[A100% 32/32 [00:11<00:00,  2.78it/s]
Meta loss on this task batch = 5.3157e-01, PNorm = 40.3957, GNorm = 0.0624

 84% 16/19 [02:44<00:34, 11.43s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.84it/s][A[A

  6% 2/32 [00:00<00:07,  3.77it/s][A[A

  9% 3/32 [00:00<00:07,  3.98it/s][A[A

 12% 4/32 [00:00<00:06,  4.32it/s][A[A

 16% 5/32 [00:01<00:06,  4.40it/s][A[A

 19% 6/32 [00:01<00:05,  4.92it/s][A[A

 22% 7/32 [00:01<00:05,  4.91it/s][A[A

 25% 8/32 [00:02<00:12,  1.90it/s][A[A

 28% 9/32 [00:03<00:10,  2.27it/s][A[A

 31% 10/32 [00:03<00:07,  2.84it/s][A[A

 34% 11/32 [00:03<00:06,  3.19it/s][A[A

 38% 12/32 [00:03<00:05,  3.58it/s][A[A

 41% 13/32 [00:03<00:04,  3.94it/s][A[A

 44% 14/32 [00:04<00:04,  4.15it/s][A[A

 47% 15/32 [00:04<00:04,  4.16it/s][A[A

 50% 16/32 [00:04<00:03,  4.09it/s][A[A

 53% 17/32 [00:04<00:03,  4.42it/s][A[A

 56% 18/32 [00:04<00:03,  4.29it/s][A[A

 59% 19/32 [00:05<00:03,  4.06it/s][A[A

 62% 20/32 [00:05<00:02,  4.14it/s][A[A

 66% 21/32 [00:06<00:05,  1.87it/s][A[A

 69% 22/32 [00:06<00:04,  2.23it/s][A[A

 72% 23/32 [00:07<00:03,  2.57it/s][A[A

 75% 24/32 [00:07<00:02,  2.81it/s][A[A

 78% 25/32 [00:07<00:02,  3.22it/s][A[A

 81% 26/32 [00:07<00:01,  3.44it/s][A[A

 84% 27/32 [00:08<00:01,  3.87it/s][A[A

 88% 28/32 [00:08<00:00,  4.20it/s][A[A

 91% 29/32 [00:08<00:00,  4.44it/s][A[A

 94% 30/32 [00:08<00:00,  4.26it/s][A[A

 97% 31/32 [00:08<00:00,  4.41it/s][A[A

100% 32/32 [00:09<00:00,  4.30it/s][A[A100% 32/32 [00:09<00:00,  3.49it/s]
Meta loss on this task batch = 4.3840e-01, PNorm = 40.4155, GNorm = 0.0748

 89% 17/19 [02:54<00:21, 10.99s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.32it/s][A[A

  6% 2/32 [00:01<00:14,  2.00it/s][A[A

  9% 3/32 [00:01<00:12,  2.41it/s][A[A

 12% 4/32 [00:01<00:10,  2.76it/s][A[A

 16% 5/32 [00:02<00:08,  3.03it/s][A[A

 19% 6/32 [00:02<00:07,  3.31it/s][A[A

 22% 7/32 [00:02<00:07,  3.46it/s][A[A

 25% 8/32 [00:02<00:06,  3.58it/s][A[A

 28% 9/32 [00:03<00:06,  3.78it/s][A[A

 31% 10/32 [00:04<00:12,  1.83it/s][A[A

 34% 11/32 [00:04<00:09,  2.23it/s][A[A

 38% 12/32 [00:04<00:07,  2.72it/s][A[A

 41% 13/32 [00:04<00:06,  2.95it/s][A[A

 44% 14/32 [00:05<00:05,  3.55it/s][A[A

 47% 15/32 [00:05<00:04,  3.60it/s][A[A

 50% 16/32 [00:05<00:04,  3.81it/s][A[A

 53% 17/32 [00:05<00:03,  4.46it/s][A[A

 56% 18/32 [00:05<00:03,  4.50it/s][A[A

 59% 19/32 [00:06<00:02,  4.38it/s][A[A

 62% 20/32 [00:06<00:02,  4.35it/s][A[A

 66% 21/32 [00:06<00:02,  4.33it/s][A[A

 69% 22/32 [00:06<00:02,  4.93it/s][A[A

 72% 23/32 [00:08<00:04,  1.92it/s][A[A

 75% 24/32 [00:08<00:03,  2.22it/s][A[A

 78% 25/32 [00:08<00:02,  2.57it/s][A[A

 81% 26/32 [00:08<00:02,  2.95it/s][A[A

 84% 27/32 [00:09<00:01,  3.14it/s][A[A

 88% 28/32 [00:09<00:01,  3.41it/s][A[A

 91% 29/32 [00:09<00:00,  3.94it/s][A[A

 94% 30/32 [00:09<00:00,  3.94it/s][A[A

 97% 31/32 [00:09<00:00,  4.17it/s][A[A

100% 32/32 [00:10<00:00,  4.04it/s][A[A100% 32/32 [00:10<00:00,  3.13it/s]
Meta loss on this task batch = 4.9516e-01, PNorm = 40.4347, GNorm = 0.0647

 95% 18/19 [03:05<00:10, 11.00s/it][A

  0% 0/23 [00:00<?, ?it/s][A[A

  4% 1/23 [00:01<00:27,  1.26s/it][A[A

  9% 2/23 [00:01<00:19,  1.07it/s][A[A

 13% 3/23 [00:01<00:14,  1.39it/s][A[A

 17% 4/23 [00:01<00:10,  1.77it/s][A[A

 22% 5/23 [00:01<00:07,  2.29it/s][A[A

 26% 6/23 [00:02<00:06,  2.71it/s][A[A

 30% 7/23 [00:02<00:05,  3.00it/s][A[A

 35% 8/23 [00:02<00:04,  3.30it/s][A[A

 39% 9/23 [00:02<00:03,  3.99it/s][A[A

 43% 10/23 [00:03<00:03,  3.94it/s][A[A

 48% 11/23 [00:03<00:02,  4.16it/s][A[A

 52% 12/23 [00:03<00:02,  4.13it/s][A[A

 57% 13/23 [00:03<00:02,  4.68it/s][A[A

 61% 14/23 [00:03<00:01,  4.87it/s][A[A

 65% 15/23 [00:05<00:04,  1.78it/s][A[A

 70% 16/23 [00:05<00:03,  2.12it/s][A[A

 74% 17/23 [00:05<00:02,  2.64it/s][A[A

 78% 18/23 [00:05<00:01,  2.94it/s][A[A

 83% 19/23 [00:06<00:01,  3.21it/s][A[A

 87% 20/23 [00:06<00:00,  3.87it/s][A[A

 91% 21/23 [00:06<00:00,  4.29it/s][A[A

 96% 22/23 [00:06<00:00,  4.91it/s][A[A

100% 23/23 [00:06<00:00,  5.27it/s][A[A100% 23/23 [00:06<00:00,  3.40it/s]
Meta loss on this task batch = 4.3569e-01, PNorm = 40.4501, GNorm = 0.0862

100% 19/19 [03:12<00:00,  9.90s/it][A100% 19/19 [03:12<00:00, 10.15s/it]
Took 192.85647678375244 seconds to complete one epoch of meta training

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/20 [00:00<?, ?it/s][A[A


  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 32.68it/s]


  5% 1/20 [00:00<00:02,  8.03it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A


100% 2/2 [00:00<00:00, 19.87it/s][A[A[A100% 2/2 [00:00<00:00, 19.82it/s]


 10% 2/20 [00:00<00:02,  6.17it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 18.25it/s][A[A[A100% 3/3 [00:00<00:00, 20.19it/s]


 15% 3/20 [00:00<00:03,  5.02it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 14.93it/s]


 20% 4/20 [00:00<00:03,  4.77it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 15.19it/s][A[A[A100% 4/4 [00:00<00:00, 17.72it/s]


 25% 5/20 [00:01<00:03,  3.83it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 12.93it/s][A[A[A100% 4/4 [00:00<00:00, 15.71it/s]


 30% 6/20 [00:02<00:08,  1.65it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.30it/s][A[A[A100% 4/4 [00:00<00:00, 21.04it/s]


 35% 7/20 [00:03<00:06,  1.90it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.32it/s][A[A[A


100% 4/4 [00:00<00:00, 19.06it/s][A[A[A100% 4/4 [00:00<00:00, 18.87it/s]


 40% 8/20 [00:03<00:05,  2.09it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.76it/s][A[A[A100% 4/4 [00:00<00:00, 23.44it/s]


 45% 9/20 [00:03<00:04,  2.28it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.89it/s][A[A[A


100% 4/4 [00:00<00:00, 19.69it/s][A[A[A100% 4/4 [00:00<00:00, 19.54it/s]


 50% 10/20 [00:04<00:04,  2.42it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 16.00it/s][A[A[A100% 4/4 [00:00<00:00, 18.31it/s]


 55% 11/20 [00:04<00:03,  2.46it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.17it/s][A[A[A100% 4/4 [00:00<00:00, 22.57it/s]


 60% 12/20 [00:04<00:03,  2.58it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 33% 1/3 [00:01<00:02,  1.07s/it][A[A[A


100% 3/3 [00:01<00:00,  1.30it/s][A[A[A100% 3/3 [00:01<00:00,  2.47it/s]


 65% 13/20 [00:06<00:04,  1.46it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 13.88it/s][A[A[A100% 3/3 [00:00<00:00, 13.51it/s]


 70% 14/20 [00:06<00:03,  1.66it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.67it/s][A[A[A100% 4/4 [00:00<00:00, 17.76it/s]


 75% 15/20 [00:07<00:02,  1.85it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.18it/s][A[A[A100% 3/3 [00:00<00:00, 17.55it/s]


 80% 16/20 [00:07<00:01,  2.08it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 12.62it/s][A[A[A100% 3/3 [00:00<00:00, 15.44it/s]


 85% 17/20 [00:07<00:01,  2.22it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A


 50% 1/2 [00:01<00:01,  1.09s/it][A[A[A100% 2/2 [00:01<00:00,  1.82it/s]


 90% 18/20 [00:08<00:01,  1.45it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 16.01it/s][A[A[A100% 3/3 [00:00<00:00, 20.26it/s]


 95% 19/20 [00:09<00:00,  1.74it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 13.30it/s]


100% 20/20 [00:09<00:00,  2.09it/s][A[A100% 20/20 [00:09<00:00,  2.10it/s]

100% 1/1 [00:09<00:00,  9.54s/it][A100% 1/1 [00:09<00:00,  9.54s/it]
Took 202.4034538269043 seconds to complete one epoch of meta training and validating
Meta validation score auc = 0.652419
Found better MAML checkpoint after meta validation, saving now
 77% 23/30 [1:14:20<22:53, 196.21s/it]Epoch 23

  0% 0/19 [00:00<?, ?it/s][A

  0% 0/32 [00:00<?, ?it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  3% 1/32 [00:00<00:05,  5.84it/s][A[A

  6% 2/32 [00:00<00:05,  5.42it/s][A[A

  9% 3/32 [00:00<00:05,  5.55it/s][A[A

 12% 4/32 [00:00<00:04,  5.88it/s][A[A

 16% 5/32 [00:00<00:04,  6.27it/s][A[A

 19% 6/32 [00:01<00:04,  6.06it/s][A[A

 22% 7/32 [00:01<00:04,  5.97it/s][A[A

 25% 8/32 [00:01<00:04,  5.58it/s][A[A

 28% 9/32 [00:01<00:03,  6.13it/s][A[A

 31% 10/32 [00:01<00:03,  6.48it/s][A[A

 34% 11/32 [00:01<00:03,  5.89it/s][A[A

 38% 12/32 [00:03<00:09,  2.00it/s][A[A

 41% 13/32 [00:03<00:07,  2.42it/s][A[A

 44% 14/32 [00:03<00:06,  2.86it/s][A[A

 47% 15/32 [00:03<00:04,  3.50it/s][A[A

 50% 16/32 [00:03<00:04,  3.77it/s][A[A

 53% 17/32 [00:04<00:03,  4.25it/s][A[A

 56% 18/32 [00:04<00:03,  4.66it/s][A[A

 59% 19/32 [00:04<00:02,  5.08it/s][A[A

 62% 20/32 [00:04<00:02,  5.09it/s][A[A

 66% 21/32 [00:04<00:01,  5.81it/s][A[A

 69% 22/32 [00:04<00:01,  5.71it/s][A[A

 72% 23/32 [00:05<00:01,  5.98it/s][A[A

 75% 24/32 [00:05<00:01,  6.13it/s][A[A

 78% 25/32 [00:05<00:01,  5.84it/s][A[A

 81% 26/32 [00:05<00:01,  5.48it/s][A[A

 84% 27/32 [00:05<00:00,  5.73it/s][A[A

 88% 28/32 [00:06<00:01,  2.10it/s][A[A

 91% 29/32 [00:07<00:01,  2.58it/s][A[A

 94% 30/32 [00:07<00:00,  3.29it/s][A[A

 97% 31/32 [00:07<00:00,  3.78it/s][A[A

100% 32/32 [00:07<00:00,  4.22it/s][A[A100% 32/32 [00:07<00:00,  4.24it/s]
Meta loss on this task batch = 4.9390e-01, PNorm = 40.4680, GNorm = 0.1642

  5% 1/19 [00:08<02:28,  8.23s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  7.33it/s][A[A

  6% 2/32 [00:00<00:04,  7.14it/s][A[A

  9% 3/32 [00:00<00:04,  6.59it/s][A[A

 12% 4/32 [00:00<00:04,  6.27it/s][A[A

 16% 5/32 [00:00<00:04,  5.96it/s][A[A

 19% 6/32 [00:00<00:04,  6.18it/s][A[A

 22% 7/32 [00:01<00:03,  6.52it/s][A[A

 25% 8/32 [00:01<00:03,  6.49it/s][A[A

 28% 9/32 [00:01<00:03,  6.33it/s][A[A

 31% 10/32 [00:01<00:03,  6.57it/s][A[A

 34% 11/32 [00:01<00:03,  6.77it/s][A[A

 38% 12/32 [00:01<00:03,  6.34it/s][A[A

 41% 13/32 [00:02<00:02,  6.53it/s][A[A

 44% 14/32 [00:02<00:02,  6.12it/s][A[A

 47% 15/32 [00:02<00:02,  6.07it/s][A[A

 50% 16/32 [00:02<00:02,  5.83it/s][A[A

 53% 17/32 [00:02<00:02,  5.96it/s][A[A

 56% 18/32 [00:02<00:02,  5.54it/s][A[A

 59% 19/32 [00:04<00:06,  2.14it/s][A[A

 62% 20/32 [00:04<00:04,  2.63it/s][A[A

 66% 21/32 [00:04<00:03,  3.22it/s][A[A

 69% 22/32 [00:04<00:02,  3.80it/s][A[A

 72% 23/32 [00:04<00:02,  4.30it/s][A[A

 75% 24/32 [00:04<00:01,  4.81it/s][A[A

 78% 25/32 [00:05<00:01,  4.97it/s][A[A

 81% 26/32 [00:05<00:01,  4.93it/s][A[A

 84% 27/32 [00:05<00:00,  5.21it/s][A[A

 88% 28/32 [00:05<00:00,  5.27it/s][A[A

 91% 29/32 [00:05<00:00,  5.89it/s][A[A

 94% 30/32 [00:05<00:00,  6.09it/s][A[A

 97% 31/32 [00:06<00:00,  6.20it/s][A[A

100% 32/32 [00:06<00:00,  6.30it/s][A[A100% 32/32 [00:06<00:00,  5.17it/s]
Meta loss on this task batch = 4.9099e-01, PNorm = 40.4896, GNorm = 0.0850

 11% 2/19 [00:15<02:12,  7.82s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.58it/s][A[A

  6% 2/32 [00:00<00:05,  5.50it/s][A[A

  9% 3/32 [00:00<00:05,  5.57it/s][A[A

 12% 4/32 [00:00<00:04,  5.96it/s][A[A

 16% 5/32 [00:01<00:12,  2.13it/s][A[A

 19% 6/32 [00:02<00:09,  2.60it/s][A[A

 22% 7/32 [00:02<00:08,  3.08it/s][A[A

 25% 8/32 [00:02<00:06,  3.66it/s][A[A

 28% 9/32 [00:02<00:05,  4.18it/s][A[A

 31% 10/32 [00:02<00:05,  4.32it/s][A[A

 34% 11/32 [00:02<00:04,  4.50it/s][A[A

 38% 12/32 [00:03<00:04,  4.77it/s][A[A

 41% 13/32 [00:03<00:03,  4.96it/s][A[A

 44% 14/32 [00:03<00:03,  5.33it/s][A[A

 47% 15/32 [00:03<00:03,  5.06it/s][A[A

 50% 16/32 [00:03<00:02,  5.58it/s][A[A

 53% 17/32 [00:03<00:02,  5.90it/s][A[A

 56% 18/32 [00:04<00:02,  6.24it/s][A[A

 59% 19/32 [00:04<00:02,  6.29it/s][A[A

 62% 20/32 [00:04<00:01,  6.48it/s][A[A

 66% 21/32 [00:04<00:01,  6.85it/s][A[A

 69% 22/32 [00:05<00:04,  2.26it/s][A[A

 72% 23/32 [00:05<00:03,  2.87it/s][A[A

 75% 24/32 [00:05<00:02,  3.32it/s][A[A

 78% 25/32 [00:06<00:01,  4.00it/s][A[A

 81% 26/32 [00:06<00:01,  4.67it/s][A[A

 84% 27/32 [00:06<00:01,  4.87it/s][A[A

 88% 28/32 [00:06<00:00,  5.11it/s][A[A

 91% 29/32 [00:06<00:00,  5.49it/s][A[A

 94% 30/32 [00:07<00:00,  4.99it/s][A[A

 97% 31/32 [00:07<00:00,  5.74it/s][A[A

100% 32/32 [00:07<00:00,  5.54it/s][A[A100% 32/32 [00:07<00:00,  4.37it/s]
Meta loss on this task batch = 5.1351e-01, PNorm = 40.5105, GNorm = 0.0791

 16% 3/19 [00:23<02:05,  7.87s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.03it/s][A[A

  6% 2/32 [00:00<00:06,  4.92it/s][A[A

  9% 3/32 [00:00<00:05,  5.15it/s][A[A

 12% 4/32 [00:00<00:04,  5.83it/s][A[A

 16% 5/32 [00:00<00:05,  5.38it/s][A[A

 19% 6/32 [00:01<00:05,  5.14it/s][A[A

 22% 7/32 [00:02<00:11,  2.10it/s][A[A

 25% 8/32 [00:02<00:09,  2.58it/s][A[A

 28% 9/32 [00:02<00:07,  3.17it/s][A[A

 31% 10/32 [00:02<00:05,  3.72it/s][A[A

 34% 11/32 [00:02<00:04,  4.31it/s][A[A

 38% 12/32 [00:03<00:04,  4.96it/s][A[A

 41% 13/32 [00:03<00:03,  5.41it/s][A[A

 44% 14/32 [00:03<00:03,  5.47it/s][A[A

 47% 15/32 [00:03<00:02,  5.76it/s][A[A

 50% 16/32 [00:03<00:02,  5.73it/s][A[A

 53% 17/32 [00:03<00:02,  5.97it/s][A[A

 56% 18/32 [00:04<00:02,  5.87it/s][A[A

 59% 19/32 [00:04<00:02,  6.19it/s][A[A

 62% 20/32 [00:04<00:01,  6.04it/s][A[A

 66% 21/32 [00:04<00:01,  5.71it/s][A[A

 69% 22/32 [00:04<00:01,  5.06it/s][A[A

 72% 23/32 [00:05<00:04,  1.98it/s][A[A

 75% 24/32 [00:06<00:03,  2.43it/s][A[A

 78% 25/32 [00:06<00:02,  3.05it/s][A[A

 81% 26/32 [00:06<00:01,  3.75it/s][A[A

 84% 27/32 [00:06<00:01,  4.24it/s][A[A

 88% 28/32 [00:06<00:00,  4.56it/s][A[A

 91% 29/32 [00:06<00:00,  5.16it/s][A[A

 94% 30/32 [00:07<00:00,  5.14it/s][A[A

 97% 31/32 [00:07<00:00,  5.56it/s][A[A

100% 32/32 [00:07<00:00,  5.38it/s][A[A100% 32/32 [00:07<00:00,  4.28it/s]
Meta loss on this task batch = 5.7012e-01, PNorm = 40.5298, GNorm = 0.1396

 21% 4/19 [00:31<01:59,  7.96s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.45it/s][A[A

  6% 2/32 [00:00<00:05,  5.94it/s][A[A

  9% 3/32 [00:00<00:04,  6.00it/s][A[A

 12% 4/32 [00:00<00:04,  5.76it/s][A[A

 16% 5/32 [00:00<00:05,  5.38it/s][A[A

 19% 6/32 [00:01<00:04,  5.58it/s][A[A

 22% 7/32 [00:01<00:04,  5.13it/s][A[A

 25% 8/32 [00:01<00:04,  5.55it/s][A[A

 28% 9/32 [00:02<00:10,  2.11it/s][A[A

 31% 10/32 [00:02<00:08,  2.62it/s][A[A

 34% 11/32 [00:02<00:06,  3.16it/s][A[A

 38% 12/32 [00:03<00:05,  3.56it/s][A[A

 41% 13/32 [00:03<00:04,  3.88it/s][A[A

 44% 14/32 [00:03<00:04,  4.47it/s][A[A

 47% 15/32 [00:03<00:03,  5.00it/s][A[A

 50% 16/32 [00:03<00:02,  5.72it/s][A[A

 53% 17/32 [00:03<00:02,  5.24it/s][A[A

 56% 18/32 [00:04<00:02,  5.21it/s][A[A

 59% 19/32 [00:04<00:02,  5.16it/s][A[A

 62% 20/32 [00:04<00:02,  5.46it/s][A[A

 66% 21/32 [00:04<00:01,  5.55it/s][A[A

 69% 22/32 [00:04<00:01,  6.18it/s][A[A

 72% 23/32 [00:04<00:01,  6.12it/s][A[A

 75% 24/32 [00:05<00:01,  5.48it/s][A[A

 78% 25/32 [00:05<00:01,  5.13it/s][A[A

 81% 26/32 [00:05<00:01,  5.55it/s][A[A

 84% 27/32 [00:05<00:00,  5.51it/s][A[A

 88% 28/32 [00:06<00:01,  2.12it/s][A[A

 91% 29/32 [00:07<00:01,  2.61it/s][A[A

 94% 30/32 [00:07<00:00,  3.18it/s][A[A

 97% 31/32 [00:07<00:00,  3.66it/s][A[A

100% 32/32 [00:07<00:00,  3.95it/s][A[A100% 32/32 [00:07<00:00,  4.21it/s]
Meta loss on this task batch = 5.2043e-01, PNorm = 40.5464, GNorm = 0.0543

 26% 5/19 [00:39<01:52,  8.06s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  6.55it/s][A[A

  6% 2/32 [00:00<00:04,  6.22it/s][A[A

  9% 3/32 [00:00<00:04,  6.26it/s][A[A

 12% 4/32 [00:00<00:04,  6.09it/s][A[A

 16% 5/32 [00:00<00:04,  6.27it/s][A[A

 19% 6/32 [00:01<00:04,  5.94it/s][A[A

 22% 7/32 [00:01<00:04,  5.83it/s][A[A

 25% 8/32 [00:01<00:04,  5.83it/s][A[A

 28% 9/32 [00:01<00:04,  5.34it/s][A[A

 31% 10/32 [00:01<00:04,  5.36it/s][A[A

 34% 11/32 [00:02<00:10,  2.06it/s][A[A

 38% 12/32 [00:03<00:07,  2.54it/s][A[A

 41% 13/32 [00:03<00:06,  3.14it/s][A[A

 44% 14/32 [00:03<00:04,  3.67it/s][A[A

 47% 15/32 [00:03<00:04,  3.89it/s][A[A

 50% 16/32 [00:03<00:03,  4.19it/s][A[A

 53% 17/32 [00:03<00:03,  4.73it/s][A[A

 56% 18/32 [00:04<00:02,  5.07it/s][A[A

 59% 19/32 [00:04<00:02,  5.30it/s][A[A

 62% 20/32 [00:04<00:02,  5.66it/s][A[A

 66% 21/32 [00:04<00:01,  5.99it/s][A[A

 69% 22/32 [00:04<00:01,  5.48it/s][A[A

 72% 23/32 [00:05<00:01,  5.47it/s][A[A

 75% 24/32 [00:05<00:01,  5.38it/s][A[A

 78% 25/32 [00:05<00:01,  4.95it/s][A[A

 81% 26/32 [00:05<00:01,  5.39it/s][A[A

 84% 27/32 [00:06<00:02,  2.00it/s][A[A

 88% 28/32 [00:06<00:01,  2.54it/s][A[A

 91% 29/32 [00:07<00:00,  3.08it/s][A[A

 94% 30/32 [00:07<00:00,  3.70it/s][A[A

 97% 31/32 [00:07<00:00,  4.27it/s][A[A

100% 32/32 [00:07<00:00,  4.82it/s][A[A100% 32/32 [00:07<00:00,  4.22it/s]
Meta loss on this task batch = 4.5359e-01, PNorm = 40.5646, GNorm = 0.0846

 32% 6/19 [00:47<01:45,  8.12s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  6.12it/s][A[A

  6% 2/32 [00:00<00:04,  6.34it/s][A[A

  9% 3/32 [00:00<00:04,  5.82it/s][A[A

 12% 4/32 [00:00<00:04,  5.94it/s][A[A

 16% 5/32 [00:00<00:04,  6.29it/s][A[A

 19% 6/32 [00:00<00:04,  6.35it/s][A[A

 22% 7/32 [00:01<00:04,  5.91it/s][A[A

 25% 8/32 [00:01<00:04,  5.83it/s][A[A

 28% 9/32 [00:01<00:03,  5.75it/s][A[A

 31% 10/32 [00:01<00:04,  5.49it/s][A[A

 34% 11/32 [00:01<00:03,  5.61it/s][A[A

 38% 12/32 [00:03<00:09,  2.12it/s][A[A

 41% 13/32 [00:03<00:07,  2.63it/s][A[A

 44% 14/32 [00:03<00:05,  3.21it/s][A[A

 47% 15/32 [00:03<00:04,  3.80it/s][A[A

 50% 16/32 [00:03<00:04,  3.85it/s][A[A

 53% 17/32 [00:03<00:03,  4.56it/s][A[A

 56% 18/32 [00:04<00:03,  4.55it/s][A[A

 59% 19/32 [00:04<00:02,  4.81it/s][A[A

 62% 20/32 [00:04<00:02,  5.25it/s][A[A

 66% 21/32 [00:04<00:02,  5.13it/s][A[A

 69% 22/32 [00:04<00:01,  5.05it/s][A[A

 72% 23/32 [00:05<00:01,  4.82it/s][A[A

 75% 24/32 [00:05<00:01,  4.95it/s][A[A

 78% 25/32 [00:05<00:01,  5.18it/s][A[A

 81% 26/32 [00:05<00:01,  5.81it/s][A[A

 84% 27/32 [00:06<00:02,  2.10it/s][A[A

 88% 28/32 [00:06<00:01,  2.55it/s][A[A

 91% 29/32 [00:07<00:00,  3.09it/s][A[A

 94% 30/32 [00:07<00:00,  3.48it/s][A[A

 97% 31/32 [00:07<00:00,  4.01it/s][A[A

100% 32/32 [00:07<00:00,  4.14it/s][A[A100% 32/32 [00:07<00:00,  4.16it/s]
Meta loss on this task batch = 4.8666e-01, PNorm = 40.5863, GNorm = 0.1220

 37% 7/19 [00:56<01:38,  8.20s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.43it/s][A[A

  6% 2/32 [00:00<00:06,  4.56it/s][A[A

  9% 3/32 [00:00<00:06,  4.55it/s][A[A

 12% 4/32 [00:00<00:06,  4.53it/s][A[A

 16% 5/32 [00:01<00:05,  4.57it/s][A[A

 19% 6/32 [00:01<00:05,  4.56it/s][A[A

 22% 7/32 [00:01<00:05,  4.92it/s][A[A

 25% 8/32 [00:01<00:05,  4.77it/s][A[A

 28% 9/32 [00:01<00:04,  4.78it/s][A[A

 31% 10/32 [00:03<00:10,  2.03it/s][A[A

 34% 11/32 [00:03<00:08,  2.54it/s][A[A

 38% 12/32 [00:03<00:06,  2.98it/s][A[A

 41% 13/32 [00:03<00:05,  3.50it/s][A[A

 44% 14/32 [00:03<00:04,  4.05it/s][A[A

 47% 15/32 [00:03<00:03,  4.38it/s][A[A

 50% 16/32 [00:04<00:03,  4.58it/s][A[A

 53% 17/32 [00:04<00:03,  4.61it/s][A[A

 56% 18/32 [00:04<00:02,  4.74it/s][A[A

 59% 19/32 [00:04<00:02,  4.70it/s][A[A

 62% 20/32 [00:04<00:02,  5.13it/s][A[A

 66% 21/32 [00:05<00:02,  5.33it/s][A[A

 69% 22/32 [00:05<00:02,  4.94it/s][A[A

 72% 23/32 [00:05<00:01,  5.30it/s][A[A

 75% 24/32 [00:05<00:01,  5.01it/s][A[A

 78% 25/32 [00:06<00:03,  1.97it/s][A[A

 81% 26/32 [00:07<00:02,  2.40it/s][A[A

 84% 27/32 [00:07<00:01,  2.85it/s][A[A

 88% 28/32 [00:07<00:01,  3.29it/s][A[A

 91% 29/32 [00:07<00:00,  3.69it/s][A[A

 94% 30/32 [00:07<00:00,  4.06it/s][A[A

 97% 31/32 [00:08<00:00,  4.34it/s][A[A

100% 32/32 [00:08<00:00,  4.54it/s][A[A100% 32/32 [00:08<00:00,  3.86it/s]
Meta loss on this task batch = 3.6370e-01, PNorm = 40.6148, GNorm = 0.1475

 42% 8/19 [01:05<01:32,  8.45s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.98it/s][A[A

  6% 2/32 [00:00<00:05,  5.14it/s][A[A

  9% 3/32 [00:00<00:05,  5.09it/s][A[A

 12% 4/32 [00:00<00:05,  5.19it/s][A[A

 16% 5/32 [00:00<00:05,  5.15it/s][A[A

 19% 6/32 [00:01<00:04,  5.32it/s][A[A

 22% 7/32 [00:01<00:04,  5.38it/s][A[A

 25% 8/32 [00:01<00:04,  5.31it/s][A[A

 28% 9/32 [00:02<00:11,  2.02it/s][A[A

 31% 10/32 [00:02<00:09,  2.40it/s][A[A

 34% 11/32 [00:03<00:07,  2.77it/s][A[A

 38% 12/32 [00:03<00:06,  3.25it/s][A[A

 41% 13/32 [00:03<00:05,  3.67it/s][A[A

 44% 14/32 [00:03<00:04,  4.06it/s][A[A

 47% 15/32 [00:03<00:03,  4.27it/s][A[A

 50% 16/32 [00:04<00:03,  4.57it/s][A[A

 53% 17/32 [00:04<00:03,  4.69it/s][A[A

 56% 18/32 [00:04<00:02,  4.91it/s][A[A

 59% 19/32 [00:04<00:02,  4.79it/s][A[A

 62% 20/32 [00:04<00:02,  4.84it/s][A[A

 66% 21/32 [00:05<00:02,  4.84it/s][A[A

 69% 22/32 [00:05<00:02,  4.85it/s][A[A

 72% 23/32 [00:05<00:01,  4.91it/s][A[A

 75% 24/32 [00:06<00:04,  1.98it/s][A[A

 78% 25/32 [00:06<00:02,  2.43it/s][A[A

 81% 26/32 [00:07<00:02,  2.91it/s][A[A

 84% 27/32 [00:07<00:01,  3.31it/s][A[A

 88% 28/32 [00:07<00:01,  3.67it/s][A[A

 91% 29/32 [00:07<00:00,  4.06it/s][A[A

 94% 30/32 [00:07<00:00,  4.36it/s][A[A

 97% 31/32 [00:08<00:00,  4.58it/s][A[A

100% 32/32 [00:08<00:00,  4.67it/s][A[A100% 32/32 [00:08<00:00,  3.85it/s]
Meta loss on this task batch = 2.1976e-01, PNorm = 40.6499, GNorm = 0.1661

 47% 9/19 [01:14<01:26,  8.64s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.66it/s][A[A

  6% 2/32 [00:00<00:06,  4.87it/s][A[A

  9% 3/32 [00:00<00:06,  4.80it/s][A[A

 12% 4/32 [00:00<00:05,  4.90it/s][A[A

 16% 5/32 [00:00<00:05,  5.02it/s][A[A

 19% 6/32 [00:01<00:05,  5.15it/s][A[A

 22% 7/32 [00:02<00:12,  1.99it/s][A[A

 25% 8/32 [00:02<00:09,  2.45it/s][A[A

 28% 9/32 [00:02<00:07,  2.91it/s][A[A

 31% 10/32 [00:03<00:06,  3.21it/s][A[A

 34% 11/32 [00:03<00:05,  3.54it/s][A[A

 38% 12/32 [00:03<00:05,  3.94it/s][A[A

 41% 13/32 [00:03<00:04,  4.16it/s][A[A

 44% 14/32 [00:03<00:04,  4.45it/s][A[A

 47% 15/32 [00:03<00:03,  4.72it/s][A[A

 50% 16/32 [00:04<00:03,  4.91it/s][A[A

 53% 17/32 [00:04<00:02,  5.12it/s][A[A

 56% 18/32 [00:04<00:02,  5.20it/s][A[A

 59% 19/32 [00:04<00:02,  5.33it/s][A[A

 62% 20/32 [00:04<00:02,  5.42it/s][A[A

 66% 21/32 [00:05<00:02,  5.48it/s][A[A

 69% 22/32 [00:05<00:01,  5.28it/s][A[A

 72% 23/32 [00:05<00:01,  5.25it/s][A[A

 75% 24/32 [00:05<00:01,  5.27it/s][A[A

 78% 25/32 [00:05<00:01,  5.37it/s][A[A

 81% 26/32 [00:06<00:01,  5.34it/s][A[A

 84% 27/32 [00:07<00:02,  2.12it/s][A[A

 88% 28/32 [00:07<00:01,  2.50it/s][A[A

 91% 29/32 [00:07<00:01,  2.99it/s][A[A

 94% 30/32 [00:07<00:00,  3.28it/s][A[A

 97% 31/32 [00:08<00:00,  3.60it/s][A[A

100% 32/32 [00:08<00:00,  4.25it/s][A[A100% 32/32 [00:08<00:00,  3.92it/s]
Meta loss on this task batch = 2.4275e-01, PNorm = 40.6895, GNorm = 0.1501

 53% 10/19 [01:23<01:18,  8.72s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.64it/s][A[A

  6% 2/32 [00:00<00:06,  4.70it/s][A[A

  9% 3/32 [00:00<00:05,  5.06it/s][A[A

 12% 4/32 [00:00<00:05,  5.16it/s][A[A

 16% 5/32 [00:00<00:05,  5.08it/s][A[A

 19% 6/32 [00:01<00:04,  5.79it/s][A[A

 22% 7/32 [00:01<00:04,  5.66it/s][A[A

 25% 8/32 [00:02<00:11,  2.02it/s][A[A

 28% 9/32 [00:02<00:09,  2.52it/s][A[A

 31% 10/32 [00:02<00:07,  2.88it/s][A[A

 34% 11/32 [00:03<00:06,  3.17it/s][A[A

 38% 12/32 [00:03<00:05,  3.58it/s][A[A

 41% 13/32 [00:03<00:04,  3.81it/s][A[A

 44% 14/32 [00:03<00:04,  4.34it/s][A[A

 47% 15/32 [00:03<00:03,  4.51it/s][A[A

 50% 16/32 [00:04<00:03,  4.49it/s][A[A

 53% 17/32 [00:04<00:03,  4.72it/s][A[A

 56% 18/32 [00:04<00:02,  5.00it/s][A[A

 59% 19/32 [00:05<00:06,  1.98it/s][A[A

 62% 20/32 [00:05<00:04,  2.45it/s][A[A

 66% 21/32 [00:06<00:03,  2.90it/s][A[A

 69% 22/32 [00:06<00:03,  3.24it/s][A[A

 72% 23/32 [00:06<00:02,  3.54it/s][A[A

 75% 24/32 [00:06<00:02,  3.78it/s][A[A

 78% 25/32 [00:06<00:01,  4.27it/s][A[A

 81% 26/32 [00:07<00:01,  4.49it/s][A[A

 84% 27/32 [00:07<00:01,  4.62it/s][A[A

 88% 28/32 [00:07<00:00,  4.69it/s][A[A

 91% 29/32 [00:07<00:00,  4.68it/s][A[A

 94% 30/32 [00:07<00:00,  4.65it/s][A[A

 97% 31/32 [00:08<00:00,  4.95it/s][A[A

100% 32/32 [00:08<00:00,  4.93it/s][A[A100% 32/32 [00:08<00:00,  3.83it/s]
Meta loss on this task batch = 6.4516e-01, PNorm = 40.7281, GNorm = 0.1540

 58% 11/19 [01:32<01:10,  8.84s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.01it/s][A[A

  6% 2/32 [00:01<00:15,  1.97it/s][A[A

  9% 3/32 [00:01<00:12,  2.38it/s][A[A

 12% 4/32 [00:01<00:10,  2.68it/s][A[A

 16% 5/32 [00:02<00:08,  3.00it/s][A[A

 19% 6/32 [00:02<00:07,  3.40it/s][A[A

 22% 7/32 [00:02<00:06,  3.89it/s][A[A

 25% 8/32 [00:02<00:05,  4.04it/s][A[A

 28% 9/32 [00:02<00:05,  4.09it/s][A[A

 31% 10/32 [00:03<00:05,  4.38it/s][A[A

 34% 11/32 [00:03<00:04,  4.73it/s][A[A

 38% 12/32 [00:03<00:04,  4.74it/s][A[A

 41% 13/32 [00:04<00:09,  1.95it/s][A[A

 44% 14/32 [00:04<00:07,  2.38it/s][A[A

 47% 15/32 [00:05<00:06,  2.76it/s][A[A

 50% 16/32 [00:05<00:04,  3.31it/s][A[A

 53% 17/32 [00:05<00:04,  3.62it/s][A[A

 56% 18/32 [00:05<00:03,  3.77it/s][A[A

 59% 19/32 [00:06<00:03,  3.94it/s][A[A

 62% 20/32 [00:06<00:02,  4.28it/s][A[A

 66% 21/32 [00:06<00:02,  4.34it/s][A[A

 69% 22/32 [00:06<00:02,  4.64it/s][A[A

 72% 23/32 [00:06<00:01,  4.71it/s][A[A

 75% 24/32 [00:07<00:01,  4.70it/s][A[A

 78% 25/32 [00:07<00:01,  4.69it/s][A[A

 81% 26/32 [00:07<00:01,  4.61it/s][A[A

 84% 27/32 [00:08<00:02,  1.95it/s][A[A

 88% 28/32 [00:08<00:01,  2.35it/s][A[A

 91% 29/32 [00:09<00:01,  2.79it/s][A[A

 94% 30/32 [00:09<00:00,  3.18it/s][A[A

 97% 31/32 [00:09<00:00,  3.56it/s][A[A

100% 32/32 [00:09<00:00,  3.74it/s][A[A100% 32/32 [00:09<00:00,  3.27it/s]
Meta loss on this task batch = 5.5057e-01, PNorm = 40.7623, GNorm = 0.2285

 63% 12/19 [01:42<01:05,  9.35s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.25it/s][A[A

  6% 2/32 [00:00<00:06,  4.48it/s][A[A

  9% 3/32 [00:00<00:05,  4.86it/s][A[A

 12% 4/32 [00:00<00:05,  4.91it/s][A[A

 16% 5/32 [00:00<00:05,  5.15it/s][A[A

 19% 6/32 [00:01<00:05,  4.85it/s][A[A

 22% 7/32 [00:01<00:05,  4.95it/s][A[A

 25% 8/32 [00:01<00:04,  5.60it/s][A[A

 28% 9/32 [00:01<00:04,  5.57it/s][A[A

 31% 10/32 [00:02<00:10,  2.02it/s][A[A

 34% 11/32 [00:03<00:08,  2.46it/s][A[A

 38% 12/32 [00:03<00:06,  2.96it/s][A[A

 41% 13/32 [00:03<00:05,  3.31it/s][A[A

 44% 14/32 [00:03<00:04,  3.75it/s][A[A

 47% 15/32 [00:03<00:04,  4.00it/s][A[A

 50% 16/32 [00:04<00:03,  4.25it/s][A[A

 53% 17/32 [00:04<00:03,  4.73it/s][A[A

 56% 18/32 [00:04<00:03,  4.50it/s][A[A

 59% 19/32 [00:04<00:02,  4.92it/s][A[A

 62% 20/32 [00:04<00:02,  4.96it/s][A[A

 66% 21/32 [00:05<00:02,  4.82it/s][A[A

 69% 22/32 [00:05<00:02,  4.83it/s][A[A

 72% 23/32 [00:05<00:01,  4.73it/s][A[A

 75% 24/32 [00:05<00:01,  4.83it/s][A[A

 78% 25/32 [00:06<00:03,  2.00it/s][A[A

 81% 26/32 [00:07<00:02,  2.42it/s][A[A

 84% 27/32 [00:07<00:01,  2.95it/s][A[A

 88% 28/32 [00:07<00:01,  3.42it/s][A[A

 91% 29/32 [00:07<00:00,  3.77it/s][A[A

 94% 30/32 [00:07<00:00,  4.15it/s][A[A

 97% 31/32 [00:08<00:00,  4.32it/s][A[A

100% 32/32 [00:08<00:00,  4.50it/s][A[A100% 32/32 [00:08<00:00,  3.87it/s]
Meta loss on this task batch = 6.0929e-01, PNorm = 40.7915, GNorm = 0.1836

 68% 13/19 [01:51<00:55,  9.25s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  7.33it/s][A[A

  6% 2/32 [00:00<00:04,  7.31it/s][A[A

  9% 3/32 [00:00<00:04,  6.17it/s][A[A

 12% 4/32 [00:00<00:05,  5.51it/s][A[A

 16% 5/32 [00:00<00:05,  5.22it/s][A[A

 19% 6/32 [00:01<00:04,  5.61it/s][A[A

 22% 7/32 [00:01<00:04,  5.21it/s][A[A

 25% 8/32 [00:01<00:04,  5.51it/s][A[A

 28% 9/32 [00:02<00:11,  2.00it/s][A[A

 31% 10/32 [00:02<00:08,  2.45it/s][A[A

 34% 11/32 [00:03<00:07,  2.97it/s][A[A

 38% 12/32 [00:03<00:05,  3.37it/s][A[A

 41% 13/32 [00:03<00:05,  3.70it/s][A[A

 44% 14/32 [00:03<00:04,  3.92it/s][A[A

 47% 15/32 [00:03<00:04,  4.15it/s][A[A

 50% 16/32 [00:04<00:03,  4.57it/s][A[A

 53% 17/32 [00:04<00:03,  4.59it/s][A[A

 56% 18/32 [00:04<00:03,  4.62it/s][A[A

 59% 19/32 [00:04<00:02,  4.70it/s][A[A

 62% 20/32 [00:04<00:02,  4.86it/s][A[A

 66% 21/32 [00:05<00:02,  4.90it/s][A[A

 69% 22/32 [00:05<00:02,  4.80it/s][A[A

 72% 23/32 [00:05<00:01,  4.61it/s][A[A

 75% 24/32 [00:06<00:04,  1.93it/s][A[A

 78% 25/32 [00:07<00:03,  2.31it/s][A[A

 81% 26/32 [00:07<00:02,  2.69it/s][A[A

 84% 27/32 [00:07<00:01,  3.10it/s][A[A

 88% 28/32 [00:07<00:01,  3.50it/s][A[A

 91% 29/32 [00:07<00:00,  3.78it/s][A[A

 94% 30/32 [00:08<00:00,  4.05it/s][A[A

 97% 31/32 [00:08<00:00,  4.26it/s][A[A

100% 32/32 [00:08<00:00,  4.46it/s][A[A100% 32/32 [00:08<00:00,  3.78it/s]
Meta loss on this task batch = 5.5798e-01, PNorm = 40.8181, GNorm = 0.1084

 74% 14/19 [02:01<00:46,  9.23s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.31it/s][A[A

  6% 2/32 [00:00<00:05,  5.31it/s][A[A

  9% 3/32 [00:01<00:14,  1.98it/s][A[A

 12% 4/32 [00:01<00:11,  2.47it/s][A[A

 16% 5/32 [00:01<00:09,  2.93it/s][A[A

 19% 6/32 [00:02<00:08,  3.23it/s][A[A

 22% 7/32 [00:02<00:07,  3.54it/s][A[A

 25% 8/32 [00:02<00:06,  3.98it/s][A[A

 28% 9/32 [00:02<00:05,  4.06it/s][A[A

 31% 10/32 [00:03<00:04,  4.47it/s][A[A

 34% 11/32 [00:03<00:04,  4.77it/s][A[A

 38% 12/32 [00:03<00:03,  5.15it/s][A[A

 41% 13/32 [00:03<00:03,  5.09it/s][A[A

 44% 14/32 [00:03<00:03,  5.26it/s][A[A

 47% 15/32 [00:04<00:08,  2.05it/s][A[A

 50% 16/32 [00:05<00:06,  2.47it/s][A[A

 53% 17/32 [00:05<00:05,  2.99it/s][A[A

 56% 18/32 [00:05<00:04,  3.46it/s][A[A

 59% 19/32 [00:05<00:03,  4.03it/s][A[A

 62% 20/32 [00:05<00:02,  4.15it/s][A[A

 66% 21/32 [00:06<00:02,  4.49it/s][A[A

 69% 22/32 [00:06<00:02,  4.73it/s][A[A

 72% 23/32 [00:06<00:02,  4.45it/s][A[A

 75% 24/32 [00:06<00:01,  4.54it/s][A[A

 78% 25/32 [00:06<00:01,  4.91it/s][A[A

 81% 26/32 [00:07<00:01,  4.63it/s][A[A

 84% 27/32 [00:07<00:01,  4.61it/s][A[A

 88% 28/32 [00:07<00:00,  4.79it/s][A[A

 91% 29/32 [00:08<00:01,  1.98it/s][A[A

 94% 30/32 [00:08<00:00,  2.35it/s][A[A

 97% 31/32 [00:09<00:00,  3.01it/s][A[A

100% 32/32 [00:09<00:00,  3.53it/s][A[A100% 32/32 [00:09<00:00,  3.47it/s]
Meta loss on this task batch = 5.0993e-01, PNorm = 40.8401, GNorm = 0.1425

 79% 15/19 [02:11<00:37,  9.45s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.55it/s][A[A

  6% 2/32 [00:00<00:05,  5.36it/s][A[A

  9% 3/32 [00:00<00:04,  5.99it/s][A[A

 12% 4/32 [00:00<00:04,  6.29it/s][A[A

 16% 5/32 [00:00<00:04,  5.99it/s][A[A

 19% 6/32 [00:01<00:04,  5.36it/s][A[A

 22% 7/32 [00:01<00:04,  5.64it/s][A[A

 25% 8/32 [00:01<00:04,  5.56it/s][A[A

 28% 9/32 [00:01<00:03,  6.06it/s][A[A

 31% 10/32 [00:01<00:03,  5.54it/s][A[A

 34% 11/32 [00:01<00:03,  5.39it/s][A[A

 38% 12/32 [00:03<00:09,  2.04it/s][A[A

 41% 13/32 [00:03<00:07,  2.50it/s][A[A

 44% 14/32 [00:03<00:06,  2.99it/s][A[A

 47% 15/32 [00:03<00:05,  3.35it/s][A[A

 50% 16/32 [00:03<00:04,  3.90it/s][A[A

 53% 17/32 [00:04<00:03,  4.01it/s][A[A

 56% 18/32 [00:04<00:03,  4.49it/s][A[A

 59% 19/32 [00:04<00:02,  4.58it/s][A[A

 62% 20/32 [00:04<00:02,  4.61it/s][A[A

 66% 21/32 [00:04<00:02,  4.72it/s][A[A

 69% 22/32 [00:05<00:02,  4.78it/s][A[A

 72% 23/32 [00:05<00:01,  4.56it/s][A[A

 75% 24/32 [00:05<00:01,  4.41it/s][A[A

 78% 25/32 [00:05<00:01,  5.21it/s][A[A

 81% 26/32 [00:05<00:01,  5.49it/s][A[A

 84% 27/32 [00:07<00:02,  1.99it/s][A[A

 88% 28/32 [00:07<00:01,  2.40it/s][A[A

 91% 29/32 [00:07<00:01,  2.82it/s][A[A

 94% 30/32 [00:07<00:00,  3.08it/s][A[A

 97% 31/32 [00:07<00:00,  3.33it/s][A[A

100% 32/32 [00:08<00:00,  3.68it/s][A[A100% 32/32 [00:08<00:00,  3.91it/s]
Meta loss on this task batch = 5.4821e-01, PNorm = 40.8609, GNorm = 0.0789

 84% 16/19 [02:19<00:27,  9.28s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.73it/s][A[A

  6% 2/32 [00:00<00:07,  3.83it/s][A[A

  9% 3/32 [00:00<00:06,  4.19it/s][A[A

 12% 4/32 [00:00<00:06,  4.40it/s][A[A

 16% 5/32 [00:01<00:05,  4.61it/s][A[A

 19% 6/32 [00:01<00:05,  5.18it/s][A[A

 22% 7/32 [00:01<00:04,  5.50it/s][A[A

 25% 8/32 [00:02<00:11,  2.00it/s][A[A

 28% 9/32 [00:02<00:09,  2.40it/s][A[A

 31% 10/32 [00:02<00:07,  3.04it/s][A[A

 34% 11/32 [00:03<00:06,  3.43it/s][A[A

 38% 12/32 [00:03<00:05,  3.78it/s][A[A

 41% 13/32 [00:03<00:04,  4.28it/s][A[A

 44% 14/32 [00:03<00:03,  4.62it/s][A[A

 47% 15/32 [00:03<00:03,  5.03it/s][A[A

 50% 16/32 [00:04<00:03,  5.19it/s][A[A

 53% 17/32 [00:04<00:02,  5.20it/s][A[A

 56% 18/32 [00:04<00:02,  4.92it/s][A[A

 59% 19/32 [00:04<00:02,  4.64it/s][A[A

 62% 20/32 [00:04<00:02,  4.60it/s][A[A

 66% 21/32 [00:05<00:02,  4.73it/s][A[A

 69% 22/32 [00:06<00:05,  1.99it/s][A[A

 72% 23/32 [00:06<00:03,  2.47it/s][A[A

 75% 24/32 [00:06<00:02,  2.85it/s][A[A

 78% 25/32 [00:06<00:02,  3.31it/s][A[A

 81% 26/32 [00:07<00:01,  3.74it/s][A[A

 84% 27/32 [00:07<00:01,  4.10it/s][A[A

 88% 28/32 [00:07<00:00,  4.34it/s][A[A

 91% 29/32 [00:07<00:00,  4.47it/s][A[A

 94% 30/32 [00:07<00:00,  4.33it/s][A[A

 97% 31/32 [00:08<00:00,  4.61it/s][A[A

100% 32/32 [00:08<00:00,  4.73it/s][A[A100% 32/32 [00:08<00:00,  3.84it/s]
Meta loss on this task batch = 4.5042e-01, PNorm = 40.8826, GNorm = 0.1074

 89% 17/19 [02:28<00:18,  9.21s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.01it/s][A[A

  6% 2/32 [00:00<00:06,  4.89it/s][A[A

  9% 3/32 [00:00<00:06,  4.76it/s][A[A

 12% 4/32 [00:00<00:05,  4.70it/s][A[A

 16% 5/32 [00:02<00:13,  1.94it/s][A[A

 19% 6/32 [00:02<00:11,  2.34it/s][A[A

 22% 7/32 [00:02<00:09,  2.63it/s][A[A

 25% 8/32 [00:02<00:08,  2.90it/s][A[A

 28% 9/32 [00:03<00:06,  3.30it/s][A[A

 31% 10/32 [00:03<00:06,  3.52it/s][A[A

 34% 11/32 [00:03<00:05,  3.98it/s][A[A

 38% 12/32 [00:03<00:04,  4.28it/s][A[A

 41% 13/32 [00:03<00:04,  4.55it/s][A[A

 44% 14/32 [00:04<00:08,  2.02it/s][A[A

 47% 15/32 [00:05<00:07,  2.38it/s][A[A

 50% 16/32 [00:05<00:05,  2.84it/s][A[A

 53% 17/32 [00:05<00:04,  3.50it/s][A[A

 56% 18/32 [00:05<00:03,  3.88it/s][A[A

 59% 19/32 [00:05<00:03,  4.04it/s][A[A

 62% 20/32 [00:06<00:02,  4.09it/s][A[A

 66% 21/32 [00:06<00:02,  4.45it/s][A[A

 69% 22/32 [00:06<00:01,  5.00it/s][A[A

 72% 23/32 [00:06<00:01,  4.72it/s][A[A

 75% 24/32 [00:07<00:01,  4.32it/s][A[A

 78% 25/32 [00:07<00:01,  4.78it/s][A[A

 81% 26/32 [00:07<00:01,  4.67it/s][A[A

 84% 27/32 [00:08<00:02,  1.93it/s][A[A

 88% 28/32 [00:08<00:01,  2.37it/s][A[A

 91% 29/32 [00:09<00:01,  2.85it/s][A[A

 94% 30/32 [00:09<00:00,  3.34it/s][A[A

 97% 31/32 [00:09<00:00,  3.76it/s][A[A

100% 32/32 [00:09<00:00,  3.86it/s][A[A100% 32/32 [00:09<00:00,  3.32it/s]
Meta loss on this task batch = 5.6526e-01, PNorm = 40.8937, GNorm = 0.2299

 95% 18/19 [02:39<00:09,  9.56s/it][A

  0% 0/23 [00:00<?, ?it/s][A[A

  4% 1/23 [00:00<00:05,  4.01it/s][A[A

  9% 2/23 [00:00<00:04,  4.42it/s][A[A

 13% 3/23 [00:00<00:04,  4.50it/s][A[A

 17% 4/23 [00:00<00:03,  5.24it/s][A[A

 22% 5/23 [00:00<00:03,  5.58it/s][A[A

 26% 6/23 [00:01<00:03,  5.35it/s][A[A

 30% 7/23 [00:01<00:02,  5.40it/s][A[A

 35% 8/23 [00:01<00:02,  5.41it/s][A[A

 39% 9/23 [00:01<00:02,  5.82it/s][A[A

 43% 10/23 [00:01<00:02,  5.00it/s][A[A

 48% 11/23 [00:02<00:02,  5.22it/s][A[A

 52% 12/23 [00:02<00:02,  5.38it/s][A[A

 57% 13/23 [00:02<00:01,  5.75it/s][A[A

 61% 14/23 [00:03<00:04,  2.09it/s][A[A

 65% 15/23 [00:03<00:03,  2.26it/s][A[A

 70% 16/23 [00:04<00:02,  2.60it/s][A[A

 74% 17/23 [00:04<00:01,  3.17it/s][A[A

 78% 18/23 [00:04<00:01,  3.51it/s][A[A

 83% 19/23 [00:04<00:01,  3.99it/s][A[A

 87% 20/23 [00:04<00:00,  4.65it/s][A[A

 91% 21/23 [00:05<00:00,  4.95it/s][A[A

 96% 22/23 [00:05<00:00,  5.52it/s][A[A

100% 23/23 [00:05<00:00,  5.74it/s][A[A100% 23/23 [00:05<00:00,  4.34it/s]
Meta loss on this task batch = 4.1550e-01, PNorm = 40.9095, GNorm = 0.1036

100% 19/19 [02:45<00:00,  8.44s/it][A100% 19/19 [02:45<00:00,  8.69s/it]
Took 165.17690992355347 seconds to complete one epoch of meta training

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/20 [00:00<?, ?it/s][A[A


  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 30.13it/s]


  5% 1/20 [00:00<00:02,  8.09it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 20.27it/s]


 10% 2/20 [00:00<00:02,  6.17it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 17.83it/s][A[A[A100% 3/3 [00:00<00:00, 19.83it/s]


 15% 3/20 [00:00<00:03,  4.97it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 15.23it/s]


 20% 4/20 [00:00<00:03,  4.94it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:01<00:01,  1.77it/s][A[A[A100% 4/4 [00:01<00:00,  3.26it/s]


 25% 5/20 [00:02<00:08,  1.78it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.51it/s][A[A[A100% 4/4 [00:00<00:00, 16.18it/s]


 30% 6/20 [00:02<00:07,  1.94it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.43it/s][A[A[A100% 4/4 [00:00<00:00, 20.24it/s]


 35% 7/20 [00:03<00:06,  2.11it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.44it/s][A[A[A


100% 4/4 [00:00<00:00, 19.00it/s][A[A[A100% 4/4 [00:00<00:00, 18.69it/s]


 40% 8/20 [00:03<00:05,  2.23it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.70it/s][A[A[A100% 4/4 [00:00<00:00, 22.25it/s]


 45% 9/20 [00:03<00:04,  2.40it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 18.96it/s][A[A[A


100% 4/4 [00:00<00:00, 18.64it/s][A[A[A100% 4/4 [00:00<00:00, 18.40it/s]


 50% 10/20 [00:04<00:04,  2.43it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:01<00:00,  2.53it/s][A[A[A100% 4/4 [00:01<00:00,  3.28it/s]


 55% 11/20 [00:05<00:06,  1.42it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.65it/s][A[A[A100% 4/4 [00:00<00:00, 22.03it/s]


 60% 12/20 [00:05<00:04,  1.68it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.67it/s][A[A[A100% 3/3 [00:00<00:00, 13.95it/s]


 65% 13/20 [00:06<00:03,  1.89it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 13.99it/s][A[A[A100% 3/3 [00:00<00:00, 13.63it/s]


 70% 14/20 [00:06<00:02,  2.08it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.82it/s][A[A[A100% 4/4 [00:00<00:00, 18.01it/s]


 75% 15/20 [00:07<00:02,  2.19it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.06it/s][A[A[A100% 3/3 [00:00<00:00, 17.41it/s]


 80% 16/20 [00:07<00:01,  2.36it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:01<00:00,  1.75it/s][A[A[A100% 3/3 [00:01<00:00,  2.54it/s]


 85% 17/20 [00:08<00:02,  1.41it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 23.06it/s]


 90% 18/20 [00:09<00:01,  1.74it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 15.75it/s][A[A[A100% 3/3 [00:00<00:00, 20.09it/s]


 95% 19/20 [00:09<00:00,  2.03it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 13.66it/s]


100% 20/20 [00:09<00:00,  2.42it/s][A[A100% 20/20 [00:09<00:00,  2.09it/s]

100% 1/1 [00:09<00:00,  9.57s/it][A100% 1/1 [00:09<00:00,  9.57s/it]
Took 174.75110006332397 seconds to complete one epoch of meta training and validating
Meta validation score auc = 0.635967
 80% 24/30 [1:17:15<18:58, 189.78s/it]Epoch 24

  0% 0/19 [00:00<?, ?it/s][A

  0% 0/32 [00:00<?, ?it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  3% 1/32 [00:00<00:06,  4.79it/s][A[A

  6% 2/32 [00:00<00:06,  4.42it/s][A[A

  9% 3/32 [00:00<00:06,  4.52it/s][A[A

 12% 4/32 [00:00<00:05,  4.75it/s][A[A

 16% 5/32 [00:01<00:05,  4.97it/s][A[A

 19% 6/32 [00:01<00:05,  5.05it/s][A[A

 22% 7/32 [00:01<00:04,  5.12it/s][A[A

 25% 8/32 [00:01<00:04,  5.11it/s][A[A

 28% 9/32 [00:02<00:11,  2.04it/s][A[A

 31% 10/32 [00:02<00:08,  2.50it/s][A[A

 34% 11/32 [00:03<00:07,  2.81it/s][A[A

 38% 12/32 [00:03<00:06,  3.01it/s][A[A

 41% 13/32 [00:03<00:05,  3.17it/s][A[A

 44% 14/32 [00:04<00:05,  3.43it/s][A[A

 47% 15/32 [00:04<00:04,  3.89it/s][A[A

 50% 16/32 [00:04<00:03,  4.09it/s][A[A

 53% 17/32 [00:04<00:03,  4.16it/s][A[A

 56% 18/32 [00:04<00:03,  4.55it/s][A[A

 59% 19/32 [00:05<00:02,  4.67it/s][A[A

 62% 20/32 [00:05<00:02,  4.52it/s][A[A

 66% 21/32 [00:06<00:05,  1.98it/s][A[A

 69% 22/32 [00:06<00:04,  2.43it/s][A[A

 72% 23/32 [00:06<00:03,  2.97it/s][A[A

 75% 24/32 [00:07<00:02,  3.26it/s][A[A

 78% 25/32 [00:07<00:01,  3.69it/s][A[A

 81% 26/32 [00:07<00:01,  3.88it/s][A[A

 84% 27/32 [00:07<00:01,  4.39it/s][A[A

 88% 28/32 [00:07<00:00,  4.80it/s][A[A

 91% 29/32 [00:08<00:00,  4.58it/s][A[A

 94% 30/32 [00:08<00:00,  4.98it/s][A[A

 97% 31/32 [00:08<00:00,  5.01it/s][A[A

100% 32/32 [00:08<00:00,  4.98it/s][A[A100% 32/32 [00:08<00:00,  3.74it/s]
Meta loss on this task batch = 5.1534e-01, PNorm = 40.9337, GNorm = 0.1333

  5% 1/19 [00:09<02:47,  9.32s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.25it/s][A[A

  6% 2/32 [00:01<00:14,  2.02it/s][A[A

  9% 3/32 [00:01<00:12,  2.36it/s][A[A

 12% 4/32 [00:01<00:10,  2.71it/s][A[A

 16% 5/32 [00:02<00:08,  3.23it/s][A[A

 19% 6/32 [00:02<00:07,  3.51it/s][A[A

 22% 7/32 [00:02<00:06,  3.98it/s][A[A

 25% 8/32 [00:02<00:05,  4.42it/s][A[A

 28% 9/32 [00:02<00:05,  4.60it/s][A[A

 31% 10/32 [00:02<00:04,  4.93it/s][A[A

 34% 11/32 [00:03<00:04,  5.04it/s][A[A

 38% 12/32 [00:03<00:04,  4.58it/s][A[A

 41% 13/32 [00:03<00:03,  4.89it/s][A[A

 44% 14/32 [00:03<00:03,  4.78it/s][A[A

 47% 15/32 [00:04<00:03,  4.76it/s][A[A

 50% 16/32 [00:05<00:07,  2.02it/s][A[A

 53% 17/32 [00:05<00:06,  2.40it/s][A[A

 56% 18/32 [00:05<00:05,  2.76it/s][A[A

 59% 19/32 [00:05<00:03,  3.33it/s][A[A

 62% 20/32 [00:06<00:03,  3.55it/s][A[A

 66% 21/32 [00:06<00:02,  3.81it/s][A[A

 69% 22/32 [00:06<00:02,  4.13it/s][A[A

 72% 23/32 [00:06<00:01,  4.53it/s][A[A

 75% 24/32 [00:06<00:01,  4.79it/s][A[A

 78% 25/32 [00:07<00:01,  4.86it/s][A[A

 81% 26/32 [00:07<00:01,  4.74it/s][A[A

 84% 27/32 [00:07<00:01,  4.72it/s][A[A

 88% 28/32 [00:07<00:00,  4.65it/s][A[A

 91% 29/32 [00:07<00:00,  5.00it/s][A[A

 94% 30/32 [00:09<00:01,  1.99it/s][A[A

 97% 31/32 [00:09<00:00,  2.41it/s][A[A

100% 32/32 [00:09<00:00,  2.93it/s][A[A100% 32/32 [00:09<00:00,  3.38it/s]
Meta loss on this task batch = 4.9122e-01, PNorm = 40.9668, GNorm = 0.1519

 11% 2/19 [00:19<02:43,  9.59s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.48it/s][A[A

  6% 2/32 [00:00<00:05,  5.61it/s][A[A

  9% 3/32 [00:00<00:05,  5.28it/s][A[A

 12% 4/32 [00:00<00:05,  4.78it/s][A[A

 16% 5/32 [00:01<00:05,  4.79it/s][A[A

 19% 6/32 [00:01<00:05,  4.62it/s][A[A

 22% 7/32 [00:01<00:04,  5.03it/s][A[A

 25% 8/32 [00:01<00:04,  5.28it/s][A[A

 28% 9/32 [00:01<00:04,  5.12it/s][A[A

 31% 10/32 [00:03<00:11,  1.99it/s][A[A

 34% 11/32 [00:03<00:08,  2.36it/s][A[A

 38% 12/32 [00:03<00:07,  2.81it/s][A[A

 41% 13/32 [00:03<00:05,  3.25it/s][A[A

 44% 14/32 [00:03<00:05,  3.57it/s][A[A

 47% 15/32 [00:04<00:04,  4.09it/s][A[A

 50% 16/32 [00:04<00:03,  4.36it/s][A[A

 53% 17/32 [00:04<00:03,  4.37it/s][A[A

 56% 18/32 [00:04<00:02,  4.82it/s][A[A

 59% 19/32 [00:04<00:02,  5.01it/s][A[A

 62% 20/32 [00:04<00:02,  5.31it/s][A[A

 66% 21/32 [00:05<00:02,  5.28it/s][A[A

 69% 22/32 [00:05<00:01,  5.21it/s][A[A

 72% 23/32 [00:05<00:01,  4.92it/s][A[A

 75% 24/32 [00:05<00:01,  4.84it/s][A[A

 78% 25/32 [00:05<00:01,  4.96it/s][A[A

 81% 26/32 [00:06<00:01,  5.14it/s][A[A

 84% 27/32 [00:07<00:02,  1.98it/s][A[A

 88% 28/32 [00:07<00:01,  2.40it/s][A[A

 91% 29/32 [00:07<00:01,  2.79it/s][A[A

 94% 30/32 [00:08<00:00,  3.07it/s][A[A

 97% 31/32 [00:08<00:00,  3.61it/s][A[A

100% 32/32 [00:08<00:00,  3.88it/s][A[A100% 32/32 [00:08<00:00,  3.79it/s]
Meta loss on this task batch = 4.9223e-01, PNorm = 41.0029, GNorm = 0.0792

 16% 3/19 [00:28<02:31,  9.48s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.01it/s][A[A

  6% 2/32 [00:00<00:07,  4.22it/s][A[A

  9% 3/32 [00:00<00:06,  4.46it/s][A[A

 12% 4/32 [00:00<00:06,  4.36it/s][A[A

 16% 5/32 [00:01<00:06,  4.41it/s][A[A

 19% 6/32 [00:01<00:05,  4.81it/s][A[A

 22% 7/32 [00:01<00:05,  4.94it/s][A[A

 25% 8/32 [00:02<00:11,  2.01it/s][A[A

 28% 9/32 [00:02<00:09,  2.49it/s][A[A

 31% 10/32 [00:02<00:07,  3.04it/s][A[A

 34% 11/32 [00:03<00:05,  3.56it/s][A[A

 38% 12/32 [00:03<00:05,  3.96it/s][A[A

 41% 13/32 [00:03<00:04,  4.12it/s][A[A

 44% 14/32 [00:03<00:04,  4.30it/s][A[A

 47% 15/32 [00:03<00:03,  4.68it/s][A[A

 50% 16/32 [00:04<00:03,  4.71it/s][A[A

 53% 17/32 [00:04<00:02,  5.11it/s][A[A

 56% 18/32 [00:04<00:02,  4.96it/s][A[A

 59% 19/32 [00:04<00:02,  5.37it/s][A[A

 62% 20/32 [00:04<00:02,  5.16it/s][A[A

 66% 21/32 [00:05<00:02,  5.36it/s][A[A

 69% 22/32 [00:05<00:01,  5.42it/s][A[A

 72% 23/32 [00:05<00:01,  4.92it/s][A[A

 75% 24/32 [00:06<00:04,  1.98it/s][A[A

 78% 25/32 [00:06<00:02,  2.47it/s][A[A

 81% 26/32 [00:07<00:02,  2.96it/s][A[A

 84% 27/32 [00:07<00:01,  3.37it/s][A[A

 88% 28/32 [00:07<00:01,  3.75it/s][A[A

 91% 29/32 [00:07<00:00,  4.25it/s][A[A

 94% 30/32 [00:07<00:00,  4.37it/s][A[A

 97% 31/32 [00:08<00:00,  4.54it/s][A[A

100% 32/32 [00:08<00:00,  4.78it/s][A[A100% 32/32 [00:08<00:00,  3.90it/s]
Meta loss on this task batch = 5.5970e-01, PNorm = 41.0375, GNorm = 0.0879

 21% 4/19 [00:37<02:19,  9.33s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.48it/s][A[A

  6% 2/32 [00:00<00:06,  4.70it/s][A[A

  9% 3/32 [00:00<00:05,  5.09it/s][A[A

 12% 4/32 [00:00<00:06,  4.65it/s][A[A

 16% 5/32 [00:01<00:05,  4.72it/s][A[A

 19% 6/32 [00:02<00:12,  2.03it/s][A[A

 22% 7/32 [00:02<00:09,  2.54it/s][A[A

 25% 8/32 [00:02<00:07,  3.03it/s][A[A

 28% 9/32 [00:02<00:06,  3.40it/s][A[A

 31% 10/32 [00:02<00:05,  3.78it/s][A[A

 34% 11/32 [00:03<00:05,  4.03it/s][A[A

 38% 12/32 [00:03<00:04,  4.14it/s][A[A

 41% 13/32 [00:03<00:04,  4.17it/s][A[A

 44% 14/32 [00:03<00:04,  4.32it/s][A[A

 47% 15/32 [00:04<00:03,  4.41it/s][A[A

 50% 16/32 [00:04<00:03,  4.83it/s][A[A

 53% 17/32 [00:04<00:03,  4.43it/s][A[A

 56% 18/32 [00:05<00:07,  1.91it/s][A[A

 59% 19/32 [00:05<00:05,  2.26it/s][A[A

 62% 20/32 [00:06<00:04,  2.59it/s][A[A

 66% 21/32 [00:06<00:03,  3.02it/s][A[A

 69% 22/32 [00:06<00:02,  3.51it/s][A[A

 72% 23/32 [00:06<00:02,  3.99it/s][A[A

 75% 24/32 [00:06<00:02,  3.98it/s][A[A

 78% 25/32 [00:07<00:01,  4.07it/s][A[A

 81% 26/32 [00:07<00:01,  4.27it/s][A[A

 84% 27/32 [00:07<00:01,  4.47it/s][A[A

 88% 28/32 [00:07<00:00,  4.91it/s][A[A

 91% 29/32 [00:07<00:00,  4.95it/s][A[A

 94% 30/32 [00:09<00:01,  2.00it/s][A[A

 97% 31/32 [00:09<00:00,  2.44it/s][A[A

100% 32/32 [00:09<00:00,  2.83it/s][A[A100% 32/32 [00:09<00:00,  3.33it/s]
Meta loss on this task batch = 5.1264e-01, PNorm = 41.0696, GNorm = 0.0925

 26% 5/19 [00:48<02:14,  9.64s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.12it/s][A[A

  6% 2/32 [00:00<00:05,  5.13it/s][A[A

  9% 3/32 [00:00<00:05,  5.46it/s][A[A

 12% 4/32 [00:00<00:05,  5.45it/s][A[A

 16% 5/32 [00:01<00:05,  4.66it/s][A[A

 19% 6/32 [00:01<00:05,  4.55it/s][A[A

 22% 7/32 [00:01<00:05,  4.68it/s][A[A

 25% 8/32 [00:01<00:04,  5.07it/s][A[A

 28% 9/32 [00:01<00:04,  4.70it/s][A[A

 31% 10/32 [00:03<00:11,  1.96it/s][A[A

 34% 11/32 [00:03<00:09,  2.33it/s][A[A

 38% 12/32 [00:03<00:07,  2.73it/s][A[A

 41% 13/32 [00:03<00:06,  3.14it/s][A[A

 44% 14/32 [00:03<00:05,  3.32it/s][A[A

 47% 15/32 [00:04<00:04,  3.57it/s][A[A

 50% 16/32 [00:04<00:04,  3.71it/s][A[A

 53% 17/32 [00:04<00:03,  4.04it/s][A[A

 56% 18/32 [00:04<00:03,  4.13it/s][A[A

 59% 19/32 [00:05<00:03,  4.25it/s][A[A

 62% 20/32 [00:05<00:02,  4.49it/s][A[A

 66% 21/32 [00:06<00:05,  1.94it/s][A[A

 69% 22/32 [00:06<00:04,  2.35it/s][A[A

 72% 23/32 [00:06<00:03,  2.74it/s][A[A

 75% 24/32 [00:07<00:02,  3.13it/s][A[A

 78% 25/32 [00:07<00:02,  3.39it/s][A[A

 81% 26/32 [00:07<00:01,  3.75it/s][A[A

 84% 27/32 [00:07<00:01,  3.87it/s][A[A

 88% 28/32 [00:08<00:01,  3.67it/s][A[A

 91% 29/32 [00:09<00:01,  1.76it/s][A[A

 94% 30/32 [00:09<00:00,  2.18it/s][A[A

 97% 31/32 [00:09<00:00,  2.75it/s][A[A

100% 32/32 [00:09<00:00,  3.18it/s][A[A100% 32/32 [00:09<00:00,  3.22it/s]
Meta loss on this task batch = 4.4189e-01, PNorm = 41.1031, GNorm = 0.1074

 32% 6/19 [00:58<02:09,  9.97s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.97it/s][A[A

  6% 2/32 [00:00<00:05,  5.09it/s][A[A

  9% 3/32 [00:00<00:06,  4.82it/s][A[A

 12% 4/32 [00:00<00:06,  4.53it/s][A[A

 16% 5/32 [00:01<00:05,  4.66it/s][A[A

 19% 6/32 [00:01<00:05,  4.76it/s][A[A

 22% 7/32 [00:01<00:05,  4.68it/s][A[A

 25% 8/32 [00:01<00:04,  4.93it/s][A[A

 28% 9/32 [00:01<00:04,  4.92it/s][A[A

 31% 10/32 [00:02<00:04,  4.78it/s][A[A

 34% 11/32 [00:02<00:04,  4.70it/s][A[A

 38% 12/32 [00:02<00:04,  4.64it/s][A[A

 41% 13/32 [00:03<00:09,  1.96it/s][A[A

 44% 14/32 [00:03<00:07,  2.40it/s][A[A

 47% 15/32 [00:04<00:06,  2.73it/s][A[A

 50% 16/32 [00:04<00:05,  3.02it/s][A[A

 53% 17/32 [00:04<00:04,  3.29it/s][A[A

 56% 18/32 [00:04<00:03,  3.66it/s][A[A

 59% 19/32 [00:05<00:03,  3.87it/s][A[A

 62% 20/32 [00:05<00:03,  3.94it/s][A[A

 66% 21/32 [00:05<00:02,  4.10it/s][A[A

 69% 22/32 [00:05<00:02,  4.20it/s][A[A

 72% 23/32 [00:06<00:02,  4.24it/s][A[A

 75% 24/32 [00:07<00:04,  1.88it/s][A[A

 78% 25/32 [00:07<00:02,  2.37it/s][A[A

 81% 26/32 [00:07<00:02,  2.74it/s][A[A

 84% 27/32 [00:07<00:01,  3.10it/s][A[A

 88% 28/32 [00:08<00:01,  3.37it/s][A[A

 91% 29/32 [00:08<00:00,  3.87it/s][A[A

 94% 30/32 [00:08<00:00,  4.14it/s][A[A

 97% 31/32 [00:08<00:00,  4.24it/s][A[A

100% 32/32 [00:08<00:00,  4.42it/s][A[A100% 32/32 [00:08<00:00,  3.60it/s]
Meta loss on this task batch = 4.7669e-01, PNorm = 41.1369, GNorm = 0.0709

 37% 7/19 [01:08<01:58,  9.89s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.50it/s][A[A

  6% 2/32 [00:00<00:07,  4.26it/s][A[A

  9% 3/32 [00:01<00:15,  1.87it/s][A[A

 12% 4/32 [00:01<00:12,  2.22it/s][A[A

 16% 5/32 [00:02<00:10,  2.61it/s][A[A

 19% 6/32 [00:02<00:08,  3.01it/s][A[A

 22% 7/32 [00:02<00:07,  3.52it/s][A[A

 25% 8/32 [00:02<00:06,  3.59it/s][A[A

 28% 9/32 [00:03<00:06,  3.72it/s][A[A

 31% 10/32 [00:03<00:05,  3.85it/s][A[A

 34% 11/32 [00:03<00:05,  4.06it/s][A[A

 38% 12/32 [00:04<00:10,  1.87it/s][A[A

 41% 13/32 [00:04<00:08,  2.31it/s][A[A

 44% 14/32 [00:05<00:06,  2.84it/s][A[A

 47% 15/32 [00:05<00:05,  3.35it/s][A[A

 50% 16/32 [00:05<00:04,  3.61it/s][A[A

 53% 17/32 [00:05<00:04,  3.65it/s][A[A

 56% 18/32 [00:06<00:03,  3.87it/s][A[A

 59% 19/32 [00:06<00:03,  3.78it/s][A[A

 62% 20/32 [00:06<00:03,  3.96it/s][A[A

 66% 21/32 [00:06<00:02,  4.14it/s][A[A

 69% 22/32 [00:07<00:05,  1.91it/s][A[A

 72% 23/32 [00:08<00:03,  2.41it/s][A[A

 75% 24/32 [00:08<00:02,  2.74it/s][A[A

 78% 25/32 [00:08<00:02,  3.04it/s][A[A

 81% 26/32 [00:08<00:01,  3.53it/s][A[A

 84% 27/32 [00:08<00:01,  3.91it/s][A[A

 88% 28/32 [00:09<00:00,  4.24it/s][A[A

 91% 29/32 [00:09<00:00,  4.45it/s][A[A

 94% 30/32 [00:09<00:00,  4.68it/s][A[A

 97% 31/32 [00:09<00:00,  5.00it/s][A[A

100% 32/32 [00:09<00:00,  4.97it/s][A[A100% 32/32 [00:09<00:00,  3.24it/s]
Meta loss on this task batch = 3.0588e-01, PNorm = 41.1712, GNorm = 0.0758

 42% 8/19 [01:19<01:51, 10.12s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:37,  1.21s/it][A[A

  6% 2/32 [00:01<00:27,  1.11it/s][A[A

  9% 3/32 [00:01<00:19,  1.46it/s][A[A

 12% 4/32 [00:01<00:14,  1.88it/s][A[A

 16% 5/32 [00:01<00:11,  2.35it/s][A[A

 19% 6/32 [00:02<00:09,  2.76it/s][A[A

 22% 7/32 [00:02<00:07,  3.20it/s][A[A

 25% 8/32 [00:02<00:06,  3.64it/s][A[A

 28% 9/32 [00:02<00:05,  4.08it/s][A[A

 31% 10/32 [00:02<00:05,  4.32it/s][A[A

 34% 11/32 [00:03<00:04,  4.54it/s][A[A

 38% 12/32 [00:03<00:04,  4.81it/s][A[A

 41% 13/32 [00:03<00:03,  5.01it/s][A[A

 44% 14/32 [00:03<00:03,  5.18it/s][A[A

 47% 15/32 [00:03<00:03,  5.18it/s][A[A

 50% 16/32 [00:03<00:03,  5.31it/s][A[A

 53% 17/32 [00:04<00:02,  5.34it/s][A[A

 56% 18/32 [00:04<00:02,  5.32it/s][A[A

 59% 19/32 [00:05<00:06,  2.05it/s][A[A

 62% 20/32 [00:05<00:04,  2.49it/s][A[A

 66% 21/32 [00:05<00:03,  2.95it/s][A[A

 69% 22/32 [00:06<00:02,  3.41it/s][A[A

 72% 23/32 [00:06<00:02,  3.68it/s][A[A

 75% 24/32 [00:06<00:01,  4.03it/s][A[A

 78% 25/32 [00:06<00:01,  4.21it/s][A[A

 81% 26/32 [00:06<00:01,  4.56it/s][A[A

 84% 27/32 [00:07<00:01,  4.80it/s][A[A

 88% 28/32 [00:07<00:00,  5.03it/s][A[A

 91% 29/32 [00:07<00:00,  5.04it/s][A[A

 94% 30/32 [00:07<00:00,  5.05it/s][A[A

 97% 31/32 [00:07<00:00,  5.23it/s][A[A

100% 32/32 [00:08<00:00,  5.38it/s][A[A100% 32/32 [00:08<00:00,  3.98it/s]
Meta loss on this task batch = 4.7729e-02, PNorm = 41.2067, GNorm = 0.1494

 47% 9/19 [01:28<01:37,  9.74s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:36,  1.17s/it][A[A

  6% 2/32 [00:01<00:26,  1.13it/s][A[A

  9% 3/32 [00:01<00:19,  1.49it/s][A[A

 12% 4/32 [00:01<00:14,  1.89it/s][A[A

 16% 5/32 [00:01<00:11,  2.34it/s][A[A

 19% 6/32 [00:02<00:09,  2.78it/s][A[A

 22% 7/32 [00:02<00:07,  3.24it/s][A[A

 25% 8/32 [00:02<00:06,  3.66it/s][A[A

 28% 9/32 [00:02<00:05,  4.03it/s][A[A

 31% 10/32 [00:02<00:05,  4.31it/s][A[A

 34% 11/32 [00:03<00:04,  4.53it/s][A[A

 38% 12/32 [00:03<00:04,  4.77it/s][A[A

 41% 13/32 [00:03<00:03,  4.91it/s][A[A

 44% 14/32 [00:04<00:08,  2.01it/s][A[A

 47% 15/32 [00:04<00:06,  2.48it/s][A[A

 50% 16/32 [00:05<00:05,  2.83it/s][A[A

 53% 17/32 [00:05<00:04,  3.27it/s][A[A

 56% 18/32 [00:05<00:03,  3.69it/s][A[A

 59% 19/32 [00:05<00:03,  3.82it/s][A[A

 62% 20/32 [00:05<00:02,  4.18it/s][A[A

 66% 21/32 [00:06<00:02,  4.36it/s][A[A

 69% 22/32 [00:06<00:02,  4.62it/s][A[A

 72% 23/32 [00:06<00:01,  4.89it/s][A[A

 75% 24/32 [00:07<00:04,  2.00it/s][A[A

 78% 25/32 [00:07<00:02,  2.46it/s][A[A

 81% 26/32 [00:08<00:02,  2.89it/s][A[A

 84% 27/32 [00:08<00:01,  3.21it/s][A[A

 88% 28/32 [00:08<00:01,  3.48it/s][A[A

 91% 29/32 [00:08<00:00,  3.62it/s][A[A

 94% 30/32 [00:08<00:00,  3.84it/s][A[A

 97% 31/32 [00:09<00:00,  4.01it/s][A[A

100% 32/32 [00:09<00:00,  4.05it/s][A[A100% 32/32 [00:09<00:00,  3.39it/s]
Meta loss on this task batch = 1.6646e-01, PNorm = 41.2454, GNorm = 0.0937

 53% 10/19 [01:38<01:29,  9.90s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.85it/s][A[A

  6% 2/32 [00:00<00:07,  3.94it/s][A[A

  9% 3/32 [00:01<00:15,  1.81it/s][A[A

 12% 4/32 [00:01<00:12,  2.27it/s][A[A

 16% 5/32 [00:02<00:10,  2.63it/s][A[A

 19% 6/32 [00:02<00:08,  2.95it/s][A[A

 22% 7/32 [00:02<00:07,  3.27it/s][A[A

 25% 8/32 [00:02<00:06,  3.46it/s][A[A

 28% 9/32 [00:03<00:06,  3.64it/s][A[A

 31% 10/32 [00:03<00:05,  3.82it/s][A[A

 34% 11/32 [00:03<00:05,  3.90it/s][A[A

 38% 12/32 [00:04<00:11,  1.81it/s][A[A

 41% 13/32 [00:05<00:08,  2.18it/s][A[A

 44% 14/32 [00:05<00:07,  2.53it/s][A[A

 47% 15/32 [00:05<00:05,  3.01it/s][A[A

 50% 16/32 [00:05<00:04,  3.48it/s][A[A

 53% 17/32 [00:05<00:04,  3.69it/s][A[A

 56% 18/32 [00:06<00:03,  3.81it/s][A[A

 59% 19/32 [00:06<00:03,  3.90it/s][A[A

 62% 20/32 [00:07<00:06,  1.82it/s][A[A

 66% 21/32 [00:07<00:04,  2.26it/s][A[A

 69% 22/32 [00:08<00:03,  2.62it/s][A[A

 72% 23/32 [00:08<00:03,  2.93it/s][A[A

 75% 24/32 [00:08<00:02,  3.17it/s][A[A

 78% 25/32 [00:08<00:02,  3.41it/s][A[A

 81% 26/32 [00:09<00:01,  3.59it/s][A[A

 84% 27/32 [00:09<00:01,  3.71it/s][A[A

 88% 28/32 [00:10<00:02,  1.81it/s][A[A

 91% 29/32 [00:10<00:01,  2.18it/s][A[A

 94% 30/32 [00:11<00:00,  2.58it/s][A[A

 97% 31/32 [00:11<00:00,  2.91it/s][A[A

100% 32/32 [00:11<00:00,  3.23it/s][A[A100% 32/32 [00:11<00:00,  2.79it/s]
Meta loss on this task batch = 5.6967e-01, PNorm = 41.2742, GNorm = 0.2840

 58% 11/19 [01:50<01:24, 10.61s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  3.91it/s][A[A

  6% 2/32 [00:00<00:07,  4.04it/s][A[A

  9% 3/32 [00:00<00:07,  4.04it/s][A[A

 12% 4/32 [00:00<00:06,  4.45it/s][A[A

 16% 5/32 [00:01<00:06,  4.36it/s][A[A

 19% 6/32 [00:02<00:13,  1.87it/s][A[A

 22% 7/32 [00:02<00:11,  2.27it/s][A[A

 25% 8/32 [00:02<00:09,  2.64it/s][A[A

 28% 9/32 [00:03<00:07,  3.00it/s][A[A

 31% 10/32 [00:03<00:06,  3.29it/s][A[A

 34% 11/32 [00:03<00:05,  3.50it/s][A[A

 38% 12/32 [00:03<00:05,  3.75it/s][A[A

 41% 13/32 [00:04<00:05,  3.80it/s][A[A

 44% 14/32 [00:04<00:04,  4.00it/s][A[A

 47% 15/32 [00:05<00:09,  1.81it/s][A[A

 50% 16/32 [00:05<00:07,  2.17it/s][A[A

 53% 17/32 [00:06<00:05,  2.51it/s][A[A

 56% 18/32 [00:06<00:04,  2.81it/s][A[A

 59% 19/32 [00:06<00:04,  3.11it/s][A[A

 62% 20/32 [00:06<00:03,  3.35it/s][A[A

 66% 21/32 [00:07<00:06,  1.72it/s][A[A

 69% 22/32 [00:08<00:04,  2.07it/s][A[A

 72% 23/32 [00:08<00:03,  2.44it/s][A[A

 75% 24/32 [00:08<00:02,  2.92it/s][A[A

 78% 25/32 [00:08<00:02,  3.23it/s][A[A

 81% 26/32 [00:09<00:01,  3.59it/s][A[A

 84% 27/32 [00:09<00:01,  3.77it/s][A[A

 88% 28/32 [00:09<00:01,  3.99it/s][A[A

 91% 29/32 [00:09<00:00,  4.28it/s][A[A

 94% 30/32 [00:11<00:01,  1.86it/s][A[A

 97% 31/32 [00:11<00:00,  2.29it/s][A[A

100% 32/32 [00:11<00:00,  2.66it/s][A[A100% 32/32 [00:11<00:00,  2.80it/s]
Meta loss on this task batch = 5.3336e-01, PNorm = 41.3034, GNorm = 0.0901

 63% 12/19 [02:02<01:17, 11.11s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.04it/s][A[A

  6% 2/32 [00:00<00:07,  4.14it/s][A[A

  9% 3/32 [00:00<00:06,  4.22it/s][A[A

 12% 4/32 [00:00<00:06,  4.22it/s][A[A

 16% 5/32 [00:01<00:06,  4.19it/s][A[A

 19% 6/32 [00:01<00:06,  4.18it/s][A[A

 22% 7/32 [00:01<00:05,  4.27it/s][A[A

 25% 8/32 [00:01<00:05,  4.23it/s][A[A

 28% 9/32 [00:02<00:05,  4.42it/s][A[A

 31% 10/32 [00:02<00:04,  4.42it/s][A[A

 34% 11/32 [00:02<00:04,  4.71it/s][A[A

 38% 12/32 [00:03<00:10,  1.93it/s][A[A

 41% 13/32 [00:03<00:08,  2.31it/s][A[A

 44% 14/32 [00:04<00:06,  2.67it/s][A[A

 47% 15/32 [00:04<00:05,  3.05it/s][A[A

 50% 16/32 [00:04<00:04,  3.44it/s][A[A

 53% 17/32 [00:04<00:04,  3.61it/s][A[A

 56% 18/32 [00:05<00:03,  4.02it/s][A[A

 59% 19/32 [00:05<00:03,  3.97it/s][A[A

 62% 20/32 [00:05<00:03,  3.99it/s][A[A

 66% 21/32 [00:06<00:06,  1.83it/s][A[A

 69% 22/32 [00:07<00:04,  2.20it/s][A[A

 72% 23/32 [00:07<00:03,  2.56it/s][A[A

 75% 24/32 [00:07<00:02,  3.00it/s][A[A

 78% 25/32 [00:07<00:02,  3.30it/s][A[A

 81% 26/32 [00:07<00:01,  3.51it/s][A[A

 84% 27/32 [00:08<00:01,  3.78it/s][A[A

 88% 28/32 [00:08<00:01,  3.91it/s][A[A

 91% 29/32 [00:08<00:00,  3.91it/s][A[A

 94% 30/32 [00:08<00:00,  4.26it/s][A[A

 97% 31/32 [00:09<00:00,  4.48it/s][A[A

100% 32/32 [00:10<00:00,  1.96it/s][A[A100% 32/32 [00:10<00:00,  3.13it/s]
Meta loss on this task batch = 5.5194e-01, PNorm = 41.3334, GNorm = 0.0754

 68% 13/19 [02:13<01:06, 11.08s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.03it/s][A[A

  6% 2/32 [00:00<00:07,  4.08it/s][A[A

  9% 3/32 [00:00<00:06,  4.23it/s][A[A

 12% 4/32 [00:00<00:06,  4.33it/s][A[A

 16% 5/32 [00:01<00:06,  4.27it/s][A[A

 19% 6/32 [00:01<00:06,  4.28it/s][A[A

 22% 7/32 [00:01<00:05,  4.20it/s][A[A

 25% 8/32 [00:01<00:05,  4.29it/s][A[A

 28% 9/32 [00:02<00:04,  4.67it/s][A[A

 31% 10/32 [00:02<00:04,  4.53it/s][A[A

 34% 11/32 [00:02<00:04,  4.48it/s][A[A

 38% 12/32 [00:02<00:04,  4.38it/s][A[A

 41% 13/32 [00:03<00:09,  1.91it/s][A[A

 44% 14/32 [00:04<00:07,  2.29it/s][A[A

 47% 15/32 [00:04<00:06,  2.66it/s][A[A

 50% 16/32 [00:04<00:05,  2.94it/s][A[A

 53% 17/32 [00:04<00:04,  3.22it/s][A[A

 56% 18/32 [00:05<00:04,  3.48it/s][A[A

 59% 19/32 [00:05<00:03,  3.86it/s][A[A

 62% 20/32 [00:05<00:03,  3.92it/s][A[A

 66% 21/32 [00:05<00:02,  4.07it/s][A[A

 69% 22/32 [00:07<00:05,  1.84it/s][A[A

 72% 23/32 [00:07<00:04,  2.20it/s][A[A

 75% 24/32 [00:07<00:02,  2.69it/s][A[A

 78% 25/32 [00:07<00:02,  3.00it/s][A[A

 81% 26/32 [00:07<00:01,  3.27it/s][A[A

 84% 27/32 [00:08<00:01,  3.48it/s][A[A

 88% 28/32 [00:09<00:02,  1.73it/s][A[A

 91% 29/32 [00:09<00:01,  2.11it/s][A[A

 94% 30/32 [00:09<00:00,  2.53it/s][A[A

 97% 31/32 [00:10<00:00,  2.88it/s][A[A

100% 32/32 [00:10<00:00,  3.13it/s][A[A100% 32/32 [00:10<00:00,  3.08it/s]
Meta loss on this task batch = 5.4731e-01, PNorm = 41.3620, GNorm = 0.0827

 74% 14/19 [02:25<00:55, 11.12s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.08it/s][A[A

  6% 2/32 [00:00<00:07,  4.01it/s][A[A

  9% 3/32 [00:00<00:07,  3.99it/s][A[A

 12% 4/32 [00:00<00:06,  4.10it/s][A[A

 16% 5/32 [00:01<00:06,  4.21it/s][A[A

 19% 6/32 [00:01<00:06,  4.10it/s][A[A

 22% 7/32 [00:01<00:06,  4.14it/s][A[A

 25% 8/32 [00:01<00:05,  4.38it/s][A[A

 28% 9/32 [00:02<00:05,  4.25it/s][A[A

 31% 10/32 [00:03<00:11,  1.92it/s][A[A

 34% 11/32 [00:03<00:08,  2.44it/s][A[A

 38% 12/32 [00:03<00:06,  2.87it/s][A[A

 41% 13/32 [00:03<00:05,  3.31it/s][A[A

 44% 14/32 [00:04<00:05,  3.55it/s][A[A

 47% 15/32 [00:04<00:04,  3.86it/s][A[A

 50% 16/32 [00:04<00:03,  4.03it/s][A[A

 53% 17/32 [00:04<00:03,  4.06it/s][A[A

 56% 18/32 [00:05<00:03,  4.15it/s][A[A

 59% 19/32 [00:05<00:02,  4.36it/s][A[A

 62% 20/32 [00:05<00:02,  4.60it/s][A[A

 66% 21/32 [00:05<00:02,  4.73it/s][A[A

 69% 22/32 [00:06<00:05,  1.93it/s][A[A

 72% 23/32 [00:07<00:03,  2.27it/s][A[A

 75% 24/32 [00:07<00:02,  2.69it/s][A[A

 78% 25/32 [00:07<00:02,  3.05it/s][A[A

 81% 26/32 [00:07<00:01,  3.21it/s][A[A

 84% 27/32 [00:08<00:01,  3.62it/s][A[A

 88% 28/32 [00:08<00:00,  4.01it/s][A[A

 91% 29/32 [00:08<00:00,  4.09it/s][A[A

 94% 30/32 [00:08<00:00,  4.15it/s][A[A

 97% 31/32 [00:08<00:00,  4.23it/s][A[A

100% 32/32 [00:09<00:00,  4.30it/s][A[A100% 32/32 [00:09<00:00,  3.51it/s]
Meta loss on this task batch = 5.0317e-01, PNorm = 41.3859, GNorm = 0.1274

 79% 15/19 [02:34<00:43, 10.76s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:37,  1.22s/it][A[A

  6% 2/32 [00:01<00:27,  1.08it/s][A[A

  9% 3/32 [00:01<00:20,  1.39it/s][A[A

 12% 4/32 [00:01<00:16,  1.73it/s][A[A

 16% 5/32 [00:02<00:12,  2.16it/s][A[A

 19% 6/32 [00:02<00:10,  2.55it/s][A[A

 22% 7/32 [00:02<00:08,  2.90it/s][A[A

 25% 8/32 [00:02<00:07,  3.23it/s][A[A

 28% 9/32 [00:03<00:06,  3.49it/s][A[A

 31% 10/32 [00:03<00:05,  3.67it/s][A[A

 34% 11/32 [00:04<00:11,  1.79it/s][A[A

 38% 12/32 [00:04<00:09,  2.18it/s][A[A

 41% 13/32 [00:04<00:07,  2.57it/s][A[A

 44% 14/32 [00:05<00:06,  2.93it/s][A[A

 47% 15/32 [00:05<00:05,  3.21it/s][A[A

 50% 16/32 [00:05<00:04,  3.49it/s][A[A

 53% 17/32 [00:05<00:04,  3.69it/s][A[A

 56% 18/32 [00:06<00:03,  4.04it/s][A[A

 59% 19/32 [00:06<00:03,  4.11it/s][A[A

 62% 20/32 [00:06<00:02,  4.24it/s][A[A

 66% 21/32 [00:06<00:02,  4.33it/s][A[A

 69% 22/32 [00:06<00:02,  4.48it/s][A[A

 72% 23/32 [00:07<00:02,  4.46it/s][A[A

 75% 24/32 [00:08<00:04,  1.87it/s][A[A

 78% 25/32 [00:08<00:03,  2.30it/s][A[A

 81% 26/32 [00:08<00:02,  2.65it/s][A[A

 84% 27/32 [00:09<00:01,  2.91it/s][A[A

 88% 28/32 [00:09<00:01,  3.25it/s][A[A

 91% 29/32 [00:09<00:00,  3.35it/s][A[A

 94% 30/32 [00:09<00:00,  3.57it/s][A[A

 97% 31/32 [00:10<00:00,  3.78it/s][A[A

100% 32/32 [00:10<00:00,  3.91it/s][A[A100% 32/32 [00:10<00:00,  3.08it/s]
Meta loss on this task batch = 5.3912e-01, PNorm = 41.4050, GNorm = 0.0696

 84% 16/19 [02:46<00:32, 10.89s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.36it/s][A[A

  6% 2/32 [00:00<00:06,  4.76it/s][A[A

  9% 3/32 [00:00<00:05,  5.05it/s][A[A

 12% 4/32 [00:01<00:13,  2.01it/s][A[A

 16% 5/32 [00:02<00:11,  2.40it/s][A[A

 19% 6/32 [00:02<00:09,  2.83it/s][A[A

 22% 7/32 [00:02<00:07,  3.27it/s][A[A

 25% 8/32 [00:02<00:07,  3.42it/s][A[A

 28% 9/32 [00:02<00:06,  3.67it/s][A[A

 31% 10/32 [00:03<00:05,  4.19it/s][A[A

 34% 11/32 [00:03<00:04,  4.23it/s][A[A

 38% 12/32 [00:03<00:04,  4.33it/s][A[A

 41% 13/32 [00:03<00:04,  4.55it/s][A[A

 44% 14/32 [00:03<00:03,  4.60it/s][A[A

 47% 15/32 [00:04<00:03,  4.45it/s][A[A

 50% 16/32 [00:05<00:08,  1.89it/s][A[A

 53% 17/32 [00:05<00:06,  2.28it/s][A[A

 56% 18/32 [00:05<00:05,  2.73it/s][A[A

 59% 19/32 [00:06<00:04,  3.12it/s][A[A

 62% 20/32 [00:06<00:03,  3.41it/s][A[A

 66% 21/32 [00:06<00:02,  3.87it/s][A[A

 69% 22/32 [00:06<00:02,  4.00it/s][A[A

 72% 23/32 [00:06<00:02,  4.04it/s][A[A

 75% 24/32 [00:08<00:04,  1.79it/s][A[A

 78% 25/32 [00:08<00:03,  2.20it/s][A[A

 81% 26/32 [00:08<00:02,  2.58it/s][A[A

 84% 27/32 [00:08<00:01,  3.02it/s][A[A

 88% 28/32 [00:09<00:01,  3.38it/s][A[A

 91% 29/32 [00:09<00:00,  3.66it/s][A[A

 94% 30/32 [00:09<00:00,  4.11it/s][A[A

 97% 31/32 [00:09<00:00,  4.35it/s][A[A

100% 32/32 [00:09<00:00,  4.21it/s][A[A100% 32/32 [00:09<00:00,  3.22it/s]
Meta loss on this task batch = 4.2458e-01, PNorm = 41.4248, GNorm = 0.0551

 89% 17/19 [02:56<00:21, 10.85s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.91it/s][A[A

  6% 2/32 [00:00<00:06,  4.78it/s][A[A

  9% 3/32 [00:01<00:15,  1.93it/s][A[A

 12% 4/32 [00:01<00:12,  2.32it/s][A[A

 16% 5/32 [00:02<00:09,  2.80it/s][A[A

 19% 6/32 [00:02<00:07,  3.28it/s][A[A

 22% 7/32 [00:02<00:07,  3.46it/s][A[A

 25% 8/32 [00:02<00:06,  3.58it/s][A[A

 28% 9/32 [00:02<00:05,  3.97it/s][A[A

 31% 10/32 [00:03<00:05,  4.11it/s][A[A

 34% 11/32 [00:03<00:04,  4.22it/s][A[A

 38% 12/32 [00:03<00:04,  4.44it/s][A[A

 41% 13/32 [00:03<00:04,  4.31it/s][A[A

 44% 14/32 [00:04<00:03,  4.50it/s][A[A

 47% 15/32 [00:04<00:03,  4.31it/s][A[A

 50% 16/32 [00:05<00:08,  1.90it/s][A[A

 53% 17/32 [00:05<00:06,  2.33it/s][A[A

 56% 18/32 [00:05<00:05,  2.72it/s][A[A

 59% 19/32 [00:06<00:04,  3.03it/s][A[A

 62% 20/32 [00:06<00:03,  3.35it/s][A[A

 66% 21/32 [00:06<00:03,  3.57it/s][A[A

 69% 22/32 [00:06<00:02,  3.90it/s][A[A

 72% 23/32 [00:07<00:02,  3.84it/s][A[A

 75% 24/32 [00:07<00:02,  3.75it/s][A[A

 78% 25/32 [00:07<00:01,  3.95it/s][A[A

 81% 26/32 [00:07<00:01,  4.13it/s][A[A

 84% 27/32 [00:09<00:02,  1.82it/s][A[A

 88% 28/32 [00:09<00:01,  2.27it/s][A[A

 91% 29/32 [00:09<00:01,  2.64it/s][A[A

 94% 30/32 [00:09<00:00,  2.95it/s][A[A

 97% 31/32 [00:09<00:00,  3.35it/s][A[A

100% 32/32 [00:10<00:00,  3.72it/s][A[A100% 32/32 [00:10<00:00,  3.15it/s]
Meta loss on this task batch = 4.9557e-01, PNorm = 41.4447, GNorm = 0.0763

 95% 18/19 [03:07<00:10, 10.89s/it][A

  0% 0/23 [00:00<?, ?it/s][A[A

  4% 1/23 [00:00<00:05,  4.15it/s][A[A

  9% 2/23 [00:00<00:04,  4.33it/s][A[A

 13% 3/23 [00:00<00:04,  4.38it/s][A[A

 17% 4/23 [00:00<00:04,  4.58it/s][A[A

 22% 5/23 [00:00<00:03,  5.21it/s][A[A

 26% 6/23 [00:01<00:03,  5.12it/s][A[A

 30% 7/23 [00:01<00:03,  4.80it/s][A[A

 35% 8/23 [00:01<00:03,  4.66it/s][A[A

 39% 9/23 [00:01<00:02,  4.95it/s][A[A

 43% 10/23 [00:02<00:02,  4.58it/s][A[A

 48% 11/23 [00:02<00:02,  5.05it/s][A[A

 52% 12/23 [00:02<00:02,  4.71it/s][A[A

 57% 13/23 [00:03<00:05,  1.96it/s][A[A

 61% 14/23 [00:03<00:03,  2.43it/s][A[A

 65% 15/23 [00:04<00:03,  2.50it/s][A[A

 70% 16/23 [00:04<00:02,  2.79it/s][A[A

 74% 17/23 [00:04<00:01,  3.09it/s][A[A

 78% 18/23 [00:04<00:01,  3.49it/s][A[A

 83% 19/23 [00:05<00:01,  3.73it/s][A[A

 87% 20/23 [00:05<00:00,  4.11it/s][A[A

 91% 21/23 [00:06<00:01,  1.88it/s][A[A

 96% 22/23 [00:06<00:00,  2.33it/s][A[A

100% 23/23 [00:06<00:00,  2.72it/s][A[A100% 23/23 [00:06<00:00,  3.29it/s]
Meta loss on this task batch = 4.3698e-01, PNorm = 41.4620, GNorm = 0.0929

100% 19/19 [03:15<00:00,  9.89s/it][A100% 19/19 [03:15<00:00, 10.29s/it]
Took 195.49500823020935 seconds to complete one epoch of meta training

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/20 [00:00<?, ?it/s][A[A


  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 30.86it/s]


  5% 1/20 [00:00<00:03,  5.68it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 21.30it/s]


 10% 2/20 [00:00<00:03,  5.58it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 18.26it/s][A[A[A100% 3/3 [00:00<00:00, 20.09it/s]


 15% 3/20 [00:00<00:03,  4.71it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 16.58it/s]


 20% 4/20 [00:00<00:03,  4.73it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 15.44it/s][A[A[A100% 4/4 [00:00<00:00, 17.77it/s]


 25% 5/20 [00:01<00:03,  3.90it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.79it/s][A[A[A100% 4/4 [00:00<00:00, 16.38it/s]


 30% 6/20 [00:01<00:04,  3.37it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.48it/s][A[A[A100% 4/4 [00:00<00:00, 21.23it/s]


 35% 7/20 [00:01<00:04,  3.19it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 17.08it/s][A[A[A100% 4/4 [00:00<00:00, 19.44it/s]


 40% 8/20 [00:02<00:04,  2.99it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:01<00:00,  2.62it/s][A[A[A100% 4/4 [00:01<00:00,  3.43it/s]


 45% 9/20 [00:03<00:06,  1.58it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.52it/s][A[A[A


100% 4/4 [00:00<00:00, 19.40it/s][A[A[A100% 4/4 [00:00<00:00, 19.29it/s]


 50% 10/20 [00:04<00:05,  1.80it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 17.19it/s][A[A[A100% 4/4 [00:00<00:00, 19.75it/s]


 55% 11/20 [00:04<00:04,  2.01it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.87it/s][A[A[A100% 4/4 [00:00<00:00, 22.31it/s]


 60% 12/20 [00:04<00:03,  2.22it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.67it/s][A[A[A100% 3/3 [00:00<00:00, 13.88it/s]


 65% 13/20 [00:05<00:03,  2.31it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.25it/s][A[A[A100% 3/3 [00:00<00:00, 13.85it/s]


 70% 14/20 [00:05<00:02,  2.38it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 14.21it/s][A[A[A100% 4/4 [00:00<00:00, 18.51it/s]


 75% 15/20 [00:05<00:02,  2.44it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 33% 1/3 [00:01<00:02,  1.07s/it][A[A[A


100% 3/3 [00:01<00:00,  1.31it/s][A[A[A100% 3/3 [00:01<00:00,  2.56it/s]


 80% 16/20 [00:07<00:02,  1.46it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 12.29it/s][A[A[A100% 3/3 [00:00<00:00, 15.19it/s]


 85% 17/20 [00:07<00:01,  1.69it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 23.45it/s]


 90% 18/20 [00:07<00:00,  2.06it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 15.68it/s][A[A[A100% 3/3 [00:00<00:00, 20.04it/s]


 95% 19/20 [00:08<00:00,  2.40it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 13.67it/s]


100% 20/20 [00:08<00:00,  2.74it/s][A[A100% 20/20 [00:08<00:00,  2.39it/s]

100% 1/1 [00:08<00:00,  8.36s/it][A100% 1/1 [00:08<00:00,  8.36s/it]
Took 203.8580231666565 seconds to complete one epoch of meta training and validating
Meta validation score auc = 0.644236
 83% 25/30 [1:20:39<16:10, 194.00s/it]Epoch 25

  0% 0/19 [00:00<?, ?it/s][A

  0% 0/32 [00:00<?, ?it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  3% 1/32 [00:00<00:05,  5.98it/s][A[A

  6% 2/32 [00:00<00:05,  5.46it/s][A[A

  9% 3/32 [00:00<00:05,  5.48it/s][A[A

 12% 4/32 [00:00<00:04,  5.83it/s][A[A

 16% 5/32 [00:00<00:04,  6.25it/s][A[A

 19% 6/32 [00:01<00:04,  6.05it/s][A[A

 22% 7/32 [00:01<00:04,  6.07it/s][A[A

 25% 8/32 [00:01<00:04,  5.77it/s][A[A

 28% 9/32 [00:01<00:03,  6.33it/s][A[A

 31% 10/32 [00:01<00:03,  6.70it/s][A[A

 34% 11/32 [00:01<00:03,  6.06it/s][A[A

 38% 12/32 [00:03<00:09,  2.04it/s][A[A

 41% 13/32 [00:03<00:07,  2.41it/s][A[A

 44% 14/32 [00:03<00:06,  2.76it/s][A[A

 47% 15/32 [00:03<00:04,  3.42it/s][A[A

 50% 16/32 [00:03<00:04,  3.75it/s][A[A

 53% 17/32 [00:04<00:03,  4.14it/s][A[A

 56% 18/32 [00:04<00:03,  4.59it/s][A[A

 59% 19/32 [00:04<00:02,  5.01it/s][A[A

 62% 20/32 [00:04<00:02,  5.16it/s][A[A

 66% 21/32 [00:04<00:01,  5.83it/s][A[A

 69% 22/32 [00:04<00:01,  5.76it/s][A[A

 72% 23/32 [00:05<00:01,  6.09it/s][A[A

 75% 24/32 [00:05<00:01,  5.83it/s][A[A

 78% 25/32 [00:05<00:01,  5.74it/s][A[A

 81% 26/32 [00:05<00:01,  5.48it/s][A[A

 84% 27/32 [00:05<00:00,  5.89it/s][A[A

 88% 28/32 [00:05<00:00,  5.91it/s][A[A

 91% 29/32 [00:07<00:01,  2.05it/s][A[A

 94% 30/32 [00:07<00:00,  2.67it/s][A[A

 97% 31/32 [00:07<00:00,  3.18it/s][A[A

100% 32/32 [00:07<00:00,  3.69it/s][A[A100% 32/32 [00:07<00:00,  4.22it/s]
Meta loss on this task batch = 4.8952e-01, PNorm = 41.4824, GNorm = 0.1411

  5% 1/19 [00:08<02:29,  8.29s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:03,  8.10it/s][A[A

  6% 2/32 [00:00<00:03,  7.76it/s][A[A

  9% 3/32 [00:00<00:04,  6.93it/s][A[A

 12% 4/32 [00:00<00:04,  6.51it/s][A[A

 16% 5/32 [00:00<00:04,  6.20it/s][A[A

 19% 6/32 [00:00<00:04,  6.45it/s][A[A

 22% 7/32 [00:01<00:03,  6.87it/s][A[A

 25% 8/32 [00:01<00:03,  6.91it/s][A[A

 28% 9/32 [00:01<00:03,  6.74it/s][A[A

 31% 10/32 [00:01<00:03,  7.02it/s][A[A

 34% 11/32 [00:01<00:02,  7.06it/s][A[A

 38% 12/32 [00:01<00:03,  6.51it/s][A[A

 41% 13/32 [00:01<00:02,  6.66it/s][A[A

 44% 14/32 [00:02<00:02,  6.15it/s][A[A

 47% 15/32 [00:02<00:02,  6.10it/s][A[A

 50% 16/32 [00:02<00:02,  5.93it/s][A[A

 53% 17/32 [00:02<00:02,  5.74it/s][A[A

 56% 18/32 [00:03<00:06,  2.04it/s][A[A

 59% 19/32 [00:04<00:05,  2.59it/s][A[A

 62% 20/32 [00:04<00:04,  2.94it/s][A[A

 66% 21/32 [00:04<00:03,  3.59it/s][A[A

 69% 22/32 [00:04<00:02,  4.23it/s][A[A

 72% 23/32 [00:04<00:01,  4.87it/s][A[A

 75% 24/32 [00:04<00:01,  5.47it/s][A[A

 78% 25/32 [00:05<00:01,  5.28it/s][A[A

 81% 26/32 [00:05<00:01,  5.16it/s][A[A

 84% 27/32 [00:05<00:00,  5.38it/s][A[A

 88% 28/32 [00:05<00:00,  5.36it/s][A[A

 91% 29/32 [00:05<00:00,  5.80it/s][A[A

 94% 30/32 [00:05<00:00,  5.68it/s][A[A

 97% 31/32 [00:06<00:00,  5.95it/s][A[A

100% 32/32 [00:06<00:00,  6.28it/s][A[A100% 32/32 [00:06<00:00,  5.16it/s]
Meta loss on this task batch = 4.8395e-01, PNorm = 41.5071, GNorm = 0.1083

 11% 2/19 [00:15<02:13,  7.86s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.67it/s][A[A

  6% 2/32 [00:00<00:05,  5.56it/s][A[A

  9% 3/32 [00:01<00:13,  2.11it/s][A[A

 12% 4/32 [00:01<00:10,  2.55it/s][A[A

 16% 5/32 [00:01<00:08,  3.08it/s][A[A

 19% 6/32 [00:02<00:07,  3.48it/s][A[A

 22% 7/32 [00:02<00:06,  3.87it/s][A[A

 25% 8/32 [00:02<00:05,  4.42it/s][A[A

 28% 9/32 [00:02<00:04,  4.86it/s][A[A

 31% 10/32 [00:02<00:04,  4.72it/s][A[A

 34% 11/32 [00:03<00:04,  4.59it/s][A[A

 38% 12/32 [00:03<00:04,  4.81it/s][A[A

 41% 13/32 [00:03<00:03,  4.76it/s][A[A

 44% 14/32 [00:03<00:03,  4.71it/s][A[A

 47% 15/32 [00:03<00:03,  4.78it/s][A[A

 50% 16/32 [00:04<00:02,  5.36it/s][A[A

 53% 17/32 [00:05<00:07,  2.04it/s][A[A

 56% 18/32 [00:05<00:05,  2.60it/s][A[A

 59% 19/32 [00:05<00:04,  3.19it/s][A[A

 62% 20/32 [00:05<00:03,  3.82it/s][A[A

 66% 21/32 [00:05<00:02,  4.48it/s][A[A

 69% 22/32 [00:05<00:01,  5.01it/s][A[A

 72% 23/32 [00:06<00:01,  5.02it/s][A[A

 75% 24/32 [00:06<00:01,  4.80it/s][A[A

 78% 25/32 [00:06<00:01,  5.43it/s][A[A

 81% 26/32 [00:06<00:01,  5.97it/s][A[A

 84% 27/32 [00:06<00:00,  5.32it/s][A[A

 88% 28/32 [00:06<00:00,  5.54it/s][A[A

 91% 29/32 [00:07<00:00,  5.49it/s][A[A

 94% 30/32 [00:07<00:00,  5.06it/s][A[A

 97% 31/32 [00:07<00:00,  5.80it/s][A[A

100% 32/32 [00:07<00:00,  5.61it/s][A[A100% 32/32 [00:07<00:00,  4.15it/s]
Meta loss on this task batch = 5.0392e-01, PNorm = 41.5334, GNorm = 0.0617

 16% 3/19 [00:23<02:08,  8.03s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.84it/s][A[A

  6% 2/32 [00:01<00:15,  1.97it/s][A[A

  9% 3/32 [00:01<00:11,  2.44it/s][A[A

 12% 4/32 [00:01<00:09,  2.90it/s][A[A

 16% 5/32 [00:02<00:08,  3.25it/s][A[A

 19% 6/32 [00:02<00:07,  3.62it/s][A[A

 22% 7/32 [00:02<00:05,  4.28it/s][A[A

 25% 8/32 [00:02<00:05,  4.63it/s][A[A

 28% 9/32 [00:02<00:04,  5.25it/s][A[A

 31% 10/32 [00:02<00:03,  5.59it/s][A[A

 34% 11/32 [00:02<00:03,  5.96it/s][A[A

 38% 12/32 [00:03<00:03,  6.30it/s][A[A

 41% 13/32 [00:03<00:03,  6.01it/s][A[A

 44% 14/32 [00:03<00:03,  5.97it/s][A[A

 47% 15/32 [00:03<00:02,  6.16it/s][A[A

 50% 16/32 [00:03<00:02,  6.10it/s][A[A

 53% 17/32 [00:03<00:02,  6.32it/s][A[A

 56% 18/32 [00:04<00:02,  5.76it/s][A[A

 59% 19/32 [00:04<00:02,  6.11it/s][A[A

 62% 20/32 [00:05<00:05,  2.15it/s][A[A

 66% 21/32 [00:05<00:04,  2.60it/s][A[A

 69% 22/32 [00:05<00:03,  3.09it/s][A[A

 72% 23/32 [00:06<00:02,  3.42it/s][A[A

 75% 24/32 [00:06<00:02,  3.70it/s][A[A

 78% 25/32 [00:06<00:01,  4.38it/s][A[A

 81% 26/32 [00:06<00:01,  5.09it/s][A[A

 84% 27/32 [00:06<00:00,  5.26it/s][A[A

 88% 28/32 [00:06<00:00,  5.31it/s][A[A

 91% 29/32 [00:06<00:00,  5.81it/s][A[A

 94% 30/32 [00:07<00:00,  5.36it/s][A[A

 97% 31/32 [00:07<00:00,  5.76it/s][A[A

100% 32/32 [00:07<00:00,  5.54it/s][A[A100% 32/32 [00:07<00:00,  4.24it/s]
Meta loss on this task batch = 5.6477e-01, PNorm = 41.5602, GNorm = 0.1242

 21% 4/19 [00:31<02:01,  8.10s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.71it/s][A[A

  6% 2/32 [00:00<00:04,  6.21it/s][A[A

  9% 3/32 [00:00<00:04,  6.32it/s][A[A

 12% 4/32 [00:00<00:04,  6.03it/s][A[A

 16% 5/32 [00:01<00:12,  2.09it/s][A[A

 19% 6/32 [00:02<00:10,  2.51it/s][A[A

 22% 7/32 [00:02<00:08,  3.04it/s][A[A

 25% 8/32 [00:02<00:06,  3.65it/s][A[A

 28% 9/32 [00:02<00:05,  3.95it/s][A[A

 31% 10/32 [00:02<00:04,  4.43it/s][A[A

 34% 11/32 [00:02<00:04,  4.66it/s][A[A

 38% 12/32 [00:03<00:04,  4.75it/s][A[A

 41% 13/32 [00:03<00:04,  4.60it/s][A[A

 44% 14/32 [00:03<00:03,  5.16it/s][A[A

 47% 15/32 [00:03<00:03,  4.97it/s][A[A

 50% 16/32 [00:03<00:02,  5.73it/s][A[A

 53% 17/32 [00:04<00:03,  4.93it/s][A[A

 56% 18/32 [00:04<00:02,  5.06it/s][A[A

 59% 19/32 [00:04<00:02,  5.14it/s][A[A

 62% 20/32 [00:04<00:02,  4.81it/s][A[A

 66% 21/32 [00:05<00:05,  2.02it/s][A[A

 69% 22/32 [00:05<00:03,  2.62it/s][A[A

 72% 23/32 [00:06<00:02,  3.02it/s][A[A

 75% 24/32 [00:06<00:02,  3.27it/s][A[A

 78% 25/32 [00:06<00:01,  3.53it/s][A[A

 81% 26/32 [00:06<00:01,  4.13it/s][A[A

 84% 27/32 [00:07<00:01,  4.46it/s][A[A

 88% 28/32 [00:07<00:00,  4.91it/s][A[A

 91% 29/32 [00:07<00:00,  5.09it/s][A[A

 94% 30/32 [00:07<00:00,  5.45it/s][A[A

 97% 31/32 [00:07<00:00,  5.46it/s][A[A

100% 32/32 [00:07<00:00,  5.16it/s][A[A100% 32/32 [00:07<00:00,  4.05it/s]
Meta loss on this task batch = 5.1511e-01, PNorm = 41.5850, GNorm = 0.1123

 26% 5/19 [00:40<01:55,  8.25s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  6.41it/s][A[A

  6% 2/32 [00:00<00:04,  6.29it/s][A[A

  9% 3/32 [00:00<00:04,  6.43it/s][A[A

 12% 4/32 [00:00<00:04,  6.30it/s][A[A

 16% 5/32 [00:01<00:12,  2.08it/s][A[A

 19% 6/32 [00:02<00:10,  2.46it/s][A[A

 22% 7/32 [00:02<00:08,  2.96it/s][A[A

 25% 8/32 [00:02<00:07,  3.33it/s][A[A

 28% 9/32 [00:02<00:06,  3.44it/s][A[A

 31% 10/32 [00:02<00:05,  3.82it/s][A[A

 34% 11/32 [00:03<00:05,  4.14it/s][A[A

 38% 12/32 [00:03<00:04,  4.41it/s][A[A

 41% 13/32 [00:03<00:03,  4.85it/s][A[A

 44% 14/32 [00:03<00:03,  4.59it/s][A[A

 47% 15/32 [00:03<00:03,  4.57it/s][A[A

 50% 16/32 [00:04<00:03,  4.49it/s][A[A

 53% 17/32 [00:04<00:02,  5.02it/s][A[A

 56% 18/32 [00:04<00:02,  5.21it/s][A[A

 59% 19/32 [00:04<00:02,  5.00it/s][A[A

 62% 20/32 [00:04<00:02,  5.09it/s][A[A

 66% 21/32 [00:06<00:05,  2.02it/s][A[A

 69% 22/32 [00:06<00:04,  2.44it/s][A[A

 72% 23/32 [00:06<00:03,  2.95it/s][A[A

 75% 24/32 [00:06<00:02,  3.35it/s][A[A

 78% 25/32 [00:06<00:01,  3.56it/s][A[A

 81% 26/32 [00:07<00:01,  4.14it/s][A[A

 84% 27/32 [00:07<00:01,  4.11it/s][A[A

 88% 28/32 [00:07<00:00,  4.14it/s][A[A

 91% 29/32 [00:07<00:00,  4.05it/s][A[A

 94% 30/32 [00:08<00:00,  4.30it/s][A[A

 97% 31/32 [00:08<00:00,  4.80it/s][A[A

100% 32/32 [00:08<00:00,  4.91it/s][A[A100% 32/32 [00:08<00:00,  3.82it/s]
Meta loss on this task batch = 4.3277e-01, PNorm = 41.6114, GNorm = 0.1094

 32% 6/19 [00:49<01:50,  8.52s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.00it/s][A[A

  6% 2/32 [00:00<00:05,  5.36it/s][A[A

  9% 3/32 [00:01<00:14,  2.01it/s][A[A

 12% 4/32 [00:01<00:11,  2.35it/s][A[A

 16% 5/32 [00:01<00:09,  2.95it/s][A[A

 19% 6/32 [00:02<00:07,  3.34it/s][A[A

 22% 7/32 [00:02<00:06,  3.75it/s][A[A

 25% 8/32 [00:02<00:06,  3.96it/s][A[A

 28% 9/32 [00:02<00:05,  4.28it/s][A[A

 31% 10/32 [00:02<00:05,  4.38it/s][A[A

 34% 11/32 [00:03<00:04,  4.75it/s][A[A

 38% 12/32 [00:03<00:04,  4.77it/s][A[A

 41% 13/32 [00:03<00:03,  4.99it/s][A[A

 44% 14/32 [00:03<00:03,  5.37it/s][A[A

 47% 15/32 [00:03<00:03,  4.90it/s][A[A

 50% 16/32 [00:05<00:08,  1.95it/s][A[A

 53% 17/32 [00:05<00:06,  2.40it/s][A[A

 56% 18/32 [00:05<00:05,  2.79it/s][A[A

 59% 19/32 [00:05<00:03,  3.27it/s][A[A

 62% 20/32 [00:06<00:03,  3.45it/s][A[A

 66% 21/32 [00:06<00:02,  3.73it/s][A[A

 69% 22/32 [00:06<00:02,  3.92it/s][A[A

 72% 23/32 [00:06<00:02,  4.02it/s][A[A

 75% 24/32 [00:06<00:01,  4.12it/s][A[A

 78% 25/32 [00:07<00:01,  4.26it/s][A[A

 81% 26/32 [00:07<00:01,  4.55it/s][A[A

 84% 27/32 [00:07<00:01,  4.88it/s][A[A

 88% 28/32 [00:07<00:00,  4.62it/s][A[A

 91% 29/32 [00:07<00:00,  5.04it/s][A[A

 94% 30/32 [00:08<00:00,  5.05it/s][A[A

 97% 31/32 [00:09<00:00,  1.99it/s][A[A

100% 32/32 [00:09<00:00,  2.39it/s][A[A100% 32/32 [00:09<00:00,  3.35it/s]
Meta loss on this task batch = 4.6335e-01, PNorm = 41.6403, GNorm = 0.0744

 37% 7/19 [00:59<01:48,  9.06s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.23it/s][A[A

  6% 2/32 [00:00<00:06,  4.42it/s][A[A

  9% 3/32 [00:00<00:06,  4.46it/s][A[A

 12% 4/32 [00:00<00:06,  4.50it/s][A[A

 16% 5/32 [00:01<00:05,  4.58it/s][A[A

 19% 6/32 [00:01<00:05,  4.62it/s][A[A

 22% 7/32 [00:01<00:04,  5.02it/s][A[A

 25% 8/32 [00:01<00:04,  4.84it/s][A[A

 28% 9/32 [00:01<00:04,  4.92it/s][A[A

 31% 10/32 [00:03<00:10,  2.05it/s][A[A

 34% 11/32 [00:03<00:08,  2.44it/s][A[A

 38% 12/32 [00:03<00:06,  2.88it/s][A[A

 41% 13/32 [00:03<00:05,  3.39it/s][A[A

 44% 14/32 [00:03<00:04,  3.92it/s][A[A

 47% 15/32 [00:04<00:04,  4.07it/s][A[A

 50% 16/32 [00:04<00:03,  4.20it/s][A[A

 53% 17/32 [00:04<00:03,  4.39it/s][A[A

 56% 18/32 [00:04<00:03,  4.60it/s][A[A

 59% 19/32 [00:04<00:02,  4.70it/s][A[A

 62% 20/32 [00:05<00:02,  4.88it/s][A[A

 66% 21/32 [00:05<00:02,  5.15it/s][A[A

 69% 22/32 [00:05<00:01,  5.31it/s][A[A

 72% 23/32 [00:05<00:01,  5.58it/s][A[A

 75% 24/32 [00:06<00:03,  2.04it/s][A[A

 78% 25/32 [00:07<00:02,  2.36it/s][A[A

 81% 26/32 [00:07<00:02,  2.65it/s][A[A

 84% 27/32 [00:07<00:01,  2.93it/s][A[A

 88% 28/32 [00:07<00:01,  3.26it/s][A[A

 91% 29/32 [00:07<00:00,  3.55it/s][A[A

 94% 30/32 [00:08<00:00,  3.80it/s][A[A

 97% 31/32 [00:08<00:00,  3.97it/s][A[A

100% 32/32 [00:08<00:00,  4.11it/s][A[A100% 32/32 [00:08<00:00,  3.70it/s]
Meta loss on this task batch = 3.6512e-01, PNorm = 41.6727, GNorm = 0.0974

 42% 8/19 [01:09<01:40,  9.17s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.70it/s][A[A

  6% 2/32 [00:01<00:16,  1.79it/s][A[A

  9% 3/32 [00:01<00:13,  2.15it/s][A[A

 12% 4/32 [00:01<00:11,  2.50it/s][A[A

 16% 5/32 [00:02<00:09,  2.85it/s][A[A

 19% 6/32 [00:02<00:08,  3.25it/s][A[A

 22% 7/32 [00:02<00:07,  3.49it/s][A[A

 25% 8/32 [00:02<00:06,  3.69it/s][A[A

 28% 9/32 [00:03<00:05,  3.93it/s][A[A

 31% 10/32 [00:03<00:05,  4.11it/s][A[A

 34% 11/32 [00:03<00:04,  4.24it/s][A[A

 38% 12/32 [00:03<00:04,  4.25it/s][A[A

 41% 13/32 [00:05<00:10,  1.88it/s][A[A

 44% 14/32 [00:05<00:08,  2.24it/s][A[A

 47% 15/32 [00:05<00:06,  2.59it/s][A[A

 50% 16/32 [00:05<00:05,  2.89it/s][A[A

 53% 17/32 [00:06<00:04,  3.17it/s][A[A

 56% 18/32 [00:06<00:04,  3.45it/s][A[A

 59% 19/32 [00:06<00:03,  3.66it/s][A[A

 62% 20/32 [00:06<00:03,  3.80it/s][A[A

 66% 21/32 [00:06<00:02,  3.97it/s][A[A

 69% 22/32 [00:07<00:02,  4.14it/s][A[A

 72% 23/32 [00:07<00:02,  4.22it/s][A[A

 75% 24/32 [00:07<00:01,  4.18it/s][A[A

 78% 25/32 [00:08<00:03,  1.88it/s][A[A

 81% 26/32 [00:09<00:02,  2.23it/s][A[A

 84% 27/32 [00:09<00:01,  2.61it/s][A[A

 88% 28/32 [00:09<00:01,  2.94it/s][A[A

 91% 29/32 [00:09<00:00,  3.20it/s][A[A

 94% 30/32 [00:10<00:00,  3.45it/s][A[A

 97% 31/32 [00:10<00:00,  3.65it/s][A[A

100% 32/32 [00:10<00:00,  3.85it/s][A[A100% 32/32 [00:10<00:00,  3.04it/s]
Meta loss on this task batch = 2.0520e-01, PNorm = 41.7089, GNorm = 0.1111

 47% 9/19 [01:20<01:38,  9.82s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  3.97it/s][A[A

  6% 2/32 [00:00<00:07,  4.13it/s][A[A

  9% 3/32 [00:00<00:06,  4.23it/s][A[A

 12% 4/32 [00:00<00:06,  4.28it/s][A[A

 16% 5/32 [00:01<00:06,  4.28it/s][A[A

 19% 6/32 [00:01<00:05,  4.45it/s][A[A

 22% 7/32 [00:02<00:13,  1.89it/s][A[A

 25% 8/32 [00:02<00:10,  2.27it/s][A[A

 28% 9/32 [00:03<00:08,  2.64it/s][A[A

 31% 10/32 [00:03<00:07,  3.04it/s][A[A

 34% 11/32 [00:03<00:06,  3.36it/s][A[A

 38% 12/32 [00:03<00:05,  3.62it/s][A[A

 41% 13/32 [00:03<00:04,  3.82it/s][A[A

 44% 14/32 [00:04<00:04,  4.04it/s][A[A

 47% 15/32 [00:04<00:04,  4.23it/s][A[A

 50% 16/32 [00:04<00:03,  4.33it/s][A[A

 53% 17/32 [00:04<00:03,  4.23it/s][A[A

 56% 18/32 [00:05<00:03,  4.09it/s][A[A

 59% 19/32 [00:06<00:07,  1.84it/s][A[A

 62% 20/32 [00:06<00:05,  2.22it/s][A[A

 66% 21/32 [00:06<00:04,  2.60it/s][A[A

 69% 22/32 [00:07<00:03,  2.95it/s][A[A

 72% 23/32 [00:07<00:02,  3.27it/s][A[A

 75% 24/32 [00:07<00:02,  3.62it/s][A[A

 78% 25/32 [00:07<00:01,  3.83it/s][A[A

 81% 26/32 [00:07<00:01,  3.88it/s][A[A

 84% 27/32 [00:08<00:01,  4.23it/s][A[A

 88% 28/32 [00:08<00:00,  4.22it/s][A[A

 91% 29/32 [00:08<00:00,  4.31it/s][A[A

 94% 30/32 [00:08<00:00,  4.23it/s][A[A

 97% 31/32 [00:10<00:00,  1.86it/s][A[A

100% 32/32 [00:10<00:00,  2.31it/s][A[A100% 32/32 [00:10<00:00,  3.11it/s]
Meta loss on this task batch = 2.5407e-01, PNorm = 41.7490, GNorm = 0.0933

 53% 10/19 [01:31<01:31, 10.20s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.13it/s][A[A

  6% 2/32 [00:00<00:07,  4.15it/s][A[A

  9% 3/32 [00:00<00:06,  4.21it/s][A[A

 12% 4/32 [00:00<00:06,  4.23it/s][A[A

 16% 5/32 [00:01<00:06,  4.33it/s][A[A

 19% 6/32 [00:01<00:05,  4.63it/s][A[A

 22% 7/32 [00:01<00:05,  4.86it/s][A[A

 25% 8/32 [00:01<00:05,  4.71it/s][A[A

 28% 9/32 [00:01<00:05,  4.52it/s][A[A

 31% 10/32 [00:03<00:11,  1.88it/s][A[A

 34% 11/32 [00:03<00:09,  2.26it/s][A[A

 38% 12/32 [00:03<00:07,  2.63it/s][A[A

 41% 13/32 [00:03<00:06,  2.98it/s][A[A

 44% 14/32 [00:04<00:05,  3.28it/s][A[A

 47% 15/32 [00:04<00:04,  3.50it/s][A[A

 50% 16/32 [00:04<00:04,  3.81it/s][A[A

 53% 17/32 [00:04<00:03,  3.82it/s][A[A

 56% 18/32 [00:05<00:03,  4.22it/s][A[A

 59% 19/32 [00:06<00:06,  1.87it/s][A[A

 62% 20/32 [00:06<00:05,  2.25it/s][A[A

 66% 21/32 [00:06<00:04,  2.61it/s][A[A

 69% 22/32 [00:07<00:03,  2.95it/s][A[A

 72% 23/32 [00:07<00:02,  3.27it/s][A[A

 75% 24/32 [00:07<00:02,  3.53it/s][A[A

 78% 25/32 [00:07<00:01,  3.93it/s][A[A

 81% 26/32 [00:07<00:01,  4.01it/s][A[A

 84% 27/32 [00:08<00:01,  4.08it/s][A[A

 88% 28/32 [00:08<00:00,  4.15it/s][A[A

 91% 29/32 [00:08<00:00,  4.15it/s][A[A

 94% 30/32 [00:08<00:00,  4.26it/s][A[A

 97% 31/32 [00:09<00:00,  4.22it/s][A[A

100% 32/32 [00:09<00:00,  4.18it/s][A[A100% 32/32 [00:09<00:00,  3.44it/s]
Meta loss on this task batch = 5.8574e-01, PNorm = 41.7809, GNorm = 0.2225

 58% 11/19 [01:41<01:21, 10.19s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  3.91it/s][A[A

  6% 2/32 [00:00<00:07,  3.92it/s][A[A

  9% 3/32 [00:00<00:07,  3.99it/s][A[A

 12% 4/32 [00:00<00:06,  4.05it/s][A[A

 16% 5/32 [00:01<00:06,  4.08it/s][A[A

 19% 6/32 [00:02<00:13,  1.86it/s][A[A

 22% 7/32 [00:02<00:10,  2.34it/s][A[A

 25% 8/32 [00:02<00:08,  2.73it/s][A[A

 28% 9/32 [00:03<00:07,  3.07it/s][A[A

 31% 10/32 [00:03<00:06,  3.35it/s][A[A

 34% 11/32 [00:03<00:05,  3.60it/s][A[A

 38% 12/32 [00:03<00:05,  3.73it/s][A[A

 41% 13/32 [00:04<00:04,  3.82it/s][A[A

 44% 14/32 [00:04<00:04,  3.90it/s][A[A

 47% 15/32 [00:04<00:04,  3.93it/s][A[A

 50% 16/32 [00:04<00:03,  4.26it/s][A[A

 53% 17/32 [00:04<00:03,  4.16it/s][A[A

 56% 18/32 [00:06<00:07,  1.86it/s][A[A

 59% 19/32 [00:06<00:05,  2.23it/s][A[A

 62% 20/32 [00:06<00:04,  2.59it/s][A[A

 66% 21/32 [00:06<00:03,  2.93it/s][A[A

 69% 22/32 [00:07<00:02,  3.47it/s][A[A

 72% 23/32 [00:07<00:02,  3.66it/s][A[A

 75% 24/32 [00:07<00:02,  3.93it/s][A[A

 78% 25/32 [00:07<00:01,  4.06it/s][A[A

 81% 26/32 [00:07<00:01,  4.20it/s][A[A

 84% 27/32 [00:08<00:01,  4.13it/s][A[A

 88% 28/32 [00:09<00:02,  1.83it/s][A[A

 91% 29/32 [00:09<00:01,  2.21it/s][A[A

 94% 30/32 [00:09<00:00,  2.56it/s][A[A

 97% 31/32 [00:10<00:00,  2.89it/s][A[A

100% 32/32 [00:10<00:00,  3.22it/s][A[A100% 32/32 [00:10<00:00,  3.07it/s]
Meta loss on this task batch = 5.4612e-01, PNorm = 41.8096, GNorm = 0.1360

 63% 12/19 [01:53<01:13, 10.50s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.16it/s][A[A

  6% 2/32 [00:00<00:06,  4.29it/s][A[A

  9% 3/32 [00:00<00:06,  4.33it/s][A[A

 12% 4/32 [00:00<00:06,  4.31it/s][A[A

 16% 5/32 [00:01<00:06,  4.30it/s][A[A

 19% 6/32 [00:02<00:13,  1.87it/s][A[A

 22% 7/32 [00:02<00:10,  2.32it/s][A[A

 25% 8/32 [00:02<00:08,  2.80it/s][A[A

 28% 9/32 [00:02<00:06,  3.32it/s][A[A

 31% 10/32 [00:03<00:06,  3.56it/s][A[A

 34% 11/32 [00:03<00:05,  3.77it/s][A[A

 38% 12/32 [00:03<00:05,  3.89it/s][A[A

 41% 13/32 [00:03<00:04,  4.00it/s][A[A

 44% 14/32 [00:05<00:09,  1.89it/s][A[A

 47% 15/32 [00:05<00:07,  2.25it/s][A[A

 50% 16/32 [00:05<00:06,  2.59it/s][A[A

 53% 17/32 [00:05<00:04,  3.04it/s][A[A

 56% 18/32 [00:05<00:03,  3.52it/s][A[A

 59% 19/32 [00:06<00:03,  3.65it/s][A[A

 62% 20/32 [00:06<00:03,  3.76it/s][A[A

 66% 21/32 [00:06<00:02,  3.83it/s][A[A

 69% 22/32 [00:07<00:05,  1.80it/s][A[A

 72% 23/32 [00:08<00:04,  2.19it/s][A[A

 75% 24/32 [00:08<00:03,  2.56it/s][A[A

 78% 25/32 [00:08<00:02,  2.91it/s][A[A

 81% 26/32 [00:08<00:01,  3.21it/s][A[A

 84% 27/32 [00:09<00:01,  3.67it/s][A[A

 88% 28/32 [00:09<00:00,  4.08it/s][A[A

 91% 29/32 [00:09<00:00,  4.06it/s][A[A

 94% 30/32 [00:09<00:00,  4.09it/s][A[A

 97% 31/32 [00:09<00:00,  4.32it/s][A[A

100% 32/32 [00:10<00:00,  4.24it/s][A[A100% 32/32 [00:10<00:00,  3.16it/s]
Meta loss on this task batch = 5.6653e-01, PNorm = 41.8371, GNorm = 0.1060

 68% 13/19 [02:04<01:03, 10.64s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:36,  1.19s/it][A[A

  6% 2/32 [00:01<00:26,  1.13it/s][A[A

  9% 3/32 [00:01<00:20,  1.45it/s][A[A

 12% 4/32 [00:01<00:15,  1.81it/s][A[A

 16% 5/32 [00:02<00:12,  2.17it/s][A[A

 19% 6/32 [00:02<00:09,  2.64it/s][A[A

 22% 7/32 [00:02<00:08,  2.96it/s][A[A

 25% 8/32 [00:02<00:06,  3.52it/s][A[A

 28% 9/32 [00:02<00:05,  4.05it/s][A[A

 31% 10/32 [00:03<00:05,  4.14it/s][A[A

 34% 11/32 [00:03<00:05,  4.17it/s][A[A

 38% 12/32 [00:03<00:04,  4.09it/s][A[A

 41% 13/32 [00:03<00:04,  4.11it/s][A[A

 44% 14/32 [00:04<00:04,  4.11it/s][A[A

 47% 15/32 [00:04<00:04,  4.11it/s][A[A

 50% 16/32 [00:04<00:03,  4.11it/s][A[A

 53% 17/32 [00:05<00:08,  1.85it/s][A[A

 56% 18/32 [00:05<00:06,  2.27it/s][A[A

 59% 19/32 [00:06<00:04,  2.70it/s][A[A

 62% 20/32 [00:06<00:03,  3.16it/s][A[A

 66% 21/32 [00:06<00:03,  3.39it/s][A[A

 69% 22/32 [00:06<00:02,  3.54it/s][A[A

 72% 23/32 [00:07<00:02,  3.68it/s][A[A

 75% 24/32 [00:08<00:04,  1.83it/s][A[A

 78% 25/32 [00:08<00:03,  2.21it/s][A[A

 81% 26/32 [00:08<00:02,  2.58it/s][A[A

 84% 27/32 [00:09<00:01,  2.89it/s][A[A

 88% 28/32 [00:09<00:01,  3.21it/s][A[A

 91% 29/32 [00:09<00:00,  3.48it/s][A[A

 94% 30/32 [00:09<00:00,  3.83it/s][A[A

 97% 31/32 [00:09<00:00,  4.11it/s][A[A

100% 32/32 [00:10<00:00,  4.39it/s][A[A100% 32/32 [00:10<00:00,  3.18it/s]
Meta loss on this task batch = 5.5273e-01, PNorm = 41.8625, GNorm = 0.1277

 74% 14/19 [02:14<00:53, 10.71s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.17it/s][A[A

  6% 2/32 [00:01<00:15,  1.89it/s][A[A

  9% 3/32 [00:01<00:12,  2.29it/s][A[A

 12% 4/32 [00:01<00:09,  2.80it/s][A[A

 16% 5/32 [00:02<00:08,  3.28it/s][A[A

 19% 6/32 [00:02<00:07,  3.40it/s][A[A

 22% 7/32 [00:02<00:06,  3.62it/s][A[A

 25% 8/32 [00:02<00:05,  4.09it/s][A[A

 28% 9/32 [00:02<00:05,  4.23it/s][A[A

 31% 10/32 [00:03<00:04,  4.58it/s][A[A

 34% 11/32 [00:03<00:04,  4.59it/s][A[A

 38% 12/32 [00:03<00:04,  4.88it/s][A[A

 41% 13/32 [00:03<00:03,  4.91it/s][A[A

 44% 14/32 [00:04<00:09,  1.98it/s][A[A

 47% 15/32 [00:05<00:06,  2.46it/s][A[A

 50% 16/32 [00:05<00:05,  2.89it/s][A[A

 53% 17/32 [00:05<00:04,  3.29it/s][A[A

 56% 18/32 [00:05<00:03,  3.74it/s][A[A

 59% 19/32 [00:05<00:03,  4.31it/s][A[A

 62% 20/32 [00:06<00:02,  4.41it/s][A[A

 66% 21/32 [00:06<00:02,  4.76it/s][A[A

 69% 22/32 [00:06<00:02,  4.97it/s][A[A

 72% 23/32 [00:06<00:01,  4.60it/s][A[A

 75% 24/32 [00:06<00:01,  4.65it/s][A[A

 78% 25/32 [00:07<00:01,  4.64it/s][A[A

 81% 26/32 [00:07<00:01,  4.47it/s][A[A

 84% 27/32 [00:07<00:01,  4.56it/s][A[A

 88% 28/32 [00:07<00:00,  4.35it/s][A[A

 91% 29/32 [00:09<00:01,  1.87it/s][A[A

 94% 30/32 [00:09<00:00,  2.24it/s][A[A

 97% 31/32 [00:09<00:00,  2.70it/s][A[A

100% 32/32 [00:09<00:00,  3.04it/s][A[A100% 32/32 [00:09<00:00,  3.31it/s]
Meta loss on this task batch = 4.7804e-01, PNorm = 41.8853, GNorm = 0.1007

 79% 15/19 [02:25<00:42, 10.62s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  3.98it/s][A[A

  6% 2/32 [00:00<00:06,  4.32it/s][A[A

  9% 3/32 [00:00<00:06,  4.54it/s][A[A

 12% 4/32 [00:00<00:05,  4.75it/s][A[A

 16% 5/32 [00:01<00:13,  2.00it/s][A[A

 19% 6/32 [00:02<00:10,  2.40it/s][A[A

 22% 7/32 [00:02<00:08,  2.89it/s][A[A

 25% 8/32 [00:02<00:07,  3.19it/s][A[A

 28% 9/32 [00:02<00:06,  3.62it/s][A[A

 31% 10/32 [00:03<00:05,  3.80it/s][A[A

 34% 11/32 [00:03<00:05,  3.87it/s][A[A

 38% 12/32 [00:03<00:04,  4.10it/s][A[A

 41% 13/32 [00:03<00:04,  4.39it/s][A[A

 44% 14/32 [00:03<00:03,  4.65it/s][A[A

 47% 15/32 [00:04<00:03,  4.61it/s][A[A

 50% 16/32 [00:04<00:03,  4.95it/s][A[A

 53% 17/32 [00:04<00:03,  4.67it/s][A[A

 56% 18/32 [00:04<00:02,  5.01it/s][A[A

 59% 19/32 [00:04<00:02,  5.00it/s][A[A

 62% 20/32 [00:05<00:02,  4.97it/s][A[A

 66% 21/32 [00:05<00:02,  5.06it/s][A[A

 69% 22/32 [00:06<00:04,  2.00it/s][A[A

 72% 23/32 [00:06<00:03,  2.37it/s][A[A

 75% 24/32 [00:06<00:02,  2.73it/s][A[A

 78% 25/32 [00:07<00:02,  3.28it/s][A[A

 81% 26/32 [00:07<00:01,  3.65it/s][A[A

 84% 27/32 [00:07<00:01,  3.78it/s][A[A

 88% 28/32 [00:07<00:01,  4.00it/s][A[A

 91% 29/32 [00:07<00:00,  4.20it/s][A[A

 94% 30/32 [00:08<00:00,  4.13it/s][A[A

 97% 31/32 [00:08<00:00,  4.14it/s][A[A

100% 32/32 [00:08<00:00,  4.33it/s][A[A100% 32/32 [00:08<00:00,  3.68it/s]
Meta loss on this task batch = 5.4490e-01, PNorm = 41.9049, GNorm = 0.1096

 84% 16/19 [02:34<00:30, 10.28s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.06it/s][A[A

  6% 2/32 [00:01<00:15,  1.98it/s][A[A

  9% 3/32 [00:01<00:12,  2.35it/s][A[A

 12% 4/32 [00:01<00:09,  2.83it/s][A[A

 16% 5/32 [00:02<00:08,  3.28it/s][A[A

 19% 6/32 [00:02<00:06,  3.93it/s][A[A

 22% 7/32 [00:02<00:05,  4.44it/s][A[A

 25% 8/32 [00:02<00:05,  4.34it/s][A[A

 28% 9/32 [00:02<00:05,  4.45it/s][A[A

 31% 10/32 [00:02<00:04,  5.16it/s][A[A

 34% 11/32 [00:03<00:04,  5.19it/s][A[A

 38% 12/32 [00:03<00:03,  5.15it/s][A[A

 41% 13/32 [00:03<00:03,  5.49it/s][A[A

 44% 14/32 [00:03<00:03,  5.50it/s][A[A

 47% 15/32 [00:03<00:03,  5.39it/s][A[A

 50% 16/32 [00:04<00:03,  5.13it/s][A[A

 53% 17/32 [00:04<00:02,  5.22it/s][A[A

 56% 18/32 [00:04<00:02,  5.05it/s][A[A

 59% 19/32 [00:04<00:02,  4.85it/s][A[A

 62% 20/32 [00:05<00:06,  1.97it/s][A[A

 66% 21/32 [00:06<00:04,  2.41it/s][A[A

 69% 22/32 [00:06<00:03,  2.86it/s][A[A

 72% 23/32 [00:06<00:02,  3.25it/s][A[A

 75% 24/32 [00:06<00:02,  3.36it/s][A[A

 78% 25/32 [00:06<00:01,  3.61it/s][A[A

 81% 26/32 [00:07<00:01,  3.91it/s][A[A

 84% 27/32 [00:07<00:01,  4.32it/s][A[A

 88% 28/32 [00:07<00:00,  4.61it/s][A[A

 91% 29/32 [00:07<00:00,  4.53it/s][A[A

 94% 30/32 [00:07<00:00,  4.80it/s][A[A

 97% 31/32 [00:08<00:00,  5.00it/s][A[A

100% 32/32 [00:09<00:00,  2.03it/s][A[A100% 32/32 [00:09<00:00,  3.44it/s]
Meta loss on this task batch = 4.6542e-01, PNorm = 41.9210, GNorm = 0.1302

 89% 17/19 [02:44<00:20, 10.20s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.38it/s][A[A

  6% 2/32 [00:00<00:05,  5.28it/s][A[A

  9% 3/32 [00:00<00:05,  5.10it/s][A[A

 12% 4/32 [00:00<00:05,  5.07it/s][A[A

 16% 5/32 [00:01<00:05,  5.00it/s][A[A

 19% 6/32 [00:01<00:05,  4.99it/s][A[A

 22% 7/32 [00:01<00:05,  4.67it/s][A[A

 25% 8/32 [00:01<00:05,  4.47it/s][A[A

 28% 9/32 [00:01<00:04,  4.70it/s][A[A

 31% 10/32 [00:02<00:04,  4.61it/s][A[A

 34% 11/32 [00:03<00:10,  2.02it/s][A[A

 38% 12/32 [00:03<00:08,  2.50it/s][A[A

 41% 13/32 [00:03<00:06,  2.80it/s][A[A

 44% 14/32 [00:03<00:05,  3.40it/s][A[A

 47% 15/32 [00:04<00:04,  3.61it/s][A[A

 50% 16/32 [00:04<00:03,  4.02it/s][A[A

 53% 17/32 [00:04<00:03,  4.69it/s][A[A

 56% 18/32 [00:04<00:02,  4.87it/s][A[A

 59% 19/32 [00:04<00:02,  4.80it/s][A[A

 62% 20/32 [00:05<00:02,  4.70it/s][A[A

 66% 21/32 [00:05<00:02,  5.03it/s][A[A

 69% 22/32 [00:05<00:01,  5.55it/s][A[A

 72% 23/32 [00:05<00:01,  4.84it/s][A[A

 75% 24/32 [00:06<00:04,  1.92it/s][A[A

 78% 25/32 [00:06<00:02,  2.42it/s][A[A

 81% 26/32 [00:07<00:02,  2.87it/s][A[A

 84% 27/32 [00:07<00:01,  3.10it/s][A[A

 88% 28/32 [00:07<00:01,  3.37it/s][A[A

 91% 29/32 [00:07<00:00,  3.60it/s][A[A

 94% 30/32 [00:08<00:00,  4.03it/s][A[A

 97% 31/32 [00:08<00:00,  4.39it/s][A[A

100% 32/32 [00:08<00:00,  4.43it/s][A[A100% 32/32 [00:08<00:00,  3.76it/s]
Meta loss on this task batch = 4.8759e-01, PNorm = 41.9389, GNorm = 0.0727

 95% 18/19 [02:54<00:09,  9.92s/it][A

  0% 0/23 [00:00<?, ?it/s][A[A

  4% 1/23 [00:00<00:05,  4.09it/s][A[A

  9% 2/23 [00:00<00:04,  4.50it/s][A[A

 13% 3/23 [00:00<00:04,  4.63it/s][A[A

 17% 4/23 [00:00<00:03,  4.97it/s][A[A

 22% 5/23 [00:00<00:03,  5.52it/s][A[A

 26% 6/23 [00:02<00:08,  2.05it/s][A[A

 30% 7/23 [00:02<00:06,  2.56it/s][A[A

 35% 8/23 [00:02<00:05,  2.91it/s][A[A

 39% 9/23 [00:02<00:03,  3.59it/s][A[A

 43% 10/23 [00:02<00:03,  3.65it/s][A[A

 48% 11/23 [00:03<00:02,  4.18it/s][A[A

 52% 12/23 [00:03<00:02,  4.32it/s][A[A

 57% 13/23 [00:03<00:02,  4.90it/s][A[A

 61% 14/23 [00:03<00:01,  5.09it/s][A[A

 65% 15/23 [00:03<00:02,  3.95it/s][A[A

 70% 16/23 [00:04<00:01,  3.91it/s][A[A

 74% 17/23 [00:04<00:01,  4.38it/s][A[A

 78% 18/23 [00:05<00:02,  1.86it/s][A[A

 83% 19/23 [00:05<00:01,  2.25it/s][A[A

 87% 20/23 [00:06<00:01,  2.85it/s][A[A

 91% 21/23 [00:06<00:00,  3.38it/s][A[A

 96% 22/23 [00:06<00:00,  4.08it/s][A[A

100% 23/23 [00:06<00:00,  4.57it/s][A[A100% 23/23 [00:06<00:00,  3.55it/s]
Meta loss on this task batch = 4.1430e-01, PNorm = 41.9568, GNorm = 0.1074

100% 19/19 [03:01<00:00,  9.05s/it][A100% 19/19 [03:01<00:00,  9.54s/it]
Took 181.21272659301758 seconds to complete one epoch of meta training

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/20 [00:00<?, ?it/s][A[A


  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 31.29it/s]


  5% 1/20 [00:00<00:02,  8.14it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 20.72it/s]


 10% 2/20 [00:00<00:02,  6.25it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 18.54it/s][A[A[A100% 3/3 [00:00<00:00, 20.30it/s]


 15% 3/20 [00:00<00:03,  5.06it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 15.83it/s]


 20% 4/20 [00:00<00:03,  4.97it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 14.69it/s][A[A[A100% 4/4 [00:00<00:00, 17.04it/s]


 25% 5/20 [00:01<00:03,  3.83it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.59it/s][A[A[A100% 4/4 [00:00<00:00, 16.37it/s]


 30% 6/20 [00:02<00:08,  1.68it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.43it/s][A[A[A100% 4/4 [00:00<00:00, 20.28it/s]


 35% 7/20 [00:03<00:06,  1.89it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.86it/s][A[A[A


100% 4/4 [00:00<00:00, 19.42it/s][A[A[A100% 4/4 [00:00<00:00, 19.12it/s]


 40% 8/20 [00:03<00:05,  2.07it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.21it/s][A[A[A100% 4/4 [00:00<00:00, 22.84it/s]


 45% 9/20 [00:03<00:04,  2.28it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.95it/s][A[A[A


100% 4/4 [00:00<00:00, 19.73it/s][A[A[A100% 4/4 [00:00<00:00, 19.57it/s]


 50% 10/20 [00:04<00:04,  2.36it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 16.67it/s][A[A[A100% 4/4 [00:00<00:00, 19.10it/s]


 55% 11/20 [00:04<00:03,  2.47it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:01<00:01,  1.86it/s][A[A[A100% 4/4 [00:01<00:00,  3.40it/s]


 60% 12/20 [00:05<00:05,  1.46it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.73it/s][A[A[A100% 3/3 [00:00<00:00, 14.05it/s]


 65% 13/20 [00:06<00:04,  1.68it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.05it/s][A[A[A100% 3/3 [00:00<00:00, 13.80it/s]


 70% 14/20 [00:06<00:03,  1.87it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.90it/s][A[A[A100% 4/4 [00:00<00:00, 18.12it/s]


 75% 15/20 [00:06<00:02,  2.13it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.34it/s][A[A[A100% 3/3 [00:00<00:00, 17.43it/s]


 80% 16/20 [00:07<00:01,  2.32it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 12.41it/s][A[A[A100% 3/3 [00:00<00:00, 15.06it/s]


 85% 17/20 [00:07<00:01,  2.38it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 22.54it/s]


 90% 18/20 [00:07<00:00,  2.65it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.95it/s][A[A[A100% 3/3 [00:00<00:00, 19.22it/s]


 95% 19/20 [00:09<00:00,  1.52it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 12.96it/s]


100% 20/20 [00:09<00:00,  1.88it/s][A[A100% 20/20 [00:09<00:00,  2.11it/s]

100% 1/1 [00:09<00:00,  9.48s/it][A100% 1/1 [00:09<00:00,  9.48s/it]
Took 190.68896913528442 seconds to complete one epoch of meta training and validating
Meta validation score auc = 0.646265
 87% 26/30 [1:23:49<12:52, 193.01s/it]Epoch 26

  0% 0/19 [00:00<?, ?it/s][A

  0% 0/32 [00:00<?, ?it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  3% 1/32 [00:00<00:06,  4.69it/s][A[A

  6% 2/32 [00:00<00:06,  4.45it/s][A[A

  9% 3/32 [00:00<00:06,  4.53it/s][A[A

 12% 4/32 [00:00<00:05,  5.02it/s][A[A

 16% 5/32 [00:00<00:04,  5.53it/s][A[A

 19% 6/32 [00:01<00:04,  5.55it/s][A[A

 22% 7/32 [00:01<00:04,  5.69it/s][A[A

 25% 8/32 [00:01<00:04,  5.48it/s][A[A

 28% 9/32 [00:01<00:03,  6.10it/s][A[A

 31% 10/32 [00:01<00:03,  6.51it/s][A[A

 34% 11/32 [00:02<00:03,  5.38it/s][A[A

 38% 12/32 [00:02<00:04,  4.79it/s][A[A

 41% 13/32 [00:03<00:09,  1.90it/s][A[A

 44% 14/32 [00:03<00:07,  2.33it/s][A[A

 47% 15/32 [00:03<00:06,  2.83it/s][A[A

 50% 16/32 [00:04<00:04,  3.22it/s][A[A

 53% 17/32 [00:04<00:04,  3.64it/s][A[A

 56% 18/32 [00:04<00:03,  4.10it/s][A[A

 59% 19/32 [00:04<00:02,  4.59it/s][A[A

 62% 20/32 [00:04<00:02,  4.49it/s][A[A

 66% 21/32 [00:05<00:02,  5.25it/s][A[A

 69% 22/32 [00:05<00:01,  5.20it/s][A[A

 72% 23/32 [00:05<00:01,  5.14it/s][A[A

 75% 24/32 [00:05<00:01,  5.13it/s][A[A

 78% 25/32 [00:05<00:01,  5.16it/s][A[A

 81% 26/32 [00:05<00:01,  5.04it/s][A[A

 84% 27/32 [00:07<00:02,  1.99it/s][A[A

 88% 28/32 [00:07<00:01,  2.49it/s][A[A

 91% 29/32 [00:07<00:00,  3.05it/s][A[A

 94% 30/32 [00:07<00:00,  3.80it/s][A[A

 97% 31/32 [00:07<00:00,  3.89it/s][A[A

100% 32/32 [00:08<00:00,  4.33it/s][A[A100% 32/32 [00:08<00:00,  3.97it/s]
Meta loss on this task batch = 5.0641e-01, PNorm = 41.9742, GNorm = 0.2062

  5% 1/19 [00:08<02:37,  8.76s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:03,  7.93it/s][A[A

  6% 2/32 [00:00<00:03,  7.65it/s][A[A

  9% 3/32 [00:00<00:04,  6.87it/s][A[A

 12% 4/32 [00:00<00:04,  5.69it/s][A[A

 16% 5/32 [00:00<00:05,  5.33it/s][A[A

 19% 6/32 [00:01<00:04,  5.77it/s][A[A

 22% 7/32 [00:01<00:04,  6.23it/s][A[A

 25% 8/32 [00:01<00:03,  6.52it/s][A[A

 28% 9/32 [00:01<00:03,  6.46it/s][A[A

 31% 10/32 [00:01<00:03,  6.64it/s][A[A

 34% 11/32 [00:02<00:09,  2.18it/s][A[A

 38% 12/32 [00:02<00:07,  2.65it/s][A[A

 41% 13/32 [00:03<00:05,  3.22it/s][A[A

 44% 14/32 [00:03<00:05,  3.55it/s][A[A

 47% 15/32 [00:03<00:04,  3.83it/s][A[A

 50% 16/32 [00:03<00:03,  4.11it/s][A[A

 53% 17/32 [00:03<00:03,  4.36it/s][A[A

 56% 18/32 [00:04<00:03,  4.31it/s][A[A

 59% 19/32 [00:04<00:02,  4.51it/s][A[A

 62% 20/32 [00:04<00:02,  4.92it/s][A[A

 66% 21/32 [00:04<00:02,  4.88it/s][A[A

 69% 22/32 [00:04<00:01,  5.39it/s][A[A

 72% 23/32 [00:05<00:01,  5.72it/s][A[A

 75% 24/32 [00:05<00:01,  6.05it/s][A[A

 78% 25/32 [00:05<00:01,  5.75it/s][A[A

 81% 26/32 [00:06<00:02,  2.06it/s][A[A

 84% 27/32 [00:06<00:01,  2.56it/s][A[A

 88% 28/32 [00:07<00:01,  2.86it/s][A[A

 91% 29/32 [00:07<00:00,  3.55it/s][A[A

 94% 30/32 [00:07<00:00,  3.97it/s][A[A

 97% 31/32 [00:07<00:00,  4.16it/s][A[A

100% 32/32 [00:07<00:00,  4.69it/s][A[A100% 32/32 [00:07<00:00,  4.16it/s]
Meta loss on this task batch = 5.2210e-01, PNorm = 41.9869, GNorm = 0.2431

 11% 2/19 [00:17<02:27,  8.65s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.66it/s][A[A

  6% 2/32 [00:00<00:05,  5.24it/s][A[A

  9% 3/32 [00:00<00:05,  4.94it/s][A[A

 12% 4/32 [00:00<00:05,  5.02it/s][A[A

 16% 5/32 [00:01<00:13,  2.06it/s][A[A

 19% 6/32 [00:02<00:10,  2.47it/s][A[A

 22% 7/32 [00:02<00:08,  2.87it/s][A[A

 25% 8/32 [00:02<00:06,  3.46it/s][A[A

 28% 9/32 [00:02<00:05,  4.07it/s][A[A

 31% 10/32 [00:02<00:05,  4.15it/s][A[A

 34% 11/32 [00:03<00:04,  4.23it/s][A[A

 38% 12/32 [00:03<00:04,  4.46it/s][A[A

 41% 13/32 [00:03<00:03,  4.98it/s][A[A

 44% 14/32 [00:03<00:03,  4.94it/s][A[A

 47% 15/32 [00:03<00:03,  4.75it/s][A[A

 50% 16/32 [00:04<00:02,  5.40it/s][A[A

 53% 17/32 [00:04<00:02,  5.02it/s][A[A

 56% 18/32 [00:04<00:02,  5.55it/s][A[A

 59% 19/32 [00:04<00:02,  5.75it/s][A[A

 62% 20/32 [00:04<00:01,  6.19it/s][A[A

 66% 21/32 [00:04<00:01,  6.72it/s][A[A

 69% 22/32 [00:05<00:04,  2.26it/s][A[A

 72% 23/32 [00:06<00:03,  2.74it/s][A[A

 75% 24/32 [00:06<00:02,  3.10it/s][A[A

 78% 25/32 [00:06<00:02,  3.49it/s][A[A

 81% 26/32 [00:06<00:01,  3.87it/s][A[A

 84% 27/32 [00:06<00:01,  4.35it/s][A[A

 88% 28/32 [00:07<00:00,  4.70it/s][A[A

 91% 29/32 [00:07<00:00,  4.88it/s][A[A

 94% 30/32 [00:07<00:00,  4.55it/s][A[A

 97% 31/32 [00:07<00:00,  5.31it/s][A[A

100% 32/32 [00:07<00:00,  5.13it/s][A[A100% 32/32 [00:07<00:00,  4.06it/s]
Meta loss on this task batch = 4.9831e-01, PNorm = 42.0077, GNorm = 0.0859

 16% 3/19 [00:25<02:18,  8.63s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.29it/s][A[A

  6% 2/32 [00:00<00:06,  4.38it/s][A[A

  9% 3/32 [00:00<00:06,  4.65it/s][A[A

 12% 4/32 [00:00<00:05,  5.00it/s][A[A

 16% 5/32 [00:00<00:05,  5.01it/s][A[A

 19% 6/32 [00:01<00:05,  4.74it/s][A[A

 22% 7/32 [00:01<00:04,  5.33it/s][A[A

 25% 8/32 [00:02<00:12,  1.96it/s][A[A

 28% 9/32 [00:02<00:09,  2.50it/s][A[A

 31% 10/32 [00:03<00:07,  2.86it/s][A[A

 34% 11/32 [00:03<00:06,  3.45it/s][A[A

 38% 12/32 [00:03<00:04,  4.09it/s][A[A

 41% 13/32 [00:03<00:04,  4.38it/s][A[A

 44% 14/32 [00:03<00:04,  4.46it/s][A[A

 47% 15/32 [00:03<00:03,  5.02it/s][A[A

 50% 16/32 [00:04<00:03,  4.89it/s][A[A

 53% 17/32 [00:04<00:03,  4.87it/s][A[A

 56% 18/32 [00:04<00:02,  5.46it/s][A[A

 59% 19/32 [00:04<00:02,  5.87it/s][A[A

 62% 20/32 [00:04<00:02,  5.43it/s][A[A

 66% 21/32 [00:04<00:02,  5.12it/s][A[A

 69% 22/32 [00:06<00:05,  1.93it/s][A[A

 72% 23/32 [00:06<00:03,  2.29it/s][A[A

 75% 24/32 [00:06<00:02,  2.69it/s][A[A

 78% 25/32 [00:06<00:02,  3.31it/s][A[A

 81% 26/32 [00:06<00:01,  4.02it/s][A[A

 84% 27/32 [00:07<00:01,  4.21it/s][A[A

 88% 28/32 [00:07<00:00,  4.44it/s][A[A

 91% 29/32 [00:07<00:00,  5.01it/s][A[A

 94% 30/32 [00:07<00:00,  4.79it/s][A[A

 97% 31/32 [00:07<00:00,  5.34it/s][A[A

100% 32/32 [00:08<00:00,  5.03it/s][A[A100% 32/32 [00:08<00:00,  3.94it/s]
Meta loss on this task batch = 5.6390e-01, PNorm = 42.0314, GNorm = 0.1099

 21% 4/19 [00:34<02:10,  8.70s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.71it/s][A[A

  6% 2/32 [00:00<00:05,  5.29it/s][A[A

  9% 3/32 [00:01<00:14,  1.98it/s][A[A

 12% 4/32 [00:01<00:12,  2.32it/s][A[A

 16% 5/32 [00:02<00:09,  2.73it/s][A[A

 19% 6/32 [00:02<00:07,  3.28it/s][A[A

 22% 7/32 [00:02<00:07,  3.55it/s][A[A

 25% 8/32 [00:02<00:05,  4.18it/s][A[A

 28% 9/32 [00:02<00:05,  4.34it/s][A[A

 31% 10/32 [00:02<00:04,  4.80it/s][A[A

 34% 11/32 [00:03<00:04,  4.92it/s][A[A

 38% 12/32 [00:03<00:04,  4.92it/s][A[A

 41% 13/32 [00:03<00:04,  4.69it/s][A[A

 44% 14/32 [00:03<00:03,  5.16it/s][A[A

 47% 15/32 [00:03<00:03,  5.00it/s][A[A

 50% 16/32 [00:04<00:03,  5.32it/s][A[A

 53% 17/32 [00:04<00:02,  5.06it/s][A[A

 56% 18/32 [00:05<00:07,  1.99it/s][A[A

 59% 19/32 [00:05<00:05,  2.34it/s][A[A

 62% 20/32 [00:06<00:04,  2.74it/s][A[A

 66% 21/32 [00:06<00:03,  3.05it/s][A[A

 69% 22/32 [00:06<00:02,  3.55it/s][A[A

 72% 23/32 [00:06<00:02,  4.00it/s][A[A

 75% 24/32 [00:06<00:02,  3.98it/s][A[A

 78% 25/32 [00:07<00:01,  4.19it/s][A[A

 81% 26/32 [00:07<00:01,  4.78it/s][A[A

 84% 27/32 [00:07<00:00,  5.03it/s][A[A

 88% 28/32 [00:07<00:00,  5.45it/s][A[A

 91% 29/32 [00:07<00:00,  5.06it/s][A[A

 94% 30/32 [00:07<00:00,  5.44it/s][A[A

 97% 31/32 [00:09<00:00,  2.07it/s][A[A

100% 32/32 [00:09<00:00,  2.44it/s][A[A100% 32/32 [00:09<00:00,  3.43it/s]
Meta loss on this task batch = 5.2531e-01, PNorm = 42.0569, GNorm = 0.0704

 26% 5/19 [00:44<02:07,  9.12s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  6.20it/s][A[A

  6% 2/32 [00:00<00:05,  5.70it/s][A[A

  9% 3/32 [00:00<00:04,  6.00it/s][A[A

 12% 4/32 [00:00<00:05,  5.36it/s][A[A

 16% 5/32 [00:00<00:05,  5.04it/s][A[A

 19% 6/32 [00:01<00:05,  4.75it/s][A[A

 22% 7/32 [00:01<00:05,  4.96it/s][A[A

 25% 8/32 [00:01<00:04,  5.17it/s][A[A

 28% 9/32 [00:01<00:04,  4.97it/s][A[A

 31% 10/32 [00:02<00:04,  4.66it/s][A[A

 34% 11/32 [00:03<00:10,  1.93it/s][A[A

 38% 12/32 [00:03<00:08,  2.33it/s][A[A

 41% 13/32 [00:03<00:06,  2.89it/s][A[A

 44% 14/32 [00:03<00:05,  3.23it/s][A[A

 47% 15/32 [00:04<00:04,  3.49it/s][A[A

 50% 16/32 [00:04<00:04,  3.63it/s][A[A

 53% 17/32 [00:04<00:03,  4.22it/s][A[A

 56% 18/32 [00:04<00:03,  4.33it/s][A[A

 59% 19/32 [00:04<00:02,  4.92it/s][A[A

 62% 20/32 [00:05<00:02,  4.95it/s][A[A

 66% 21/32 [00:05<00:02,  5.00it/s][A[A

 69% 22/32 [00:05<00:02,  4.86it/s][A[A

 72% 23/32 [00:05<00:01,  4.71it/s][A[A

 75% 24/32 [00:05<00:01,  4.87it/s][A[A

 78% 25/32 [00:06<00:01,  4.72it/s][A[A

 81% 26/32 [00:06<00:01,  5.21it/s][A[A

 84% 27/32 [00:07<00:02,  1.96it/s][A[A

 88% 28/32 [00:07<00:01,  2.34it/s][A[A

 91% 29/32 [00:07<00:01,  2.77it/s][A[A

 94% 30/32 [00:08<00:00,  3.24it/s][A[A

 97% 31/32 [00:08<00:00,  3.60it/s][A[A

100% 32/32 [00:08<00:00,  3.96it/s][A[A100% 32/32 [00:08<00:00,  3.75it/s]
Meta loss on this task batch = 4.4896e-01, PNorm = 42.0853, GNorm = 0.1476

 32% 6/19 [00:53<01:59,  9.17s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.03it/s][A[A

  6% 2/32 [00:00<00:05,  5.01it/s][A[A

  9% 3/32 [00:00<00:05,  4.95it/s][A[A

 12% 4/32 [00:00<00:05,  4.86it/s][A[A

 16% 5/32 [00:00<00:04,  5.46it/s][A[A

 19% 6/32 [00:01<00:04,  5.39it/s][A[A

 22% 7/32 [00:01<00:04,  5.08it/s][A[A

 25% 8/32 [00:01<00:04,  5.26it/s][A[A

 28% 9/32 [00:02<00:11,  2.05it/s][A[A

 31% 10/32 [00:02<00:08,  2.47it/s][A[A

 34% 11/32 [00:03<00:07,  2.87it/s][A[A

 38% 12/32 [00:03<00:06,  3.24it/s][A[A

 41% 13/32 [00:03<00:05,  3.47it/s][A[A

 44% 14/32 [00:03<00:04,  4.03it/s][A[A

 47% 15/32 [00:03<00:04,  4.09it/s][A[A

 50% 16/32 [00:04<00:03,  4.03it/s][A[A

 53% 17/32 [00:04<00:03,  4.43it/s][A[A

 56% 18/32 [00:04<00:03,  4.35it/s][A[A

 59% 19/32 [00:04<00:02,  4.64it/s][A[A

 62% 20/32 [00:06<00:06,  1.93it/s][A[A

 66% 21/32 [00:06<00:04,  2.33it/s][A[A

 69% 22/32 [00:06<00:03,  2.78it/s][A[A

 72% 23/32 [00:06<00:02,  3.10it/s][A[A

 75% 24/32 [00:06<00:02,  3.41it/s][A[A

 78% 25/32 [00:07<00:01,  3.87it/s][A[A

 81% 26/32 [00:07<00:01,  4.33it/s][A[A

 84% 27/32 [00:07<00:01,  4.30it/s][A[A

 88% 28/32 [00:07<00:00,  4.24it/s][A[A

 91% 29/32 [00:07<00:00,  4.40it/s][A[A

 94% 30/32 [00:08<00:00,  4.59it/s][A[A

 97% 31/32 [00:08<00:00,  5.48it/s][A[A

100% 32/32 [00:08<00:00,  5.01it/s][A[A100% 32/32 [00:08<00:00,  3.76it/s]
Meta loss on this task batch = 4.9089e-01, PNorm = 42.1160, GNorm = 0.1593

 37% 7/19 [01:03<01:50,  9.21s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:38,  1.25s/it][A[A

  6% 2/32 [00:01<00:28,  1.05it/s][A[A

  9% 3/32 [00:01<00:21,  1.35it/s][A[A

 12% 4/32 [00:02<00:16,  1.68it/s][A[A

 16% 5/32 [00:02<00:13,  2.07it/s][A[A

 19% 6/32 [00:02<00:10,  2.40it/s][A[A

 22% 7/32 [00:02<00:08,  2.79it/s][A[A

 25% 8/32 [00:02<00:07,  3.03it/s][A[A

 28% 9/32 [00:03<00:06,  3.34it/s][A[A

 31% 10/32 [00:03<00:06,  3.59it/s][A[A

 34% 11/32 [00:03<00:05,  4.17it/s][A[A

 38% 12/32 [00:04<00:10,  1.86it/s][A[A

 41% 13/32 [00:05<00:08,  2.29it/s][A[A

 44% 14/32 [00:05<00:06,  2.70it/s][A[A

 47% 15/32 [00:05<00:05,  3.19it/s][A[A

 50% 16/32 [00:05<00:04,  3.46it/s][A[A

 53% 17/32 [00:05<00:04,  3.55it/s][A[A

 56% 18/32 [00:06<00:03,  3.80it/s][A[A

 59% 19/32 [00:06<00:03,  3.84it/s][A[A

 62% 20/32 [00:06<00:02,  4.22it/s][A[A

 66% 21/32 [00:06<00:02,  4.24it/s][A[A

 69% 22/32 [00:07<00:02,  4.22it/s][A[A

 72% 23/32 [00:07<00:02,  4.35it/s][A[A

 75% 24/32 [00:08<00:04,  1.87it/s][A[A

 78% 25/32 [00:08<00:03,  2.24it/s][A[A

 81% 26/32 [00:08<00:02,  2.67it/s][A[A

 84% 27/32 [00:09<00:01,  2.97it/s][A[A

 88% 28/32 [00:09<00:01,  3.23it/s][A[A

 91% 29/32 [00:09<00:00,  3.44it/s][A[A

 94% 30/32 [00:09<00:00,  3.57it/s][A[A

 97% 31/32 [00:10<00:00,  3.77it/s][A[A

100% 32/32 [00:10<00:00,  3.85it/s][A[A100% 32/32 [00:10<00:00,  3.07it/s]
Meta loss on this task batch = 3.8056e-01, PNorm = 42.1525, GNorm = 0.1736

 42% 8/19 [01:14<01:48,  9.82s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:38,  1.25s/it][A[A

  6% 2/32 [00:01<00:28,  1.06it/s][A[A

  9% 3/32 [00:01<00:21,  1.37it/s][A[A

 12% 4/32 [00:01<00:16,  1.70it/s][A[A

 16% 5/32 [00:02<00:13,  2.05it/s][A[A

 19% 6/32 [00:02<00:10,  2.40it/s][A[A

 22% 7/32 [00:02<00:09,  2.77it/s][A[A

 25% 8/32 [00:02<00:07,  3.08it/s][A[A

 28% 9/32 [00:03<00:06,  3.31it/s][A[A

 31% 10/32 [00:03<00:06,  3.53it/s][A[A

 34% 11/32 [00:04<00:11,  1.76it/s][A[A

 38% 12/32 [00:04<00:09,  2.14it/s][A[A

 41% 13/32 [00:05<00:07,  2.50it/s][A[A

 44% 14/32 [00:05<00:06,  2.82it/s][A[A

 47% 15/32 [00:05<00:05,  3.16it/s][A[A

 50% 16/32 [00:05<00:04,  3.42it/s][A[A

 53% 17/32 [00:06<00:04,  3.60it/s][A[A

 56% 18/32 [00:06<00:03,  3.72it/s][A[A

 59% 19/32 [00:06<00:03,  3.75it/s][A[A

 62% 20/32 [00:06<00:03,  3.81it/s][A[A

 66% 21/32 [00:07<00:02,  3.94it/s][A[A

 69% 22/32 [00:07<00:02,  3.98it/s][A[A

 72% 23/32 [00:08<00:04,  1.80it/s][A[A

 75% 24/32 [00:08<00:03,  2.20it/s][A[A

 78% 25/32 [00:09<00:02,  2.63it/s][A[A

 81% 26/32 [00:09<00:01,  3.02it/s][A[A

 84% 27/32 [00:09<00:01,  3.31it/s][A[A

 88% 28/32 [00:09<00:01,  3.49it/s][A[A

 91% 29/32 [00:09<00:00,  3.69it/s][A[A

 94% 30/32 [00:10<00:00,  3.80it/s][A[A

 97% 31/32 [00:10<00:00,  3.84it/s][A[A

100% 32/32 [00:10<00:00,  3.94it/s][A[A100% 32/32 [00:10<00:00,  2.99it/s]
Meta loss on this task batch = 2.3884e-01, PNorm = 42.1977, GNorm = 0.1890

 47% 9/19 [01:26<01:43, 10.34s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.04it/s][A[A

  6% 2/32 [00:01<00:16,  1.85it/s][A[A

  9% 3/32 [00:01<00:13,  2.23it/s][A[A

 12% 4/32 [00:01<00:10,  2.61it/s][A[A

 16% 5/32 [00:02<00:09,  2.97it/s][A[A

 19% 6/32 [00:02<00:08,  3.22it/s][A[A

 22% 7/32 [00:02<00:07,  3.49it/s][A[A

 25% 8/32 [00:02<00:06,  3.64it/s][A[A

 28% 9/32 [00:03<00:06,  3.72it/s][A[A

 31% 10/32 [00:03<00:05,  3.84it/s][A[A

 34% 11/32 [00:03<00:05,  3.90it/s][A[A

 38% 12/32 [00:03<00:05,  3.87it/s][A[A

 41% 13/32 [00:05<00:10,  1.79it/s][A[A

 44% 14/32 [00:05<00:08,  2.14it/s][A[A

 47% 15/32 [00:05<00:06,  2.49it/s][A[A

 50% 16/32 [00:05<00:05,  2.87it/s][A[A

 53% 17/32 [00:06<00:04,  3.14it/s][A[A

 56% 18/32 [00:06<00:04,  3.40it/s][A[A

 59% 19/32 [00:06<00:03,  3.60it/s][A[A

 62% 20/32 [00:06<00:03,  3.73it/s][A[A

 66% 21/32 [00:07<00:02,  3.95it/s][A[A

 69% 22/32 [00:07<00:02,  4.02it/s][A[A

 72% 23/32 [00:07<00:02,  4.01it/s][A[A

 75% 24/32 [00:07<00:01,  4.02it/s][A[A

 78% 25/32 [00:08<00:01,  3.98it/s][A[A

 81% 26/32 [00:09<00:03,  1.86it/s][A[A

 84% 27/32 [00:09<00:02,  2.35it/s][A[A

 88% 28/32 [00:09<00:01,  2.72it/s][A[A

 91% 29/32 [00:09<00:00,  3.02it/s][A[A

 94% 30/32 [00:10<00:00,  3.30it/s][A[A

 97% 31/32 [00:10<00:00,  3.55it/s][A[A

100% 32/32 [00:10<00:00,  3.96it/s][A[A100% 32/32 [00:10<00:00,  3.03it/s]
Meta loss on this task batch = 2.8461e-01, PNorm = 42.2472, GNorm = 0.1107

 53% 10/19 [01:37<01:35, 10.65s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.85it/s][A[A

  6% 2/32 [00:00<00:07,  3.93it/s][A[A

  9% 3/32 [00:00<00:06,  4.61it/s][A[A

 12% 4/32 [00:00<00:05,  4.78it/s][A[A

 16% 5/32 [00:01<00:05,  4.64it/s][A[A

 19% 6/32 [00:01<00:05,  4.94it/s][A[A

 22% 7/32 [00:01<00:05,  4.70it/s][A[A

 25% 8/32 [00:02<00:12,  1.91it/s][A[A

 28% 9/32 [00:02<00:09,  2.48it/s][A[A

 31% 10/32 [00:03<00:07,  2.82it/s][A[A

 34% 11/32 [00:03<00:06,  3.10it/s][A[A

 38% 12/32 [00:03<00:05,  3.36it/s][A[A

 41% 13/32 [00:03<00:05,  3.56it/s][A[A

 44% 14/32 [00:04<00:04,  3.76it/s][A[A

 47% 15/32 [00:04<00:04,  4.06it/s][A[A

 50% 16/32 [00:04<00:03,  4.01it/s][A[A

 53% 17/32 [00:04<00:03,  4.64it/s][A[A

 56% 18/32 [00:05<00:07,  1.90it/s][A[A

 59% 19/32 [00:06<00:05,  2.25it/s][A[A

 62% 20/32 [00:06<00:04,  2.92it/s][A[A

 66% 21/32 [00:06<00:03,  3.35it/s][A[A

 69% 22/32 [00:06<00:02,  3.53it/s][A[A

 72% 23/32 [00:06<00:02,  3.73it/s][A[A

 75% 24/32 [00:07<00:02,  3.91it/s][A[A

 78% 25/32 [00:07<00:01,  4.16it/s][A[A

 81% 26/32 [00:07<00:01,  4.14it/s][A[A

 84% 27/32 [00:07<00:01,  4.16it/s][A[A

 88% 28/32 [00:08<00:00,  4.36it/s][A[A

 91% 29/32 [00:08<00:00,  4.31it/s][A[A

 94% 30/32 [00:08<00:00,  4.21it/s][A[A

 97% 31/32 [00:08<00:00,  4.95it/s][A[A

100% 32/32 [00:08<00:00,  4.89it/s][A[A100% 32/32 [00:08<00:00,  3.62it/s]
Meta loss on this task batch = 6.5505e-01, PNorm = 42.2864, GNorm = 0.2266

 58% 11/19 [01:47<01:22, 10.35s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:38,  1.25s/it][A[A

  6% 2/32 [00:01<00:28,  1.06it/s][A[A

  9% 3/32 [00:01<00:21,  1.38it/s][A[A

 12% 4/32 [00:01<00:16,  1.72it/s][A[A

 16% 5/32 [00:02<00:12,  2.09it/s][A[A

 19% 6/32 [00:02<00:10,  2.52it/s][A[A

 22% 7/32 [00:02<00:08,  2.83it/s][A[A

 25% 8/32 [00:02<00:07,  3.09it/s][A[A

 28% 9/32 [00:04<00:13,  1.65it/s][A[A

 31% 10/32 [00:04<00:10,  2.09it/s][A[A

 34% 11/32 [00:04<00:08,  2.45it/s][A[A

 38% 12/32 [00:04<00:06,  2.88it/s][A[A

 41% 13/32 [00:05<00:06,  3.16it/s][A[A

 44% 14/32 [00:05<00:05,  3.58it/s][A[A

 47% 15/32 [00:05<00:04,  3.81it/s][A[A

 50% 16/32 [00:05<00:04,  3.91it/s][A[A

 53% 17/32 [00:05<00:03,  3.97it/s][A[A

 56% 18/32 [00:06<00:03,  4.05it/s][A[A

 59% 19/32 [00:06<00:03,  4.13it/s][A[A

 62% 20/32 [00:06<00:02,  4.91it/s][A[A

 66% 21/32 [00:07<00:05,  1.97it/s][A[A

 69% 22/32 [00:07<00:04,  2.34it/s][A[A

 72% 23/32 [00:08<00:03,  2.69it/s][A[A

 75% 24/32 [00:08<00:02,  3.02it/s][A[A

 78% 25/32 [00:08<00:02,  3.26it/s][A[A

 81% 26/32 [00:08<00:01,  3.47it/s][A[A

 84% 27/32 [00:09<00:01,  3.65it/s][A[A

 88% 28/32 [00:10<00:02,  1.82it/s][A[A

 91% 29/32 [00:10<00:01,  2.25it/s][A[A

 94% 30/32 [00:10<00:00,  2.67it/s][A[A

 97% 31/32 [00:10<00:00,  3.10it/s][A[A

100% 32/32 [00:11<00:00,  3.33it/s][A[A100% 32/32 [00:11<00:00,  2.85it/s]
Meta loss on this task batch = 5.1992e-01, PNorm = 42.3270, GNorm = 0.1255

 63% 12/19 [01:59<01:16, 10.86s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.29it/s][A[A

  6% 2/32 [00:00<00:07,  4.24it/s][A[A

  9% 3/32 [00:00<00:06,  4.26it/s][A[A

 12% 4/32 [00:00<00:06,  4.28it/s][A[A

 16% 5/32 [00:01<00:05,  5.00it/s][A[A

 19% 6/32 [00:01<00:05,  4.69it/s][A[A

 22% 7/32 [00:02<00:13,  1.91it/s][A[A

 25% 8/32 [00:02<00:09,  2.40it/s][A[A

 28% 9/32 [00:02<00:08,  2.74it/s][A[A

 31% 10/32 [00:03<00:07,  3.06it/s][A[A

 34% 11/32 [00:03<00:06,  3.44it/s][A[A

 38% 12/32 [00:03<00:05,  3.61it/s][A[A

 41% 13/32 [00:03<00:05,  3.72it/s][A[A

 44% 14/32 [00:04<00:04,  3.82it/s][A[A

 47% 15/32 [00:04<00:04,  4.10it/s][A[A

 50% 16/32 [00:04<00:03,  4.34it/s][A[A

 53% 17/32 [00:04<00:03,  4.49it/s][A[A

 56% 18/32 [00:04<00:03,  4.48it/s][A[A

 59% 19/32 [00:05<00:02,  4.51it/s][A[A

 62% 20/32 [00:06<00:06,  1.90it/s][A[A

 66% 21/32 [00:06<00:04,  2.28it/s][A[A

 69% 22/32 [00:06<00:03,  2.65it/s][A[A

 72% 23/32 [00:07<00:02,  3.02it/s][A[A

 75% 24/32 [00:07<00:02,  3.41it/s][A[A

 78% 25/32 [00:07<00:01,  3.98it/s][A[A

 81% 26/32 [00:07<00:01,  3.96it/s][A[A

 84% 27/32 [00:07<00:01,  4.16it/s][A[A

 88% 28/32 [00:08<00:00,  4.13it/s][A[A

 91% 29/32 [00:08<00:00,  4.18it/s][A[A

 94% 30/32 [00:08<00:00,  4.46it/s][A[A

 97% 31/32 [00:09<00:00,  1.90it/s][A[A

100% 32/32 [00:10<00:00,  2.35it/s][A[A100% 32/32 [00:10<00:00,  3.19it/s]
Meta loss on this task batch = 6.0041e-01, PNorm = 42.3590, GNorm = 0.1953

 68% 13/19 [02:09<01:05, 10.85s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.60it/s][A[A

  6% 2/32 [00:00<00:05,  5.45it/s][A[A

  9% 3/32 [00:00<00:05,  5.02it/s][A[A

 12% 4/32 [00:00<00:05,  4.72it/s][A[A

 16% 5/32 [00:01<00:05,  4.68it/s][A[A

 19% 6/32 [00:01<00:05,  4.79it/s][A[A

 22% 7/32 [00:01<00:05,  4.63it/s][A[A

 25% 8/32 [00:01<00:05,  4.63it/s][A[A

 28% 9/32 [00:01<00:05,  4.48it/s][A[A

 31% 10/32 [00:02<00:05,  4.36it/s][A[A

 34% 11/32 [00:02<00:04,  4.39it/s][A[A

 38% 12/32 [00:02<00:04,  4.28it/s][A[A

 41% 13/32 [00:03<00:09,  1.90it/s][A[A

 44% 14/32 [00:04<00:07,  2.28it/s][A[A

 47% 15/32 [00:04<00:06,  2.71it/s][A[A

 50% 16/32 [00:04<00:05,  3.10it/s][A[A

 53% 17/32 [00:04<00:04,  3.37it/s][A[A

 56% 18/32 [00:05<00:03,  3.54it/s][A[A

 59% 19/32 [00:05<00:03,  3.70it/s][A[A

 62% 20/32 [00:05<00:03,  3.77it/s][A[A

 66% 21/32 [00:05<00:02,  4.02it/s][A[A

 69% 22/32 [00:05<00:02,  4.13it/s][A[A

 72% 23/32 [00:06<00:02,  4.14it/s][A[A

 75% 24/32 [00:06<00:01,  4.21it/s][A[A

 78% 25/32 [00:07<00:03,  1.85it/s][A[A

 81% 26/32 [00:07<00:02,  2.25it/s][A[A

 84% 27/32 [00:08<00:01,  2.69it/s][A[A

 88% 28/32 [00:08<00:01,  2.99it/s][A[A

 91% 29/32 [00:08<00:00,  3.30it/s][A[A

 94% 30/32 [00:08<00:00,  3.63it/s][A[A

 97% 31/32 [00:09<00:00,  3.79it/s][A[A

100% 32/32 [00:09<00:00,  3.78it/s][A[A100% 32/32 [00:09<00:00,  3.45it/s]
Meta loss on this task batch = 5.5997e-01, PNorm = 42.3895, GNorm = 0.1000

 74% 14/19 [02:20<00:53, 10.62s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  6.71it/s][A[A

  6% 2/32 [00:00<00:05,  5.42it/s][A[A

  9% 3/32 [00:00<00:05,  4.87it/s][A[A

 12% 4/32 [00:01<00:14,  1.95it/s][A[A

 16% 5/32 [00:02<00:11,  2.34it/s][A[A

 19% 6/32 [00:02<00:09,  2.72it/s][A[A

 22% 7/32 [00:02<00:08,  2.97it/s][A[A

 25% 8/32 [00:02<00:06,  3.45it/s][A[A

 28% 9/32 [00:03<00:06,  3.50it/s][A[A

 31% 10/32 [00:03<00:05,  3.75it/s][A[A

 34% 11/32 [00:03<00:04,  4.21it/s][A[A

 38% 12/32 [00:03<00:04,  4.69it/s][A[A

 41% 13/32 [00:03<00:04,  4.59it/s][A[A

 44% 14/32 [00:04<00:04,  4.43it/s][A[A

 47% 15/32 [00:05<00:08,  1.94it/s][A[A

 50% 16/32 [00:05<00:06,  2.34it/s][A[A

 53% 17/32 [00:05<00:05,  2.77it/s][A[A

 56% 18/32 [00:05<00:04,  3.12it/s][A[A

 59% 19/32 [00:06<00:03,  3.72it/s][A[A

 62% 20/32 [00:06<00:03,  3.80it/s][A[A

 66% 21/32 [00:06<00:02,  4.21it/s][A[A

 69% 22/32 [00:06<00:02,  4.29it/s][A[A

 72% 23/32 [00:06<00:02,  4.25it/s][A[A

 75% 24/32 [00:07<00:01,  4.33it/s][A[A

 78% 25/32 [00:07<00:01,  5.06it/s][A[A

 81% 26/32 [00:08<00:03,  1.93it/s][A[A

 84% 27/32 [00:08<00:02,  2.28it/s][A[A

 88% 28/32 [00:09<00:01,  2.72it/s][A[A

 91% 29/32 [00:09<00:01,  3.00it/s][A[A

 94% 30/32 [00:09<00:00,  3.26it/s][A[A

 97% 31/32 [00:09<00:00,  3.79it/s][A[A

100% 32/32 [00:09<00:00,  3.87it/s][A[A100% 32/32 [00:09<00:00,  3.22it/s]
Meta loss on this task batch = 4.7450e-01, PNorm = 42.4155, GNorm = 0.1351

 79% 15/19 [02:30<00:42, 10.66s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.87it/s][A[A

  6% 2/32 [00:00<00:05,  5.20it/s][A[A

  9% 3/32 [00:00<00:05,  5.45it/s][A[A

 12% 4/32 [00:00<00:05,  5.43it/s][A[A

 16% 5/32 [00:00<00:05,  4.89it/s][A[A

 19% 6/32 [00:02<00:13,  1.94it/s][A[A

 22% 7/32 [00:02<00:10,  2.37it/s][A[A

 25% 8/32 [00:02<00:08,  2.99it/s][A[A

 28% 9/32 [00:02<00:06,  3.46it/s][A[A

 31% 10/32 [00:02<00:06,  3.66it/s][A[A

 34% 11/32 [00:03<00:05,  3.75it/s][A[A

 38% 12/32 [00:03<00:05,  3.94it/s][A[A

 41% 13/32 [00:03<00:04,  4.09it/s][A[A

 44% 14/32 [00:03<00:04,  4.17it/s][A[A

 47% 15/32 [00:04<00:04,  4.22it/s][A[A

 50% 16/32 [00:04<00:03,  4.72it/s][A[A

 53% 17/32 [00:04<00:03,  4.61it/s][A[A

 56% 18/32 [00:04<00:02,  4.86it/s][A[A

 59% 19/32 [00:04<00:02,  4.64it/s][A[A

 62% 20/32 [00:06<00:06,  1.91it/s][A[A

 66% 21/32 [00:06<00:04,  2.29it/s][A[A

 69% 22/32 [00:06<00:03,  2.74it/s][A[A

 72% 23/32 [00:06<00:02,  3.05it/s][A[A

 75% 24/32 [00:07<00:02,  3.23it/s][A[A

 78% 25/32 [00:07<00:01,  3.83it/s][A[A

 81% 26/32 [00:07<00:01,  4.11it/s][A[A

 84% 27/32 [00:07<00:01,  4.02it/s][A[A

 88% 28/32 [00:07<00:00,  4.14it/s][A[A

 91% 29/32 [00:08<00:00,  3.94it/s][A[A

 94% 30/32 [00:09<00:01,  1.81it/s][A[A

 97% 31/32 [00:09<00:00,  2.18it/s][A[A

100% 32/32 [00:09<00:00,  2.53it/s][A[A100% 32/32 [00:09<00:00,  3.21it/s]
Meta loss on this task batch = 5.5164e-01, PNorm = 42.4359, GNorm = 0.1471

 84% 16/19 [02:41<00:32, 10.68s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.87it/s][A[A

  6% 2/32 [00:00<00:07,  3.83it/s][A[A

  9% 3/32 [00:00<00:06,  4.21it/s][A[A

 12% 4/32 [00:00<00:06,  4.48it/s][A[A

 16% 5/32 [00:01<00:06,  4.31it/s][A[A

 19% 6/32 [00:01<00:05,  4.88it/s][A[A

 22% 7/32 [00:01<00:05,  4.72it/s][A[A

 25% 8/32 [00:01<00:05,  4.43it/s][A[A

 28% 9/32 [00:03<00:12,  1.88it/s][A[A

 31% 10/32 [00:03<00:09,  2.41it/s][A[A

 34% 11/32 [00:03<00:07,  2.79it/s][A[A

 38% 12/32 [00:03<00:06,  3.24it/s][A[A

 41% 13/32 [00:03<00:05,  3.51it/s][A[A

 44% 14/32 [00:04<00:04,  3.84it/s][A[A

 47% 15/32 [00:04<00:04,  4.14it/s][A[A

 50% 16/32 [00:04<00:03,  4.33it/s][A[A

 53% 17/32 [00:04<00:03,  4.59it/s][A[A

 56% 18/32 [00:04<00:03,  4.42it/s][A[A

 59% 19/32 [00:05<00:03,  4.22it/s][A[A

 62% 20/32 [00:05<00:02,  4.29it/s][A[A

 66% 21/32 [00:06<00:05,  1.86it/s][A[A

 69% 22/32 [00:06<00:04,  2.22it/s][A[A

 72% 23/32 [00:07<00:03,  2.66it/s][A[A

 75% 24/32 [00:07<00:02,  2.92it/s][A[A

 78% 25/32 [00:07<00:02,  3.29it/s][A[A

 81% 26/32 [00:07<00:01,  3.51it/s][A[A

 84% 27/32 [00:07<00:01,  3.95it/s][A[A

 88% 28/32 [00:08<00:00,  4.27it/s][A[A

 91% 29/32 [00:08<00:00,  4.28it/s][A[A

 94% 30/32 [00:08<00:00,  4.20it/s][A[A

 97% 31/32 [00:08<00:00,  4.34it/s][A[A

100% 32/32 [00:09<00:00,  4.26it/s][A[A100% 32/32 [00:09<00:00,  3.53it/s]
Meta loss on this task batch = 4.1682e-01, PNorm = 42.4581, GNorm = 0.0793

 89% 17/19 [02:51<00:20, 10.43s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.03it/s][A[A

  6% 2/32 [00:00<00:06,  4.80it/s][A[A

  9% 3/32 [00:01<00:14,  1.95it/s][A[A

 12% 4/32 [00:01<00:11,  2.35it/s][A[A

 16% 5/32 [00:02<00:10,  2.68it/s][A[A

 19% 6/32 [00:02<00:08,  3.07it/s][A[A

 22% 7/32 [00:02<00:07,  3.28it/s][A[A

 25% 8/32 [00:02<00:06,  3.45it/s][A[A

 28% 9/32 [00:03<00:06,  3.69it/s][A[A

 31% 10/32 [00:03<00:05,  3.82it/s][A[A

 34% 11/32 [00:03<00:05,  4.00it/s][A[A

 38% 12/32 [00:03<00:04,  4.38it/s][A[A

 41% 13/32 [00:04<00:10,  1.86it/s][A[A

 44% 14/32 [00:05<00:07,  2.38it/s][A[A

 47% 15/32 [00:05<00:06,  2.68it/s][A[A

 50% 16/32 [00:05<00:05,  3.06it/s][A[A

 53% 17/32 [00:05<00:04,  3.73it/s][A[A

 56% 18/32 [00:05<00:03,  3.95it/s][A[A

 59% 19/32 [00:06<00:03,  3.99it/s][A[A

 62% 20/32 [00:06<00:02,  4.08it/s][A[A

 66% 21/32 [00:06<00:02,  4.16it/s][A[A

 69% 22/32 [00:06<00:02,  4.80it/s][A[A

 72% 23/32 [00:07<00:01,  4.65it/s][A[A

 75% 24/32 [00:07<00:01,  4.22it/s][A[A

 78% 25/32 [00:07<00:01,  4.28it/s][A[A

 81% 26/32 [00:08<00:03,  1.89it/s][A[A

 84% 27/32 [00:09<00:02,  2.20it/s][A[A

 88% 28/32 [00:09<00:01,  2.65it/s][A[A

 94% 30/32 [00:09<00:00,  3.15it/s][A[A

 97% 31/32 [00:09<00:00,  3.49it/s][A[A

100% 32/32 [00:10<00:00,  3.53it/s][A[A100% 32/32 [00:10<00:00,  3.17it/s]
Meta loss on this task batch = 4.8075e-01, PNorm = 42.4804, GNorm = 0.0787

 95% 18/19 [03:02<00:10, 10.57s/it][A

  0% 0/23 [00:00<?, ?it/s][A[A

  4% 1/23 [00:00<00:05,  4.11it/s][A[A

  9% 2/23 [00:00<00:04,  4.55it/s][A[A

 13% 3/23 [00:00<00:04,  4.59it/s][A[A

 17% 4/23 [00:00<00:03,  5.02it/s][A[A

 22% 5/23 [00:00<00:03,  5.12it/s][A[A

 26% 6/23 [00:01<00:03,  4.98it/s][A[A

 30% 7/23 [00:01<00:03,  4.72it/s][A[A

 35% 8/23 [00:01<00:03,  4.58it/s][A[A

 39% 9/23 [00:01<00:02,  5.18it/s][A[A

 43% 10/23 [00:03<00:06,  1.93it/s][A[A

 48% 11/23 [00:03<00:05,  2.35it/s][A[A

 52% 12/23 [00:03<00:04,  2.72it/s][A[A

 57% 13/23 [00:03<00:02,  3.33it/s][A[A

 61% 14/23 [00:03<00:02,  3.53it/s][A[A

 65% 15/23 [00:04<00:02,  3.20it/s][A[A

 70% 16/23 [00:04<00:02,  3.35it/s][A[A

 74% 17/23 [00:04<00:01,  3.87it/s][A[A

 78% 18/23 [00:04<00:01,  4.09it/s][A[A

 83% 19/23 [00:05<00:00,  4.09it/s][A[A

 87% 20/23 [00:06<00:01,  1.97it/s][A[A

 91% 21/23 [00:06<00:00,  2.46it/s][A[A

 96% 22/23 [00:06<00:00,  3.09it/s][A[A

100% 23/23 [00:06<00:00,  3.66it/s][A[A100% 23/23 [00:06<00:00,  3.42it/s]
Meta loss on this task batch = 4.2814e-01, PNorm = 42.5025, GNorm = 0.0907

100% 19/19 [03:09<00:00,  9.59s/it][A100% 19/19 [03:09<00:00,  9.98s/it]
Took 189.59641075134277 seconds to complete one epoch of meta training

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/20 [00:00<?, ?it/s][A[A


  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 30.70it/s]


  5% 1/20 [00:00<00:02,  8.03it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 20.73it/s]


 10% 2/20 [00:00<00:02,  6.12it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 18.09it/s][A[A[A100% 3/3 [00:00<00:00, 19.76it/s]


 15% 3/20 [00:00<00:03,  4.92it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 15.59it/s]


 20% 4/20 [00:00<00:03,  4.77it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 14.58it/s][A[A[A


100% 4/4 [00:00<00:00, 15.86it/s][A[A[A100% 4/4 [00:00<00:00, 16.82it/s]


 25% 5/20 [00:01<00:03,  3.78it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.01it/s][A[A[A100% 4/4 [00:00<00:00, 15.79it/s]


 30% 6/20 [00:01<00:04,  3.25it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.03it/s][A[A[A100% 4/4 [00:00<00:00, 20.81it/s]


 35% 7/20 [00:02<00:04,  3.10it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 17.16it/s][A[A[A100% 4/4 [00:00<00:00, 19.53it/s]


 40% 8/20 [00:03<00:07,  1.58it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.72it/s][A[A[A100% 4/4 [00:00<00:00, 23.40it/s]


 45% 9/20 [00:03<00:05,  1.85it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.16it/s][A[A[A


100% 4/4 [00:00<00:00, 18.98it/s][A[A[A100% 4/4 [00:00<00:00, 18.84it/s]


 50% 10/20 [00:04<00:04,  2.04it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 17.20it/s][A[A[A100% 4/4 [00:00<00:00, 19.70it/s]


 55% 11/20 [00:04<00:04,  2.20it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.39it/s][A[A[A100% 4/4 [00:00<00:00, 21.68it/s]


 60% 12/20 [00:04<00:03,  2.37it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.25it/s][A[A[A100% 3/3 [00:00<00:00, 13.52it/s]


 65% 13/20 [00:05<00:02,  2.40it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 13.42it/s][A[A[A


100% 3/3 [00:01<00:00,  2.63it/s][A[A[A100% 3/3 [00:01<00:00,  2.41it/s]


 70% 14/20 [00:06<00:04,  1.39it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.92it/s][A[A[A100% 4/4 [00:00<00:00, 18.16it/s]


 75% 15/20 [00:07<00:03,  1.61it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.27it/s][A[A[A100% 3/3 [00:00<00:00, 17.66it/s]


 80% 16/20 [00:07<00:02,  1.87it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 12.17it/s][A[A[A100% 3/3 [00:00<00:00, 14.98it/s]


 85% 17/20 [00:07<00:01,  2.03it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 22.47it/s]


 90% 18/20 [00:08<00:00,  2.35it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 15.86it/s][A[A[A100% 3/3 [00:00<00:00, 20.05it/s]


 95% 19/20 [00:08<00:00,  2.57it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 13.70it/s]


100% 20/20 [00:08<00:00,  2.88it/s][A[A100% 20/20 [00:08<00:00,  2.32it/s]

100% 1/1 [00:08<00:00,  8.60s/it][A100% 1/1 [00:08<00:00,  8.60s/it]
Took 198.2026343345642 seconds to complete one epoch of meta training and validating
Meta validation score auc = 0.643305
 90% 27/30 [1:27:08<09:43, 194.57s/it]Epoch 27

  0% 0/19 [00:00<?, ?it/s][A

  0% 0/32 [00:00<?, ?it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  3% 1/32 [00:00<00:04,  6.45it/s][A[A

  6% 2/32 [00:00<00:05,  5.87it/s][A[A

  9% 3/32 [00:00<00:04,  5.96it/s][A[A

 12% 4/32 [00:00<00:04,  5.70it/s][A[A

 16% 5/32 [00:00<00:04,  5.56it/s][A[A

 19% 6/32 [00:02<00:12,  2.05it/s][A[A

 22% 7/32 [00:02<00:09,  2.51it/s][A[A

 25% 8/32 [00:02<00:08,  2.95it/s][A[A

 28% 9/32 [00:02<00:06,  3.43it/s][A[A

 31% 10/32 [00:02<00:05,  3.86it/s][A[A

 34% 11/32 [00:03<00:05,  4.17it/s][A[A

 38% 12/32 [00:03<00:04,  4.08it/s][A[A

 41% 13/32 [00:03<00:04,  4.13it/s][A[A

 44% 14/32 [00:03<00:04,  4.14it/s][A[A

 47% 15/32 [00:03<00:03,  4.80it/s][A[A

 50% 16/32 [00:04<00:03,  4.80it/s][A[A

 53% 17/32 [00:04<00:03,  4.94it/s][A[A

 56% 18/32 [00:04<00:02,  5.08it/s][A[A

 59% 19/32 [00:04<00:02,  5.11it/s][A[A

 62% 20/32 [00:04<00:02,  5.19it/s][A[A

 66% 21/32 [00:06<00:05,  2.05it/s][A[A

 69% 22/32 [00:06<00:03,  2.51it/s][A[A

 72% 23/32 [00:06<00:02,  3.22it/s][A[A

 75% 24/32 [00:06<00:02,  3.64it/s][A[A

 78% 25/32 [00:06<00:01,  4.03it/s][A[A

 81% 26/32 [00:06<00:01,  4.21it/s][A[A

 84% 27/32 [00:07<00:00,  5.09it/s][A[A

 88% 28/32 [00:07<00:00,  5.33it/s][A[A

 91% 29/32 [00:07<00:00,  4.98it/s][A[A

 94% 30/32 [00:07<00:00,  5.31it/s][A[A

 97% 31/32 [00:07<00:00,  6.00it/s][A[A

100% 32/32 [00:07<00:00,  5.71it/s][A[A100% 32/32 [00:07<00:00,  4.04it/s]
Meta loss on this task batch = 4.9333e-01, PNorm = 42.5308, GNorm = 0.1520

  5% 1/19 [00:08<02:35,  8.64s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.60it/s][A[A

  6% 2/32 [00:00<00:05,  5.23it/s][A[A

  9% 3/32 [00:00<00:06,  4.73it/s][A[A

 12% 4/32 [00:00<00:05,  5.03it/s][A[A

 16% 5/32 [00:00<00:04,  5.44it/s][A[A

 19% 6/32 [00:01<00:05,  5.15it/s][A[A

 22% 7/32 [00:01<00:04,  5.34it/s][A[A

 25% 8/32 [00:01<00:04,  5.53it/s][A[A

 28% 9/32 [00:02<00:11,  2.05it/s][A[A

 31% 10/32 [00:02<00:08,  2.55it/s][A[A

 34% 11/32 [00:03<00:06,  3.18it/s][A[A

 38% 12/32 [00:03<00:05,  3.36it/s][A[A

 41% 13/32 [00:03<00:04,  3.82it/s][A[A

 44% 14/32 [00:03<00:04,  4.18it/s][A[A

 47% 15/32 [00:03<00:03,  4.60it/s][A[A

 50% 16/32 [00:03<00:03,  5.15it/s][A[A

 53% 17/32 [00:04<00:02,  5.20it/s][A[A

 56% 18/32 [00:04<00:02,  4.92it/s][A[A

 59% 19/32 [00:04<00:02,  5.78it/s][A[A

 62% 20/32 [00:04<00:02,  5.11it/s][A[A

 66% 21/32 [00:04<00:01,  5.52it/s][A[A

 69% 22/32 [00:05<00:01,  5.46it/s][A[A

 72% 23/32 [00:05<00:01,  5.72it/s][A[A

 75% 24/32 [00:05<00:01,  5.76it/s][A[A

 78% 25/32 [00:05<00:01,  5.47it/s][A[A

 81% 26/32 [00:05<00:01,  5.08it/s][A[A

 84% 27/32 [00:07<00:02,  1.98it/s][A[A

 88% 28/32 [00:07<00:01,  2.46it/s][A[A

 91% 29/32 [00:07<00:01,  2.99it/s][A[A

 94% 30/32 [00:07<00:00,  3.49it/s][A[A

 97% 31/32 [00:07<00:00,  4.07it/s][A[A

100% 32/32 [00:07<00:00,  4.53it/s][A[A100% 32/32 [00:07<00:00,  4.06it/s]
Meta loss on this task batch = 4.6685e-01, PNorm = 42.5642, GNorm = 0.1170

 11% 2/19 [00:17<02:26,  8.63s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.63it/s][A[A

  6% 2/32 [00:00<00:05,  5.87it/s][A[A

  9% 3/32 [00:00<00:04,  6.01it/s][A[A

 12% 4/32 [00:00<00:04,  5.66it/s][A[A

 16% 5/32 [00:00<00:05,  5.39it/s][A[A

 19% 6/32 [00:01<00:04,  5.39it/s][A[A

 22% 7/32 [00:01<00:04,  6.01it/s][A[A

 25% 8/32 [00:01<00:03,  6.01it/s][A[A

 28% 9/32 [00:01<00:04,  5.61it/s][A[A

 31% 10/32 [00:01<00:04,  5.14it/s][A[A

 34% 11/32 [00:03<00:10,  1.99it/s][A[A

 38% 12/32 [00:03<00:08,  2.46it/s][A[A

 41% 13/32 [00:03<00:06,  2.88it/s][A[A

 44% 14/32 [00:03<00:05,  3.24it/s][A[A

 47% 15/32 [00:03<00:04,  3.86it/s][A[A

 50% 16/32 [00:03<00:03,  4.17it/s][A[A

 53% 17/32 [00:04<00:03,  4.77it/s][A[A

 56% 18/32 [00:04<00:02,  5.10it/s][A[A

 59% 19/32 [00:04<00:02,  5.26it/s][A[A

 62% 20/32 [00:04<00:02,  5.51it/s][A[A

 66% 21/32 [00:04<00:02,  5.37it/s][A[A

 69% 22/32 [00:05<00:01,  5.30it/s][A[A

 72% 23/32 [00:05<00:01,  5.34it/s][A[A

 75% 24/32 [00:05<00:01,  5.36it/s][A[A

 78% 25/32 [00:05<00:01,  5.89it/s][A[A

 81% 26/32 [00:05<00:00,  6.35it/s][A[A

 84% 27/32 [00:05<00:00,  5.43it/s][A[A

 88% 28/32 [00:07<00:01,  2.03it/s][A[A

 91% 29/32 [00:07<00:01,  2.49it/s][A[A

 94% 30/32 [00:07<00:00,  2.84it/s][A[A

 97% 31/32 [00:07<00:00,  3.38it/s][A[A

100% 32/32 [00:07<00:00,  3.78it/s][A[A100% 32/32 [00:07<00:00,  4.06it/s]
Meta loss on this task batch = 5.1821e-01, PNorm = 42.5931, GNorm = 0.1720

 16% 3/19 [00:25<02:18,  8.63s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.87it/s][A[A

  6% 2/32 [00:00<00:06,  4.87it/s][A[A

  9% 3/32 [00:00<00:05,  4.98it/s][A[A

 12% 4/32 [00:00<00:05,  5.15it/s][A[A

 16% 5/32 [00:01<00:05,  4.92it/s][A[A

 19% 6/32 [00:01<00:04,  5.40it/s][A[A

 22% 7/32 [00:01<00:04,  5.26it/s][A[A

 25% 8/32 [00:01<00:04,  5.93it/s][A[A

 28% 9/32 [00:01<00:03,  5.83it/s][A[A

 34% 11/32 [00:02<00:06,  3.22it/s][A[A

 38% 12/32 [00:03<00:05,  3.65it/s][A[A

 41% 13/32 [00:03<00:04,  4.05it/s][A[A

 44% 14/32 [00:03<00:03,  4.50it/s][A[A

 47% 15/32 [00:03<00:03,  4.88it/s][A[A

 50% 16/32 [00:03<00:03,  5.17it/s][A[A

 53% 17/32 [00:03<00:02,  6.03it/s][A[A

 56% 18/32 [00:04<00:02,  5.44it/s][A[A

 59% 19/32 [00:04<00:02,  5.75it/s][A[A

 62% 20/32 [00:04<00:02,  5.80it/s][A[A

 66% 21/32 [00:04<00:01,  5.97it/s][A[A

 69% 22/32 [00:04<00:01,  5.39it/s][A[A

 72% 23/32 [00:05<00:01,  5.14it/s][A[A

 75% 24/32 [00:05<00:01,  5.15it/s][A[A

 78% 25/32 [00:05<00:01,  5.29it/s][A[A

 81% 26/32 [00:05<00:01,  5.26it/s][A[A

 84% 27/32 [00:05<00:00,  5.43it/s][A[A

 88% 28/32 [00:05<00:00,  5.43it/s][A[A

 91% 29/32 [00:06<00:00,  5.50it/s][A[A

 94% 30/32 [00:06<00:00,  5.28it/s][A[A

 97% 31/32 [00:07<00:00,  2.01it/s][A[A

100% 32/32 [00:07<00:00,  2.52it/s][A[A100% 32/32 [00:07<00:00,  4.15it/s]
Meta loss on this task batch = 5.3929e-01, PNorm = 42.6248, GNorm = 0.1393

 21% 4/19 [00:34<02:08,  8.57s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.83it/s][A[A

  6% 2/32 [00:00<00:05,  5.61it/s][A[A

 12% 4/32 [00:00<00:04,  6.02it/s][A[A

 16% 5/32 [00:00<00:04,  5.65it/s][A[A

 19% 6/32 [00:01<00:04,  5.57it/s][A[A

 22% 7/32 [00:01<00:04,  5.21it/s][A[A

 25% 8/32 [00:01<00:04,  5.33it/s][A[A

 28% 9/32 [00:01<00:04,  5.54it/s][A[A

 31% 10/32 [00:01<00:04,  5.34it/s][A[A

 34% 11/32 [00:01<00:03,  5.39it/s][A[A

 38% 12/32 [00:02<00:03,  5.07it/s][A[A

 41% 13/32 [00:02<00:03,  5.00it/s][A[A

 44% 14/32 [00:03<00:09,  1.98it/s][A[A

 47% 15/32 [00:03<00:07,  2.40it/s][A[A

 50% 16/32 [00:03<00:05,  3.07it/s][A[A

 53% 17/32 [00:04<00:04,  3.53it/s][A[A

 56% 18/32 [00:04<00:03,  3.81it/s][A[A

 59% 19/32 [00:04<00:03,  4.16it/s][A[A

 62% 20/32 [00:04<00:02,  4.14it/s][A[A

 66% 21/32 [00:04<00:02,  4.72it/s][A[A

 69% 22/32 [00:05<00:01,  5.43it/s][A[A

 72% 23/32 [00:05<00:01,  5.19it/s][A[A

 75% 24/32 [00:05<00:01,  5.08it/s][A[A

 78% 25/32 [00:05<00:01,  4.95it/s][A[A

 81% 26/32 [00:05<00:01,  4.91it/s][A[A

 84% 27/32 [00:06<00:01,  4.93it/s][A[A

 88% 28/32 [00:06<00:00,  5.25it/s][A[A

 91% 29/32 [00:07<00:01,  2.09it/s][A[A

 94% 30/32 [00:07<00:00,  2.51it/s][A[A

 97% 31/32 [00:07<00:00,  2.95it/s][A[A

100% 32/32 [00:08<00:00,  3.37it/s][A[A100% 32/32 [00:08<00:00,  4.00it/s]
Meta loss on this task batch = 5.1499e-01, PNorm = 42.6568, GNorm = 0.0964

 26% 5/19 [00:43<02:00,  8.62s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.97it/s][A[A

  6% 2/32 [00:00<00:05,  5.29it/s][A[A

  9% 3/32 [00:00<00:05,  5.56it/s][A[A

 12% 4/32 [00:00<00:04,  6.09it/s][A[A

 16% 5/32 [00:00<00:05,  5.29it/s][A[A

 19% 6/32 [00:01<00:04,  5.35it/s][A[A

 22% 7/32 [00:01<00:04,  5.31it/s][A[A

 25% 8/32 [00:01<00:04,  5.19it/s][A[A

 28% 9/32 [00:01<00:04,  5.21it/s][A[A

 31% 10/32 [00:01<00:04,  5.44it/s][A[A

 34% 11/32 [00:03<00:10,  2.05it/s][A[A

 38% 12/32 [00:03<00:07,  2.52it/s][A[A

 41% 13/32 [00:03<00:06,  2.97it/s][A[A

 44% 14/32 [00:03<00:05,  3.24it/s][A[A

 47% 15/32 [00:03<00:04,  3.59it/s][A[A

 50% 16/32 [00:04<00:04,  3.99it/s][A[A

 53% 17/32 [00:04<00:03,  4.28it/s][A[A

 56% 18/32 [00:04<00:03,  4.66it/s][A[A

 59% 19/32 [00:04<00:02,  4.62it/s][A[A

 62% 20/32 [00:04<00:02,  5.12it/s][A[A

 66% 21/32 [00:04<00:02,  5.06it/s][A[A

 69% 22/32 [00:05<00:02,  4.99it/s][A[A

 72% 23/32 [00:06<00:04,  2.04it/s][A[A

 75% 24/32 [00:06<00:03,  2.53it/s][A[A

 78% 25/32 [00:06<00:02,  2.89it/s][A[A

 81% 26/32 [00:06<00:01,  3.32it/s][A[A

 84% 27/32 [00:07<00:01,  3.57it/s][A[A

 88% 28/32 [00:07<00:01,  3.68it/s][A[A

 91% 29/32 [00:07<00:00,  3.78it/s][A[A

 94% 30/32 [00:07<00:00,  4.07it/s][A[A

100% 32/32 [00:08<00:00,  4.64it/s][A[A100% 32/32 [00:08<00:00,  3.92it/s]
Meta loss on this task batch = 4.5674e-01, PNorm = 42.6907, GNorm = 0.1539

 32% 6/19 [00:51<01:53,  8.71s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  6.07it/s][A[A

  6% 2/32 [00:00<00:04,  6.45it/s][A[A

  9% 3/32 [00:00<00:05,  5.77it/s][A[A

 12% 4/32 [00:00<00:05,  5.14it/s][A[A

 16% 5/32 [00:01<00:13,  2.03it/s][A[A

 19% 6/32 [00:02<00:10,  2.45it/s][A[A

 22% 7/32 [00:02<00:08,  2.94it/s][A[A

 25% 8/32 [00:02<00:07,  3.30it/s][A[A

 28% 9/32 [00:02<00:06,  3.63it/s][A[A

 31% 10/32 [00:02<00:05,  3.82it/s][A[A

 34% 11/32 [00:03<00:04,  4.29it/s][A[A

 38% 12/32 [00:03<00:04,  4.77it/s][A[A

 41% 13/32 [00:03<00:03,  5.28it/s][A[A

 44% 14/32 [00:03<00:03,  5.25it/s][A[A

 47% 15/32 [00:03<00:03,  5.06it/s][A[A

 50% 16/32 [00:04<00:03,  4.82it/s][A[A

 53% 17/32 [00:05<00:07,  2.00it/s][A[A

 56% 18/32 [00:05<00:05,  2.47it/s][A[A

 59% 19/32 [00:05<00:04,  2.85it/s][A[A

 62% 20/32 [00:05<00:03,  3.20it/s][A[A

 66% 21/32 [00:06<00:03,  3.56it/s][A[A

 69% 22/32 [00:06<00:02,  3.95it/s][A[A

 72% 23/32 [00:06<00:02,  4.11it/s][A[A

 75% 24/32 [00:06<00:01,  4.47it/s][A[A

 78% 25/32 [00:06<00:01,  4.52it/s][A[A

 81% 26/32 [00:07<00:01,  4.75it/s][A[A

 84% 27/32 [00:07<00:00,  5.02it/s][A[A

 88% 28/32 [00:07<00:00,  5.02it/s][A[A

 91% 29/32 [00:07<00:00,  5.81it/s][A[A

 94% 30/32 [00:07<00:00,  5.54it/s][A[A

 97% 31/32 [00:09<00:00,  2.03it/s][A[A

100% 32/32 [00:09<00:00,  2.50it/s][A[A100% 32/32 [00:09<00:00,  3.48it/s]
Meta loss on this task batch = 4.8387e-01, PNorm = 42.7273, GNorm = 0.1713

 37% 7/19 [01:01<01:48,  9.08s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.02it/s][A[A

  6% 2/32 [00:00<00:06,  4.95it/s][A[A

  9% 3/32 [00:00<00:06,  4.80it/s][A[A

 12% 4/32 [00:00<00:05,  4.69it/s][A[A

 16% 5/32 [00:01<00:05,  4.78it/s][A[A

 19% 6/32 [00:01<00:04,  5.21it/s][A[A

 22% 7/32 [00:01<00:04,  6.05it/s][A[A

 25% 8/32 [00:01<00:04,  5.52it/s][A[A

 28% 9/32 [00:01<00:04,  5.37it/s][A[A

 31% 10/32 [00:01<00:03,  5.67it/s][A[A

 34% 11/32 [00:02<00:03,  5.28it/s][A[A

 38% 12/32 [00:02<00:03,  5.20it/s][A[A

 41% 13/32 [00:02<00:03,  5.39it/s][A[A

 44% 14/32 [00:02<00:02,  6.16it/s][A[A

 47% 15/32 [00:03<00:08,  2.08it/s][A[A

 50% 16/32 [00:03<00:06,  2.57it/s][A[A

 53% 17/32 [00:04<00:05,  2.98it/s][A[A

 56% 18/32 [00:04<00:04,  3.46it/s][A[A

 59% 19/32 [00:04<00:03,  3.76it/s][A[A

 62% 20/32 [00:04<00:02,  4.15it/s][A[A

 66% 21/32 [00:04<00:02,  4.63it/s][A[A

 69% 22/32 [00:05<00:02,  4.49it/s][A[A

 72% 23/32 [00:05<00:01,  5.32it/s][A[A

 75% 24/32 [00:05<00:01,  5.07it/s][A[A

 78% 25/32 [00:05<00:01,  4.97it/s][A[A

 81% 26/32 [00:06<00:03,  1.94it/s][A[A

 84% 27/32 [00:07<00:02,  2.47it/s][A[A

 88% 28/32 [00:07<00:01,  3.05it/s][A[A

 91% 29/32 [00:07<00:00,  3.63it/s][A[A

 94% 30/32 [00:07<00:00,  4.27it/s][A[A

 97% 31/32 [00:07<00:00,  4.71it/s][A[A

100% 32/32 [00:07<00:00,  5.17it/s][A[A100% 32/32 [00:07<00:00,  4.08it/s]
Meta loss on this task batch = 3.6084e-01, PNorm = 42.7673, GNorm = 0.1056

 42% 8/19 [01:10<01:38,  8.91s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  6.99it/s][A[A

  6% 2/32 [00:00<00:04,  7.19it/s][A[A

  9% 3/32 [00:00<00:04,  6.59it/s][A[A

 12% 4/32 [00:00<00:04,  6.79it/s][A[A

 16% 5/32 [00:00<00:03,  6.89it/s][A[A

 19% 6/32 [00:00<00:03,  7.20it/s][A[A

 22% 7/32 [00:01<00:03,  7.07it/s][A[A

 25% 8/32 [00:01<00:03,  7.14it/s][A[A

 28% 9/32 [00:01<00:03,  6.98it/s][A[A

 31% 10/32 [00:01<00:03,  7.03it/s][A[A

 34% 11/32 [00:01<00:02,  7.09it/s][A[A

 38% 12/32 [00:01<00:02,  6.94it/s][A[A

 41% 13/32 [00:01<00:02,  6.97it/s][A[A

 44% 14/32 [00:02<00:02,  6.99it/s][A[A

 47% 15/32 [00:02<00:02,  7.05it/s][A[A

 50% 16/32 [00:02<00:02,  7.27it/s][A[A

 53% 17/32 [00:02<00:02,  7.35it/s][A[A

 56% 18/32 [00:02<00:01,  7.62it/s][A[A

 59% 19/32 [00:02<00:01,  7.44it/s][A[A

 62% 20/32 [00:03<00:05,  2.26it/s][A[A

 66% 21/32 [00:03<00:03,  2.81it/s][A[A

 69% 22/32 [00:04<00:02,  3.34it/s][A[A

 72% 23/32 [00:04<00:02,  4.08it/s][A[A

 75% 24/32 [00:04<00:01,  4.70it/s][A[A

 78% 25/32 [00:04<00:01,  5.23it/s][A[A

 81% 26/32 [00:04<00:01,  5.61it/s][A[A

 84% 27/32 [00:04<00:00,  5.88it/s][A[A

 88% 28/32 [00:04<00:00,  6.07it/s][A[A

 91% 29/32 [00:05<00:00,  6.38it/s][A[A

 94% 30/32 [00:05<00:00,  6.36it/s][A[A

 97% 31/32 [00:05<00:00,  6.44it/s][A[A

100% 32/32 [00:05<00:00,  6.58it/s][A[A100% 32/32 [00:05<00:00,  5.73it/s]
Meta loss on this task batch = 2.0029e-01, PNorm = 42.8093, GNorm = 0.1119

 47% 9/19 [01:16<01:20,  8.09s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  6.27it/s][A[A

  6% 2/32 [00:00<00:04,  6.75it/s][A[A

  9% 3/32 [00:00<00:04,  6.74it/s][A[A

 12% 4/32 [00:00<00:04,  6.61it/s][A[A

 16% 5/32 [00:00<00:03,  6.93it/s][A[A

 19% 6/32 [00:00<00:03,  6.99it/s][A[A

 22% 7/32 [00:00<00:03,  7.05it/s][A[A

 25% 8/32 [00:01<00:03,  7.09it/s][A[A

 28% 9/32 [00:01<00:03,  7.37it/s][A[A

 31% 10/32 [00:01<00:03,  7.29it/s][A[A

 34% 11/32 [00:01<00:02,  7.07it/s][A[A

 38% 12/32 [00:01<00:02,  6.90it/s][A[A

 41% 13/32 [00:01<00:02,  6.76it/s][A[A

 44% 14/32 [00:02<00:02,  6.79it/s][A[A

 47% 15/32 [00:03<00:07,  2.22it/s][A[A

 50% 16/32 [00:03<00:05,  2.87it/s][A[A

 53% 17/32 [00:03<00:04,  3.52it/s][A[A

 56% 18/32 [00:03<00:03,  4.11it/s][A[A

 59% 19/32 [00:03<00:02,  4.71it/s][A[A

 62% 20/32 [00:03<00:02,  5.37it/s][A[A

 66% 21/32 [00:04<00:02,  5.24it/s][A[A

 69% 22/32 [00:04<00:01,  5.74it/s][A[A

 72% 23/32 [00:04<00:01,  6.02it/s][A[A

 75% 24/32 [00:04<00:01,  6.27it/s][A[A

 78% 25/32 [00:04<00:01,  6.59it/s][A[A

 81% 26/32 [00:04<00:01,  5.63it/s][A[A

 84% 27/32 [00:05<00:00,  5.52it/s][A[A

 88% 28/32 [00:05<00:00,  5.06it/s][A[A

 91% 29/32 [00:05<00:00,  5.18it/s][A[A

 94% 30/32 [00:05<00:00,  4.90it/s][A[A

 97% 31/32 [00:06<00:00,  1.95it/s][A[A

100% 32/32 [00:07<00:00,  2.40it/s][A[A100% 32/32 [00:07<00:00,  4.52it/s]
Meta loss on this task batch = 2.1662e-01, PNorm = 42.8542, GNorm = 0.0990

 53% 10/19 [01:24<01:11,  7.98s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.99it/s][A[A

  6% 2/32 [00:00<00:05,  5.37it/s][A[A

  9% 3/32 [00:00<00:05,  4.93it/s][A[A

 12% 4/32 [00:00<00:05,  4.68it/s][A[A

 16% 5/32 [00:01<00:05,  4.58it/s][A[A

 19% 6/32 [00:01<00:05,  4.79it/s][A[A

 22% 7/32 [00:01<00:04,  5.14it/s][A[A

 25% 8/32 [00:01<00:04,  4.95it/s][A[A

 28% 9/32 [00:01<00:05,  4.59it/s][A[A

 31% 10/32 [00:03<00:11,  1.91it/s][A[A

 34% 11/32 [00:03<00:09,  2.29it/s][A[A

 38% 12/32 [00:03<00:07,  2.65it/s][A[A

 41% 13/32 [00:03<00:06,  2.95it/s][A[A

 44% 14/32 [00:04<00:05,  3.23it/s][A[A

 47% 15/32 [00:04<00:04,  3.44it/s][A[A

 50% 16/32 [00:04<00:04,  3.98it/s][A[A

 53% 17/32 [00:04<00:03,  4.02it/s][A[A

 56% 18/32 [00:04<00:03,  4.47it/s][A[A

 59% 19/32 [00:05<00:02,  4.57it/s][A[A

 62% 20/32 [00:05<00:02,  4.45it/s][A[A

 66% 21/32 [00:05<00:02,  4.35it/s][A[A

 69% 22/32 [00:05<00:02,  4.34it/s][A[A

 72% 23/32 [00:06<00:02,  4.40it/s][A[A

 75% 24/32 [00:07<00:04,  1.90it/s][A[A

 78% 25/32 [00:07<00:03,  2.32it/s][A[A

 81% 26/32 [00:07<00:02,  2.89it/s][A[A

 84% 27/32 [00:07<00:01,  3.39it/s][A[A

 88% 28/32 [00:08<00:01,  3.59it/s][A[A

 91% 29/32 [00:08<00:00,  3.86it/s][A[A

 94% 30/32 [00:08<00:00,  4.02it/s][A[A

 97% 31/32 [00:08<00:00,  4.06it/s][A[A

100% 32/32 [00:08<00:00,  4.45it/s][A[A100% 32/32 [00:08<00:00,  3.58it/s]
Meta loss on this task batch = 5.5079e-01, PNorm = 42.8979, GNorm = 0.1348

 58% 11/19 [01:34<01:08,  8.50s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.85it/s][A[A

  6% 2/32 [00:00<00:07,  4.12it/s][A[A

  9% 3/32 [00:00<00:07,  4.14it/s][A[A

 12% 4/32 [00:00<00:06,  4.18it/s][A[A

 16% 5/32 [00:01<00:06,  4.17it/s][A[A

 19% 6/32 [00:01<00:06,  4.20it/s][A[A

 22% 7/32 [00:01<00:05,  4.64it/s][A[A

 25% 8/32 [00:02<00:12,  1.90it/s][A[A

 28% 9/32 [00:03<00:09,  2.32it/s][A[A

 31% 10/32 [00:03<00:08,  2.66it/s][A[A

 34% 11/32 [00:03<00:06,  3.19it/s][A[A

 38% 12/32 [00:03<00:05,  3.38it/s][A[A

 41% 13/32 [00:03<00:05,  3.67it/s][A[A

 44% 14/32 [00:04<00:04,  3.76it/s][A[A

 47% 15/32 [00:04<00:04,  3.87it/s][A[A

 50% 16/32 [00:04<00:03,  4.32it/s][A[A

 53% 17/32 [00:04<00:03,  4.42it/s][A[A

 56% 18/32 [00:06<00:07,  1.89it/s][A[A

 59% 19/32 [00:06<00:05,  2.35it/s][A[A

 62% 20/32 [00:06<00:04,  2.67it/s][A[A

 66% 21/32 [00:06<00:03,  3.00it/s][A[A

 69% 22/32 [00:06<00:02,  3.52it/s][A[A

 72% 23/32 [00:07<00:02,  3.82it/s][A[A

 75% 24/32 [00:07<00:01,  4.40it/s][A[A

 78% 25/32 [00:07<00:01,  4.30it/s][A[A

 81% 26/32 [00:07<00:01,  4.56it/s][A[A

 84% 27/32 [00:07<00:01,  4.40it/s][A[A

 88% 28/32 [00:08<00:00,  4.31it/s][A[A

 91% 29/32 [00:08<00:00,  4.29it/s][A[A

 94% 30/32 [00:08<00:00,  4.61it/s][A[A

 97% 31/32 [00:09<00:00,  1.92it/s][A[A

100% 32/32 [00:10<00:00,  2.30it/s][A[A100% 32/32 [00:10<00:00,  3.19it/s]
Meta loss on this task batch = 5.7427e-01, PNorm = 42.9339, GNorm = 0.2507

 63% 12/19 [01:44<01:04,  9.19s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.49it/s][A[A

  6% 2/32 [00:00<00:06,  4.74it/s][A[A

  9% 3/32 [00:00<00:06,  4.71it/s][A[A

 12% 4/32 [00:00<00:05,  4.79it/s][A[A

 16% 5/32 [00:01<00:05,  4.57it/s][A[A

 19% 6/32 [00:01<00:05,  4.70it/s][A[A

 22% 7/32 [00:01<00:04,  5.09it/s][A[A

 25% 8/32 [00:01<00:04,  5.19it/s][A[A

 28% 9/32 [00:01<00:04,  5.49it/s][A[A

 31% 10/32 [00:01<00:04,  5.06it/s][A[A

 34% 11/32 [00:02<00:04,  4.73it/s][A[A

 38% 12/32 [00:02<00:04,  4.64it/s][A[A

 41% 13/32 [00:03<00:09,  1.90it/s][A[A

 44% 14/32 [00:03<00:07,  2.37it/s][A[A

 47% 15/32 [00:04<00:06,  2.72it/s][A[A

 50% 16/32 [00:04<00:05,  3.02it/s][A[A

 53% 17/32 [00:04<00:04,  3.42it/s][A[A

 56% 18/32 [00:04<00:03,  3.62it/s][A[A

 59% 19/32 [00:05<00:03,  3.69it/s][A[A

 62% 20/32 [00:05<00:02,  4.07it/s][A[A

 66% 21/32 [00:05<00:02,  4.03it/s][A[A

 69% 22/32 [00:06<00:05,  1.83it/s][A[A

 72% 23/32 [00:06<00:03,  2.28it/s][A[A

 75% 24/32 [00:07<00:03,  2.62it/s][A[A

 78% 25/32 [00:07<00:02,  2.95it/s][A[A

 81% 26/32 [00:07<00:01,  3.34it/s][A[A

 84% 27/32 [00:07<00:01,  3.75it/s][A[A

 88% 28/32 [00:08<00:00,  4.16it/s][A[A

 91% 29/32 [00:08<00:00,  4.12it/s][A[A

 94% 30/32 [00:08<00:00,  4.12it/s][A[A

 97% 31/32 [00:08<00:00,  4.57it/s][A[A

100% 32/32 [00:08<00:00,  4.36it/s][A[A100% 32/32 [00:08<00:00,  3.59it/s]
Meta loss on this task batch = 5.6909e-01, PNorm = 42.9674, GNorm = 0.1599

 68% 13/19 [01:54<00:56,  9.35s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:37,  1.20s/it][A[A

  6% 2/32 [00:01<00:26,  1.11it/s][A[A

  9% 3/32 [00:01<00:20,  1.42it/s][A[A

 12% 4/32 [00:01<00:15,  1.77it/s][A[A

 16% 5/32 [00:02<00:12,  2.17it/s][A[A

 19% 6/32 [00:02<00:09,  2.65it/s][A[A

 22% 7/32 [00:02<00:08,  2.98it/s][A[A

 25% 8/32 [00:02<00:06,  3.53it/s][A[A

 28% 9/32 [00:02<00:06,  3.68it/s][A[A

 31% 10/32 [00:03<00:05,  4.04it/s][A[A

 34% 11/32 [00:03<00:05,  4.01it/s][A[A

 38% 12/32 [00:04<00:10,  1.96it/s][A[A

 41% 13/32 [00:04<00:08,  2.34it/s][A[A

 44% 14/32 [00:04<00:06,  2.68it/s][A[A

 47% 15/32 [00:05<00:05,  2.97it/s][A[A

 50% 16/32 [00:05<00:05,  3.19it/s][A[A

 53% 17/32 [00:05<00:04,  3.52it/s][A[A

 56% 18/32 [00:05<00:03,  3.89it/s][A[A

 59% 19/32 [00:06<00:02,  4.36it/s][A[A

 62% 20/32 [00:06<00:02,  4.62it/s][A[A

 66% 21/32 [00:06<00:02,  4.44it/s][A[A

 69% 22/32 [00:06<00:02,  4.53it/s][A[A

 72% 23/32 [00:06<00:02,  4.39it/s][A[A

 75% 24/32 [00:07<00:01,  4.92it/s][A[A

 78% 25/32 [00:07<00:01,  4.76it/s][A[A

 81% 26/32 [00:07<00:01,  4.73it/s][A[A

 84% 27/32 [00:07<00:01,  4.92it/s][A[A

 88% 28/32 [00:08<00:02,  1.95it/s][A[A

 91% 29/32 [00:09<00:01,  2.33it/s][A[A

 94% 30/32 [00:09<00:00,  2.74it/s][A[A

 97% 31/32 [00:09<00:00,  3.14it/s][A[A

100% 32/32 [00:09<00:00,  3.54it/s][A[A100% 32/32 [00:09<00:00,  3.26it/s]
Meta loss on this task batch = 5.4762e-01, PNorm = 42.9989, GNorm = 0.1180

 74% 14/19 [02:05<00:48,  9.72s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.10it/s][A[A

  6% 2/32 [00:00<00:06,  4.33it/s][A[A

  9% 3/32 [00:00<00:06,  4.41it/s][A[A

 12% 4/32 [00:00<00:05,  4.78it/s][A[A

 16% 5/32 [00:01<00:05,  4.93it/s][A[A

 19% 6/32 [00:01<00:05,  4.41it/s][A[A

 22% 7/32 [00:01<00:05,  4.17it/s][A[A

 25% 8/32 [00:02<00:12,  1.89it/s][A[A

 28% 9/32 [00:03<00:10,  2.28it/s][A[A

 31% 10/32 [00:03<00:07,  2.92it/s][A[A

 34% 11/32 [00:03<00:06,  3.30it/s][A[A

 38% 12/32 [00:03<00:05,  3.57it/s][A[A

 41% 13/32 [00:03<00:04,  4.04it/s][A[A

 44% 14/32 [00:03<00:04,  4.42it/s][A[A

 47% 15/32 [00:04<00:03,  4.57it/s][A[A

 50% 16/32 [00:04<00:03,  4.62it/s][A[A

 53% 17/32 [00:04<00:03,  4.62it/s][A[A

 56% 18/32 [00:04<00:02,  4.83it/s][A[A

 59% 19/32 [00:04<00:02,  4.83it/s][A[A

 62% 20/32 [00:05<00:02,  5.16it/s][A[A

 66% 21/32 [00:05<00:02,  5.08it/s][A[A

 69% 22/32 [00:05<00:01,  5.13it/s][A[A

 72% 23/32 [00:05<00:01,  4.51it/s][A[A

 75% 24/32 [00:06<00:04,  1.93it/s][A[A

 78% 25/32 [00:07<00:03,  2.31it/s][A[A

 81% 26/32 [00:07<00:02,  2.61it/s][A[A

 84% 27/32 [00:07<00:01,  3.13it/s][A[A

 88% 28/32 [00:07<00:01,  3.38it/s][A[A

 91% 29/32 [00:08<00:00,  3.58it/s][A[A

 94% 30/32 [00:08<00:00,  3.70it/s][A[A

 97% 31/32 [00:08<00:00,  4.07it/s][A[A

100% 32/32 [00:08<00:00,  4.15it/s][A[A100% 32/32 [00:08<00:00,  3.64it/s]
Meta loss on this task batch = 5.1626e-01, PNorm = 43.0216, GNorm = 0.1399

 79% 15/19 [02:14<00:38,  9.67s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  3.91it/s][A[A

  6% 2/32 [00:00<00:07,  4.24it/s][A[A

  9% 3/32 [00:00<00:06,  4.54it/s][A[A

 12% 4/32 [00:00<00:05,  4.69it/s][A[A

 16% 5/32 [00:01<00:13,  2.04it/s][A[A

 19% 6/32 [00:02<00:10,  2.43it/s][A[A

 22% 7/32 [00:02<00:08,  2.87it/s][A[A

 25% 8/32 [00:02<00:07,  3.13it/s][A[A

 28% 9/32 [00:02<00:06,  3.55it/s][A[A

 31% 10/32 [00:03<00:05,  3.83it/s][A[A

 34% 11/32 [00:03<00:04,  4.21it/s][A[A

 38% 12/32 [00:03<00:04,  4.43it/s][A[A

 41% 13/32 [00:03<00:04,  4.72it/s][A[A

 44% 14/32 [00:03<00:03,  4.97it/s][A[A

 47% 15/32 [00:04<00:03,  4.86it/s][A[A

 50% 16/32 [00:04<00:03,  4.68it/s][A[A

 53% 17/32 [00:04<00:03,  4.66it/s][A[A

 56% 18/32 [00:05<00:06,  2.00it/s][A[A

 59% 19/32 [00:05<00:05,  2.44it/s][A[A

 62% 20/32 [00:06<00:04,  2.87it/s][A[A

 66% 21/32 [00:06<00:03,  3.32it/s][A[A

 69% 22/32 [00:06<00:02,  3.66it/s][A[A

 72% 23/32 [00:06<00:02,  3.83it/s][A[A

 75% 24/32 [00:06<00:02,  3.92it/s][A[A

 78% 25/32 [00:07<00:01,  4.42it/s][A[A

 81% 26/32 [00:07<00:01,  4.59it/s][A[A

 84% 27/32 [00:07<00:01,  4.45it/s][A[A

 88% 28/32 [00:07<00:00,  4.50it/s][A[A

 91% 29/32 [00:07<00:00,  4.61it/s][A[A

 94% 30/32 [00:08<00:00,  4.46it/s][A[A

 97% 31/32 [00:08<00:00,  4.39it/s][A[A

100% 32/32 [00:08<00:00,  4.52it/s][A[A100% 32/32 [00:08<00:00,  3.72it/s]
Meta loss on this task batch = 5.3547e-01, PNorm = 43.0431, GNorm = 0.0911

 84% 16/19 [02:24<00:28,  9.58s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:38,  1.25s/it][A[A

  6% 2/32 [00:01<00:28,  1.06it/s][A[A

  9% 3/32 [00:01<00:21,  1.38it/s][A[A

 12% 4/32 [00:01<00:15,  1.76it/s][A[A

 16% 5/32 [00:02<00:11,  2.25it/s][A[A

 19% 6/32 [00:02<00:09,  2.69it/s][A[A

 22% 7/32 [00:02<00:07,  3.34it/s][A[A

 25% 8/32 [00:02<00:06,  3.56it/s][A[A

 28% 9/32 [00:02<00:05,  3.86it/s][A[A

 31% 10/32 [00:02<00:04,  4.54it/s][A[A

 34% 11/32 [00:03<00:04,  4.75it/s][A[A

 38% 12/32 [00:03<00:04,  4.64it/s][A[A

 41% 13/32 [00:03<00:03,  5.23it/s][A[A

 44% 14/32 [00:03<00:03,  5.35it/s][A[A

 47% 15/32 [00:03<00:03,  5.27it/s][A[A

 50% 16/32 [00:05<00:07,  2.04it/s][A[A

 53% 17/32 [00:05<00:06,  2.42it/s][A[A

 56% 18/32 [00:05<00:04,  2.92it/s][A[A

 59% 19/32 [00:05<00:03,  3.47it/s][A[A

 62% 20/32 [00:05<00:03,  3.78it/s][A[A

 66% 21/32 [00:06<00:02,  4.35it/s][A[A

 69% 22/32 [00:06<00:02,  4.70it/s][A[A

 72% 23/32 [00:06<00:01,  4.72it/s][A[A

 75% 24/32 [00:06<00:01,  4.73it/s][A[A

 78% 25/32 [00:06<00:01,  5.15it/s][A[A

 81% 26/32 [00:06<00:01,  5.33it/s][A[A

 84% 27/32 [00:07<00:00,  5.24it/s][A[A

 88% 28/32 [00:07<00:00,  5.12it/s][A[A

 91% 29/32 [00:08<00:01,  2.03it/s][A[A

 94% 30/32 [00:08<00:00,  2.37it/s][A[A

 97% 31/32 [00:08<00:00,  2.87it/s][A[A

100% 32/32 [00:09<00:00,  3.36it/s][A[A100% 32/32 [00:09<00:00,  3.50it/s]
Meta loss on this task batch = 4.4969e-01, PNorm = 43.0638, GNorm = 0.1224

 89% 17/19 [02:33<00:19,  9.67s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.83it/s][A[A

  6% 2/32 [00:00<00:06,  4.80it/s][A[A

  9% 3/32 [00:00<00:06,  4.63it/s][A[A

 12% 4/32 [00:00<00:06,  4.61it/s][A[A

 16% 5/32 [00:01<00:05,  4.97it/s][A[A

 19% 6/32 [00:01<00:04,  5.21it/s][A[A

 22% 7/32 [00:01<00:05,  4.78it/s][A[A

 25% 8/32 [00:02<00:12,  1.92it/s][A[A

 28% 9/32 [00:02<00:09,  2.39it/s][A[A

 31% 10/32 [00:03<00:07,  2.78it/s][A[A

 34% 11/32 [00:03<00:06,  3.32it/s][A[A

 38% 12/32 [00:03<00:05,  3.71it/s][A[A

 41% 13/32 [00:03<00:05,  3.76it/s][A[A

 44% 14/32 [00:03<00:04,  4.02it/s][A[A

 47% 15/32 [00:04<00:04,  4.05it/s][A[A

 50% 16/32 [00:04<00:03,  4.35it/s][A[A

 53% 17/32 [00:04<00:03,  4.52it/s][A[A

 56% 18/32 [00:04<00:02,  4.73it/s][A[A

 59% 19/32 [00:04<00:02,  4.63it/s][A[A

 62% 20/32 [00:06<00:06,  1.92it/s][A[A

 66% 21/32 [00:06<00:04,  2.40it/s][A[A

 69% 22/32 [00:06<00:03,  2.86it/s][A[A

 72% 23/32 [00:06<00:02,  3.30it/s][A[A

 75% 24/32 [00:07<00:02,  3.46it/s][A[A

 78% 25/32 [00:07<00:01,  4.02it/s][A[A

 81% 26/32 [00:07<00:01,  4.22it/s][A[A

 84% 27/32 [00:07<00:01,  4.41it/s][A[A

 88% 28/32 [00:07<00:00,  4.28it/s][A[A

 91% 29/32 [00:08<00:00,  4.24it/s][A[A

 94% 30/32 [00:08<00:00,  4.60it/s][A[A

 97% 31/32 [00:08<00:00,  4.84it/s][A[A

100% 32/32 [00:08<00:00,  5.17it/s][A[A100% 32/32 [00:08<00:00,  3.72it/s]
Meta loss on this task batch = 5.2426e-01, PNorm = 43.0842, GNorm = 0.1427

 95% 18/19 [02:43<00:09,  9.57s/it][A

  0% 0/23 [00:00<?, ?it/s][A[A

  4% 1/23 [00:00<00:05,  4.20it/s][A[A

  9% 2/23 [00:01<00:11,  1.90it/s][A[A

 13% 3/23 [00:01<00:08,  2.32it/s][A[A

 17% 4/23 [00:01<00:06,  2.85it/s][A[A

 26% 6/23 [00:02<00:04,  3.46it/s][A[A

 30% 7/23 [00:02<00:04,  3.93it/s][A[A

 35% 8/23 [00:02<00:03,  4.34it/s][A[A

 39% 9/23 [00:02<00:02,  4.68it/s][A[A

 43% 10/23 [00:02<00:02,  4.42it/s][A[A

 48% 11/23 [00:02<00:02,  5.18it/s][A[A

 52% 12/23 [00:03<00:02,  5.45it/s][A[A

 57% 13/23 [00:03<00:01,  5.26it/s][A[A

 61% 14/23 [00:03<00:01,  5.90it/s][A[A

 65% 15/23 [00:03<00:01,  4.40it/s][A[A

 70% 16/23 [00:04<00:01,  4.24it/s][A[A

 74% 17/23 [00:05<00:03,  1.87it/s][A[A

 78% 18/23 [00:05<00:02,  2.19it/s][A[A

 83% 19/23 [00:05<00:01,  2.70it/s][A[A

 87% 20/23 [00:05<00:00,  3.17it/s][A[A

 91% 21/23 [00:06<00:00,  3.55it/s][A[A

 96% 22/23 [00:06<00:00,  3.97it/s][A[A

100% 23/23 [00:06<00:00,  4.06it/s][A[A100% 23/23 [00:06<00:00,  3.49it/s]
Meta loss on this task batch = 4.3690e-01, PNorm = 43.1018, GNorm = 0.1291

100% 19/19 [02:50<00:00,  8.84s/it][A100% 19/19 [02:50<00:00,  8.97s/it]
Took 170.42776656150818 seconds to complete one epoch of meta training

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/20 [00:00<?, ?it/s][A[A


  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 31.22it/s]


  5% 1/20 [00:00<00:03,  5.74it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 20.52it/s]


 10% 2/20 [00:00<00:03,  5.57it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 18.24it/s][A[A[A100% 3/3 [00:00<00:00, 20.27it/s]


 15% 3/20 [00:00<00:03,  4.84it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 15.16it/s]


 20% 4/20 [00:00<00:03,  4.81it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 15.11it/s][A[A[A


100% 4/4 [00:00<00:00, 16.30it/s][A[A[A100% 4/4 [00:00<00:00, 17.19it/s]


 25% 5/20 [00:01<00:03,  3.98it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 25% 1/4 [00:01<00:03,  1.07s/it][A[A[A


 75% 3/4 [00:01<00:00,  1.29it/s][A[A[A100% 4/4 [00:01<00:00,  3.20it/s]


 30% 6/20 [00:02<00:08,  1.72it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.15it/s][A[A[A100% 4/4 [00:00<00:00, 20.90it/s]


 35% 7/20 [00:02<00:06,  2.02it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.75it/s][A[A[A


100% 4/4 [00:00<00:00, 19.39it/s][A[A[A100% 4/4 [00:00<00:00, 19.13it/s]


 40% 8/20 [00:03<00:05,  2.24it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.66it/s][A[A[A100% 4/4 [00:00<00:00, 23.34it/s]


 45% 9/20 [00:03<00:04,  2.54it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 18.77it/s][A[A[A


100% 4/4 [00:00<00:00, 18.79it/s][A[A[A100% 4/4 [00:00<00:00, 18.79it/s]


 50% 10/20 [00:03<00:03,  2.68it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 16.12it/s][A[A[A100% 4/4 [00:00<00:00, 18.54it/s]


 55% 11/20 [00:04<00:03,  2.74it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.00it/s][A[A[A100% 4/4 [00:00<00:00, 22.40it/s]


 60% 12/20 [00:04<00:02,  2.90it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.06it/s][A[A[A


100% 3/3 [00:01<00:00,  2.69it/s][A[A[A100% 3/3 [00:01<00:00,  2.47it/s]


 65% 13/20 [00:05<00:04,  1.52it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 13.64it/s][A[A[A100% 3/3 [00:00<00:00, 13.39it/s]


 70% 14/20 [00:06<00:03,  1.72it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.35it/s][A[A[A100% 4/4 [00:00<00:00, 17.26it/s]


 75% 15/20 [00:06<00:02,  1.90it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 13.98it/s][A[A[A100% 3/3 [00:00<00:00, 17.21it/s]


 80% 16/20 [00:06<00:01,  2.12it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 12.32it/s][A[A[A


100% 3/3 [00:01<00:00,  2.75it/s][A[A[A100% 3/3 [00:01<00:00,  2.53it/s]


 85% 17/20 [00:08<00:02,  1.34it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 21.47it/s]


 90% 18/20 [00:08<00:01,  1.68it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 15.41it/s][A[A[A100% 3/3 [00:00<00:00, 19.55it/s]


 95% 19/20 [00:08<00:00,  1.95it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 13.00it/s]


100% 20/20 [00:09<00:00,  2.33it/s][A[A100% 20/20 [00:09<00:00,  2.19it/s]

100% 1/1 [00:09<00:00,  9.14s/it][A100% 1/1 [00:09<00:00,  9.14s/it]
Took 179.56940364837646 seconds to complete one epoch of meta training and validating
Meta validation score auc = 0.633248
 93% 28/30 [1:30:07<06:20, 190.07s/it]Epoch 28

  0% 0/19 [00:00<?, ?it/s][A

  0% 0/32 [00:00<?, ?it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  3% 1/32 [00:00<00:06,  4.88it/s][A[A

  6% 2/32 [00:00<00:06,  4.43it/s][A[A

  9% 3/32 [00:00<00:06,  4.52it/s][A[A

 12% 4/32 [00:00<00:05,  4.99it/s][A[A

 16% 5/32 [00:00<00:04,  5.47it/s][A[A

 19% 6/32 [00:01<00:04,  5.51it/s][A[A

 22% 7/32 [00:01<00:04,  5.64it/s][A[A

 25% 8/32 [00:01<00:04,  5.41it/s][A[A

 28% 9/32 [00:02<00:10,  2.16it/s][A[A

 31% 10/32 [00:02<00:07,  2.75it/s][A[A

 34% 11/32 [00:03<00:06,  3.02it/s][A[A

 38% 12/32 [00:03<00:06,  3.20it/s][A[A

 41% 13/32 [00:03<00:05,  3.33it/s][A[A

 44% 14/32 [00:03<00:05,  3.58it/s][A[A

 47% 15/32 [00:03<00:04,  4.07it/s][A[A

 50% 16/32 [00:04<00:03,  4.25it/s][A[A

 53% 17/32 [00:04<00:03,  4.31it/s][A[A

 56% 18/32 [00:04<00:02,  4.70it/s][A[A

 59% 19/32 [00:04<00:02,  5.07it/s][A[A

 62% 20/32 [00:04<00:02,  4.82it/s][A[A

 66% 21/32 [00:05<00:01,  5.52it/s][A[A

 69% 22/32 [00:06<00:04,  2.07it/s][A[A

 72% 23/32 [00:06<00:03,  2.53it/s][A[A

 75% 24/32 [00:06<00:02,  2.90it/s][A[A

 78% 25/32 [00:06<00:02,  3.37it/s][A[A

 81% 26/32 [00:07<00:01,  3.64it/s][A[A

 84% 27/32 [00:07<00:01,  3.92it/s][A[A

 88% 28/32 [00:07<00:00,  4.33it/s][A[A

 91% 29/32 [00:07<00:00,  4.28it/s][A[A

 94% 30/32 [00:07<00:00,  5.04it/s][A[A

 97% 31/32 [00:08<00:00,  4.65it/s][A[A

100% 32/32 [00:08<00:00,  4.90it/s][A[A100% 32/32 [00:08<00:00,  3.87it/s]
Meta loss on this task batch = 5.1314e-01, PNorm = 43.1245, GNorm = 0.1984

  5% 1/19 [00:09<02:42,  9.00s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  7.64it/s][A[A

  6% 2/32 [00:00<00:04,  7.39it/s][A[A

  9% 3/32 [00:00<00:04,  6.71it/s][A[A

 12% 4/32 [00:01<00:13,  2.09it/s][A[A

 16% 5/32 [00:01<00:10,  2.48it/s][A[A

 19% 6/32 [00:02<00:08,  3.05it/s][A[A

 22% 7/32 [00:02<00:06,  3.70it/s][A[A

 25% 8/32 [00:02<00:05,  4.23it/s][A[A

 28% 9/32 [00:02<00:04,  4.65it/s][A[A

 31% 10/32 [00:02<00:04,  5.19it/s][A[A

 34% 11/32 [00:02<00:04,  5.22it/s][A[A

 38% 12/32 [00:03<00:03,  5.25it/s][A[A

 41% 13/32 [00:03<00:03,  5.77it/s][A[A

 44% 14/32 [00:03<00:03,  5.24it/s][A[A

 47% 15/32 [00:03<00:03,  5.01it/s][A[A

 50% 16/32 [00:03<00:03,  4.95it/s][A[A

 53% 17/32 [00:04<00:03,  4.79it/s][A[A

 56% 18/32 [00:04<00:03,  4.65it/s][A[A

 59% 19/32 [00:04<00:02,  4.75it/s][A[A

 62% 20/32 [00:04<00:02,  4.59it/s][A[A

 66% 21/32 [00:05<00:05,  1.96it/s][A[A

 69% 22/32 [00:06<00:04,  2.50it/s][A[A

 72% 23/32 [00:06<00:02,  3.04it/s][A[A

 75% 24/32 [00:06<00:02,  3.70it/s][A[A

 78% 25/32 [00:06<00:01,  4.04it/s][A[A

 81% 26/32 [00:06<00:01,  4.30it/s][A[A

 84% 27/32 [00:06<00:01,  4.72it/s][A[A

 88% 28/32 [00:07<00:00,  4.62it/s][A[A

 91% 29/32 [00:07<00:00,  5.30it/s][A[A

 94% 30/32 [00:07<00:00,  5.12it/s][A[A

 97% 31/32 [00:07<00:00,  5.03it/s][A[A

100% 32/32 [00:07<00:00,  5.42it/s][A[A100% 32/32 [00:07<00:00,  4.08it/s]
Meta loss on this task batch = 4.7267e-01, PNorm = 43.1536, GNorm = 0.1254

 11% 2/19 [00:17<02:30,  8.87s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.45it/s][A[A

  6% 2/32 [00:00<00:05,  5.13it/s][A[A

  9% 3/32 [00:00<00:06,  4.80it/s][A[A

 12% 4/32 [00:01<00:14,  1.89it/s][A[A

 16% 5/32 [00:02<00:11,  2.37it/s][A[A

 19% 6/32 [00:02<00:09,  2.72it/s][A[A

 22% 7/32 [00:02<00:08,  3.10it/s][A[A

 25% 8/32 [00:02<00:06,  3.66it/s][A[A

 28% 9/32 [00:02<00:05,  4.20it/s][A[A

 31% 10/32 [00:03<00:05,  4.23it/s][A[A

 34% 11/32 [00:03<00:04,  4.24it/s][A[A

 38% 12/32 [00:03<00:04,  4.37it/s][A[A

 41% 13/32 [00:03<00:04,  4.38it/s][A[A

 44% 14/32 [00:04<00:04,  4.33it/s][A[A

 47% 15/32 [00:04<00:03,  4.27it/s][A[A

 50% 16/32 [00:05<00:08,  1.96it/s][A[A

 53% 17/32 [00:05<00:06,  2.32it/s][A[A

 56% 18/32 [00:05<00:04,  2.89it/s][A[A

 59% 19/32 [00:05<00:03,  3.41it/s][A[A

 62% 20/32 [00:06<00:03,  3.98it/s][A[A

 66% 21/32 [00:06<00:02,  4.67it/s][A[A

 69% 22/32 [00:06<00:01,  5.19it/s][A[A

 72% 23/32 [00:06<00:01,  4.84it/s][A[A

 75% 24/32 [00:06<00:01,  4.72it/s][A[A

 78% 25/32 [00:07<00:01,  4.89it/s][A[A

 81% 26/32 [00:07<00:01,  5.03it/s][A[A

 84% 27/32 [00:07<00:01,  4.78it/s][A[A

 88% 28/32 [00:07<00:00,  5.08it/s][A[A

 91% 29/32 [00:07<00:00,  4.87it/s][A[A

 94% 30/32 [00:08<00:00,  4.49it/s][A[A

 97% 31/32 [00:08<00:00,  5.20it/s][A[A

100% 32/32 [00:08<00:00,  5.04it/s][A[A100% 32/32 [00:08<00:00,  3.79it/s]
Meta loss on this task batch = 5.0100e-01, PNorm = 43.1852, GNorm = 0.0798

 16% 3/19 [00:26<02:23,  8.97s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.79it/s][A[A

  6% 2/32 [00:01<00:16,  1.80it/s][A[A

  9% 3/32 [00:01<00:12,  2.27it/s][A[A

 12% 4/32 [00:01<00:10,  2.64it/s][A[A

 16% 5/32 [00:02<00:08,  3.04it/s][A[A

 19% 6/32 [00:02<00:07,  3.31it/s][A[A

 22% 7/32 [00:02<00:06,  3.98it/s][A[A

 25% 8/32 [00:02<00:06,  3.98it/s][A[A

 28% 9/32 [00:02<00:04,  4.61it/s][A[A

 31% 10/32 [00:03<00:04,  4.56it/s][A[A

 34% 11/32 [00:03<00:04,  5.02it/s][A[A

 38% 12/32 [00:03<00:03,  5.45it/s][A[A

 41% 13/32 [00:03<00:03,  5.10it/s][A[A

 44% 14/32 [00:03<00:03,  4.96it/s][A[A

 47% 15/32 [00:04<00:03,  5.30it/s][A[A

 50% 16/32 [00:04<00:03,  4.98it/s][A[A

 53% 17/32 [00:05<00:07,  1.98it/s][A[A

 56% 18/32 [00:05<00:05,  2.39it/s][A[A

 59% 19/32 [00:05<00:04,  2.95it/s][A[A

 62% 20/32 [00:06<00:03,  3.27it/s][A[A

 66% 21/32 [00:06<00:03,  3.55it/s][A[A

 69% 22/32 [00:06<00:02,  3.67it/s][A[A

 72% 23/32 [00:06<00:02,  3.77it/s][A[A

 75% 24/32 [00:06<00:02,  3.98it/s][A[A

 78% 25/32 [00:07<00:01,  4.68it/s][A[A

 81% 26/32 [00:07<00:01,  5.34it/s][A[A

 84% 27/32 [00:07<00:00,  5.02it/s][A[A

 88% 28/32 [00:08<00:02,  2.00it/s][A[A

 91% 29/32 [00:08<00:01,  2.55it/s][A[A

 94% 30/32 [00:09<00:00,  2.87it/s][A[A

 97% 31/32 [00:09<00:00,  3.47it/s][A[A

100% 32/32 [00:09<00:00,  3.73it/s][A[A100% 32/32 [00:09<00:00,  3.40it/s]
Meta loss on this task batch = 5.4760e-01, PNorm = 43.2181, GNorm = 0.0773

 21% 4/19 [00:36<02:20,  9.33s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.37it/s][A[A

  6% 2/32 [00:00<00:05,  5.02it/s][A[A

  9% 3/32 [00:00<00:06,  4.83it/s][A[A

 12% 4/32 [00:00<00:06,  4.44it/s][A[A

 16% 5/32 [00:01<00:05,  4.51it/s][A[A

 19% 6/32 [00:01<00:05,  4.56it/s][A[A

 22% 7/32 [00:01<00:05,  4.50it/s][A[A

 25% 8/32 [00:01<00:04,  5.12it/s][A[A

 28% 9/32 [00:01<00:04,  5.00it/s][A[A

 31% 10/32 [00:03<00:10,  2.02it/s][A[A

 34% 11/32 [00:03<00:08,  2.44it/s][A[A

 38% 12/32 [00:03<00:06,  2.88it/s][A[A

 41% 13/32 [00:03<00:05,  3.21it/s][A[A

 44% 14/32 [00:03<00:04,  3.81it/s][A[A

 47% 15/32 [00:04<00:04,  4.02it/s][A[A

 50% 16/32 [00:04<00:03,  4.47it/s][A[A

 53% 17/32 [00:04<00:03,  4.22it/s][A[A

 56% 18/32 [00:04<00:03,  4.48it/s][A[A

 59% 19/32 [00:04<00:03,  4.29it/s][A[A

 62% 20/32 [00:05<00:02,  4.24it/s][A[A

 66% 21/32 [00:05<00:02,  4.28it/s][A[A

 69% 22/32 [00:05<00:02,  4.65it/s][A[A

 72% 23/32 [00:05<00:01,  4.55it/s][A[A

 75% 24/32 [00:06<00:01,  4.27it/s][A[A

 78% 25/32 [00:07<00:03,  1.86it/s][A[A

 81% 26/32 [00:07<00:02,  2.38it/s][A[A

 84% 27/32 [00:07<00:01,  2.88it/s][A[A

 88% 28/32 [00:07<00:01,  3.46it/s][A[A

 91% 29/32 [00:08<00:00,  3.66it/s][A[A

 94% 30/32 [00:08<00:00,  4.20it/s][A[A

 97% 31/32 [00:08<00:00,  4.53it/s][A[A

100% 32/32 [00:08<00:00,  4.40it/s][A[A100% 32/32 [00:08<00:00,  3.72it/s]
Meta loss on this task batch = 5.1180e-01, PNorm = 43.2497, GNorm = 0.0637

 26% 5/19 [00:46<02:10,  9.34s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.99it/s][A[A

  6% 2/32 [00:00<00:05,  5.44it/s][A[A

  9% 3/32 [00:00<00:05,  5.72it/s][A[A

 12% 4/32 [00:00<00:05,  5.15it/s][A[A

 16% 5/32 [00:01<00:06,  4.42it/s][A[A

 19% 6/32 [00:01<00:06,  4.31it/s][A[A

 22% 7/32 [00:01<00:05,  4.62it/s][A[A

 25% 8/32 [00:02<00:12,  1.92it/s][A[A

 28% 9/32 [00:03<00:10,  2.25it/s][A[A

 31% 10/32 [00:03<00:08,  2.60it/s][A[A

 34% 11/32 [00:03<00:07,  2.91it/s][A[A

 38% 12/32 [00:03<00:06,  3.24it/s][A[A

 41% 13/32 [00:03<00:05,  3.79it/s][A[A

 44% 14/32 [00:04<00:04,  3.79it/s][A[A

 47% 15/32 [00:04<00:04,  3.96it/s][A[A

 50% 16/32 [00:04<00:04,  3.98it/s][A[A

 53% 17/32 [00:04<00:03,  4.55it/s][A[A

 56% 18/32 [00:06<00:07,  1.90it/s][A[A

 59% 19/32 [00:06<00:05,  2.28it/s][A[A

 62% 20/32 [00:06<00:04,  2.75it/s][A[A

 66% 21/32 [00:06<00:03,  3.19it/s][A[A

 69% 22/32 [00:06<00:02,  3.46it/s][A[A

 72% 23/32 [00:07<00:02,  3.61it/s][A[A

 75% 24/32 [00:07<00:02,  3.83it/s][A[A

 78% 25/32 [00:07<00:01,  3.91it/s][A[A

 81% 26/32 [00:07<00:01,  4.47it/s][A[A

 84% 27/32 [00:07<00:01,  4.37it/s][A[A

 88% 28/32 [00:08<00:01,  3.99it/s][A[A

 91% 29/32 [00:08<00:00,  4.03it/s][A[A

 94% 30/32 [00:09<00:01,  1.87it/s][A[A

 97% 31/32 [00:09<00:00,  2.31it/s][A[A

100% 32/32 [00:10<00:00,  2.76it/s][A[A100% 32/32 [00:10<00:00,  3.16it/s]
Meta loss on this task batch = 4.3768e-01, PNorm = 43.2799, GNorm = 0.1204

 32% 6/19 [00:57<02:07,  9.81s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.91it/s][A[A

  6% 2/32 [00:00<00:06,  4.94it/s][A[A

  9% 3/32 [00:00<00:06,  4.75it/s][A[A

 12% 4/32 [00:00<00:06,  4.53it/s][A[A

 16% 5/32 [00:01<00:05,  5.16it/s][A[A

 19% 6/32 [00:01<00:05,  5.11it/s][A[A

 22% 7/32 [00:01<00:05,  4.88it/s][A[A

 25% 8/32 [00:01<00:04,  4.82it/s][A[A

 28% 9/32 [00:02<00:11,  2.01it/s][A[A

 31% 10/32 [00:03<00:09,  2.40it/s][A[A

 34% 11/32 [00:03<00:07,  2.76it/s][A[A

 38% 12/32 [00:03<00:06,  3.10it/s][A[A

 41% 13/32 [00:03<00:05,  3.36it/s][A[A

 44% 14/32 [00:03<00:04,  3.94it/s][A[A

 47% 15/32 [00:04<00:04,  3.99it/s][A[A

 50% 16/32 [00:04<00:04,  4.00it/s][A[A

 53% 17/32 [00:04<00:03,  4.05it/s][A[A

 56% 18/32 [00:04<00:03,  4.15it/s][A[A

 59% 19/32 [00:05<00:02,  4.54it/s][A[A

 62% 20/32 [00:05<00:02,  4.43it/s][A[A

 66% 21/32 [00:06<00:05,  1.90it/s][A[A

 69% 22/32 [00:06<00:04,  2.26it/s][A[A

 72% 23/32 [00:07<00:03,  2.59it/s][A[A

 75% 24/32 [00:07<00:02,  2.93it/s][A[A

 78% 25/32 [00:07<00:02,  3.25it/s][A[A

 81% 26/32 [00:07<00:01,  3.47it/s][A[A

 84% 27/32 [00:07<00:01,  3.73it/s][A[A

 88% 28/32 [00:08<00:01,  3.89it/s][A[A

 91% 29/32 [00:08<00:00,  4.09it/s][A[A

 94% 30/32 [00:08<00:00,  4.37it/s][A[A

 97% 31/32 [00:08<00:00,  4.42it/s][A[A

100% 32/32 [00:10<00:00,  1.90it/s][A[A100% 32/32 [00:10<00:00,  3.19it/s]
Meta loss on this task batch = 4.8205e-01, PNorm = 43.3105, GNorm = 0.1102

 37% 7/19 [01:08<02:01, 10.12s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.66it/s][A[A

  6% 2/32 [00:00<00:08,  3.65it/s][A[A

  9% 3/32 [00:00<00:07,  3.72it/s][A[A

 12% 4/32 [00:01<00:07,  3.77it/s][A[A

 16% 5/32 [00:01<00:06,  3.93it/s][A[A

 19% 6/32 [00:01<00:06,  3.87it/s][A[A

 22% 7/32 [00:02<00:13,  1.83it/s][A[A

 25% 8/32 [00:03<00:10,  2.18it/s][A[A

 28% 9/32 [00:03<00:09,  2.55it/s][A[A

 31% 10/32 [00:03<00:07,  2.87it/s][A[A

 34% 11/32 [00:03<00:06,  3.23it/s][A[A

 38% 12/32 [00:03<00:05,  3.58it/s][A[A

 41% 13/32 [00:04<00:04,  3.86it/s][A[A

 44% 14/32 [00:04<00:04,  4.03it/s][A[A

 47% 15/32 [00:04<00:04,  4.12it/s][A[A

 50% 16/32 [00:05<00:08,  1.85it/s][A[A

 53% 17/32 [00:06<00:06,  2.18it/s][A[A

 56% 18/32 [00:06<00:05,  2.57it/s][A[A

 59% 19/32 [00:06<00:04,  2.84it/s][A[A

 62% 20/32 [00:06<00:03,  3.17it/s][A[A

 66% 21/32 [00:07<00:03,  3.45it/s][A[A

 69% 22/32 [00:07<00:02,  3.68it/s][A[A

 72% 23/32 [00:07<00:02,  3.92it/s][A[A

 75% 24/32 [00:07<00:02,  3.93it/s][A[A

 78% 25/32 [00:08<00:01,  3.92it/s][A[A

 81% 26/32 [00:09<00:03,  1.79it/s][A[A

 84% 27/32 [00:09<00:02,  2.25it/s][A[A

 88% 28/32 [00:09<00:01,  2.73it/s][A[A

 91% 29/32 [00:09<00:00,  3.21it/s][A[A

 94% 30/32 [00:10<00:00,  3.62it/s][A[A

 97% 31/32 [00:10<00:00,  3.96it/s][A[A

100% 32/32 [00:10<00:00,  4.23it/s][A[A100% 32/32 [00:10<00:00,  3.07it/s]
Meta loss on this task batch = 3.2729e-01, PNorm = 43.3419, GNorm = 0.1123

 42% 8/19 [01:19<01:54, 10.45s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  5.02it/s][A[A

  6% 2/32 [00:00<00:05,  5.26it/s][A[A

  9% 3/32 [00:00<00:05,  5.40it/s][A[A

 12% 4/32 [00:00<00:05,  5.49it/s][A[A

 16% 5/32 [00:00<00:05,  5.34it/s][A[A

 19% 6/32 [00:02<00:12,  2.04it/s][A[A

 22% 7/32 [00:02<00:10,  2.48it/s][A[A

 25% 8/32 [00:02<00:08,  2.92it/s][A[A

 28% 9/32 [00:02<00:06,  3.36it/s][A[A

 31% 10/32 [00:02<00:05,  3.78it/s][A[A

 34% 11/32 [00:03<00:05,  4.14it/s][A[A

 38% 12/32 [00:03<00:04,  4.39it/s][A[A

 41% 13/32 [00:03<00:04,  4.53it/s][A[A

 44% 14/32 [00:03<00:03,  4.71it/s][A[A

 47% 15/32 [00:03<00:03,  4.83it/s][A[A

 50% 16/32 [00:04<00:03,  5.00it/s][A[A

 53% 17/32 [00:04<00:02,  5.12it/s][A[A

 56% 18/32 [00:04<00:02,  5.28it/s][A[A

 59% 19/32 [00:04<00:02,  5.46it/s][A[A

 62% 20/32 [00:04<00:02,  5.45it/s][A[A

 66% 21/32 [00:05<00:05,  2.04it/s][A[A

 69% 22/32 [00:06<00:04,  2.49it/s][A[A

 72% 23/32 [00:06<00:03,  2.86it/s][A[A

 75% 24/32 [00:06<00:02,  3.31it/s][A[A

 78% 25/32 [00:06<00:01,  3.63it/s][A[A

 81% 26/32 [00:06<00:01,  3.99it/s][A[A

 84% 27/32 [00:07<00:01,  4.33it/s][A[A

 88% 28/32 [00:07<00:00,  4.65it/s][A[A

 91% 29/32 [00:07<00:00,  4.86it/s][A[A

 94% 30/32 [00:07<00:00,  5.06it/s][A[A

 97% 31/32 [00:07<00:00,  5.22it/s][A[A

100% 32/32 [00:08<00:00,  5.18it/s][A[A100% 32/32 [00:08<00:00,  3.96it/s]
Meta loss on this task batch = 5.5499e-02, PNorm = 43.3750, GNorm = 0.2354

 47% 9/19 [01:28<01:39,  9.99s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:06,  4.91it/s][A[A

  6% 2/32 [00:00<00:06,  4.63it/s][A[A

  9% 3/32 [00:01<00:14,  1.95it/s][A[A

 12% 4/32 [00:01<00:11,  2.40it/s][A[A

 16% 5/32 [00:02<00:09,  2.86it/s][A[A

 19% 6/32 [00:02<00:07,  3.30it/s][A[A

 22% 7/32 [00:02<00:06,  3.78it/s][A[A

 25% 8/32 [00:02<00:05,  4.16it/s][A[A

 28% 9/32 [00:02<00:05,  4.37it/s][A[A

 31% 10/32 [00:02<00:04,  4.57it/s][A[A

 34% 11/32 [00:03<00:04,  4.87it/s][A[A

 38% 12/32 [00:03<00:03,  5.01it/s][A[A

 41% 13/32 [00:03<00:03,  5.12it/s][A[A

 44% 14/32 [00:03<00:03,  5.05it/s][A[A

 47% 15/32 [00:03<00:03,  5.15it/s][A[A

 50% 16/32 [00:04<00:03,  4.90it/s][A[A

 53% 17/32 [00:05<00:07,  1.98it/s][A[A

 56% 18/32 [00:05<00:05,  2.44it/s][A[A

 59% 19/32 [00:05<00:04,  2.83it/s][A[A

 62% 20/32 [00:05<00:03,  3.23it/s][A[A

 66% 21/32 [00:06<00:02,  3.70it/s][A[A

 69% 22/32 [00:06<00:02,  4.03it/s][A[A

 72% 23/32 [00:06<00:02,  4.29it/s][A[A

 75% 24/32 [00:06<00:01,  4.57it/s][A[A

 78% 25/32 [00:06<00:01,  4.84it/s][A[A

 81% 26/32 [00:07<00:01,  4.52it/s][A[A

 84% 27/32 [00:08<00:02,  1.90it/s][A[A

 88% 28/32 [00:08<00:01,  2.29it/s][A[A

 91% 29/32 [00:08<00:01,  2.65it/s][A[A

 94% 30/32 [00:09<00:00,  3.00it/s][A[A

 97% 31/32 [00:09<00:00,  3.33it/s][A[A

100% 32/32 [00:09<00:00,  3.59it/s][A[A100% 32/32 [00:09<00:00,  3.35it/s]
Meta loss on this task batch = 1.6706e-01, PNorm = 43.4096, GNorm = 0.0723

 53% 10/19 [01:38<01:30, 10.10s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  3.97it/s][A[A

  6% 2/32 [00:00<00:07,  4.05it/s][A[A

  9% 3/32 [00:00<00:07,  4.10it/s][A[A

 12% 4/32 [00:00<00:06,  4.06it/s][A[A

 16% 5/32 [00:02<00:14,  1.83it/s][A[A

 19% 6/32 [00:02<00:11,  2.19it/s][A[A

 22% 7/32 [00:02<00:09,  2.57it/s][A[A

 25% 8/32 [00:02<00:08,  2.87it/s][A[A

 28% 9/32 [00:03<00:07,  3.17it/s][A[A

 31% 10/32 [00:03<00:06,  3.37it/s][A[A

 34% 11/32 [00:04<00:12,  1.72it/s][A[A

 38% 12/32 [00:04<00:09,  2.06it/s][A[A

 41% 13/32 [00:05<00:07,  2.42it/s][A[A

 44% 14/32 [00:05<00:06,  2.74it/s][A[A

 47% 15/32 [00:05<00:05,  3.09it/s][A[A

 50% 16/32 [00:05<00:04,  3.38it/s][A[A

 53% 17/32 [00:06<00:04,  3.58it/s][A[A

 56% 18/32 [00:06<00:03,  3.75it/s][A[A

 59% 19/32 [00:06<00:03,  3.93it/s][A[A

 62% 20/32 [00:06<00:02,  4.00it/s][A[A

 66% 21/32 [00:07<00:02,  4.07it/s][A[A

 69% 22/32 [00:08<00:05,  1.85it/s][A[A

 72% 23/32 [00:08<00:04,  2.21it/s][A[A

 75% 24/32 [00:08<00:03,  2.57it/s][A[A

 78% 25/32 [00:09<00:02,  2.89it/s][A[A

 81% 26/32 [00:09<00:01,  3.19it/s][A[A

 84% 27/32 [00:09<00:01,  3.45it/s][A[A

 88% 28/32 [00:09<00:01,  3.62it/s][A[A

 91% 29/32 [00:10<00:00,  3.78it/s][A[A

 94% 30/32 [00:10<00:00,  3.91it/s][A[A

 97% 31/32 [00:11<00:00,  1.82it/s][A[A

100% 32/32 [00:11<00:00,  2.17it/s][A[A100% 32/32 [00:11<00:00,  2.73it/s]
Meta loss on this task batch = 5.6139e-01, PNorm = 43.4441, GNorm = 0.1040

 58% 11/19 [01:51<01:26, 10.84s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.82it/s][A[A

  6% 2/32 [00:00<00:07,  3.92it/s][A[A

  9% 3/32 [00:00<00:07,  3.99it/s][A[A

 12% 4/32 [00:00<00:06,  4.02it/s][A[A

 16% 5/32 [00:02<00:14,  1.83it/s][A[A

 19% 6/32 [00:02<00:11,  2.19it/s][A[A

 22% 7/32 [00:02<00:09,  2.55it/s][A[A

 25% 8/32 [00:02<00:08,  2.88it/s][A[A

 28% 9/32 [00:03<00:07,  3.22it/s][A[A

 31% 10/32 [00:03<00:06,  3.48it/s][A[A

 34% 11/32 [00:03<00:05,  3.66it/s][A[A

 38% 12/32 [00:03<00:05,  3.82it/s][A[A

 41% 13/32 [00:04<00:04,  3.91it/s][A[A

 44% 14/32 [00:04<00:04,  4.00it/s][A[A

 47% 15/32 [00:05<00:09,  1.83it/s][A[A

 50% 16/32 [00:05<00:07,  2.20it/s][A[A

 53% 17/32 [00:06<00:05,  2.53it/s][A[A

 56% 18/32 [00:06<00:04,  2.87it/s][A[A

 59% 19/32 [00:06<00:04,  3.15it/s][A[A

 62% 20/32 [00:06<00:03,  3.41it/s][A[A

 66% 21/32 [00:07<00:03,  3.60it/s][A[A

 69% 22/32 [00:07<00:02,  3.74it/s][A[A

 72% 23/32 [00:07<00:02,  3.90it/s][A[A

 75% 24/32 [00:08<00:04,  1.81it/s][A[A

 78% 25/32 [00:09<00:03,  2.19it/s][A[A

 81% 26/32 [00:09<00:02,  2.54it/s][A[A

 84% 27/32 [00:09<00:01,  2.89it/s][A[A

 88% 28/32 [00:09<00:01,  3.21it/s][A[A

 91% 29/32 [00:09<00:00,  3.52it/s][A[A

 94% 30/32 [00:10<00:00,  3.72it/s][A[A

 97% 31/32 [00:10<00:00,  3.90it/s][A[A

100% 32/32 [00:10<00:00,  3.98it/s][A[A100% 32/32 [00:10<00:00,  3.00it/s]
Meta loss on this task batch = 5.2615e-01, PNorm = 43.4769, GNorm = 0.0983

 63% 12/19 [02:02<01:17, 11.04s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  3.98it/s][A[A

  6% 2/32 [00:00<00:07,  4.05it/s][A[A

  9% 3/32 [00:00<00:07,  4.03it/s][A[A

 12% 4/32 [00:01<00:15,  1.83it/s][A[A

 16% 5/32 [00:02<00:12,  2.22it/s][A[A

 19% 6/32 [00:02<00:10,  2.57it/s][A[A

 22% 7/32 [00:02<00:08,  2.90it/s][A[A

 25% 8/32 [00:02<00:07,  3.19it/s][A[A

 28% 9/32 [00:03<00:06,  3.47it/s][A[A

 31% 10/32 [00:03<00:05,  3.72it/s][A[A

 34% 11/32 [00:03<00:05,  3.91it/s][A[A

 38% 12/32 [00:03<00:05,  3.97it/s][A[A

 41% 13/32 [00:05<00:10,  1.82it/s][A[A

 44% 14/32 [00:05<00:08,  2.18it/s][A[A

 47% 15/32 [00:05<00:06,  2.55it/s][A[A

 50% 16/32 [00:05<00:05,  2.90it/s][A[A

 53% 17/32 [00:06<00:04,  3.18it/s][A[A

 56% 18/32 [00:06<00:04,  3.43it/s][A[A

 59% 19/32 [00:06<00:03,  3.63it/s][A[A

 62% 20/32 [00:06<00:03,  3.76it/s][A[A

 66% 21/32 [00:08<00:06,  1.78it/s][A[A

 69% 22/32 [00:08<00:04,  2.15it/s][A[A

 72% 23/32 [00:08<00:03,  2.51it/s][A[A

 75% 24/32 [00:08<00:02,  2.83it/s][A[A

 78% 25/32 [00:09<00:02,  3.12it/s][A[A

 81% 26/32 [00:09<00:01,  3.38it/s][A[A

 84% 27/32 [00:10<00:02,  1.75it/s][A[A

 88% 28/32 [00:10<00:01,  2.12it/s][A[A

 91% 29/32 [00:10<00:01,  2.50it/s][A[A

 94% 30/32 [00:11<00:00,  2.84it/s][A[A

 97% 31/32 [00:11<00:00,  3.18it/s][A[A

100% 32/32 [00:11<00:00,  3.41it/s][A[A100% 32/32 [00:11<00:00,  2.75it/s]
Meta loss on this task batch = 5.4584e-01, PNorm = 43.5089, GNorm = 0.0878

 68% 13/19 [02:15<01:08, 11.47s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.07it/s][A[A

  6% 2/32 [00:00<00:07,  4.13it/s][A[A

  9% 3/32 [00:00<00:06,  4.21it/s][A[A

 12% 4/32 [00:00<00:06,  4.27it/s][A[A

 16% 5/32 [00:01<00:06,  4.22it/s][A[A

 19% 6/32 [00:02<00:13,  1.87it/s][A[A

 22% 7/32 [00:02<00:11,  2.23it/s][A[A

 25% 8/32 [00:02<00:09,  2.63it/s][A[A

 28% 9/32 [00:03<00:07,  2.98it/s][A[A

 31% 10/32 [00:03<00:06,  3.27it/s][A[A

 34% 11/32 [00:03<00:05,  3.51it/s][A[A

 38% 12/32 [00:03<00:05,  3.64it/s][A[A

 41% 13/32 [00:04<00:05,  3.79it/s][A[A

 44% 14/32 [00:04<00:04,  3.86it/s][A[A

 47% 15/32 [00:05<00:09,  1.81it/s][A[A

 50% 16/32 [00:05<00:07,  2.18it/s][A[A

 53% 17/32 [00:06<00:05,  2.54it/s][A[A

 56% 18/32 [00:06<00:04,  2.86it/s][A[A

 59% 19/32 [00:06<00:04,  3.21it/s][A[A

 62% 20/32 [00:06<00:03,  3.39it/s][A[A

 66% 21/32 [00:07<00:03,  3.58it/s][A[A

 69% 22/32 [00:08<00:05,  1.76it/s][A[A

 72% 23/32 [00:08<00:04,  2.13it/s][A[A

 75% 24/32 [00:08<00:03,  2.53it/s][A[A

 78% 25/32 [00:08<00:02,  2.86it/s][A[A

 81% 26/32 [00:09<00:01,  3.17it/s][A[A

 84% 27/32 [00:09<00:01,  3.45it/s][A[A

 88% 28/32 [00:09<00:01,  3.69it/s][A[A

 91% 29/32 [00:09<00:00,  3.88it/s][A[A

 94% 30/32 [00:10<00:00,  4.04it/s][A[A

 97% 31/32 [00:10<00:00,  4.11it/s][A[A

100% 32/32 [00:11<00:00,  1.82it/s][A[A100% 32/32 [00:11<00:00,  2.76it/s]
Meta loss on this task batch = 5.4893e-01, PNorm = 43.5375, GNorm = 0.1154

 74% 14/19 [02:27<00:58, 11.76s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.04it/s][A[A

  6% 2/32 [00:00<00:07,  3.99it/s][A[A

  9% 3/32 [00:00<00:07,  4.03it/s][A[A

 12% 4/32 [00:00<00:06,  4.10it/s][A[A

 16% 5/32 [00:01<00:06,  4.29it/s][A[A

 19% 6/32 [00:01<00:06,  4.14it/s][A[A

 22% 7/32 [00:01<00:05,  4.20it/s][A[A

 25% 8/32 [00:01<00:05,  4.54it/s][A[A

 28% 9/32 [00:03<00:12,  1.88it/s][A[A

 31% 10/32 [00:03<00:09,  2.28it/s][A[A

 34% 11/32 [00:03<00:07,  2.69it/s][A[A

 38% 12/32 [00:03<00:06,  3.28it/s][A[A

 41% 13/32 [00:03<00:05,  3.58it/s][A[A

 44% 14/32 [00:04<00:04,  3.81it/s][A[A

 47% 15/32 [00:04<00:04,  4.23it/s][A[A

 50% 16/32 [00:04<00:03,  4.30it/s][A[A

 53% 17/32 [00:04<00:03,  4.26it/s][A[A

 56% 18/32 [00:05<00:03,  4.34it/s][A[A

 59% 19/32 [00:06<00:06,  1.97it/s][A[A

 62% 20/32 [00:06<00:05,  2.33it/s][A[A

 66% 21/32 [00:06<00:03,  2.83it/s][A[A

 69% 22/32 [00:06<00:03,  3.18it/s][A[A

 72% 23/32 [00:07<00:02,  3.34it/s][A[A

 75% 24/32 [00:07<00:02,  3.65it/s][A[A

 78% 25/32 [00:07<00:01,  3.85it/s][A[A

 81% 26/32 [00:07<00:01,  3.79it/s][A[A

 84% 27/32 [00:08<00:01,  3.85it/s][A[A

 88% 28/32 [00:09<00:02,  1.80it/s][A[A

 91% 29/32 [00:09<00:01,  2.16it/s][A[A

 94% 30/32 [00:09<00:00,  2.51it/s][A[A

 97% 31/32 [00:10<00:00,  2.85it/s][A[A

100% 32/32 [00:10<00:00,  3.18it/s][A[A100% 32/32 [00:10<00:00,  3.12it/s]
Meta loss on this task batch = 4.5817e-01, PNorm = 43.5634, GNorm = 0.1011

 79% 15/19 [02:38<00:46, 11.55s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.18it/s][A[A

  6% 2/32 [00:00<00:07,  4.20it/s][A[A

  9% 3/32 [00:00<00:06,  4.25it/s][A[A

 12% 4/32 [00:00<00:06,  4.31it/s][A[A

 16% 5/32 [00:02<00:14,  1.88it/s][A[A

 19% 6/32 [00:02<00:11,  2.26it/s][A[A

 22% 7/32 [00:02<00:09,  2.65it/s][A[A

 25% 8/32 [00:02<00:08,  2.98it/s][A[A

 28% 9/32 [00:03<00:07,  3.24it/s][A[A

 31% 10/32 [00:03<00:06,  3.48it/s][A[A

 34% 11/32 [00:03<00:05,  3.66it/s][A[A

 38% 12/32 [00:03<00:05,  3.85it/s][A[A

 41% 13/32 [00:04<00:04,  4.03it/s][A[A

 44% 14/32 [00:04<00:04,  4.18it/s][A[A

 47% 15/32 [00:04<00:04,  4.20it/s][A[A

 50% 16/32 [00:05<00:08,  1.94it/s][A[A

 53% 17/32 [00:05<00:06,  2.32it/s][A[A

 56% 18/32 [00:06<00:05,  2.80it/s][A[A

 59% 19/32 [00:06<00:04,  3.13it/s][A[A

 62% 20/32 [00:06<00:03,  3.44it/s][A[A

 66% 21/32 [00:06<00:02,  3.70it/s][A[A

 69% 22/32 [00:06<00:02,  4.04it/s][A[A

 72% 23/32 [00:07<00:02,  4.15it/s][A[A

 75% 24/32 [00:08<00:04,  1.82it/s][A[A

 78% 25/32 [00:08<00:03,  2.25it/s][A[A

 81% 26/32 [00:08<00:02,  2.61it/s][A[A

 84% 27/32 [00:09<00:01,  2.91it/s][A[A

 88% 28/32 [00:09<00:01,  3.26it/s][A[A

 91% 29/32 [00:09<00:00,  3.35it/s][A[A

 94% 30/32 [00:09<00:00,  3.58it/s][A[A

 97% 31/32 [00:10<00:00,  3.77it/s][A[A

100% 32/32 [00:10<00:00,  3.88it/s][A[A100% 32/32 [00:10<00:00,  3.10it/s]
Meta loss on this task batch = 5.4706e-01, PNorm = 43.5832, GNorm = 0.1082

 84% 16/19 [02:49<00:34, 11.43s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:08,  3.78it/s][A[A

  6% 2/32 [00:00<00:07,  3.79it/s][A[A

  9% 3/32 [00:01<00:16,  1.80it/s][A[A

 12% 4/32 [00:01<00:12,  2.25it/s][A[A

 16% 5/32 [00:02<00:10,  2.58it/s][A[A

 19% 6/32 [00:02<00:08,  3.22it/s][A[A

 22% 7/32 [00:02<00:07,  3.50it/s][A[A

 25% 8/32 [00:02<00:06,  3.60it/s][A[A

 28% 9/32 [00:03<00:06,  3.81it/s][A[A

 31% 10/32 [00:03<00:05,  4.33it/s][A[A

 34% 11/32 [00:03<00:04,  4.41it/s][A[A

 38% 12/32 [00:03<00:04,  4.61it/s][A[A

 41% 13/32 [00:03<00:04,  4.55it/s][A[A

 44% 14/32 [00:05<00:09,  1.92it/s][A[A

 47% 15/32 [00:05<00:07,  2.29it/s][A[A

 50% 16/32 [00:05<00:06,  2.66it/s][A[A

 53% 17/32 [00:05<00:04,  3.15it/s][A[A

 56% 18/32 [00:05<00:04,  3.40it/s][A[A

 59% 19/32 [00:06<00:03,  3.47it/s][A[A

 62% 20/32 [00:06<00:03,  3.72it/s][A[A

 66% 21/32 [00:07<00:06,  1.79it/s][A[A

 69% 22/32 [00:07<00:04,  2.16it/s][A[A

 72% 23/32 [00:08<00:03,  2.53it/s][A[A

 75% 24/32 [00:08<00:02,  2.80it/s][A[A

 78% 25/32 [00:08<00:02,  3.20it/s][A[A

 81% 26/32 [00:08<00:01,  3.47it/s][A[A

 84% 27/32 [00:09<00:01,  3.93it/s][A[A

 88% 28/32 [00:09<00:00,  4.29it/s][A[A

 91% 29/32 [00:09<00:00,  4.36it/s][A[A

 94% 30/32 [00:09<00:00,  4.29it/s][A[A

 97% 31/32 [00:09<00:00,  4.45it/s][A[A

100% 32/32 [00:11<00:00,  1.88it/s][A[A100% 32/32 [00:11<00:00,  2.87it/s]
Meta loss on this task batch = 4.2719e-01, PNorm = 43.6030, GNorm = 0.0707

 89% 17/19 [03:01<00:23, 11.60s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.26it/s][A[A

  6% 2/32 [00:00<00:05,  5.03it/s][A[A

  9% 3/32 [00:00<00:05,  4.92it/s][A[A

 12% 4/32 [00:00<00:05,  4.77it/s][A[A

 16% 5/32 [00:01<00:05,  4.57it/s][A[A

 19% 6/32 [00:01<00:05,  4.60it/s][A[A

 22% 7/32 [00:01<00:05,  4.38it/s][A[A

 25% 8/32 [00:01<00:05,  4.23it/s][A[A

 28% 9/32 [00:03<00:12,  1.90it/s][A[A

 31% 10/32 [00:03<00:09,  2.29it/s][A[A

 34% 11/32 [00:03<00:07,  2.68it/s][A[A

 38% 12/32 [00:03<00:06,  3.14it/s][A[A

 41% 13/32 [00:03<00:05,  3.31it/s][A[A

 44% 14/32 [00:04<00:04,  3.93it/s][A[A

 47% 15/32 [00:04<00:04,  3.95it/s][A[A

 50% 16/32 [00:04<00:03,  4.14it/s][A[A

 53% 17/32 [00:04<00:03,  4.81it/s][A[A

 56% 18/32 [00:04<00:02,  4.75it/s][A[A

 59% 19/32 [00:05<00:02,  4.55it/s][A[A

 62% 20/32 [00:05<00:02,  4.53it/s][A[A

 66% 21/32 [00:06<00:05,  1.92it/s][A[A

 69% 22/32 [00:06<00:04,  2.48it/s][A[A

 72% 23/32 [00:06<00:03,  2.74it/s][A[A

 75% 24/32 [00:07<00:02,  2.94it/s][A[A

 78% 25/32 [00:07<00:02,  3.27it/s][A[A

 81% 26/32 [00:07<00:01,  3.55it/s][A[A

 84% 27/32 [00:07<00:01,  3.59it/s][A[A

 88% 28/32 [00:08<00:01,  3.81it/s][A[A

 91% 29/32 [00:08<00:00,  3.95it/s][A[A

 94% 30/32 [00:08<00:00,  3.99it/s][A[A

 97% 31/32 [00:08<00:00,  4.22it/s][A[A

100% 32/32 [00:10<00:00,  1.83it/s][A[A100% 32/32 [00:10<00:00,  3.15it/s]
Meta loss on this task batch = 4.9146e-01, PNorm = 43.6233, GNorm = 0.0864

 95% 18/19 [03:12<00:11, 11.41s/it][A

  0% 0/23 [00:00<?, ?it/s][A[A

  4% 1/23 [00:00<00:05,  4.16it/s][A[A

  9% 2/23 [00:00<00:04,  4.61it/s][A[A

 13% 3/23 [00:00<00:04,  4.56it/s][A[A

 17% 4/23 [00:00<00:04,  4.66it/s][A[A

 22% 5/23 [00:01<00:03,  4.84it/s][A[A

 26% 6/23 [00:01<00:03,  4.82it/s][A[A

 30% 7/23 [00:01<00:03,  4.63it/s][A[A

 35% 8/23 [00:01<00:03,  4.49it/s][A[A

 39% 9/23 [00:01<00:02,  5.10it/s][A[A

 43% 10/23 [00:02<00:02,  4.62it/s][A[A

 48% 11/23 [00:02<00:02,  4.79it/s][A[A

 52% 12/23 [00:03<00:05,  1.94it/s][A[A

 57% 13/23 [00:03<00:04,  2.50it/s][A[A

 61% 14/23 [00:03<00:03,  2.86it/s][A[A

 65% 15/23 [00:04<00:02,  2.79it/s][A[A

 70% 16/23 [00:04<00:02,  3.04it/s][A[A

 74% 17/23 [00:04<00:01,  3.60it/s][A[A

 78% 18/23 [00:04<00:01,  3.72it/s][A[A

 83% 19/23 [00:05<00:01,  3.88it/s][A[A

 87% 20/23 [00:05<00:00,  4.56it/s][A[A

 91% 21/23 [00:05<00:00,  4.87it/s][A[A

 96% 22/23 [00:05<00:00,  5.49it/s][A[A

100% 23/23 [00:05<00:00,  5.73it/s][A[A100% 23/23 [00:05<00:00,  4.00it/s]
Meta loss on this task batch = 4.2876e-01, PNorm = 43.6389, GNorm = 0.1337

100% 19/19 [03:19<00:00,  9.88s/it][A100% 19/19 [03:19<00:00, 10.47s/it]
Took 199.02486038208008 seconds to complete one epoch of meta training

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/20 [00:00<?, ?it/s][A[A


  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 31.11it/s]


  5% 1/20 [00:00<00:02,  8.14it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 21.34it/s]


 10% 2/20 [00:01<00:08,  2.19it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 18.70it/s][A[A[A100% 3/3 [00:00<00:00, 20.75it/s]


 15% 3/20 [00:01<00:06,  2.48it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 16.36it/s]


 20% 4/20 [00:01<00:05,  2.87it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 15.26it/s][A[A[A100% 4/4 [00:00<00:00, 17.62it/s]


 25% 5/20 [00:02<00:05,  2.77it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.13it/s][A[A[A100% 4/4 [00:00<00:00, 15.96it/s]


 30% 6/20 [00:02<00:05,  2.67it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.60it/s][A[A[A100% 4/4 [00:00<00:00, 21.42it/s]


 35% 7/20 [00:02<00:04,  2.74it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.96it/s][A[A[A


100% 4/4 [00:00<00:00, 19.48it/s][A[A[A100% 4/4 [00:00<00:00, 19.14it/s]


 40% 8/20 [00:03<00:04,  2.76it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.58it/s][A[A[A


100% 4/4 [00:01<00:00,  2.92it/s][A[A[A100% 4/4 [00:01<00:00,  3.40it/s]


 45% 9/20 [00:04<00:07,  1.52it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.74it/s][A[A[A


100% 4/4 [00:00<00:00, 19.68it/s][A[A[A100% 4/4 [00:00<00:00, 19.61it/s]


 50% 10/20 [00:05<00:05,  1.76it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 17.19it/s][A[A[A100% 4/4 [00:00<00:00, 19.66it/s]


 55% 11/20 [00:05<00:04,  1.98it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.65it/s][A[A[A100% 4/4 [00:00<00:00, 22.08it/s]


 60% 12/20 [00:05<00:03,  2.21it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.79it/s][A[A[A100% 3/3 [00:00<00:00, 14.06it/s]


 65% 13/20 [00:06<00:03,  2.32it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.27it/s][A[A[A100% 3/3 [00:00<00:00, 13.96it/s]


 70% 14/20 [00:06<00:02,  2.39it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.61it/s][A[A[A100% 4/4 [00:00<00:00, 17.78it/s]


 75% 15/20 [00:07<00:03,  1.41it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.01it/s][A[A[A100% 3/3 [00:00<00:00, 17.37it/s]


 80% 16/20 [00:08<00:02,  1.66it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 12.28it/s][A[A[A100% 3/3 [00:00<00:00, 15.10it/s]


 85% 17/20 [00:08<00:01,  1.86it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 23.40it/s]


 90% 18/20 [00:08<00:00,  2.18it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 16.16it/s][A[A[A100% 3/3 [00:00<00:00, 20.60it/s]


 95% 19/20 [00:09<00:00,  2.43it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 14.14it/s]


100% 20/20 [00:09<00:00,  2.78it/s][A[A100% 20/20 [00:09<00:00,  2.11it/s]

100% 1/1 [00:09<00:00,  9.46s/it][A100% 1/1 [00:09<00:00,  9.46s/it]
Took 208.48746585845947 seconds to complete one epoch of meta training and validating
Meta validation score auc = 0.637505
 97% 29/30 [1:33:36<03:15, 195.60s/it]Epoch 29

  0% 0/19 [00:00<?, ?it/s][A

  0% 0/32 [00:00<?, ?it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  3% 1/32 [00:00<00:04,  6.28it/s][A[A

  6% 2/32 [00:00<00:05,  5.84it/s][A[A

  9% 3/32 [00:00<00:04,  5.91it/s][A[A

 12% 4/32 [00:00<00:04,  6.18it/s][A[A

 16% 5/32 [00:00<00:04,  6.49it/s][A[A

 19% 6/32 [00:00<00:04,  6.21it/s][A[A

 22% 7/32 [00:01<00:04,  6.11it/s][A[A

 25% 8/32 [00:01<00:04,  5.85it/s][A[A

 28% 9/32 [00:02<00:10,  2.19it/s][A[A

 31% 10/32 [00:02<00:07,  2.78it/s][A[A

 34% 11/32 [00:02<00:06,  3.20it/s][A[A

 38% 12/32 [00:03<00:05,  3.39it/s][A[A

 41% 13/32 [00:03<00:05,  3.70it/s][A[A

 44% 14/32 [00:03<00:04,  4.04it/s][A[A

 47% 15/32 [00:03<00:03,  4.68it/s][A[A

 50% 16/32 [00:03<00:03,  4.74it/s][A[A

 53% 17/32 [00:03<00:02,  5.16it/s][A[A

 56% 18/32 [00:04<00:02,  5.44it/s][A[A

 59% 19/32 [00:04<00:02,  5.66it/s][A[A

 62% 20/32 [00:04<00:02,  5.59it/s][A[A

 66% 21/32 [00:04<00:01,  6.22it/s][A[A

 69% 22/32 [00:04<00:01,  5.99it/s][A[A

 72% 23/32 [00:04<00:01,  6.76it/s][A[A

 75% 24/32 [00:05<00:01,  6.73it/s][A[A

 78% 25/32 [00:06<00:03,  2.16it/s][A[A

 81% 26/32 [00:06<00:02,  2.58it/s][A[A

 88% 28/32 [00:06<00:01,  3.23it/s][A[A

 91% 29/32 [00:06<00:00,  3.74it/s][A[A

 94% 30/32 [00:06<00:00,  4.57it/s][A[A

 97% 31/32 [00:07<00:00,  5.35it/s][A[A

100% 32/32 [00:07<00:00,  5.48it/s][A[A100% 32/32 [00:07<00:00,  4.42it/s]
Meta loss on this task batch = 4.9631e-01, PNorm = 43.6539, GNorm = 0.1998

  5% 1/19 [00:07<02:22,  7.90s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:03,  8.15it/s][A[A

  6% 2/32 [00:00<00:03,  7.80it/s][A[A

  9% 3/32 [00:00<00:04,  6.91it/s][A[A

 12% 4/32 [00:00<00:04,  6.57it/s][A[A

 16% 5/32 [00:00<00:04,  6.14it/s][A[A

 19% 6/32 [00:00<00:04,  6.40it/s][A[A

 22% 7/32 [00:01<00:03,  6.68it/s][A[A

 25% 8/32 [00:01<00:03,  6.79it/s][A[A

 28% 9/32 [00:01<00:03,  6.61it/s][A[A

 31% 10/32 [00:01<00:03,  6.74it/s][A[A

 34% 11/32 [00:01<00:03,  6.85it/s][A[A

 38% 12/32 [00:01<00:03,  6.30it/s][A[A

 41% 13/32 [00:02<00:02,  6.40it/s][A[A

 44% 14/32 [00:03<00:08,  2.15it/s][A[A

 47% 15/32 [00:03<00:06,  2.68it/s][A[A

 50% 16/32 [00:03<00:05,  3.16it/s][A[A

 53% 17/32 [00:03<00:04,  3.73it/s][A[A

 56% 18/32 [00:03<00:03,  3.93it/s][A[A

 59% 19/32 [00:04<00:02,  4.79it/s][A[A

 62% 20/32 [00:04<00:02,  5.06it/s][A[A

 66% 21/32 [00:04<00:01,  5.57it/s][A[A

 69% 22/32 [00:04<00:01,  5.98it/s][A[A

 72% 23/32 [00:04<00:01,  6.24it/s][A[A

 75% 24/32 [00:04<00:01,  6.46it/s][A[A

 78% 25/32 [00:04<00:01,  5.99it/s][A[A

 81% 26/32 [00:05<00:01,  5.52it/s][A[A

 84% 27/32 [00:05<00:00,  5.63it/s][A[A

 88% 28/32 [00:05<00:00,  5.65it/s][A[A

 91% 29/32 [00:05<00:00,  6.21it/s][A[A

 94% 30/32 [00:05<00:00,  6.38it/s][A[A

 97% 31/32 [00:05<00:00,  6.46it/s][A[A

100% 32/32 [00:06<00:00,  6.61it/s][A[A100% 32/32 [00:06<00:00,  5.28it/s]
Meta loss on this task batch = 5.0898e-01, PNorm = 43.6683, GNorm = 0.1922

 11% 2/19 [00:14<02:08,  7.55s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.82it/s][A[A

  6% 2/32 [00:00<00:05,  5.63it/s][A[A

  9% 3/32 [00:00<00:05,  5.80it/s][A[A

 12% 4/32 [00:01<00:12,  2.17it/s][A[A

 16% 5/32 [00:01<00:10,  2.68it/s][A[A

 19% 6/32 [00:02<00:08,  3.15it/s][A[A

 22% 7/32 [00:02<00:07,  3.57it/s][A[A

 25% 8/32 [00:02<00:05,  4.16it/s][A[A

 28% 9/32 [00:02<00:04,  4.69it/s][A[A

 31% 10/32 [00:02<00:04,  4.63it/s][A[A

 34% 11/32 [00:02<00:04,  4.99it/s][A[A

 38% 12/32 [00:03<00:03,  5.16it/s][A[A

 41% 13/32 [00:03<00:03,  5.43it/s][A[A

 44% 14/32 [00:03<00:03,  5.79it/s][A[A

 47% 15/32 [00:03<00:03,  5.54it/s][A[A

 50% 16/32 [00:03<00:02,  6.02it/s][A[A

 53% 17/32 [00:03<00:02,  5.58it/s][A[A

 56% 18/32 [00:04<00:02,  5.96it/s][A[A

 59% 19/32 [00:04<00:02,  6.08it/s][A[A

 62% 20/32 [00:04<00:01,  6.42it/s][A[A

 66% 21/32 [00:04<00:01,  6.81it/s][A[A

 69% 22/32 [00:04<00:01,  6.96it/s][A[A

 72% 23/32 [00:05<00:03,  2.27it/s][A[A

 75% 24/32 [00:05<00:02,  2.78it/s][A[A

 78% 25/32 [00:06<00:02,  3.42it/s][A[A

 81% 26/32 [00:06<00:01,  4.14it/s][A[A

 84% 27/32 [00:06<00:01,  4.51it/s][A[A

 88% 28/32 [00:06<00:00,  4.90it/s][A[A

 91% 29/32 [00:06<00:00,  5.35it/s][A[A

 94% 30/32 [00:06<00:00,  4.93it/s][A[A

 97% 31/32 [00:07<00:00,  5.71it/s][A[A

100% 32/32 [00:07<00:00,  5.60it/s][A[A100% 32/32 [00:07<00:00,  4.44it/s]
Meta loss on this task batch = 4.9203e-01, PNorm = 43.6860, GNorm = 0.0743

 16% 3/19 [00:22<02:02,  7.64s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.20it/s][A[A

  6% 2/32 [00:00<00:05,  5.07it/s][A[A

  9% 3/32 [00:00<00:05,  5.25it/s][A[A

 12% 4/32 [00:00<00:04,  5.90it/s][A[A

 16% 5/32 [00:00<00:04,  5.57it/s][A[A

 19% 6/32 [00:02<00:12,  2.05it/s][A[A

 22% 7/32 [00:02<00:09,  2.63it/s][A[A

 25% 8/32 [00:02<00:07,  3.33it/s][A[A

 28% 9/32 [00:02<00:05,  3.97it/s][A[A

 34% 11/32 [00:02<00:04,  4.73it/s][A[A

 38% 12/32 [00:02<00:03,  5.34it/s][A[A

 41% 13/32 [00:03<00:03,  5.72it/s][A[A

 44% 14/32 [00:03<00:03,  5.79it/s][A[A

 47% 15/32 [00:03<00:02,  6.18it/s][A[A

 50% 16/32 [00:03<00:02,  6.12it/s][A[A

 53% 17/32 [00:03<00:02,  6.86it/s][A[A

 56% 18/32 [00:03<00:02,  6.65it/s][A[A

 59% 19/32 [00:03<00:01,  6.90it/s][A[A

 62% 20/32 [00:04<00:01,  6.55it/s][A[A

 66% 21/32 [00:04<00:01,  5.98it/s][A[A

 69% 22/32 [00:04<00:01,  5.95it/s][A[A

 72% 23/32 [00:04<00:01,  5.58it/s][A[A

 75% 24/32 [00:04<00:01,  5.24it/s][A[A

 78% 25/32 [00:04<00:01,  5.69it/s][A[A

 81% 26/32 [00:05<00:00,  6.24it/s][A[A

 84% 27/32 [00:05<00:00,  6.15it/s][A[A

 88% 28/32 [00:06<00:01,  2.13it/s][A[A

 91% 29/32 [00:06<00:01,  2.72it/s][A[A

 94% 30/32 [00:06<00:00,  3.23it/s][A[A

 97% 31/32 [00:06<00:00,  3.87it/s][A[A

100% 32/32 [00:07<00:00,  4.19it/s][A[A100% 32/32 [00:07<00:00,  4.51it/s]
Meta loss on this task batch = 5.5322e-01, PNorm = 43.7054, GNorm = 0.1310

 21% 4/19 [00:30<01:55,  7.68s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.70it/s][A[A

  6% 2/32 [00:00<00:04,  6.20it/s][A[A

 12% 4/32 [00:00<00:04,  6.47it/s][A[A

 16% 5/32 [00:00<00:04,  5.91it/s][A[A

 19% 6/32 [00:00<00:03,  6.56it/s][A[A

 22% 7/32 [00:01<00:03,  6.42it/s][A[A

 25% 8/32 [00:01<00:03,  6.50it/s][A[A

 28% 9/32 [00:01<00:03,  5.94it/s][A[A

 31% 10/32 [00:01<00:03,  6.00it/s][A[A

 34% 11/32 [00:01<00:03,  6.03it/s][A[A

 38% 12/32 [00:01<00:03,  5.59it/s][A[A

 41% 13/32 [00:02<00:03,  5.62it/s][A[A

 44% 14/32 [00:03<00:08,  2.13it/s][A[A

 47% 15/32 [00:03<00:06,  2.70it/s][A[A

 50% 16/32 [00:03<00:04,  3.42it/s][A[A

 53% 17/32 [00:03<00:04,  3.70it/s][A[A

 56% 18/32 [00:03<00:03,  4.02it/s][A[A

 59% 19/32 [00:04<00:02,  4.34it/s][A[A

 62% 20/32 [00:04<00:02,  4.86it/s][A[A

 66% 21/32 [00:04<00:02,  5.38it/s][A[A

 69% 22/32 [00:04<00:01,  6.02it/s][A[A

 72% 23/32 [00:04<00:01,  6.43it/s][A[A

 75% 24/32 [00:04<00:01,  5.98it/s][A[A

 78% 25/32 [00:05<00:01,  5.49it/s][A[A

 81% 26/32 [00:05<00:01,  5.93it/s][A[A

 84% 27/32 [00:05<00:00,  5.88it/s][A[A

 88% 28/32 [00:05<00:00,  6.11it/s][A[A

 91% 29/32 [00:05<00:00,  6.32it/s][A[A

 94% 30/32 [00:05<00:00,  6.34it/s][A[A

 97% 31/32 [00:06<00:00,  6.01it/s][A[A

100% 32/32 [00:06<00:00,  5.56it/s][A[A100% 32/32 [00:06<00:00,  5.13it/s]
Meta loss on this task batch = 4.8501e-01, PNorm = 43.7283, GNorm = 0.0692

 26% 5/19 [00:37<01:44,  7.45s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:35,  1.14s/it][A[A

  6% 2/32 [00:01<00:25,  1.18it/s][A[A

  9% 3/32 [00:01<00:18,  1.56it/s][A[A

 12% 4/32 [00:01<00:13,  2.05it/s][A[A

 16% 5/32 [00:01<00:10,  2.59it/s][A[A

 19% 6/32 [00:01<00:08,  2.93it/s][A[A

 22% 7/32 [00:02<00:07,  3.41it/s][A[A

 25% 8/32 [00:02<00:05,  4.08it/s][A[A

 28% 9/32 [00:02<00:05,  4.19it/s][A[A

 31% 10/32 [00:02<00:04,  4.52it/s][A[A

 34% 11/32 [00:02<00:04,  4.76it/s][A[A

 38% 12/32 [00:03<00:04,  4.99it/s][A[A

 41% 13/32 [00:03<00:03,  5.50it/s][A[A

 44% 14/32 [00:03<00:03,  5.77it/s][A[A

 47% 15/32 [00:03<00:03,  5.35it/s][A[A

 50% 16/32 [00:04<00:08,  1.98it/s][A[A

 53% 17/32 [00:04<00:05,  2.53it/s][A[A

 56% 18/32 [00:05<00:04,  3.09it/s][A[A

 59% 19/32 [00:05<00:03,  3.61it/s][A[A

 62% 20/32 [00:05<00:03,  3.99it/s][A[A

 66% 21/32 [00:05<00:02,  4.59it/s][A[A

 69% 22/32 [00:05<00:02,  4.63it/s][A[A

 72% 23/32 [00:05<00:01,  4.90it/s][A[A

 75% 24/32 [00:06<00:01,  5.04it/s][A[A

 78% 25/32 [00:06<00:01,  4.80it/s][A[A

 81% 26/32 [00:06<00:01,  5.33it/s][A[A

 84% 27/32 [00:06<00:01,  4.99it/s][A[A

 88% 28/32 [00:06<00:00,  5.33it/s][A[A

 91% 29/32 [00:07<00:00,  5.69it/s][A[A

 94% 30/32 [00:07<00:00,  6.01it/s][A[A

100% 32/32 [00:07<00:00,  6.55it/s][A[A100% 32/32 [00:07<00:00,  4.28it/s]
Meta loss on this task batch = 4.8286e-01, PNorm = 43.7548, GNorm = 0.1799

 32% 6/19 [00:45<01:39,  7.66s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:36,  1.18s/it][A[A

  6% 2/32 [00:01<00:26,  1.15it/s][A[A

  9% 3/32 [00:01<00:19,  1.50it/s][A[A

 12% 4/32 [00:01<00:14,  1.95it/s][A[A

 16% 5/32 [00:01<00:10,  2.52it/s][A[A

 19% 6/32 [00:01<00:08,  3.13it/s][A[A

 22% 7/32 [00:02<00:06,  3.62it/s][A[A

 25% 8/32 [00:02<00:05,  4.25it/s][A[A

 28% 9/32 [00:02<00:05,  4.57it/s][A[A

 31% 10/32 [00:02<00:04,  4.70it/s][A[A

 34% 11/32 [00:02<00:04,  5.04it/s][A[A

 38% 12/32 [00:02<00:04,  4.97it/s][A[A

 41% 13/32 [00:03<00:03,  5.36it/s][A[A

 44% 14/32 [00:03<00:03,  5.59it/s][A[A

 47% 15/32 [00:03<00:03,  5.02it/s][A[A

 50% 16/32 [00:03<00:03,  4.71it/s][A[A

 53% 17/32 [00:03<00:02,  5.41it/s][A[A

 56% 18/32 [00:04<00:02,  5.23it/s][A[A

 59% 19/32 [00:05<00:06,  2.07it/s][A[A

 62% 20/32 [00:05<00:04,  2.44it/s][A[A

 66% 21/32 [00:05<00:03,  2.88it/s][A[A

 69% 22/32 [00:05<00:03,  3.28it/s][A[A

 72% 23/32 [00:06<00:02,  3.56it/s][A[A

 75% 24/32 [00:06<00:02,  3.79it/s][A[A

 78% 25/32 [00:06<00:01,  4.45it/s][A[A

 81% 26/32 [00:06<00:01,  5.21it/s][A[A

 84% 27/32 [00:06<00:00,  5.36it/s][A[A

 88% 28/32 [00:07<00:00,  4.96it/s][A[A

 91% 29/32 [00:07<00:00,  5.80it/s][A[A

 94% 30/32 [00:07<00:00,  5.51it/s][A[A

 97% 31/32 [00:07<00:00,  5.74it/s][A[A

100% 32/32 [00:07<00:00,  5.48it/s][A[A100% 32/32 [00:07<00:00,  4.15it/s]
Meta loss on this task batch = 4.6724e-01, PNorm = 43.7854, GNorm = 0.1198

 37% 7/19 [00:53<01:34,  7.89s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.28it/s][A[A

  6% 2/32 [00:01<00:15,  1.92it/s][A[A

  9% 3/32 [00:01<00:12,  2.32it/s][A[A

 12% 4/32 [00:01<00:10,  2.70it/s][A[A

 16% 5/32 [00:02<00:08,  3.12it/s][A[A

 19% 6/32 [00:02<00:07,  3.44it/s][A[A

 22% 7/32 [00:02<00:05,  4.23it/s][A[A

 25% 8/32 [00:02<00:05,  4.25it/s][A[A

 28% 9/32 [00:02<00:05,  4.40it/s][A[A

 31% 10/32 [00:03<00:04,  4.87it/s][A[A

 34% 11/32 [00:03<00:04,  5.17it/s][A[A

 38% 12/32 [00:03<00:03,  5.15it/s][A[A

 41% 13/32 [00:03<00:03,  5.37it/s][A[A

 44% 14/32 [00:03<00:02,  6.19it/s][A[A

 47% 15/32 [00:03<00:02,  6.56it/s][A[A

 50% 16/32 [00:03<00:02,  5.88it/s][A[A

 53% 17/32 [00:04<00:02,  5.52it/s][A[A

 56% 18/32 [00:05<00:06,  2.09it/s][A[A

 59% 19/32 [00:05<00:05,  2.51it/s][A[A

 62% 20/32 [00:05<00:03,  3.11it/s][A[A

 66% 21/32 [00:05<00:02,  3.70it/s][A[A

 69% 22/32 [00:06<00:02,  4.14it/s][A[A

 72% 23/32 [00:06<00:01,  4.98it/s][A[A

 75% 24/32 [00:06<00:01,  4.86it/s][A[A

 78% 25/32 [00:06<00:01,  4.71it/s][A[A

 81% 26/32 [00:06<00:01,  5.30it/s][A[A

 84% 27/32 [00:06<00:01,  4.86it/s][A[A

 88% 28/32 [00:07<00:00,  4.77it/s][A[A

 91% 29/32 [00:07<00:00,  4.67it/s][A[A

 94% 30/32 [00:07<00:00,  4.69it/s][A[A

 97% 31/32 [00:07<00:00,  4.69it/s][A[A

100% 32/32 [00:08<00:00,  4.56it/s][A[A100% 32/32 [00:08<00:00,  3.97it/s]
Meta loss on this task batch = 3.6858e-01, PNorm = 43.8215, GNorm = 0.1544

 42% 8/19 [01:02<01:29,  8.15s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:38,  1.25s/it][A[A

  6% 2/32 [00:01<00:28,  1.05it/s][A[A

  9% 3/32 [00:01<00:21,  1.35it/s][A[A

 12% 4/32 [00:01<00:16,  1.69it/s][A[A

 16% 5/32 [00:02<00:12,  2.08it/s][A[A

 19% 6/32 [00:02<00:10,  2.44it/s][A[A

 22% 7/32 [00:02<00:08,  2.82it/s][A[A

 25% 8/32 [00:02<00:07,  3.18it/s][A[A

 28% 9/32 [00:03<00:06,  3.45it/s][A[A

 31% 10/32 [00:03<00:05,  3.72it/s][A[A

 34% 11/32 [00:04<00:11,  1.81it/s][A[A

 38% 12/32 [00:04<00:09,  2.16it/s][A[A

 41% 13/32 [00:05<00:07,  2.51it/s][A[A

 44% 14/32 [00:05<00:06,  2.88it/s][A[A

 47% 15/32 [00:05<00:05,  3.21it/s][A[A

 50% 16/32 [00:05<00:04,  3.42it/s][A[A

 53% 17/32 [00:06<00:04,  3.63it/s][A[A

 56% 18/32 [00:06<00:03,  3.84it/s][A[A

 59% 19/32 [00:06<00:03,  3.91it/s][A[A

 62% 20/32 [00:06<00:02,  4.10it/s][A[A

 66% 21/32 [00:07<00:06,  1.82it/s][A[A

 69% 22/32 [00:08<00:04,  2.23it/s][A[A

 72% 23/32 [00:08<00:03,  2.63it/s][A[A

 75% 24/32 [00:08<00:02,  2.99it/s][A[A

 78% 25/32 [00:08<00:02,  3.35it/s][A[A

 81% 26/32 [00:09<00:01,  3.56it/s][A[A

 84% 27/32 [00:09<00:01,  3.67it/s][A[A

 88% 28/32 [00:09<00:01,  3.91it/s][A[A

 91% 29/32 [00:09<00:00,  4.00it/s][A[A

 94% 30/32 [00:10<00:00,  4.12it/s][A[A

 97% 31/32 [00:10<00:00,  4.19it/s][A[A

100% 32/32 [00:11<00:00,  1.88it/s][A[A100% 32/32 [00:11<00:00,  2.79it/s]
Meta loss on this task batch = 2.2382e-01, PNorm = 43.8633, GNorm = 0.2022

 47% 9/19 [01:14<01:33,  9.39s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.13it/s][A[A

  6% 2/32 [00:00<00:07,  4.27it/s][A[A

  9% 3/32 [00:00<00:06,  4.28it/s][A[A

 12% 4/32 [00:00<00:06,  4.32it/s][A[A

 16% 5/32 [00:01<00:06,  4.22it/s][A[A

 19% 6/32 [00:01<00:05,  4.38it/s][A[A

 22% 7/32 [00:01<00:05,  4.36it/s][A[A

 25% 8/32 [00:01<00:05,  4.32it/s][A[A

 28% 9/32 [00:02<00:05,  4.32it/s][A[A

 31% 10/32 [00:02<00:05,  4.25it/s][A[A

 34% 11/32 [00:02<00:04,  4.33it/s][A[A

 38% 12/32 [00:03<00:10,  1.89it/s][A[A

 41% 13/32 [00:03<00:08,  2.27it/s][A[A

 44% 14/32 [00:04<00:06,  2.62it/s][A[A

 47% 15/32 [00:04<00:05,  2.96it/s][A[A

 50% 16/32 [00:04<00:04,  3.29it/s][A[A

 53% 17/32 [00:04<00:04,  3.54it/s][A[A

 56% 18/32 [00:05<00:03,  3.69it/s][A[A

 59% 19/32 [00:05<00:03,  3.88it/s][A[A

 62% 20/32 [00:05<00:03,  3.95it/s][A[A

 66% 21/32 [00:05<00:02,  4.11it/s][A[A

 69% 22/32 [00:06<00:02,  4.17it/s][A[A

 72% 23/32 [00:06<00:02,  4.29it/s][A[A

 75% 24/32 [00:06<00:01,  4.42it/s][A[A

 78% 25/32 [00:07<00:03,  1.90it/s][A[A

 81% 26/32 [00:07<00:02,  2.42it/s][A[A

 84% 27/32 [00:08<00:01,  3.07it/s][A[A

 88% 28/32 [00:08<00:01,  3.37it/s][A[A

 91% 29/32 [00:08<00:00,  3.59it/s][A[A

 94% 30/32 [00:08<00:00,  3.67it/s][A[A

 97% 31/32 [00:08<00:00,  4.04it/s][A[A

100% 32/32 [00:09<00:00,  4.61it/s][A[A100% 32/32 [00:09<00:00,  3.52it/s]
Meta loss on this task batch = 2.7207e-01, PNorm = 43.9088, GNorm = 0.1285

 53% 10/19 [01:24<01:25,  9.54s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  3.95it/s][A[A

  6% 2/32 [00:00<00:07,  4.22it/s][A[A

  9% 3/32 [00:00<00:06,  4.63it/s][A[A

 12% 4/32 [00:00<00:05,  5.24it/s][A[A

 16% 5/32 [00:00<00:05,  5.18it/s][A[A

 19% 6/32 [00:01<00:04,  5.83it/s][A[A

 22% 7/32 [00:01<00:04,  5.16it/s][A[A

 25% 8/32 [00:02<00:11,  2.02it/s][A[A

 28% 9/32 [00:02<00:09,  2.50it/s][A[A

 31% 10/32 [00:02<00:07,  2.86it/s][A[A

 34% 11/32 [00:03<00:06,  3.15it/s][A[A

 38% 12/32 [00:03<00:05,  3.51it/s][A[A

 41% 13/32 [00:03<00:05,  3.73it/s][A[A

 44% 14/32 [00:03<00:04,  4.28it/s][A[A

 47% 15/32 [00:03<00:03,  4.90it/s][A[A

 50% 16/32 [00:04<00:03,  4.85it/s][A[A

 53% 17/32 [00:04<00:03,  4.98it/s][A[A

 56% 18/32 [00:04<00:02,  4.75it/s][A[A

 59% 19/32 [00:04<00:02,  5.04it/s][A[A

 62% 20/32 [00:04<00:02,  5.23it/s][A[A

 66% 21/32 [00:05<00:01,  5.71it/s][A[A

 69% 22/32 [00:05<00:01,  5.18it/s][A[A

 72% 23/32 [00:06<00:04,  2.00it/s][A[A

 75% 24/32 [00:06<00:03,  2.38it/s][A[A

 78% 25/32 [00:06<00:02,  2.91it/s][A[A

 81% 26/32 [00:07<00:01,  3.17it/s][A[A

 84% 27/32 [00:07<00:01,  3.40it/s][A[A

 88% 28/32 [00:07<00:01,  3.82it/s][A[A

 91% 29/32 [00:07<00:00,  3.98it/s][A[A

 94% 30/32 [00:08<00:00,  4.06it/s][A[A

 97% 31/32 [00:08<00:00,  4.40it/s][A[A

100% 32/32 [00:08<00:00,  4.55it/s][A[A100% 32/32 [00:08<00:00,  3.82it/s]
Meta loss on this task batch = 6.7491e-01, PNorm = 43.9398, GNorm = 0.4044

 58% 11/19 [01:33<01:15,  9.42s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.41it/s][A[A

  6% 2/32 [00:00<00:05,  5.09it/s][A[A

  9% 3/32 [00:00<00:05,  5.02it/s][A[A

 12% 4/32 [00:00<00:05,  5.23it/s][A[A

 16% 5/32 [00:01<00:05,  4.91it/s][A[A

 19% 6/32 [00:02<00:13,  1.98it/s][A[A

 22% 7/32 [00:02<00:10,  2.35it/s][A[A

 25% 8/32 [00:02<00:08,  2.69it/s][A[A

 28% 9/32 [00:02<00:07,  3.21it/s][A[A

 31% 10/32 [00:03<00:06,  3.66it/s][A[A

 34% 11/32 [00:03<00:05,  3.82it/s][A[A

 38% 12/32 [00:03<00:04,  4.23it/s][A[A

 41% 13/32 [00:03<00:04,  4.35it/s][A[A

 44% 14/32 [00:03<00:03,  4.74it/s][A[A

 47% 15/32 [00:04<00:03,  4.61it/s][A[A

 50% 16/32 [00:04<00:03,  4.47it/s][A[A

 53% 17/32 [00:04<00:03,  4.79it/s][A[A

 56% 18/32 [00:05<00:07,  1.94it/s][A[A

 59% 19/32 [00:05<00:05,  2.33it/s][A[A

 62% 20/32 [00:06<00:04,  2.83it/s][A[A

 66% 21/32 [00:06<00:03,  3.20it/s][A[A

 69% 22/32 [00:06<00:02,  3.40it/s][A[A

 72% 23/32 [00:06<00:02,  3.84it/s][A[A

 75% 24/32 [00:07<00:01,  4.07it/s][A[A

 78% 25/32 [00:07<00:01,  4.26it/s][A[A

 81% 26/32 [00:07<00:01,  4.32it/s][A[A

 84% 27/32 [00:07<00:01,  4.46it/s][A[A

 88% 28/32 [00:07<00:00,  4.79it/s][A[A

 91% 29/32 [00:08<00:01,  2.05it/s][A[A

 94% 30/32 [00:09<00:00,  2.44it/s][A[A

 97% 31/32 [00:09<00:00,  2.93it/s][A[A

100% 32/32 [00:09<00:00,  3.21it/s][A[A100% 32/32 [00:09<00:00,  3.33it/s]
Meta loss on this task batch = 5.3879e-01, PNorm = 43.9689, GNorm = 0.2193

 63% 12/19 [01:44<01:07,  9.70s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:07,  4.05it/s][A[A

  6% 2/32 [00:00<00:07,  4.04it/s][A[A

  9% 3/32 [00:00<00:07,  4.06it/s][A[A

 12% 4/32 [00:00<00:06,  4.36it/s][A[A

 16% 5/32 [00:01<00:05,  4.64it/s][A[A

 19% 6/32 [00:01<00:05,  4.63it/s][A[A

 22% 7/32 [00:01<00:05,  4.89it/s][A[A

 25% 8/32 [00:01<00:04,  5.53it/s][A[A

 28% 9/32 [00:01<00:03,  5.78it/s][A[A

 31% 10/32 [00:02<00:04,  5.08it/s][A[A

 34% 11/32 [00:03<00:10,  2.10it/s][A[A

 38% 12/32 [00:03<00:08,  2.45it/s][A[A

 41% 13/32 [00:03<00:06,  2.80it/s][A[A

 44% 14/32 [00:03<00:05,  3.04it/s][A[A

 47% 15/32 [00:04<00:04,  3.42it/s][A[A

 50% 16/32 [00:04<00:04,  3.92it/s][A[A

 53% 17/32 [00:04<00:03,  4.41it/s][A[A

 56% 18/32 [00:04<00:03,  4.64it/s][A[A

 59% 19/32 [00:04<00:02,  5.03it/s][A[A

 62% 20/32 [00:05<00:02,  4.60it/s][A[A

 66% 21/32 [00:05<00:02,  4.49it/s][A[A

 69% 22/32 [00:06<00:05,  1.96it/s][A[A

 72% 23/32 [00:06<00:03,  2.37it/s][A[A

 75% 24/32 [00:06<00:02,  2.94it/s][A[A

 78% 25/32 [00:07<00:02,  3.42it/s][A[A

 81% 26/32 [00:07<00:01,  3.61it/s][A[A

 84% 27/32 [00:07<00:01,  4.08it/s][A[A

 88% 28/32 [00:07<00:00,  4.06it/s][A[A

 91% 29/32 [00:07<00:00,  4.32it/s][A[A

 94% 30/32 [00:08<00:00,  4.87it/s][A[A

 97% 31/32 [00:08<00:00,  4.82it/s][A[A

100% 32/32 [00:08<00:00,  5.44it/s][A[A100% 32/32 [00:08<00:00,  3.82it/s]
Meta loss on this task batch = 6.1369e-01, PNorm = 43.9963, GNorm = 0.2362

 68% 13/19 [01:53<00:57,  9.52s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:04,  7.38it/s][A[A

  6% 2/32 [00:00<00:04,  7.25it/s][A[A

  9% 3/32 [00:00<00:04,  5.85it/s][A[A

 12% 4/32 [00:01<00:13,  2.04it/s][A[A

 16% 5/32 [00:01<00:11,  2.45it/s][A[A

 19% 6/32 [00:02<00:08,  3.03it/s][A[A

 22% 7/32 [00:02<00:07,  3.34it/s][A[A

 25% 8/32 [00:02<00:06,  3.89it/s][A[A

 28% 9/32 [00:02<00:05,  4.31it/s][A[A

 31% 10/32 [00:02<00:05,  4.24it/s][A[A

 34% 11/32 [00:03<00:04,  4.67it/s][A[A

 38% 12/32 [00:03<00:04,  4.45it/s][A[A

 41% 13/32 [00:03<00:03,  4.75it/s][A[A

 44% 14/32 [00:03<00:03,  4.75it/s][A[A

 47% 15/32 [00:03<00:03,  4.80it/s][A[A

 50% 16/32 [00:04<00:03,  5.16it/s][A[A

 53% 17/32 [00:05<00:07,  2.04it/s][A[A

 56% 18/32 [00:05<00:05,  2.46it/s][A[A

 59% 19/32 [00:05<00:04,  2.90it/s][A[A

 62% 20/32 [00:05<00:03,  3.20it/s][A[A

 66% 21/32 [00:06<00:02,  3.68it/s][A[A

 69% 22/32 [00:06<00:02,  3.94it/s][A[A

 72% 23/32 [00:06<00:02,  4.09it/s][A[A

 75% 24/32 [00:06<00:01,  4.39it/s][A[A

 78% 25/32 [00:06<00:01,  4.50it/s][A[A

 81% 26/32 [00:07<00:01,  4.52it/s][A[A

 84% 27/32 [00:07<00:01,  4.63it/s][A[A

 88% 28/32 [00:07<00:00,  4.78it/s][A[A

 91% 29/32 [00:07<00:00,  4.66it/s][A[A

 94% 30/32 [00:08<00:01,  1.97it/s][A[A

 97% 31/32 [00:09<00:00,  2.39it/s][A[A

100% 32/32 [00:09<00:00,  2.85it/s][A[A100% 32/32 [00:09<00:00,  3.41it/s]
Meta loss on this task batch = 5.3656e-01, PNorm = 44.0233, GNorm = 0.1270

 74% 14/19 [02:03<00:48,  9.69s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.92it/s][A[A

  6% 2/32 [00:00<00:05,  5.68it/s][A[A

  9% 3/32 [00:00<00:05,  5.28it/s][A[A

 12% 4/32 [00:00<00:05,  5.50it/s][A[A

 16% 5/32 [00:00<00:04,  5.40it/s][A[A

 19% 6/32 [00:01<00:05,  5.13it/s][A[A

 22% 7/32 [00:01<00:05,  4.69it/s][A[A

 25% 8/32 [00:01<00:04,  4.98it/s][A[A

 28% 9/32 [00:01<00:04,  4.72it/s][A[A

 31% 10/32 [00:01<00:04,  5.34it/s][A[A

 34% 11/32 [00:02<00:03,  6.20it/s][A[A

 38% 12/32 [00:02<00:03,  5.99it/s][A[A

 41% 13/32 [00:03<00:09,  2.10it/s][A[A

 44% 14/32 [00:03<00:07,  2.49it/s][A[A

 47% 15/32 [00:03<00:05,  2.99it/s][A[A

 50% 16/32 [00:04<00:04,  3.38it/s][A[A

 53% 17/32 [00:04<00:03,  3.88it/s][A[A

 56% 18/32 [00:04<00:03,  4.23it/s][A[A

 59% 19/32 [00:04<00:02,  4.70it/s][A[A

 62% 20/32 [00:04<00:02,  4.69it/s][A[A

 66% 21/32 [00:04<00:02,  4.99it/s][A[A

 69% 22/32 [00:05<00:01,  5.09it/s][A[A

 72% 23/32 [00:05<00:01,  4.61it/s][A[A

 75% 24/32 [00:05<00:01,  4.66it/s][A[A

 78% 25/32 [00:05<00:01,  4.98it/s][A[A

 81% 26/32 [00:06<00:01,  4.45it/s][A[A

 84% 27/32 [00:06<00:01,  4.50it/s][A[A

 88% 28/32 [00:06<00:00,  5.17it/s][A[A

 91% 29/32 [00:07<00:01,  1.96it/s][A[A

 94% 30/32 [00:07<00:00,  2.35it/s][A[A

 97% 31/32 [00:07<00:00,  3.01it/s][A[A

100% 32/32 [00:08<00:00,  3.26it/s][A[A100% 32/32 [00:08<00:00,  3.89it/s]
Meta loss on this task batch = 4.7768e-01, PNorm = 44.0523, GNorm = 0.0984

 79% 15/19 [02:12<00:37,  9.47s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.17it/s][A[A

  6% 2/32 [00:00<00:05,  5.92it/s][A[A

  9% 3/32 [00:00<00:04,  6.49it/s][A[A

 12% 4/32 [00:00<00:04,  6.69it/s][A[A

 16% 5/32 [00:00<00:03,  6.91it/s][A[A

 19% 6/32 [00:00<00:04,  5.93it/s][A[A

 22% 7/32 [00:01<00:04,  6.17it/s][A[A

 25% 8/32 [00:01<00:03,  6.08it/s][A[A

 28% 9/32 [00:01<00:03,  6.56it/s][A[A

 31% 10/32 [00:02<00:10,  2.11it/s][A[A

 34% 11/32 [00:02<00:08,  2.45it/s][A[A

 38% 12/32 [00:03<00:06,  2.88it/s][A[A

 41% 13/32 [00:03<00:05,  3.36it/s][A[A

 44% 14/32 [00:03<00:04,  3.80it/s][A[A

 47% 15/32 [00:03<00:04,  4.04it/s][A[A

 50% 16/32 [00:03<00:03,  4.52it/s][A[A

 53% 17/32 [00:04<00:03,  4.39it/s][A[A

 56% 18/32 [00:04<00:02,  4.90it/s][A[A

 59% 19/32 [00:04<00:02,  4.91it/s][A[A

 62% 20/32 [00:04<00:02,  4.92it/s][A[A

 66% 21/32 [00:04<00:02,  4.97it/s][A[A

 69% 22/32 [00:04<00:02,  4.96it/s][A[A

 72% 23/32 [00:06<00:04,  1.95it/s][A[A

 75% 24/32 [00:06<00:03,  2.30it/s][A[A

 78% 25/32 [00:06<00:02,  2.97it/s][A[A

 81% 26/32 [00:06<00:01,  3.55it/s][A[A

 84% 27/32 [00:06<00:01,  3.68it/s][A[A

 88% 28/32 [00:07<00:01,  3.93it/s][A[A

 91% 29/32 [00:07<00:00,  4.15it/s][A[A

 94% 30/32 [00:07<00:00,  4.17it/s][A[A

 97% 31/32 [00:07<00:00,  4.16it/s][A[A

100% 32/32 [00:08<00:00,  4.43it/s][A[A100% 32/32 [00:08<00:00,  3.96it/s]
Meta loss on this task batch = 5.5647e-01, PNorm = 44.0790, GNorm = 0.1131

 84% 16/19 [02:21<00:27,  9.27s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:00<00:05,  5.21it/s][A[A

  6% 2/32 [00:00<00:06,  4.78it/s][A[A

  9% 3/32 [00:00<00:05,  5.33it/s][A[A

 12% 4/32 [00:01<00:13,  2.05it/s][A[A

 16% 5/32 [00:01<00:10,  2.56it/s][A[A

 19% 6/32 [00:02<00:08,  3.16it/s][A[A

 22% 7/32 [00:02<00:06,  3.83it/s][A[A

 25% 8/32 [00:02<00:06,  3.87it/s][A[A

 28% 9/32 [00:02<00:05,  4.09it/s][A[A

 31% 10/32 [00:02<00:04,  4.93it/s][A[A

 34% 11/32 [00:02<00:04,  4.94it/s][A[A

 38% 12/32 [00:03<00:03,  5.01it/s][A[A

 41% 13/32 [00:03<00:03,  5.53it/s][A[A

 44% 14/32 [00:03<00:03,  5.59it/s][A[A

 47% 15/32 [00:03<00:02,  5.86it/s][A[A

 50% 16/32 [00:03<00:02,  5.96it/s][A[A

 53% 17/32 [00:03<00:02,  5.78it/s][A[A

 56% 18/32 [00:04<00:02,  5.36it/s][A[A

 59% 19/32 [00:04<00:02,  4.92it/s][A[A

 62% 20/32 [00:04<00:02,  4.84it/s][A[A

 66% 21/32 [00:04<00:02,  4.92it/s][A[A

 69% 22/32 [00:06<00:05,  1.93it/s][A[A

 72% 23/32 [00:06<00:03,  2.43it/s][A[A

 75% 24/32 [00:06<00:02,  2.69it/s][A[A

 78% 25/32 [00:06<00:02,  3.05it/s][A[A

 81% 26/32 [00:07<00:01,  3.26it/s][A[A

 84% 27/32 [00:07<00:01,  3.75it/s][A[A

 88% 28/32 [00:07<00:00,  4.09it/s][A[A

 91% 29/32 [00:07<00:00,  4.39it/s][A[A

 94% 30/32 [00:07<00:00,  4.66it/s][A[A

 97% 31/32 [00:07<00:00,  4.88it/s][A[A

100% 32/32 [00:08<00:00,  4.98it/s][A[A100% 32/32 [00:08<00:00,  3.93it/s]
Meta loss on this task batch = 4.3744e-01, PNorm = 44.1089, GNorm = 0.1307

 89% 17/19 [02:29<00:18,  9.15s/it][A

  0% 0/32 [00:00<?, ?it/s][A[A

  3% 1/32 [00:01<00:36,  1.18s/it][A[A

  6% 2/32 [00:01<00:26,  1.13it/s][A[A

  9% 3/32 [00:01<00:19,  1.46it/s][A[A

 12% 4/32 [00:01<00:15,  1.85it/s][A[A

 16% 5/32 [00:02<00:12,  2.23it/s][A[A

 19% 6/32 [00:02<00:09,  2.68it/s][A[A

 22% 7/32 [00:02<00:08,  2.97it/s][A[A

 25% 8/32 [00:02<00:07,  3.22it/s][A[A

 28% 9/32 [00:02<00:06,  3.59it/s][A[A

 31% 10/32 [00:03<00:05,  3.82it/s][A[A

 34% 11/32 [00:03<00:04,  4.26it/s][A[A

 38% 12/32 [00:03<00:04,  4.58it/s][A[A

 41% 13/32 [00:03<00:03,  4.79it/s][A[A

 44% 14/32 [00:03<00:03,  5.24it/s][A[A

 47% 15/32 [00:04<00:03,  4.86it/s][A[A

 50% 16/32 [00:05<00:07,  2.01it/s][A[A

 53% 17/32 [00:05<00:05,  2.57it/s][A[A

 56% 18/32 [00:05<00:04,  3.04it/s][A[A

 59% 19/32 [00:05<00:03,  3.36it/s][A[A

 62% 20/32 [00:06<00:03,  3.60it/s][A[A

 66% 21/32 [00:06<00:02,  3.81it/s][A[A

 69% 22/32 [00:06<00:02,  4.46it/s][A[A

 72% 23/32 [00:06<00:02,  4.39it/s][A[A

 75% 24/32 [00:06<00:01,  4.13it/s][A[A

 78% 25/32 [00:07<00:01,  4.60it/s][A[A

 81% 26/32 [00:07<00:01,  4.71it/s][A[A

 84% 27/32 [00:07<00:01,  4.43it/s][A[A

 88% 28/32 [00:08<00:01,  2.03it/s][A[A

 91% 29/32 [00:08<00:01,  2.53it/s][A[A

 94% 30/32 [00:09<00:00,  3.03it/s][A[A

 97% 31/32 [00:09<00:00,  3.52it/s][A[A

100% 32/32 [00:09<00:00,  3.71it/s][A[A100% 32/32 [00:09<00:00,  3.40it/s]
Meta loss on this task batch = 5.2830e-01, PNorm = 44.1317, GNorm = 0.1445

 95% 18/19 [02:40<00:09,  9.45s/it][A

  0% 0/23 [00:00<?, ?it/s][A[A

  4% 1/23 [00:00<00:05,  4.03it/s][A[A

  9% 2/23 [00:00<00:04,  4.43it/s][A[A

 13% 3/23 [00:00<00:04,  4.54it/s][A[A

 17% 4/23 [00:00<00:03,  5.31it/s][A[A

 26% 6/23 [00:01<00:02,  5.71it/s][A[A

 30% 7/23 [00:01<00:02,  5.68it/s][A[A

 35% 8/23 [00:01<00:02,  5.15it/s][A[A

 39% 9/23 [00:01<00:02,  5.67it/s][A[A

 43% 10/23 [00:01<00:02,  4.96it/s][A[A

 48% 11/23 [00:02<00:02,  5.17it/s][A[A

 52% 12/23 [00:02<00:02,  4.91it/s][A[A

 57% 13/23 [00:02<00:01,  5.42it/s][A[A

 61% 14/23 [00:02<00:01,  5.96it/s][A[A

 65% 15/23 [00:03<00:04,  1.87it/s][A[A

 70% 16/23 [00:04<00:03,  2.19it/s][A[A

 74% 17/23 [00:04<00:02,  2.69it/s][A[A

 78% 18/23 [00:04<00:01,  3.25it/s][A[A

 83% 19/23 [00:04<00:01,  3.53it/s][A[A

 87% 20/23 [00:04<00:00,  4.23it/s][A[A

 91% 21/23 [00:05<00:00,  4.60it/s][A[A

 96% 22/23 [00:05<00:00,  5.25it/s][A[A

100% 23/23 [00:05<00:00,  5.61it/s][A[A100% 23/23 [00:05<00:00,  4.33it/s]
Meta loss on this task batch = 4.2762e-01, PNorm = 44.1542, GNorm = 0.1286

100% 19/19 [02:45<00:00,  8.36s/it][A100% 19/19 [02:45<00:00,  8.73s/it]
Took 165.95382237434387 seconds to complete one epoch of meta training

  0% 0/1 [00:00<?, ?it/s][A

  0% 0/20 [00:00<?, ?it/s][A[A


  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 31.47it/s]


  5% 1/20 [00:00<00:02,  8.16it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 21.25it/s]


 10% 2/20 [00:00<00:02,  6.21it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 18.62it/s][A[A[A100% 3/3 [00:00<00:00, 20.33it/s]


 15% 3/20 [00:00<00:03,  5.01it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A


100% 1/1 [00:01<00:00,  1.05s/it][A[A[A100% 1/1 [00:01<00:00,  1.05s/it]


 20% 4/20 [00:01<00:07,  2.02it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 14.50it/s][A[A[A100% 4/4 [00:00<00:00, 16.91it/s]


 25% 5/20 [00:02<00:07,  2.13it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.24it/s][A[A[A


100% 4/4 [00:00<00:00, 14.72it/s][A[A[A100% 4/4 [00:00<00:00, 15.89it/s]


 30% 6/20 [00:02<00:06,  2.23it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.89it/s][A[A[A100% 4/4 [00:00<00:00, 20.68it/s]


 35% 7/20 [00:03<00:05,  2.37it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 17.22it/s][A[A[A100% 4/4 [00:00<00:00, 19.63it/s]


 40% 8/20 [00:03<00:04,  2.45it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 19.33it/s][A[A[A100% 4/4 [00:00<00:00, 22.97it/s]


 45% 9/20 [00:03<00:04,  2.57it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:01<00:01,  1.81it/s][A[A[A


100% 4/4 [00:01<00:00,  2.48it/s][A[A[A100% 4/4 [00:01<00:00,  3.28it/s]


 50% 10/20 [00:05<00:06,  1.46it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 19.98it/s][A[A[A


100% 4/4 [00:00<00:00, 18.28it/s][A[A[A100% 4/4 [00:00<00:00, 17.27it/s]


 55% 11/20 [00:05<00:05,  1.66it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 75% 3/4 [00:00<00:00, 18.10it/s][A[A[A100% 4/4 [00:00<00:00, 21.27it/s]


 60% 12/20 [00:05<00:04,  1.88it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.16it/s][A[A[A100% 3/3 [00:00<00:00, 13.57it/s]


 65% 13/20 [00:06<00:03,  2.05it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 13.84it/s][A[A[A100% 3/3 [00:00<00:00, 13.49it/s]


 70% 14/20 [00:06<00:02,  2.19it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/4 [00:00<?, ?it/s][A[A[A


 50% 2/4 [00:00<00:00, 13.56it/s][A[A[A100% 4/4 [00:00<00:00, 17.66it/s]


 75% 15/20 [00:07<00:02,  2.27it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 13.31it/s][A[A[A100% 3/3 [00:00<00:00, 16.53it/s]


 80% 16/20 [00:07<00:01,  2.45it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 12.24it/s][A[A[A100% 3/3 [00:00<00:00, 15.05it/s]


 85% 17/20 [00:07<00:01,  2.48it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/2 [00:00<?, ?it/s][A[A[A100% 2/2 [00:00<00:00, 21.97it/s]


 90% 18/20 [00:08<00:00,  2.82it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/3 [00:00<?, ?it/s][A[A[A


 67% 2/3 [00:00<00:00, 14.95it/s][A[A[A100% 3/3 [00:00<00:00, 19.17it/s]


 95% 19/20 [00:09<00:00,  1.58it/s][A[A/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "



  0% 0/1 [00:00<?, ?it/s][A[A[A100% 1/1 [00:00<00:00, 13.13it/s]


100% 20/20 [00:09<00:00,  1.95it/s][A[A100% 20/20 [00:09<00:00,  2.09it/s]

100% 1/1 [00:09<00:00,  9.55s/it][A100% 1/1 [00:09<00:00,  9.55s/it]
Took 175.50374007225037 seconds to complete one epoch of meta training and validating
Meta validation score auc = 0.658839
Found better MAML checkpoint after meta validation, saving now
100% 30/30 [1:36:31<00:00, 189.58s/it]100% 30/30 [1:36:31<00:00, 193.05s/it]
Best validation auc = 0.658839 on epoch 29
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Beginning meta testing
  0% 0/26 [00:00<?, ?it/s]
  0% 0/1 [00:00<?, ?it/s][AMeta testing on task: CHEMBL1614170
/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  0% 0/30 [00:00<?, ?it/s][A[ANew best model for test task CHEMBL1614170 at epoch 1 with val loss 0.7145342230796814


  3% 1/30 [00:00<00:24,  1.17it/s][A[AVal loss: 0.71555495262146


  7% 2/30 [00:01<00:23,  1.19it/s][A[AVal loss: 0.7165097892284393


 10% 3/30 [00:03<00:30,  1.14s/it][A[AVal loss: 0.7173820734024048


 13% 4/30 [00:04<00:26,  1.04s/it][A[AVal loss: 0.7182314395904541


 17% 5/30 [00:05<00:24,  1.03it/s][A[AVal loss: 0.7189873158931732


 20% 6/30 [00:06<00:29,  1.23s/it][A[AVal loss: 0.7197145819664001


 23% 7/30 [00:07<00:25,  1.10s/it][A[AVal loss: 0.7203500270843506


 27% 8/30 [00:08<00:22,  1.02s/it][A[AVal loss: 0.7209215462207794


 30% 9/30 [00:09<00:20,  1.04it/s][A[AVal loss: 0.7213636636734009


 33% 10/30 [00:11<00:24,  1.22s/it][A[AVal loss: 0.7218669652938843


 37% 11/30 [00:12<00:20,  1.10s/it][A[AVal loss: 0.7222191095352173


 40% 12/30 [00:12<00:18,  1.02s/it][A[AVal loss: 0.7225145697593689


 43% 13/30 [00:14<00:21,  1.27s/it][A[AVal loss: 0.7228385210037231


 47% 14/30 [00:15<00:18,  1.14s/it][A[AVal loss: 0.7231128811836243


 50% 15/30 [00:17<00:20,  1.35s/it][A[AVal loss: 0.7232339978218079


 53% 16/30 [00:18<00:16,  1.19s/it][A[AVal loss: 0.7234120965003967


 57% 17/30 [00:19<00:14,  1.08s/it][A[AVal loss: 0.7236047983169556


 60% 18/30 [00:19<00:12,  1.01s/it][A[AVal loss: 0.7236983776092529


 63% 19/30 [00:21<00:13,  1.25s/it][A[AVal loss: 0.7238047420978546


 67% 20/30 [00:22<00:11,  1.13s/it][A[AVal loss: 0.7238060534000397


 70% 21/30 [00:23<00:09,  1.04s/it][A[AVal loss: 0.723856121301651


 73% 22/30 [00:25<00:10,  1.28s/it][A[AVal loss: 0.7239653468132019


 77% 23/30 [00:26<00:08,  1.15s/it][A[AVal loss: 0.723987340927124


 80% 24/30 [00:26<00:06,  1.06s/it][A[AVal loss: 0.7239986062049866


 83% 25/30 [00:27<00:04,  1.00it/s][A[AVal loss: 0.7240894138813019


 87% 26/30 [00:29<00:05,  1.25s/it][A[AVal loss: 0.7240901291370392


 90% 27/30 [00:30<00:03,  1.14s/it][A[AVal loss: 0.7240737974643707


 93% 28/30 [00:32<00:02,  1.35s/it][A[AVal loss: 0.7240810692310333


 97% 29/30 [00:33<00:01,  1.21s/it][A[AVal loss: 0.7240599989891052


100% 30/30 [00:34<00:00,  1.11s/it][A[A100% 30/30 [00:34<00:00,  1.14s/it]
Finished early stopping for task CHEMBL1614170, beginning testing


  0% 0/2 [00:00<?, ?it/s][A[A100% 2/2 [00:00<00:00, 23.33it/s]

100% 1/1 [00:34<00:00, 34.49s/it][A100% 1/1 [00:34<00:00, 34.49s/it]
  4% 1/26 [00:34<14:22, 34.49s/it]
  0% 0/1 [00:00<?, ?it/s][AMeta testing on task: CHEMBL1614202
/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  0% 0/30 [00:00<?, ?it/s][A[ANew best model for test task CHEMBL1614202 at epoch 1 with val loss 0.5663109421730042


  3% 1/30 [00:00<00:14,  1.99it/s][A[ANew best model for test task CHEMBL1614202 at epoch 2 with val loss 0.5433353185653687


  7% 2/30 [00:02<00:22,  1.23it/s][A[ANew best model for test task CHEMBL1614202 at epoch 3 with val loss 0.5208944082260132


 10% 3/30 [00:02<00:19,  1.37it/s][A[ANew best model for test task CHEMBL1614202 at epoch 4 with val loss 0.49818453192710876


 13% 4/30 [00:03<00:17,  1.50it/s][A[ANew best model for test task CHEMBL1614202 at epoch 5 with val loss 0.47690778970718384


 17% 5/30 [00:03<00:15,  1.63it/s][A[ANew best model for test task CHEMBL1614202 at epoch 6 with val loss 0.4572369456291199


 20% 6/30 [00:04<00:13,  1.72it/s][A[ANew best model for test task CHEMBL1614202 at epoch 7 with val loss 0.43991392850875854


 23% 7/30 [00:04<00:12,  1.77it/s][A[ANew best model for test task CHEMBL1614202 at epoch 8 with val loss 0.42694729566574097


 27% 8/30 [00:06<00:18,  1.17it/s][A[ANew best model for test task CHEMBL1614202 at epoch 9 with val loss 0.4162810444831848


 30% 9/30 [00:06<00:15,  1.32it/s][A[ANew best model for test task CHEMBL1614202 at epoch 10 with val loss 0.41081878542900085


 33% 10/30 [00:07<00:13,  1.46it/s][A[ANew best model for test task CHEMBL1614202 at epoch 11 with val loss 0.4012771248817444


 37% 11/30 [00:07<00:11,  1.59it/s][A[ANew best model for test task CHEMBL1614202 at epoch 12 with val loss 0.3928716480731964


 40% 12/30 [00:08<00:10,  1.68it/s][A[ANew best model for test task CHEMBL1614202 at epoch 13 with val loss 0.38628700375556946


 43% 13/30 [00:09<00:14,  1.15it/s][A[ANew best model for test task CHEMBL1614202 at epoch 14 with val loss 0.38029077649116516


 47% 14/30 [00:10<00:12,  1.29it/s][A[ANew best model for test task CHEMBL1614202 at epoch 15 with val loss 0.3749631941318512


 50% 15/30 [00:10<00:10,  1.43it/s][A[ANew best model for test task CHEMBL1614202 at epoch 16 with val loss 0.37032371759414673


 53% 16/30 [00:11<00:09,  1.55it/s][A[ANew best model for test task CHEMBL1614202 at epoch 17 with val loss 0.3670867681503296


 57% 17/30 [00:11<00:07,  1.66it/s][A[ANew best model for test task CHEMBL1614202 at epoch 18 with val loss 0.3639254868030548


 60% 18/30 [00:12<00:06,  1.73it/s][A[AVal loss: 0.3642880618572235


 63% 19/30 [00:13<00:09,  1.18it/s][A[AVal loss: 0.36423879861831665


 67% 20/30 [00:14<00:07,  1.35it/s][A[ANew best model for test task CHEMBL1614202 at epoch 21 with val loss 0.36160147190093994


 70% 21/30 [00:14<00:06,  1.47it/s][A[ANew best model for test task CHEMBL1614202 at epoch 22 with val loss 0.359086275100708


 73% 22/30 [00:15<00:05,  1.58it/s][A[ANew best model for test task CHEMBL1614202 at epoch 23 with val loss 0.35705941915512085


 77% 23/30 [00:15<00:04,  1.68it/s][A[ANew best model for test task CHEMBL1614202 at epoch 24 with val loss 0.35503071546554565


 80% 24/30 [00:16<00:03,  1.75it/s][A[ANew best model for test task CHEMBL1614202 at epoch 25 with val loss 0.3531935214996338


 83% 25/30 [00:17<00:04,  1.17it/s][A[ANew best model for test task CHEMBL1614202 at epoch 26 with val loss 0.351113885641098


 87% 26/30 [00:18<00:03,  1.32it/s][A[ANew best model for test task CHEMBL1614202 at epoch 27 with val loss 0.3495759665966034


 90% 27/30 [00:18<00:02,  1.44it/s][A[AVal loss: 0.35047081112861633


 93% 28/30 [00:19<00:01,  1.58it/s][A[ANew best model for test task CHEMBL1614202 at epoch 29 with val loss 0.3489447236061096


 97% 29/30 [00:20<00:00,  1.66it/s][A[ANew best model for test task CHEMBL1614202 at epoch 30 with val loss 0.34750062227249146


100% 30/30 [00:20<00:00,  1.74it/s][A[A100% 30/30 [00:20<00:00,  1.46it/s]
Finished early stopping for task CHEMBL1614202, beginning testing


  0% 0/1 [00:00<?, ?it/s][A[A100% 1/1 [00:00<00:00, 21.24it/s]

100% 1/1 [00:20<00:00, 20.68s/it][A100% 1/1 [00:20<00:00, 20.68s/it]
  8% 2/26 [00:55<12:08, 30.35s/it]
  0% 0/1 [00:00<?, ?it/s][AMeta testing on task: CHEMBL1614359
/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  0% 0/30 [00:00<?, ?it/s][A[ANew best model for test task CHEMBL1614359 at epoch 1 with val loss 0.7016342878341675


  3% 1/30 [00:01<00:45,  1.57s/it][A[AVal loss: 0.7031518220901489


  7% 2/30 [00:02<00:35,  1.27s/it][A[AVal loss: 0.7048969864845276


 10% 3/30 [00:02<00:28,  1.06s/it][A[AVal loss: 0.7069663405418396


 13% 4/30 [00:03<00:23,  1.11it/s][A[AVal loss: 0.708999514579773


 17% 5/30 [00:03<00:19,  1.26it/s][A[AVal loss: 0.7107281684875488


 20% 6/30 [00:05<00:24,  1.01s/it][A[AVal loss: 0.7125306725502014


 23% 7/30 [00:05<00:20,  1.14it/s][A[AVal loss: 0.7139726281166077


 27% 8/30 [00:06<00:17,  1.28it/s][A[AVal loss: 0.7153605818748474


 30% 9/30 [00:06<00:15,  1.40it/s][A[AVal loss: 0.7167404294013977


 33% 10/30 [00:07<00:13,  1.51it/s][A[AVal loss: 0.718253493309021


 37% 11/30 [00:08<00:11,  1.60it/s][A[AVal loss: 0.7199885845184326


 40% 12/30 [00:09<00:16,  1.11it/s][A[AVal loss: 0.7213542461395264


 43% 13/30 [00:10<00:13,  1.25it/s][A[AVal loss: 0.7225999236106873


 47% 14/30 [00:10<00:11,  1.37it/s][A[AVal loss: 0.7234237194061279


 50% 15/30 [00:11<00:10,  1.48it/s][A[AVal loss: 0.7248235940933228


 53% 16/30 [00:11<00:08,  1.57it/s][A[AVal loss: 0.7259472012519836


 57% 17/30 [00:13<00:11,  1.10it/s][A[AVal loss: 0.7265247106552124


 60% 18/30 [00:13<00:09,  1.23it/s][A[AVal loss: 0.7269850373268127


 63% 19/30 [00:14<00:08,  1.36it/s][A[AVal loss: 0.7278376817703247


 67% 20/30 [00:15<00:06,  1.47it/s][A[AVal loss: 0.728609025478363


 70% 21/30 [00:15<00:05,  1.56it/s][A[AVal loss: 0.7294079065322876


 73% 22/30 [00:17<00:07,  1.08it/s][A[AVal loss: 0.7297924757003784


 77% 23/30 [00:17<00:05,  1.23it/s][A[AVal loss: 0.7298975586891174


 80% 24/30 [00:18<00:04,  1.37it/s][A[AVal loss: 0.7299476265907288


 83% 25/30 [00:18<00:03,  1.47it/s][A[AVal loss: 0.7300834655761719


 87% 26/30 [00:19<00:02,  1.56it/s][A[AVal loss: 0.7302878499031067


 90% 27/30 [00:19<00:01,  1.61it/s][A[AVal loss: 0.7307485342025757


 93% 28/30 [00:21<00:01,  1.11it/s][A[AVal loss: 0.7308787107467651


 97% 29/30 [00:22<00:00,  1.25it/s][A[AVal loss: 0.7310552000999451


100% 30/30 [00:22<00:00,  1.38it/s][A[A100% 30/30 [00:22<00:00,  1.32it/s]
Finished early stopping for task CHEMBL1614359, beginning testing


  0% 0/1 [00:00<?, ?it/s][A[A100% 1/1 [00:00<00:00, 18.96it/s]

100% 1/1 [00:22<00:00, 22.82s/it][A100% 1/1 [00:22<00:00, 22.82s/it]
 12% 3/26 [01:17<10:46, 28.09s/it]
  0% 0/1 [00:00<?, ?it/s][AMeta testing on task: CHEMBL1738019
/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  0% 0/30 [00:00<?, ?it/s][A[ANew best model for test task CHEMBL1738019 at epoch 1 with val loss 0.6200709939002991


  3% 1/30 [00:00<00:07,  3.78it/s][A[ANew best model for test task CHEMBL1738019 at epoch 2 with val loss 0.6163210272789001


  7% 2/30 [00:00<00:07,  3.79it/s][A[ANew best model for test task CHEMBL1738019 at epoch 3 with val loss 0.6125842928886414


 10% 3/30 [00:00<00:07,  3.83it/s][A[ANew best model for test task CHEMBL1738019 at epoch 4 with val loss 0.6090579032897949


 13% 4/30 [00:01<00:06,  3.86it/s][A[ANew best model for test task CHEMBL1738019 at epoch 5 with val loss 0.6056496500968933


 17% 5/30 [00:01<00:06,  3.90it/s][A[ANew best model for test task CHEMBL1738019 at epoch 6 with val loss 0.6022241115570068


 20% 6/30 [00:01<00:06,  3.92it/s][A[ANew best model for test task CHEMBL1738019 at epoch 7 with val loss 0.5990396738052368


 23% 7/30 [00:01<00:05,  3.92it/s][A[ANew best model for test task CHEMBL1738019 at epoch 8 with val loss 0.5959381461143494


 27% 8/30 [00:02<00:05,  3.88it/s][A[ANew best model for test task CHEMBL1738019 at epoch 9 with val loss 0.5929844379425049


 30% 9/30 [00:02<00:05,  3.93it/s][A[ANew best model for test task CHEMBL1738019 at epoch 10 with val loss 0.5901058316230774


 33% 10/30 [00:03<00:11,  1.79it/s][A[ANew best model for test task CHEMBL1738019 at epoch 11 with val loss 0.5874520540237427


 37% 11/30 [00:03<00:08,  2.15it/s][A[ANew best model for test task CHEMBL1738019 at epoch 12 with val loss 0.5848791599273682


 40% 12/30 [00:04<00:07,  2.49it/s][A[ANew best model for test task CHEMBL1738019 at epoch 13 with val loss 0.582366406917572


 43% 13/30 [00:04<00:06,  2.80it/s][A[ANew best model for test task CHEMBL1738019 at epoch 14 with val loss 0.5801090002059937


 47% 14/30 [00:04<00:05,  3.07it/s][A[ANew best model for test task CHEMBL1738019 at epoch 15 with val loss 0.577906608581543


 50% 15/30 [00:04<00:04,  3.28it/s][A[ANew best model for test task CHEMBL1738019 at epoch 16 with val loss 0.5758116841316223


 53% 16/30 [00:05<00:04,  3.42it/s][A[ANew best model for test task CHEMBL1738019 at epoch 17 with val loss 0.5737939476966858


 57% 17/30 [00:05<00:03,  3.59it/s][A[ANew best model for test task CHEMBL1738019 at epoch 18 with val loss 0.5719733834266663


 60% 18/30 [00:05<00:03,  3.71it/s][A[ANew best model for test task CHEMBL1738019 at epoch 19 with val loss 0.5701171159744263


 63% 19/30 [00:05<00:02,  3.73it/s][A[ANew best model for test task CHEMBL1738019 at epoch 20 with val loss 0.5684323310852051


 67% 20/30 [00:06<00:02,  3.71it/s][A[ANew best model for test task CHEMBL1738019 at epoch 21 with val loss 0.5668413639068604


 70% 21/30 [00:06<00:02,  3.72it/s][A[ANew best model for test task CHEMBL1738019 at epoch 22 with val loss 0.5653785467147827


 73% 22/30 [00:06<00:02,  3.76it/s][A[ANew best model for test task CHEMBL1738019 at epoch 23 with val loss 0.5640654563903809


 77% 23/30 [00:06<00:01,  3.81it/s][A[ANew best model for test task CHEMBL1738019 at epoch 24 with val loss 0.5627814531326294


 80% 24/30 [00:07<00:01,  3.83it/s][A[ANew best model for test task CHEMBL1738019 at epoch 25 with val loss 0.5615367889404297


 83% 25/30 [00:08<00:02,  1.80it/s][A[ANew best model for test task CHEMBL1738019 at epoch 26 with val loss 0.5604711771011353


 87% 26/30 [00:08<00:01,  2.16it/s][A[ANew best model for test task CHEMBL1738019 at epoch 27 with val loss 0.5593186616897583


 90% 27/30 [00:08<00:01,  2.51it/s][A[ANew best model for test task CHEMBL1738019 at epoch 28 with val loss 0.5582805871963501


 93% 28/30 [00:09<00:00,  2.78it/s][A[ANew best model for test task CHEMBL1738019 at epoch 29 with val loss 0.5573751330375671


 97% 29/30 [00:09<00:00,  3.00it/s][A[ANew best model for test task CHEMBL1738019 at epoch 30 with val loss 0.5564230680465698


100% 30/30 [00:09<00:00,  3.19it/s][A[A100% 30/30 [00:09<00:00,  3.09it/s]
Finished early stopping for task CHEMBL1738019, beginning testing


  0% 0/1 [00:00<?, ?it/s][A[A100% 1/1 [00:00<00:00, 38.71it/s]

100% 1/1 [00:09<00:00,  9.85s/it][A100% 1/1 [00:09<00:00,  9.85s/it]
 15% 4/26 [01:27<08:17, 22.62s/it]
  0% 0/1 [00:00<?, ?it/s][AMeta testing on task: CHEMBL1738021
/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  0% 0/30 [00:00<?, ?it/s][A[ANew best model for test task CHEMBL1738021 at epoch 1 with val loss 0.5995704531669617


  3% 1/30 [00:00<00:07,  3.97it/s][A[ANew best model for test task CHEMBL1738021 at epoch 2 with val loss 0.5926330089569092


  7% 2/30 [00:00<00:06,  4.12it/s][A[ANew best model for test task CHEMBL1738021 at epoch 3 with val loss 0.5857630968093872


 10% 3/30 [00:00<00:06,  4.20it/s][A[ANew best model for test task CHEMBL1738021 at epoch 4 with val loss 0.5791565179824829


 13% 4/30 [00:00<00:06,  4.28it/s][A[ANew best model for test task CHEMBL1738021 at epoch 5 with val loss 0.5716887712478638


 17% 5/30 [00:01<00:05,  4.34it/s][A[ANew best model for test task CHEMBL1738021 at epoch 6 with val loss 0.565413773059845


 20% 6/30 [00:01<00:05,  4.36it/s][A[ANew best model for test task CHEMBL1738021 at epoch 7 with val loss 0.5587500929832458


 23% 7/30 [00:01<00:05,  4.37it/s][A[ANew best model for test task CHEMBL1738021 at epoch 8 with val loss 0.5525592565536499


 27% 8/30 [00:01<00:05,  4.38it/s][A[ANew best model for test task CHEMBL1738021 at epoch 9 with val loss 0.5463387966156006


 30% 9/30 [00:02<00:04,  4.37it/s][A[ANew best model for test task CHEMBL1738021 at epoch 10 with val loss 0.5405758619308472


 33% 10/30 [00:02<00:04,  4.42it/s][A[ANew best model for test task CHEMBL1738021 at epoch 11 with val loss 0.534762978553772


 37% 11/30 [00:02<00:04,  4.46it/s][A[ANew best model for test task CHEMBL1738021 at epoch 12 with val loss 0.5300288200378418


 40% 12/30 [00:02<00:04,  4.40it/s][A[ANew best model for test task CHEMBL1738021 at epoch 13 with val loss 0.5244740843772888


 43% 13/30 [00:02<00:03,  4.32it/s][A[ANew best model for test task CHEMBL1738021 at epoch 14 with val loss 0.5193350315093994


 47% 14/30 [00:04<00:08,  1.87it/s][A[ANew best model for test task CHEMBL1738021 at epoch 15 with val loss 0.5142642855644226


 50% 15/30 [00:04<00:06,  2.27it/s][A[ANew best model for test task CHEMBL1738021 at epoch 16 with val loss 0.5096048712730408


 53% 16/30 [00:04<00:05,  2.66it/s][A[ANew best model for test task CHEMBL1738021 at epoch 17 with val loss 0.5053321719169617


 57% 17/30 [00:04<00:04,  3.01it/s][A[ANew best model for test task CHEMBL1738021 at epoch 18 with val loss 0.501594066619873


 60% 18/30 [00:05<00:03,  3.33it/s][A[ANew best model for test task CHEMBL1738021 at epoch 19 with val loss 0.4981677234172821


 63% 19/30 [00:05<00:03,  3.56it/s][A[ANew best model for test task CHEMBL1738021 at epoch 20 with val loss 0.4948239028453827


 67% 20/30 [00:05<00:02,  3.79it/s][A[ANew best model for test task CHEMBL1738021 at epoch 21 with val loss 0.49194976687431335


 70% 21/30 [00:05<00:02,  3.99it/s][A[ANew best model for test task CHEMBL1738021 at epoch 22 with val loss 0.4895932972431183


 73% 22/30 [00:06<00:01,  4.12it/s][A[ANew best model for test task CHEMBL1738021 at epoch 23 with val loss 0.4865780174732208


 77% 23/30 [00:06<00:01,  4.12it/s][A[ANew best model for test task CHEMBL1738021 at epoch 24 with val loss 0.4837813973426819


 80% 24/30 [00:06<00:01,  4.15it/s][A[ANew best model for test task CHEMBL1738021 at epoch 25 with val loss 0.48099783062934875


 83% 25/30 [00:06<00:01,  4.19it/s][A[ANew best model for test task CHEMBL1738021 at epoch 26 with val loss 0.4785209000110626


 87% 26/30 [00:06<00:00,  4.22it/s][A[ANew best model for test task CHEMBL1738021 at epoch 27 with val loss 0.476357102394104


 90% 27/30 [00:07<00:00,  4.26it/s][A[ANew best model for test task CHEMBL1738021 at epoch 28 with val loss 0.47445833683013916


 93% 28/30 [00:07<00:00,  4.30it/s][A[ANew best model for test task CHEMBL1738021 at epoch 29 with val loss 0.47264569997787476


 97% 29/30 [00:07<00:00,  4.34it/s][A[ANew best model for test task CHEMBL1738021 at epoch 30 with val loss 0.4712318480014801


100% 30/30 [00:07<00:00,  4.33it/s][A[A100% 30/30 [00:07<00:00,  3.81it/s]
Finished early stopping for task CHEMBL1738021, beginning testing


  0% 0/1 [00:00<?, ?it/s][A[A100% 1/1 [00:00<00:00, 54.93it/s]
Warning: Found a task with targets all 0s or all 1s

100% 1/1 [00:08<00:00,  8.01s/it][A100% 1/1 [00:08<00:00,  8.01s/it]
 19% 5/26 [01:35<06:22, 18.24s/it]
  0% 0/1 [00:00<?, ?it/s][AMeta testing on task: CHEMBL1738131


  0% 0/30 [00:00<?, ?it/s][A[ANew best model for test task CHEMBL1738131 at epoch 1 with val loss 0.7270075082778931


  3% 1/30 [00:01<00:50,  1.75s/it][A[AVal loss: 0.7293777465820312


  7% 2/30 [00:02<00:40,  1.44s/it][A[AVal loss: 0.7316045761108398


 10% 3/30 [00:03<00:33,  1.23s/it][A[AVal loss: 0.7336704730987549


 13% 4/30 [00:03<00:27,  1.07s/it][A[AVal loss: 0.7355595827102661


 17% 5/30 [00:05<00:31,  1.27s/it][A[AVal loss: 0.7374597787857056


 20% 6/30 [00:06<00:26,  1.10s/it][A[AVal loss: 0.7389005422592163


 23% 7/30 [00:07<00:22,  1.00it/s][A[AVal loss: 0.7407181859016418


 27% 8/30 [00:08<00:26,  1.21s/it][A[AVal loss: 0.7421110272407532


 30% 9/30 [00:09<00:22,  1.06s/it][A[AVal loss: 0.7434374690055847


 33% 10/30 [00:10<00:19,  1.04it/s][A[AVal loss: 0.7446202039718628


 37% 11/30 [00:10<00:16,  1.12it/s][A[AVal loss: 0.7458968758583069


 40% 12/30 [00:12<00:20,  1.15s/it][A[AVal loss: 0.7470898032188416


 43% 13/30 [00:13<00:17,  1.02s/it][A[AVal loss: 0.7480049133300781


 47% 14/30 [00:14<00:14,  1.07it/s][A[AVal loss: 0.7489223480224609


 50% 15/30 [00:14<00:13,  1.15it/s][A[AVal loss: 0.749707043170929


 53% 16/30 [00:16<00:15,  1.14s/it][A[AVal loss: 0.75046306848526


 57% 17/30 [00:17<00:13,  1.02s/it][A[AVal loss: 0.751145601272583


 60% 18/30 [00:18<00:11,  1.07it/s][A[AVal loss: 0.7519350647926331


 63% 19/30 [00:19<00:12,  1.17s/it][A[AVal loss: 0.7520214915275574


 67% 20/30 [00:20<00:10,  1.04s/it][A[AVal loss: 0.7524651288986206


 70% 21/30 [00:21<00:08,  1.06it/s][A[AVal loss: 0.7527796626091003


 73% 22/30 [00:23<00:09,  1.19s/it][A[AVal loss: 0.7528631687164307


 77% 23/30 [00:23<00:07,  1.06s/it][A[AVal loss: 0.75358647108078


 80% 24/30 [00:24<00:05,  1.04it/s][A[AVal loss: 0.7540744543075562


 83% 25/30 [00:25<00:04,  1.11it/s][A[AVal loss: 0.7540270090103149


 87% 26/30 [00:27<00:04,  1.16s/it][A[AVal loss: 0.7541875839233398


 90% 27/30 [00:27<00:03,  1.04s/it][A[AVal loss: 0.7543097138404846


 93% 28/30 [00:28<00:01,  1.04it/s][A[AVal loss: 0.7546552419662476


 97% 29/30 [00:30<00:01,  1.19s/it][A[AVal loss: 0.7548515796661377


100% 30/30 [00:31<00:00,  1.05s/it][A[A100% 30/30 [00:31<00:00,  1.04s/it]
Finished early stopping for task CHEMBL1738131, beginning testing


  0% 0/1 [00:00<?, ?it/s][A[A100% 1/1 [00:00<00:00, 14.54it/s]

100% 1/1 [00:31<00:00, 31.26s/it][A100% 1/1 [00:31<00:00, 31.26s/it]
 23% 6/26 [02:07<07:22, 22.14s/it]
  0% 0/1 [00:00<?, ?it/s][AMeta testing on task: CHEMBL1738202
/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  0% 0/30 [00:00<?, ?it/s][A[ANew best model for test task CHEMBL1738202 at epoch 1 with val loss 0.6263175010681152


  3% 1/30 [00:00<00:08,  3.61it/s][A[ANew best model for test task CHEMBL1738202 at epoch 2 with val loss 0.6160827875137329


  7% 2/30 [00:00<00:07,  3.65it/s][A[ANew best model for test task CHEMBL1738202 at epoch 3 with val loss 0.6055585145950317


 10% 3/30 [00:00<00:07,  3.68it/s][A[ANew best model for test task CHEMBL1738202 at epoch 4 with val loss 0.5951563119888306


 13% 4/30 [00:01<00:06,  3.72it/s][A[ANew best model for test task CHEMBL1738202 at epoch 5 with val loss 0.5843137502670288


 17% 5/30 [00:01<00:06,  3.75it/s][A[ANew best model for test task CHEMBL1738202 at epoch 6 with val loss 0.573316752910614


 20% 6/30 [00:02<00:13,  1.77it/s][A[ANew best model for test task CHEMBL1738202 at epoch 7 with val loss 0.5629975199699402


 23% 7/30 [00:02<00:10,  2.12it/s][A[ANew best model for test task CHEMBL1738202 at epoch 8 with val loss 0.5524703860282898


 27% 8/30 [00:03<00:08,  2.47it/s][A[ANew best model for test task CHEMBL1738202 at epoch 9 with val loss 0.5423856973648071


 30% 9/30 [00:03<00:07,  2.75it/s][A[ANew best model for test task CHEMBL1738202 at epoch 10 with val loss 0.5329418182373047


 33% 10/30 [00:03<00:06,  2.96it/s][A[ANew best model for test task CHEMBL1738202 at epoch 11 with val loss 0.5239986181259155


 37% 11/30 [00:03<00:06,  3.15it/s][A[ANew best model for test task CHEMBL1738202 at epoch 12 with val loss 0.5161016583442688


 40% 12/30 [00:04<00:05,  3.30it/s][A[ANew best model for test task CHEMBL1738202 at epoch 13 with val loss 0.5079734325408936


 43% 13/30 [00:04<00:04,  3.45it/s][A[ANew best model for test task CHEMBL1738202 at epoch 14 with val loss 0.5008663535118103


 47% 14/30 [00:04<00:04,  3.55it/s][A[ANew best model for test task CHEMBL1738202 at epoch 15 with val loss 0.49426549673080444


 50% 15/30 [00:04<00:04,  3.62it/s][A[ANew best model for test task CHEMBL1738202 at epoch 16 with val loss 0.48792946338653564


 53% 16/30 [00:05<00:03,  3.69it/s][A[ANew best model for test task CHEMBL1738202 at epoch 17 with val loss 0.48237645626068115


 57% 17/30 [00:05<00:03,  3.73it/s][A[ANew best model for test task CHEMBL1738202 at epoch 18 with val loss 0.477278470993042


 60% 18/30 [00:05<00:03,  3.74it/s][A[ANew best model for test task CHEMBL1738202 at epoch 19 with val loss 0.47263696789741516


 63% 19/30 [00:07<00:06,  1.76it/s][A[ANew best model for test task CHEMBL1738202 at epoch 20 with val loss 0.46863120794296265


 67% 20/30 [00:07<00:04,  2.10it/s][A[ANew best model for test task CHEMBL1738202 at epoch 21 with val loss 0.4643540680408478


 70% 21/30 [00:07<00:03,  2.43it/s][A[ANew best model for test task CHEMBL1738202 at epoch 22 with val loss 0.4608083665370941


 73% 22/30 [00:07<00:02,  2.73it/s][A[ANew best model for test task CHEMBL1738202 at epoch 23 with val loss 0.45707622170448303


 77% 23/30 [00:08<00:02,  2.98it/s][A[ANew best model for test task CHEMBL1738202 at epoch 24 with val loss 0.45384660363197327


 80% 24/30 [00:08<00:01,  3.18it/s][A[ANew best model for test task CHEMBL1738202 at epoch 25 with val loss 0.45074811577796936


 83% 25/30 [00:08<00:01,  3.35it/s][A[ANew best model for test task CHEMBL1738202 at epoch 26 with val loss 0.44770124554634094


 87% 26/30 [00:08<00:01,  3.45it/s][A[ANew best model for test task CHEMBL1738202 at epoch 27 with val loss 0.4448537230491638


 90% 27/30 [00:09<00:00,  3.56it/s][A[ANew best model for test task CHEMBL1738202 at epoch 28 with val loss 0.4422721266746521


 93% 28/30 [00:09<00:00,  3.66it/s][A[ANew best model for test task CHEMBL1738202 at epoch 29 with val loss 0.43985694646835327


 97% 29/30 [00:09<00:00,  3.69it/s][A[ANew best model for test task CHEMBL1738202 at epoch 30 with val loss 0.4375447630882263


100% 30/30 [00:09<00:00,  3.67it/s][A[A100% 30/30 [00:09<00:00,  3.02it/s]
Finished early stopping for task CHEMBL1738202, beginning testing


  0% 0/1 [00:00<?, ?it/s][A[A100% 1/1 [00:00<00:00, 40.68it/s]
Warning: Found a task with targets all 0s or all 1s

100% 1/1 [00:10<00:00, 10.06s/it][A100% 1/1 [00:10<00:00, 10.06s/it]
 27% 7/26 [02:17<05:51, 18.52s/it]
  0% 0/1 [00:00<?, ?it/s][AMeta testing on task: CHEMBL1794355


  0% 0/30 [00:00<?, ?it/s][A[ANew best model for test task CHEMBL1794355 at epoch 1 with val loss 0.5720587968826294


  3% 1/30 [00:00<00:14,  2.05it/s][A[ANew best model for test task CHEMBL1794355 at epoch 2 with val loss 0.5543316006660461


  7% 2/30 [00:01<00:21,  1.28it/s][A[ANew best model for test task CHEMBL1794355 at epoch 3 with val loss 0.5366721153259277


 10% 3/30 [00:02<00:18,  1.46it/s][A[ANew best model for test task CHEMBL1794355 at epoch 4 with val loss 0.5197513103485107


 13% 4/30 [00:02<00:16,  1.59it/s][A[ANew best model for test task CHEMBL1794355 at epoch 5 with val loss 0.5039465427398682


 17% 5/30 [00:03<00:14,  1.71it/s][A[ANew best model for test task CHEMBL1794355 at epoch 6 with val loss 0.4895000159740448


 20% 6/30 [00:03<00:13,  1.82it/s][A[ANew best model for test task CHEMBL1794355 at epoch 7 with val loss 0.4764831066131592


 23% 7/30 [00:04<00:12,  1.90it/s][A[ANew best model for test task CHEMBL1794355 at epoch 8 with val loss 0.4649121165275574


 27% 8/30 [00:05<00:17,  1.23it/s][A[ANew best model for test task CHEMBL1794355 at epoch 9 with val loss 0.45461466908454895


 30% 9/30 [00:06<00:15,  1.39it/s][A[ANew best model for test task CHEMBL1794355 at epoch 10 with val loss 0.44550904631614685


 33% 10/30 [00:06<00:13,  1.54it/s][A[ANew best model for test task CHEMBL1794355 at epoch 11 with val loss 0.43752557039260864


 37% 11/30 [00:07<00:11,  1.67it/s][A[ANew best model for test task CHEMBL1794355 at epoch 12 with val loss 0.43048757314682007


 40% 12/30 [00:07<00:10,  1.78it/s][A[ANew best model for test task CHEMBL1794355 at epoch 13 with val loss 0.42427510023117065


 43% 13/30 [00:08<00:09,  1.87it/s][A[ANew best model for test task CHEMBL1794355 at epoch 14 with val loss 0.4187118709087372


 47% 14/30 [00:08<00:08,  1.94it/s][A[ANew best model for test task CHEMBL1794355 at epoch 15 with val loss 0.4138360023498535


 50% 15/30 [00:10<00:12,  1.24it/s][A[ANew best model for test task CHEMBL1794355 at epoch 16 with val loss 0.4094131588935852


 53% 16/30 [00:10<00:10,  1.40it/s][A[ANew best model for test task CHEMBL1794355 at epoch 17 with val loss 0.4054924249649048


 57% 17/30 [00:11<00:08,  1.55it/s][A[ANew best model for test task CHEMBL1794355 at epoch 18 with val loss 0.40196260809898376


 60% 18/30 [00:11<00:07,  1.68it/s][A[ANew best model for test task CHEMBL1794355 at epoch 19 with val loss 0.39868953824043274


 63% 19/30 [00:12<00:06,  1.78it/s][A[ANew best model for test task CHEMBL1794355 at epoch 20 with val loss 0.3957182765007019


 67% 20/30 [00:12<00:05,  1.87it/s][A[ANew best model for test task CHEMBL1794355 at epoch 21 with val loss 0.39300036430358887


 70% 21/30 [00:14<00:07,  1.22it/s][A[ANew best model for test task CHEMBL1794355 at epoch 22 with val loss 0.39058026671409607


 73% 22/30 [00:14<00:05,  1.38it/s][A[ANew best model for test task CHEMBL1794355 at epoch 23 with val loss 0.38837459683418274


 77% 23/30 [00:15<00:04,  1.52it/s][A[ANew best model for test task CHEMBL1794355 at epoch 24 with val loss 0.3863928020000458


 80% 24/30 [00:15<00:03,  1.65it/s][A[ANew best model for test task CHEMBL1794355 at epoch 25 with val loss 0.38454312086105347


 83% 25/30 [00:16<00:02,  1.77it/s][A[ANew best model for test task CHEMBL1794355 at epoch 26 with val loss 0.38286909461021423


 87% 26/30 [00:16<00:02,  1.86it/s][A[ANew best model for test task CHEMBL1794355 at epoch 27 with val loss 0.38131454586982727


 90% 27/30 [00:18<00:02,  1.22it/s][A[ANew best model for test task CHEMBL1794355 at epoch 28 with val loss 0.379885196685791


 93% 28/30 [00:18<00:01,  1.38it/s][A[ANew best model for test task CHEMBL1794355 at epoch 29 with val loss 0.37856560945510864


 97% 29/30 [00:19<00:00,  1.52it/s][A[ANew best model for test task CHEMBL1794355 at epoch 30 with val loss 0.377359539270401


100% 30/30 [00:19<00:00,  1.64it/s][A[A100% 30/30 [00:19<00:00,  1.54it/s]
Finished early stopping for task CHEMBL1794355, beginning testing


  0% 0/1 [00:00<?, ?it/s][A[A100% 1/1 [00:00<00:00, 24.04it/s]

100% 1/1 [00:19<00:00, 19.66s/it][A100% 1/1 [00:19<00:00, 19.66s/it]
 31% 8/26 [02:36<05:39, 18.86s/it]
  0% 0/1 [00:00<?, ?it/s][AMeta testing on task: CHEMBL1794358
/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  0% 0/30 [00:00<?, ?it/s][A[ANew best model for test task CHEMBL1794358 at epoch 1 with val loss 0.7904971837997437


  3% 1/30 [00:00<00:12,  2.37it/s][A[ANew best model for test task CHEMBL1794358 at epoch 2 with val loss 0.7878926992416382


  7% 2/30 [00:00<00:11,  2.37it/s][A[ANew best model for test task CHEMBL1794358 at epoch 3 with val loss 0.7852285504341125


 10% 3/30 [00:02<00:19,  1.39it/s][A[ANew best model for test task CHEMBL1794358 at epoch 4 with val loss 0.7825539708137512


 13% 4/30 [00:02<00:16,  1.60it/s][A[ANew best model for test task CHEMBL1794358 at epoch 5 with val loss 0.7800891399383545


 17% 5/30 [00:03<00:14,  1.76it/s][A[ANew best model for test task CHEMBL1794358 at epoch 6 with val loss 0.7778115272521973


 20% 6/30 [00:03<00:12,  1.89it/s][A[ANew best model for test task CHEMBL1794358 at epoch 7 with val loss 0.775648832321167


 23% 7/30 [00:03<00:11,  1.99it/s][A[ANew best model for test task CHEMBL1794358 at epoch 8 with val loss 0.7735377550125122


 27% 8/30 [00:04<00:10,  2.09it/s][A[ANew best model for test task CHEMBL1794358 at epoch 9 with val loss 0.7716876864433289


 30% 9/30 [00:04<00:09,  2.16it/s][A[ANew best model for test task CHEMBL1794358 at epoch 10 with val loss 0.7700854539871216


 33% 10/30 [00:05<00:08,  2.22it/s][A[ANew best model for test task CHEMBL1794358 at epoch 11 with val loss 0.7684686779975891


 37% 11/30 [00:06<00:14,  1.35it/s][A[ANew best model for test task CHEMBL1794358 at epoch 12 with val loss 0.7668312191963196


 40% 12/30 [00:07<00:11,  1.54it/s][A[ANew best model for test task CHEMBL1794358 at epoch 13 with val loss 0.7652740478515625


 43% 13/30 [00:07<00:09,  1.70it/s][A[ANew best model for test task CHEMBL1794358 at epoch 14 with val loss 0.7638129591941833


 47% 14/30 [00:07<00:08,  1.83it/s][A[ANew best model for test task CHEMBL1794358 at epoch 15 with val loss 0.7624300122261047


 50% 15/30 [00:08<00:07,  1.97it/s][A[ANew best model for test task CHEMBL1794358 at epoch 16 with val loss 0.7612834572792053


 53% 16/30 [00:08<00:06,  2.07it/s][A[ANew best model for test task CHEMBL1794358 at epoch 17 with val loss 0.7600507140159607


 57% 17/30 [00:09<00:06,  2.16it/s][A[ANew best model for test task CHEMBL1794358 at epoch 18 with val loss 0.7588823437690735


 60% 18/30 [00:10<00:09,  1.33it/s][A[ANew best model for test task CHEMBL1794358 at epoch 19 with val loss 0.75770103931427


 63% 19/30 [00:11<00:07,  1.53it/s][A[ANew best model for test task CHEMBL1794358 at epoch 20 with val loss 0.7567849159240723


 67% 20/30 [00:11<00:05,  1.69it/s][A[ANew best model for test task CHEMBL1794358 at epoch 21 with val loss 0.7557634115219116


 70% 21/30 [00:11<00:04,  1.83it/s][A[ANew best model for test task CHEMBL1794358 at epoch 22 with val loss 0.754810631275177


 73% 22/30 [00:12<00:04,  1.95it/s][A[ANew best model for test task CHEMBL1794358 at epoch 23 with val loss 0.7539136409759521


 77% 23/30 [00:12<00:03,  2.06it/s][A[ANew best model for test task CHEMBL1794358 at epoch 24 with val loss 0.7530089616775513


 80% 24/30 [00:13<00:02,  2.15it/s][A[ANew best model for test task CHEMBL1794358 at epoch 25 with val loss 0.7522912621498108


 83% 25/30 [00:14<00:03,  1.33it/s][A[ANew best model for test task CHEMBL1794358 at epoch 26 with val loss 0.751440167427063


 87% 26/30 [00:15<00:02,  1.53it/s][A[ANew best model for test task CHEMBL1794358 at epoch 27 with val loss 0.7506622076034546


 90% 27/30 [00:15<00:01,  1.69it/s][A[ANew best model for test task CHEMBL1794358 at epoch 28 with val loss 0.7497983574867249


 93% 28/30 [00:15<00:01,  1.83it/s][A[ANew best model for test task CHEMBL1794358 at epoch 29 with val loss 0.7490096092224121


 97% 29/30 [00:16<00:00,  1.94it/s][A[ANew best model for test task CHEMBL1794358 at epoch 30 with val loss 0.7483403086662292


100% 30/30 [00:16<00:00,  2.05it/s][A[A100% 30/30 [00:16<00:00,  1.78it/s]
Finished early stopping for task CHEMBL1794358, beginning testing


  0% 0/1 [00:00<?, ?it/s][A[A100% 1/1 [00:00<00:00, 27.00it/s]
Warning: Found a task with targets all 0s or all 1s

100% 1/1 [00:17<00:00, 17.00s/it][A100% 1/1 [00:17<00:00, 17.00s/it]
 35% 9/26 [02:53<05:11, 18.30s/it]
  0% 0/1 [00:00<?, ?it/s][AMeta testing on task: CHEMBL1794567


  0% 0/30 [00:00<?, ?it/s][A[ANew best model for test task CHEMBL1794567 at epoch 1 with val loss 0.5863078236579895


  3% 1/30 [00:00<00:15,  1.90it/s][A[ANew best model for test task CHEMBL1794567 at epoch 2 with val loss 0.5675783753395081


  7% 2/30 [00:02<00:22,  1.22it/s][A[ANew best model for test task CHEMBL1794567 at epoch 3 with val loss 0.5488299131393433


 10% 3/30 [00:02<00:19,  1.36it/s][A[ANew best model for test task CHEMBL1794567 at epoch 4 with val loss 0.5322589874267578


 13% 4/30 [00:03<00:17,  1.47it/s][A[ANew best model for test task CHEMBL1794567 at epoch 5 with val loss 0.5172918438911438


 17% 5/30 [00:03<00:15,  1.57it/s][A[ANew best model for test task CHEMBL1794567 at epoch 6 with val loss 0.5046120882034302


 20% 6/30 [00:04<00:14,  1.66it/s][A[ANew best model for test task CHEMBL1794567 at epoch 7 with val loss 0.4929044544696808


 23% 7/30 [00:04<00:13,  1.73it/s][A[ANew best model for test task CHEMBL1794567 at epoch 8 with val loss 0.4832410216331482


 27% 8/30 [00:06<00:19,  1.15it/s][A[ANew best model for test task CHEMBL1794567 at epoch 9 with val loss 0.47522681951522827


 30% 9/30 [00:06<00:16,  1.29it/s][A[ANew best model for test task CHEMBL1794567 at epoch 10 with val loss 0.4682590067386627


 33% 10/30 [00:07<00:14,  1.43it/s][A[ANew best model for test task CHEMBL1794567 at epoch 11 with val loss 0.4619067311286926


 37% 11/30 [00:07<00:12,  1.54it/s][A[ANew best model for test task CHEMBL1794567 at epoch 12 with val loss 0.4564899206161499


 40% 12/30 [00:08<00:11,  1.62it/s][A[ANew best model for test task CHEMBL1794567 at epoch 13 with val loss 0.4518788456916809


 43% 13/30 [00:08<00:09,  1.70it/s][A[ANew best model for test task CHEMBL1794567 at epoch 14 with val loss 0.4476991891860962


 47% 14/30 [00:10<00:13,  1.14it/s][A[ANew best model for test task CHEMBL1794567 at epoch 15 with val loss 0.4455146789550781


 50% 15/30 [00:10<00:11,  1.29it/s][A[ANew best model for test task CHEMBL1794567 at epoch 16 with val loss 0.44206246733665466


 53% 16/30 [00:11<00:09,  1.42it/s][A[ANew best model for test task CHEMBL1794567 at epoch 17 with val loss 0.43942150473594666


 57% 17/30 [00:12<00:08,  1.54it/s][A[ANew best model for test task CHEMBL1794567 at epoch 18 with val loss 0.4373149871826172


 60% 18/30 [00:12<00:07,  1.60it/s][A[ANew best model for test task CHEMBL1794567 at epoch 19 with val loss 0.43485552072525024


 63% 19/30 [00:13<00:06,  1.66it/s][A[ANew best model for test task CHEMBL1794567 at epoch 20 with val loss 0.43267571926116943


 67% 20/30 [00:14<00:08,  1.13it/s][A[ANew best model for test task CHEMBL1794567 at epoch 21 with val loss 0.4308110177516937


 70% 21/30 [00:15<00:07,  1.28it/s][A[ANew best model for test task CHEMBL1794567 at epoch 22 with val loss 0.4292752146720886


 73% 22/30 [00:15<00:05,  1.41it/s][A[ANew best model for test task CHEMBL1794567 at epoch 23 with val loss 0.4283301830291748


 77% 23/30 [00:16<00:04,  1.50it/s][A[ANew best model for test task CHEMBL1794567 at epoch 24 with val loss 0.42718276381492615


 80% 24/30 [00:16<00:03,  1.57it/s][A[ANew best model for test task CHEMBL1794567 at epoch 25 with val loss 0.425993412733078


 83% 25/30 [00:17<00:03,  1.65it/s][A[ANew best model for test task CHEMBL1794567 at epoch 26 with val loss 0.424959272146225


 87% 26/30 [00:18<00:02,  1.70it/s][A[ANew best model for test task CHEMBL1794567 at epoch 27 with val loss 0.42377251386642456


 90% 27/30 [00:19<00:02,  1.15it/s][A[ANew best model for test task CHEMBL1794567 at epoch 28 with val loss 0.42300179600715637


 93% 28/30 [00:20<00:01,  1.28it/s][A[ANew best model for test task CHEMBL1794567 at epoch 29 with val loss 0.422406405210495


 97% 29/30 [00:20<00:00,  1.39it/s][A[ANew best model for test task CHEMBL1794567 at epoch 30 with val loss 0.42164573073387146


100% 30/30 [00:21<00:00,  1.50it/s][A[A100% 30/30 [00:21<00:00,  1.41it/s]
Finished early stopping for task CHEMBL1794567, beginning testing


  0% 0/1 [00:00<?, ?it/s][A[A100% 1/1 [00:00<00:00, 20.64it/s]

100% 1/1 [00:21<00:00, 21.40s/it][A100% 1/1 [00:21<00:00, 21.40s/it]
 38% 10/26 [03:15<05:07, 19.23s/it]
  0% 0/1 [00:00<?, ?it/s][AMeta testing on task: CHEMBL1909085
/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  0% 0/30 [00:00<?, ?it/s][A[ANew best model for test task CHEMBL1909085 at epoch 1 with val loss 0.7556661665439606


  3% 1/30 [00:02<01:03,  2.18s/it][A[ANew best model for test task CHEMBL1909085 at epoch 2 with val loss 0.7505618333816528


  7% 2/30 [00:03<00:53,  1.90s/it][A[ANew best model for test task CHEMBL1909085 at epoch 3 with val loss 0.7465987801551819


 10% 3/30 [00:04<00:45,  1.68s/it][A[ANew best model for test task CHEMBL1909085 at epoch 4 with val loss 0.7432450354099274


 13% 4/30 [00:06<00:47,  1.84s/it][A[ANew best model for test task CHEMBL1909085 at epoch 5 with val loss 0.7405465841293335


 17% 5/30 [00:08<00:41,  1.66s/it][A[ANew best model for test task CHEMBL1909085 at epoch 6 with val loss 0.738311380147934


 20% 6/30 [00:10<00:43,  1.81s/it][A[ANew best model for test task CHEMBL1909085 at epoch 7 with val loss 0.7364112734794617


 23% 7/30 [00:11<00:37,  1.65s/it][A[ANew best model for test task CHEMBL1909085 at epoch 8 with val loss 0.7348154783248901


 27% 8/30 [00:13<00:39,  1.82s/it][A[ANew best model for test task CHEMBL1909085 at epoch 9 with val loss 0.7336902916431427


 30% 9/30 [00:14<00:34,  1.64s/it][A[ANew best model for test task CHEMBL1909085 at epoch 10 with val loss 0.7325540781021118


 33% 10/30 [00:16<00:30,  1.52s/it][A[ANew best model for test task CHEMBL1909085 at epoch 11 with val loss 0.7315948307514191


 37% 11/30 [00:18<00:32,  1.73s/it][A[ANew best model for test task CHEMBL1909085 at epoch 12 with val loss 0.7305794656276703


 40% 12/30 [00:19<00:28,  1.58s/it][A[ANew best model for test task CHEMBL1909085 at epoch 13 with val loss 0.7297996580600739


 43% 13/30 [00:20<00:25,  1.48s/it][A[ANew best model for test task CHEMBL1909085 at epoch 14 with val loss 0.729136198759079


 47% 14/30 [00:23<00:27,  1.71s/it][A[ANew best model for test task CHEMBL1909085 at epoch 15 with val loss 0.728501945734024


 50% 15/30 [00:24<00:23,  1.57s/it][A[ANew best model for test task CHEMBL1909085 at epoch 16 with val loss 0.727837473154068


 53% 16/30 [00:26<00:24,  1.76s/it][A[ANew best model for test task CHEMBL1909085 at epoch 17 with val loss 0.7274045050144196


 57% 17/30 [00:27<00:20,  1.59s/it][A[ANew best model for test task CHEMBL1909085 at epoch 18 with val loss 0.7269457876682281


 60% 18/30 [00:29<00:21,  1.76s/it][A[ANew best model for test task CHEMBL1909085 at epoch 19 with val loss 0.7264916598796844


 63% 19/30 [00:31<00:17,  1.59s/it][A[ANew best model for test task CHEMBL1909085 at epoch 20 with val loss 0.7260861694812775


 67% 20/30 [00:32<00:14,  1.48s/it][A[ANew best model for test task CHEMBL1909085 at epoch 21 with val loss 0.7256589233875275


 70% 21/30 [00:34<00:15,  1.70s/it][A[ANew best model for test task CHEMBL1909085 at epoch 22 with val loss 0.7253671884536743


 73% 22/30 [00:35<00:12,  1.55s/it][A[ANew best model for test task CHEMBL1909085 at epoch 23 with val loss 0.7251542210578918


 77% 23/30 [00:37<00:12,  1.74s/it][A[ANew best model for test task CHEMBL1909085 at epoch 24 with val loss 0.7248672246932983


 80% 24/30 [00:39<00:09,  1.59s/it][A[ANew best model for test task CHEMBL1909085 at epoch 25 with val loss 0.724699467420578


 83% 25/30 [00:40<00:07,  1.47s/it][A[ANew best model for test task CHEMBL1909085 at epoch 26 with val loss 0.7245468199253082


 87% 26/30 [00:42<00:06,  1.69s/it][A[ANew best model for test task CHEMBL1909085 at epoch 27 with val loss 0.7244193255901337


 90% 27/30 [00:43<00:04,  1.56s/it][A[ANew best model for test task CHEMBL1909085 at epoch 28 with val loss 0.7242770791053772


 93% 28/30 [00:45<00:02,  1.46s/it][A[ANew best model for test task CHEMBL1909085 at epoch 29 with val loss 0.7240989506244659


 97% 29/30 [00:47<00:01,  1.68s/it][A[ANew best model for test task CHEMBL1909085 at epoch 30 with val loss 0.7238863408565521


100% 30/30 [00:48<00:00,  1.54s/it][A[A100% 30/30 [00:48<00:00,  1.61s/it]
Finished early stopping for task CHEMBL1909085, beginning testing


  0% 0/2 [00:00<?, ?it/s][A[A

100% 2/2 [00:00<00:00, 14.24it/s][A[A100% 2/2 [00:00<00:00, 14.21it/s]

100% 1/1 [00:48<00:00, 48.67s/it][A100% 1/1 [00:48<00:00, 48.67s/it]
 42% 11/26 [04:03<07:00, 28.06s/it]
  0% 0/1 [00:00<?, ?it/s][AMeta testing on task: CHEMBL1909092
/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  0% 0/30 [00:00<?, ?it/s][A[ANew best model for test task CHEMBL1909092 at epoch 1 with val loss 0.797378808259964


  3% 1/30 [00:02<01:02,  2.14s/it][A[ANew best model for test task CHEMBL1909092 at epoch 2 with val loss 0.7858900129795074


  7% 2/30 [00:03<00:51,  1.84s/it][A[ANew best model for test task CHEMBL1909092 at epoch 3 with val loss 0.7767178118228912


 10% 3/30 [00:05<00:52,  1.93s/it][A[ANew best model for test task CHEMBL1909092 at epoch 4 with val loss 0.7692097425460815


 13% 4/30 [00:06<00:44,  1.70s/it][A[ANew best model for test task CHEMBL1909092 at epoch 5 with val loss 0.7632456421852112


 17% 5/30 [00:07<00:38,  1.54s/it][A[ANew best model for test task CHEMBL1909092 at epoch 6 with val loss 0.75822314620018


 20% 6/30 [00:09<00:41,  1.72s/it][A[ANew best model for test task CHEMBL1909092 at epoch 7 with val loss 0.753923773765564


 23% 7/30 [00:11<00:35,  1.56s/it][A[ANew best model for test task CHEMBL1909092 at epoch 8 with val loss 0.7505665421485901


 27% 8/30 [00:13<00:38,  1.73s/it][A[ANew best model for test task CHEMBL1909092 at epoch 9 with val loss 0.7473963499069214


 30% 9/30 [00:14<00:32,  1.56s/it][A[ANew best model for test task CHEMBL1909092 at epoch 10 with val loss 0.7446286678314209


 33% 10/30 [00:15<00:28,  1.44s/it][A[ANew best model for test task CHEMBL1909092 at epoch 11 with val loss 0.7423287630081177


 37% 11/30 [00:17<00:31,  1.66s/it][A[ANew best model for test task CHEMBL1909092 at epoch 12 with val loss 0.7402412593364716


 40% 12/30 [00:18<00:27,  1.52s/it][A[ANew best model for test task CHEMBL1909092 at epoch 13 with val loss 0.7384725213050842


 43% 13/30 [00:21<00:29,  1.71s/it][A[ANew best model for test task CHEMBL1909092 at epoch 14 with val loss 0.736802726984024


 47% 14/30 [00:22<00:24,  1.55s/it][A[ANew best model for test task CHEMBL1909092 at epoch 15 with val loss 0.7353191673755646


 50% 15/30 [00:23<00:21,  1.44s/it][A[ANew best model for test task CHEMBL1909092 at epoch 16 with val loss 0.7341382503509521


 53% 16/30 [00:25<00:23,  1.66s/it][A[ANew best model for test task CHEMBL1909092 at epoch 17 with val loss 0.7328982353210449


 57% 17/30 [00:26<00:19,  1.52s/it][A[ANew best model for test task CHEMBL1909092 at epoch 18 with val loss 0.7317456007003784


 60% 18/30 [00:27<00:17,  1.42s/it][A[ANew best model for test task CHEMBL1909092 at epoch 19 with val loss 0.7307120859622955


 63% 19/30 [00:30<00:18,  1.64s/it][A[ANew best model for test task CHEMBL1909092 at epoch 20 with val loss 0.7298798859119415


 67% 20/30 [00:31<00:15,  1.51s/it][A[ANew best model for test task CHEMBL1909092 at epoch 21 with val loss 0.7290172278881073


 70% 21/30 [00:32<00:12,  1.41s/it][A[ANew best model for test task CHEMBL1909092 at epoch 22 with val loss 0.7282216846942902


 73% 22/30 [00:34<00:13,  1.65s/it][A[ANew best model for test task CHEMBL1909092 at epoch 23 with val loss 0.7275004982948303


 77% 23/30 [00:35<00:10,  1.51s/it][A[ANew best model for test task CHEMBL1909092 at epoch 24 with val loss 0.7269706130027771


 80% 24/30 [00:38<00:10,  1.72s/it][A[ANew best model for test task CHEMBL1909092 at epoch 25 with val loss 0.7264043092727661


 83% 25/30 [00:39<00:07,  1.56s/it][A[ANew best model for test task CHEMBL1909092 at epoch 26 with val loss 0.7260383665561676


 87% 26/30 [00:40<00:05,  1.46s/it][A[ANew best model for test task CHEMBL1909092 at epoch 27 with val loss 0.725483238697052


 90% 27/30 [00:42<00:05,  1.68s/it][A[ANew best model for test task CHEMBL1909092 at epoch 28 with val loss 0.7249763906002045


 93% 28/30 [00:43<00:03,  1.54s/it][A[ANew best model for test task CHEMBL1909092 at epoch 29 with val loss 0.7245557010173798


 97% 29/30 [00:45<00:01,  1.44s/it][A[ANew best model for test task CHEMBL1909092 at epoch 30 with val loss 0.7241645753383636


100% 30/30 [00:47<00:00,  1.67s/it][A[A100% 30/30 [00:47<00:00,  1.58s/it]
Finished early stopping for task CHEMBL1909092, beginning testing


  0% 0/2 [00:00<?, ?it/s][A[A

100% 2/2 [00:00<00:00, 14.90it/s][A[A100% 2/2 [00:00<00:00, 14.88it/s]

100% 1/1 [00:47<00:00, 47.55s/it][A100% 1/1 [00:47<00:00, 47.55s/it]
 46% 12/26 [04:51<07:54, 33.91s/it]
  0% 0/1 [00:00<?, ?it/s][AMeta testing on task: CHEMBL1909192
/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  0% 0/30 [00:00<?, ?it/s][A[ANew best model for test task CHEMBL1909192 at epoch 1 with val loss 0.7979736924171448


  3% 1/30 [00:01<00:33,  1.16s/it][A[ANew best model for test task CHEMBL1909192 at epoch 2 with val loss 0.7841993570327759


  7% 2/30 [00:03<00:40,  1.45s/it][A[ANew best model for test task CHEMBL1909192 at epoch 3 with val loss 0.7735866606235504


 10% 3/30 [00:04<00:36,  1.36s/it][A[ANew best model for test task CHEMBL1909192 at epoch 4 with val loss 0.7650910019874573


 13% 4/30 [00:05<00:33,  1.30s/it][A[ANew best model for test task CHEMBL1909192 at epoch 5 with val loss 0.7581751048564911


 17% 5/30 [00:07<00:38,  1.56s/it][A[ANew best model for test task CHEMBL1909192 at epoch 6 with val loss 0.7524890303611755


 20% 6/30 [00:08<00:34,  1.44s/it][A[ANew best model for test task CHEMBL1909192 at epoch 7 with val loss 0.7477455139160156


 23% 7/30 [00:11<00:38,  1.66s/it][A[ANew best model for test task CHEMBL1909192 at epoch 8 with val loss 0.7437583804130554


 27% 8/30 [00:12<00:33,  1.51s/it][A[ANew best model for test task CHEMBL1909192 at epoch 9 with val loss 0.7403178513050079


 30% 9/30 [00:13<00:29,  1.41s/it][A[ANew best model for test task CHEMBL1909192 at epoch 10 with val loss 0.7373428344726562


 33% 10/30 [00:15<00:32,  1.64s/it][A[ANew best model for test task CHEMBL1909192 at epoch 11 with val loss 0.7347553968429565


 37% 11/30 [00:16<00:28,  1.50s/it][A[ANew best model for test task CHEMBL1909192 at epoch 12 with val loss 0.7324724793434143


 40% 12/30 [00:18<00:30,  1.70s/it][A[ANew best model for test task CHEMBL1909192 at epoch 13 with val loss 0.7304570078849792


 43% 13/30 [00:20<00:26,  1.55s/it][A[ANew best model for test task CHEMBL1909192 at epoch 14 with val loss 0.7286754548549652


 47% 14/30 [00:21<00:23,  1.44s/it][A[ANew best model for test task CHEMBL1909192 at epoch 15 with val loss 0.7271268963813782


 50% 15/30 [00:23<00:24,  1.67s/it][A[ANew best model for test task CHEMBL1909192 at epoch 16 with val loss 0.7256602346897125


 53% 16/30 [00:24<00:21,  1.53s/it][A[ANew best model for test task CHEMBL1909192 at epoch 17 with val loss 0.7243329584598541


 57% 17/30 [00:26<00:22,  1.72s/it][A[ANew best model for test task CHEMBL1909192 at epoch 18 with val loss 0.7231196165084839


 60% 18/30 [00:28<00:18,  1.57s/it][A[ANew best model for test task CHEMBL1909192 at epoch 19 with val loss 0.7220061719417572


 63% 19/30 [00:29<00:16,  1.46s/it][A[ANew best model for test task CHEMBL1909192 at epoch 20 with val loss 0.7210023701190948


 67% 20/30 [00:31<00:16,  1.68s/it][A[ANew best model for test task CHEMBL1909192 at epoch 21 with val loss 0.7200746834278107


 70% 21/30 [00:32<00:13,  1.53s/it][A[ANew best model for test task CHEMBL1909192 at epoch 22 with val loss 0.7192180454730988


 73% 22/30 [00:33<00:11,  1.43s/it][A[ANew best model for test task CHEMBL1909192 at epoch 23 with val loss 0.7184026539325714


 77% 23/30 [00:36<00:11,  1.66s/it][A[ANew best model for test task CHEMBL1909192 at epoch 24 with val loss 0.7176883518695831


 80% 24/30 [00:37<00:09,  1.53s/it][A[ANew best model for test task CHEMBL1909192 at epoch 25 with val loss 0.7170571982860565


 83% 25/30 [00:39<00:08,  1.73s/it][A[ANew best model for test task CHEMBL1909192 at epoch 26 with val loss 0.7164137959480286


 87% 26/30 [00:40<00:06,  1.57s/it][A[ANew best model for test task CHEMBL1909192 at epoch 27 with val loss 0.7158128321170807


 90% 27/30 [00:42<00:05,  1.76s/it][A[ANew best model for test task CHEMBL1909192 at epoch 28 with val loss 0.7152498364448547


 93% 28/30 [00:44<00:03,  1.60s/it][A[ANew best model for test task CHEMBL1909192 at epoch 29 with val loss 0.7147201001644135


 97% 29/30 [00:45<00:01,  1.48s/it][A[ANew best model for test task CHEMBL1909192 at epoch 30 with val loss 0.714213490486145


100% 30/30 [00:47<00:00,  1.70s/it][A[A100% 30/30 [00:47<00:00,  1.59s/it]
Finished early stopping for task CHEMBL1909192, beginning testing


  0% 0/2 [00:00<?, ?it/s][A[A

100% 2/2 [00:00<00:00, 16.00it/s][A[A100% 2/2 [00:00<00:00, 15.96it/s]
Warning: Found a task with targets all 0s or all 1s

100% 1/1 [00:47<00:00, 47.80s/it][A100% 1/1 [00:47<00:00, 47.80s/it]
 50% 13/26 [05:39<08:15, 38.08s/it]
  0% 0/1 [00:00<?, ?it/s][AMeta testing on task: CHEMBL1909209


  0% 0/30 [00:00<?, ?it/s][A[ANew best model for test task CHEMBL1909209 at epoch 1 with val loss 0.7809534966945648


  3% 1/30 [00:01<00:33,  1.14s/it][A[ANew best model for test task CHEMBL1909209 at epoch 2 with val loss 0.773421436548233


  7% 2/30 [00:03<00:40,  1.44s/it][A[ANew best model for test task CHEMBL1909209 at epoch 3 with val loss 0.7670171856880188


 10% 3/30 [00:04<00:36,  1.35s/it][A[ANew best model for test task CHEMBL1909209 at epoch 4 with val loss 0.762033611536026


 13% 4/30 [00:05<00:33,  1.29s/it][A[ANew best model for test task CHEMBL1909209 at epoch 5 with val loss 0.7581414878368378


 17% 5/30 [00:07<00:38,  1.55s/it][A[ANew best model for test task CHEMBL1909209 at epoch 6 with val loss 0.754498302936554


 20% 6/30 [00:08<00:34,  1.43s/it][A[ANew best model for test task CHEMBL1909209 at epoch 7 with val loss 0.7512765526771545


 23% 7/30 [00:11<00:37,  1.65s/it][A[ANew best model for test task CHEMBL1909209 at epoch 8 with val loss 0.7489532828330994


 27% 8/30 [00:12<00:33,  1.50s/it][A[ANew best model for test task CHEMBL1909209 at epoch 9 with val loss 0.7464951872825623


 30% 9/30 [00:13<00:29,  1.40s/it][A[ANew best model for test task CHEMBL1909209 at epoch 10 with val loss 0.7446079552173615


 33% 10/30 [00:15<00:32,  1.63s/it][A[ANew best model for test task CHEMBL1909209 at epoch 11 with val loss 0.7428484261035919


 37% 11/30 [00:16<00:28,  1.49s/it][A[ANew best model for test task CHEMBL1909209 at epoch 12 with val loss 0.7415471374988556


 40% 12/30 [00:18<00:30,  1.69s/it][A[ANew best model for test task CHEMBL1909209 at epoch 13 with val loss 0.7404207587242126


 43% 13/30 [00:20<00:26,  1.54s/it][A[ANew best model for test task CHEMBL1909209 at epoch 14 with val loss 0.7395716607570648


 47% 14/30 [00:21<00:22,  1.43s/it][A[ANew best model for test task CHEMBL1909209 at epoch 15 with val loss 0.7384573817253113


 50% 15/30 [00:23<00:24,  1.65s/it][A[ANew best model for test task CHEMBL1909209 at epoch 16 with val loss 0.7375534176826477


 53% 16/30 [00:24<00:21,  1.51s/it][A[ANew best model for test task CHEMBL1909209 at epoch 17 with val loss 0.7367894947528839


 57% 17/30 [00:26<00:22,  1.71s/it][A[ANew best model for test task CHEMBL1909209 at epoch 18 with val loss 0.7359394431114197


 60% 18/30 [00:27<00:18,  1.55s/it][A[ANew best model for test task CHEMBL1909209 at epoch 19 with val loss 0.7352778315544128


 63% 19/30 [00:29<00:15,  1.44s/it][A[ANew best model for test task CHEMBL1909209 at epoch 20 with val loss 0.7348255813121796


 67% 20/30 [00:31<00:16,  1.66s/it][A[ANew best model for test task CHEMBL1909209 at epoch 21 with val loss 0.7341914176940918


 70% 21/30 [00:32<00:13,  1.52s/it][A[ANew best model for test task CHEMBL1909209 at epoch 22 with val loss 0.7335861623287201


 73% 22/30 [00:34<00:13,  1.72s/it][A[ANew best model for test task CHEMBL1909209 at epoch 23 with val loss 0.7331316769123077


 77% 23/30 [00:35<00:10,  1.56s/it][A[ANew best model for test task CHEMBL1909209 at epoch 24 with val loss 0.7328233420848846


 80% 24/30 [00:37<00:08,  1.45s/it][A[ANew best model for test task CHEMBL1909209 at epoch 25 with val loss 0.7324246764183044


 83% 25/30 [00:39<00:08,  1.67s/it][A[ANew best model for test task CHEMBL1909209 at epoch 26 with val loss 0.7320538759231567


 87% 26/30 [00:40<00:06,  1.53s/it][A[ANew best model for test task CHEMBL1909209 at epoch 27 with val loss 0.7319289147853851


 90% 27/30 [00:41<00:04,  1.43s/it][A[ANew best model for test task CHEMBL1909209 at epoch 28 with val loss 0.7315934896469116


 93% 28/30 [00:43<00:03,  1.66s/it][A[ANew best model for test task CHEMBL1909209 at epoch 29 with val loss 0.7313007414340973


 97% 29/30 [00:45<00:01,  1.53s/it][A[ANew best model for test task CHEMBL1909209 at epoch 30 with val loss 0.7310746014118195


100% 30/30 [00:47<00:00,  1.73s/it][A[A100% 30/30 [00:47<00:00,  1.57s/it]
Finished early stopping for task CHEMBL1909209, beginning testing


  0% 0/2 [00:00<?, ?it/s][A[A

100% 2/2 [00:00<00:00, 15.13it/s][A[A100% 2/2 [00:00<00:00, 15.10it/s]

100% 1/1 [00:47<00:00, 47.48s/it][A100% 1/1 [00:47<00:00, 47.49s/it]
 54% 14/26 [06:26<08:10, 40.90s/it]
  0% 0/1 [00:00<?, ?it/s][AMeta testing on task: CHEMBL1909211
/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  0% 0/30 [00:00<?, ?it/s][A[ANew best model for test task CHEMBL1909211 at epoch 1 with val loss 0.784692108631134


  3% 1/30 [00:01<00:33,  1.15s/it][A[ANew best model for test task CHEMBL1909211 at epoch 2 with val loss 0.7788301706314087


  7% 2/30 [00:03<00:40,  1.45s/it][A[ANew best model for test task CHEMBL1909211 at epoch 3 with val loss 0.7746412456035614


 10% 3/30 [00:04<00:36,  1.36s/it][A[ANew best model for test task CHEMBL1909211 at epoch 4 with val loss 0.7708097398281097


 13% 4/30 [00:05<00:33,  1.30s/it][A[ANew best model for test task CHEMBL1909211 at epoch 5 with val loss 0.7674059271812439


 17% 5/30 [00:07<00:38,  1.56s/it][A[ANew best model for test task CHEMBL1909211 at epoch 6 with val loss 0.7646718621253967


 20% 6/30 [00:08<00:34,  1.44s/it][A[ANew best model for test task CHEMBL1909211 at epoch 7 with val loss 0.7623127698898315


 23% 7/30 [00:10<00:31,  1.36s/it][A[ANew best model for test task CHEMBL1909211 at epoch 8 with val loss 0.7602588534355164


 27% 8/30 [00:12<00:35,  1.60s/it][A[ANew best model for test task CHEMBL1909211 at epoch 9 with val loss 0.7585498988628387


 30% 9/30 [00:13<00:30,  1.47s/it][A[ANew best model for test task CHEMBL1909211 at epoch 10 with val loss 0.7573502659797668


 33% 10/30 [00:15<00:33,  1.68s/it][A[ANew best model for test task CHEMBL1909211 at epoch 11 with val loss 0.7560821175575256


 37% 11/30 [00:16<00:29,  1.53s/it][A[ANew best model for test task CHEMBL1909211 at epoch 12 with val loss 0.7549843192100525


 40% 12/30 [00:17<00:25,  1.43s/it][A[ANew best model for test task CHEMBL1909211 at epoch 13 with val loss 0.7539204955101013


 43% 13/30 [00:20<00:28,  1.65s/it][A[ANew best model for test task CHEMBL1909211 at epoch 14 with val loss 0.7533664107322693


 47% 14/30 [00:21<00:24,  1.51s/it][A[ANew best model for test task CHEMBL1909211 at epoch 15 with val loss 0.7526995837688446


 50% 15/30 [00:23<00:25,  1.71s/it][A[ANew best model for test task CHEMBL1909211 at epoch 16 with val loss 0.7522163987159729


 53% 16/30 [00:24<00:21,  1.56s/it][A[ANew best model for test task CHEMBL1909211 at epoch 17 with val loss 0.751501590013504


 57% 17/30 [00:25<00:18,  1.45s/it][A[ANew best model for test task CHEMBL1909211 at epoch 18 with val loss 0.7509101629257202


 60% 18/30 [00:28<00:20,  1.67s/it][A[ANew best model for test task CHEMBL1909211 at epoch 19 with val loss 0.7502016425132751


 63% 19/30 [00:29<00:16,  1.53s/it][A[ANew best model for test task CHEMBL1909211 at epoch 20 with val loss 0.7496406435966492


 67% 20/30 [00:31<00:17,  1.73s/it][A[ANew best model for test task CHEMBL1909211 at epoch 21 with val loss 0.7492503225803375


 70% 21/30 [00:32<00:14,  1.57s/it][A[AVal loss: 0.7492638528347015


 73% 22/30 [00:33<00:11,  1.45s/it][A[ANew best model for test task CHEMBL1909211 at epoch 23 with val loss 0.7489627003669739


 77% 23/30 [00:36<00:11,  1.67s/it][A[ANew best model for test task CHEMBL1909211 at epoch 24 with val loss 0.7489041686058044


 80% 24/30 [00:37<00:09,  1.53s/it][A[ANew best model for test task CHEMBL1909211 at epoch 25 with val loss 0.7487221956253052


 83% 25/30 [00:39<00:08,  1.73s/it][A[ANew best model for test task CHEMBL1909211 at epoch 26 with val loss 0.7484673857688904


 87% 26/30 [00:40<00:06,  1.57s/it][A[ANew best model for test task CHEMBL1909211 at epoch 27 with val loss 0.7482087314128876


 90% 27/30 [00:41<00:04,  1.47s/it][A[ANew best model for test task CHEMBL1909211 at epoch 28 with val loss 0.7477869987487793


 93% 28/30 [00:44<00:03,  1.69s/it][A[ANew best model for test task CHEMBL1909211 at epoch 29 with val loss 0.7477507889270782


 97% 29/30 [00:45<00:01,  1.54s/it][A[ANew best model for test task CHEMBL1909211 at epoch 30 with val loss 0.7474049627780914


100% 30/30 [00:47<00:00,  1.74s/it][A[A100% 30/30 [00:47<00:00,  1.58s/it]
Finished early stopping for task CHEMBL1909211, beginning testing


  0% 0/2 [00:00<?, ?it/s][A[A

100% 2/2 [00:00<00:00, 15.23it/s][A[A100% 2/2 [00:00<00:00, 15.20it/s]

100% 1/1 [00:47<00:00, 47.71s/it][A100% 1/1 [00:47<00:00, 47.71s/it]
 58% 15/26 [07:14<07:52, 42.94s/it]
  0% 0/1 [00:00<?, ?it/s][AMeta testing on task: CHEMBL1909212
/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  0% 0/30 [00:00<?, ?it/s][A[ANew best model for test task CHEMBL1909212 at epoch 1 with val loss 0.7858286798000336


  3% 1/30 [00:01<00:33,  1.15s/it][A[ANew best model for test task CHEMBL1909212 at epoch 2 with val loss 0.7743472754955292


  7% 2/30 [00:02<00:32,  1.15s/it][A[ANew best model for test task CHEMBL1909212 at epoch 3 with val loss 0.7652206718921661


 10% 3/30 [00:04<00:39,  1.45s/it][A[ANew best model for test task CHEMBL1909212 at epoch 4 with val loss 0.7580707669258118


 13% 4/30 [00:05<00:35,  1.36s/it][A[ANew best model for test task CHEMBL1909212 at epoch 5 with val loss 0.7521340847015381


 17% 5/30 [00:07<00:39,  1.60s/it][A[ANew best model for test task CHEMBL1909212 at epoch 6 with val loss 0.7473507225513458


 20% 6/30 [00:08<00:35,  1.47s/it][A[ANew best model for test task CHEMBL1909212 at epoch 7 with val loss 0.7440127730369568


 23% 7/30 [00:10<00:31,  1.38s/it][A[ANew best model for test task CHEMBL1909212 at epoch 8 with val loss 0.7404064238071442


 27% 8/30 [00:12<00:35,  1.61s/it][A[ANew best model for test task CHEMBL1909212 at epoch 9 with val loss 0.7372540831565857


 30% 9/30 [00:13<00:31,  1.48s/it][A[ANew best model for test task CHEMBL1909212 at epoch 10 with val loss 0.7346248924732208


 33% 10/30 [00:14<00:27,  1.39s/it][A[ANew best model for test task CHEMBL1909212 at epoch 11 with val loss 0.7323076128959656


 37% 11/30 [00:16<00:30,  1.62s/it][A[ANew best model for test task CHEMBL1909212 at epoch 12 with val loss 0.730155885219574


 40% 12/30 [00:17<00:26,  1.48s/it][A[ANew best model for test task CHEMBL1909212 at epoch 13 with val loss 0.7283259928226471


 43% 13/30 [00:20<00:28,  1.69s/it][A[ANew best model for test task CHEMBL1909212 at epoch 14 with val loss 0.7268980741500854


 47% 14/30 [00:21<00:24,  1.54s/it][A[ANew best model for test task CHEMBL1909212 at epoch 15 with val loss 0.725595623254776


 50% 15/30 [00:22<00:21,  1.43s/it][A[ANew best model for test task CHEMBL1909212 at epoch 16 with val loss 0.724239319562912


 53% 16/30 [00:24<00:23,  1.65s/it][A[ANew best model for test task CHEMBL1909212 at epoch 17 with val loss 0.723004162311554


 57% 17/30 [00:25<00:19,  1.52s/it][A[ANew best model for test task CHEMBL1909212 at epoch 18 with val loss 0.7219153344631195


 60% 18/30 [00:28<00:20,  1.72s/it][A[ANew best model for test task CHEMBL1909212 at epoch 19 with val loss 0.7208907902240753


 63% 19/30 [00:29<00:17,  1.56s/it][A[ANew best model for test task CHEMBL1909212 at epoch 20 with val loss 0.719966322183609


 67% 20/30 [00:30<00:14,  1.45s/it][A[ANew best model for test task CHEMBL1909212 at epoch 21 with val loss 0.7191076576709747


 70% 21/30 [00:32<00:15,  1.67s/it][A[ANew best model for test task CHEMBL1909212 at epoch 22 with val loss 0.7182977497577667


 73% 22/30 [00:33<00:12,  1.53s/it][A[ANew best model for test task CHEMBL1909212 at epoch 23 with val loss 0.7175735235214233


 77% 23/30 [00:35<00:12,  1.73s/it][A[ANew best model for test task CHEMBL1909212 at epoch 24 with val loss 0.7168928980827332


 80% 24/30 [00:37<00:09,  1.57s/it][A[ANew best model for test task CHEMBL1909212 at epoch 25 with val loss 0.7163740992546082


 83% 25/30 [00:39<00:08,  1.76s/it][A[ANew best model for test task CHEMBL1909212 at epoch 26 with val loss 0.715766042470932


 87% 26/30 [00:40<00:06,  1.60s/it][A[ANew best model for test task CHEMBL1909212 at epoch 27 with val loss 0.7152182459831238


 90% 27/30 [00:41<00:04,  1.48s/it][A[ANew best model for test task CHEMBL1909212 at epoch 28 with val loss 0.7148350775241852


 93% 28/30 [00:44<00:03,  1.70s/it][A[ANew best model for test task CHEMBL1909212 at epoch 29 with val loss 0.7144843935966492


 97% 29/30 [00:45<00:01,  1.55s/it][A[ANew best model for test task CHEMBL1909212 at epoch 30 with val loss 0.7139926850795746


100% 30/30 [00:47<00:00,  1.75s/it][A[A100% 30/30 [00:47<00:00,  1.58s/it]
Finished early stopping for task CHEMBL1909212, beginning testing
Meta learning, so output of FFN is 1
Meta learning, so output of FFN is 1
Meta learning, so output of FFN is 1
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Meta learning, so output of FFN is 1
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Meta learning, so output of FFN is 1
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Meta learning, so output of FFN is 1
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Meta learning, so output of FFN is 1
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Meta learning, so output of FFN is 1
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Meta learning, so output of FFN is 1
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Meta learning, so output of FFN is 1
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Meta learning, so output of FFN is 1
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Meta learning, so output of FFN is 1
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Meta learning, so output of FFN is 1
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Meta learning, so output of FFN is 1
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Meta learning, so output of FFN is 1
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Meta learning, so output of FFN is 1
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Meta learning, so output of FFN is 1
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Meta learning, so output of FFN is 1
Loading pretrained parameter "encoder.encoder.cached_zero_vector".

  0% 0/2 [00:00<?, ?it/s][A[A

100% 2/2 [00:00<00:00, 17.18it/s][A[A100% 2/2 [00:00<00:00, 17.15it/s]

100% 1/1 [00:47<00:00, 47.67s/it][A100% 1/1 [00:47<00:00, 47.67s/it]
 62% 16/26 [08:02<07:23, 44.36s/it]
  0% 0/1 [00:00<?, ?it/s][AMeta testing on task: CHEMBL1963705
/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  0% 0/30 [00:00<?, ?it/s][A[ANew best model for test task CHEMBL1963705 at epoch 1 with val loss 0.6706514060497284


  3% 1/30 [00:01<00:30,  1.06s/it][A[ANew best model for test task CHEMBL1963705 at epoch 2 with val loss 0.6705734431743622


  7% 2/30 [00:03<00:38,  1.36s/it][A[AVal loss: 0.6705767214298248


 10% 3/30 [00:04<00:34,  1.28s/it][A[AVal loss: 0.6706328690052032


 13% 4/30 [00:06<00:39,  1.51s/it][A[AVal loss: 0.670732855796814


 17% 5/30 [00:07<00:34,  1.38s/it][A[AVal loss: 0.6708547174930573


 20% 6/30 [00:09<00:37,  1.58s/it][A[AVal loss: 0.6709305942058563


 23% 7/30 [00:10<00:32,  1.42s/it][A[AVal loss: 0.6713177263736725


 27% 8/30 [00:11<00:29,  1.33s/it][A[AVal loss: 0.6714857220649719


 30% 9/30 [00:13<00:32,  1.54s/it][A[AVal loss: 0.6714635789394379


 33% 10/30 [00:14<00:27,  1.38s/it][A[AVal loss: 0.6717703938484192


 37% 11/30 [00:16<00:30,  1.60s/it][A[AVal loss: 0.6717858016490936


 40% 12/30 [00:17<00:25,  1.44s/it][A[AVal loss: 0.6715506017208099


 43% 13/30 [00:18<00:22,  1.33s/it][A[AVal loss: 0.6715672314167023


 47% 14/30 [00:20<00:24,  1.55s/it][A[AVal loss: 0.6715405881404877


 50% 15/30 [00:21<00:21,  1.40s/it][A[AVal loss: 0.6716175377368927


 53% 16/30 [00:23<00:18,  1.30s/it][A[AVal loss: 0.671707957983017


 57% 17/30 [00:24<00:16,  1.24s/it][A[AVal loss: 0.6712155938148499


 60% 18/30 [00:26<00:17,  1.49s/it][A[AVal loss: 0.6709020435810089


 63% 19/30 [00:27<00:15,  1.37s/it][A[AVal loss: 0.670726478099823


 67% 20/30 [00:29<00:15,  1.59s/it][A[AVal loss: 0.670775443315506


 70% 21/30 [00:30<00:13,  1.45s/it][A[AVal loss: 0.6708396375179291


 73% 22/30 [00:32<00:13,  1.65s/it][A[ANew best model for test task CHEMBL1963705 at epoch 23 with val loss 0.6705405712127686


 77% 23/30 [00:33<00:10,  1.49s/it][A[ANew best model for test task CHEMBL1963705 at epoch 24 with val loss 0.6701735258102417


 80% 24/30 [00:35<00:10,  1.68s/it][A[AVal loss: 0.6702999174594879


 83% 25/30 [00:36<00:07,  1.51s/it][A[ANew best model for test task CHEMBL1963705 at epoch 26 with val loss 0.6698951721191406


 87% 26/30 [00:39<00:06,  1.70s/it][A[ANew best model for test task CHEMBL1963705 at epoch 27 with val loss 0.6696242988109589


 90% 27/30 [00:40<00:04,  1.54s/it][A[ANew best model for test task CHEMBL1963705 at epoch 28 with val loss 0.6694891452789307


 93% 28/30 [00:41<00:02,  1.43s/it][A[ANew best model for test task CHEMBL1963705 at epoch 29 with val loss 0.6694101393222809


 97% 29/30 [00:43<00:01,  1.64s/it][A[AVal loss: 0.6695496439933777


100% 30/30 [00:44<00:00,  1.49s/it][A[A100% 30/30 [00:44<00:00,  1.49s/it]
Finished early stopping for task CHEMBL1963705, beginning testing


  0% 0/2 [00:00<?, ?it/s][A[A100% 2/2 [00:00<00:00, 20.68it/s]

100% 1/1 [00:44<00:00, 44.91s/it][A100% 1/1 [00:44<00:00, 44.91s/it]
 65% 17/26 [08:47<06:40, 44.53s/it]
  0% 0/1 [00:00<?, ?it/s][AMeta testing on task: CHEMBL1963741
/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  0% 0/30 [00:00<?, ?it/s][A[ANew best model for test task CHEMBL1963741 at epoch 1 with val loss 0.7305128872394562


  3% 1/30 [00:02<01:11,  2.46s/it][A[AVal loss: 0.7321763932704926


  7% 2/30 [00:03<00:59,  2.12s/it][A[AVal loss: 0.7337439060211182


 10% 3/30 [00:06<00:59,  2.20s/it][A[AVal loss: 0.7351276278495789


 13% 4/30 [00:07<00:50,  1.95s/it][A[AVal loss: 0.7361827194690704


 17% 5/30 [00:09<00:51,  2.08s/it][A[AVal loss: 0.7371208667755127


 20% 6/30 [00:11<00:44,  1.87s/it][A[AVal loss: 0.7378296256065369


 23% 7/30 [00:13<00:46,  2.04s/it][A[AVal loss: 0.7383888065814972


 27% 8/30 [00:15<00:41,  1.87s/it][A[AVal loss: 0.73896723985672


 30% 9/30 [00:17<00:42,  2.04s/it][A[AVal loss: 0.7394908964633942


 33% 10/30 [00:20<00:42,  2.15s/it][A[AVal loss: 0.7398256361484528


 37% 11/30 [00:22<00:42,  2.23s/it][A[AVal loss: 0.7399798631668091


 40% 12/30 [00:23<00:35,  1.98s/it][A[AVal loss: 0.7401018440723419


 43% 13/30 [00:26<00:35,  2.10s/it][A[AVal loss: 0.7402009069919586


 47% 14/30 [00:27<00:30,  1.88s/it][A[AVal loss: 0.740249365568161


 50% 15/30 [00:29<00:30,  2.02s/it][A[AVal loss: 0.7402562201023102


 53% 16/30 [00:31<00:25,  1.83s/it][A[AVal loss: 0.7402996718883514


 57% 17/30 [00:33<00:25,  1.99s/it][A[AVal loss: 0.7403744161128998


 60% 18/30 [00:35<00:21,  1.81s/it][A[AVal loss: 0.7404573857784271


 63% 19/30 [00:37<00:21,  2.00s/it][A[AVal loss: 0.7403917908668518


 67% 20/30 [00:39<00:21,  2.12s/it][A[AVal loss: 0.7402778565883636


 70% 21/30 [00:41<00:17,  1.91s/it][A[AVal loss: 0.7403232157230377


 73% 22/30 [00:43<00:16,  2.06s/it][A[AVal loss: 0.7402782738208771


 77% 23/30 [00:45<00:13,  1.87s/it][A[AVal loss: 0.7401953339576721


 80% 24/30 [00:47<00:12,  2.04s/it][A[AVal loss: 0.7400400042533875


 83% 25/30 [00:49<00:09,  1.86s/it][A[AVal loss: 0.7398518919944763


 87% 26/30 [00:51<00:08,  2.03s/it][A[AVal loss: 0.7397842407226562


 90% 27/30 [00:52<00:05,  1.86s/it][A[AVal loss: 0.7397246360778809


 93% 28/30 [00:55<00:04,  2.04s/it][A[AVal loss: 0.7395344376564026


 97% 29/30 [00:57<00:02,  2.16s/it][A[AVal loss: 0.7394067943096161


100% 30/30 [00:59<00:00,  1.95s/it][A[A100% 30/30 [00:59<00:00,  1.98s/it]
Finished early stopping for task CHEMBL1963741, beginning testing


  0% 0/2 [00:00<?, ?it/s][A[A

100% 2/2 [00:01<00:00,  1.80it/s][A[A100% 2/2 [00:01<00:00,  1.80it/s]

100% 1/1 [01:00<00:00, 60.53s/it][A100% 1/1 [01:00<00:00, 60.53s/it]
 69% 18/26 [09:47<06:34, 49.33s/it]
  0% 0/1 [00:00<?, ?it/s][AMeta testing on task: CHEMBL1963934
/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  0% 0/30 [00:00<?, ?it/s][A[ANew best model for test task CHEMBL1963934 at epoch 1 with val loss 0.5797326564788818


  3% 1/30 [00:00<00:07,  3.79it/s][A[ANew best model for test task CHEMBL1963934 at epoch 2 with val loss 0.5687124729156494


  7% 2/30 [00:00<00:07,  3.80it/s][A[ANew best model for test task CHEMBL1963934 at epoch 3 with val loss 0.5576679706573486


 10% 3/30 [00:00<00:07,  3.79it/s][A[ANew best model for test task CHEMBL1963934 at epoch 4 with val loss 0.5465806126594543


 13% 4/30 [00:01<00:06,  3.79it/s][A[ANew best model for test task CHEMBL1963934 at epoch 5 with val loss 0.535637378692627


 17% 5/30 [00:01<00:06,  3.77it/s][A[ANew best model for test task CHEMBL1963934 at epoch 6 with val loss 0.5250059366226196


 20% 6/30 [00:01<00:06,  3.78it/s][A[ANew best model for test task CHEMBL1963934 at epoch 7 with val loss 0.514691948890686


 23% 7/30 [00:01<00:06,  3.75it/s][A[ANew best model for test task CHEMBL1963934 at epoch 8 with val loss 0.5046048164367676


 27% 8/30 [00:02<00:05,  3.78it/s][A[ANew best model for test task CHEMBL1963934 at epoch 9 with val loss 0.4952246844768524


 30% 9/30 [00:02<00:05,  3.78it/s][A[ANew best model for test task CHEMBL1963934 at epoch 10 with val loss 0.4863027334213257


 33% 10/30 [00:02<00:05,  3.79it/s][A[ANew best model for test task CHEMBL1963934 at epoch 11 with val loss 0.4777601361274719


 37% 11/30 [00:02<00:05,  3.78it/s][A[ANew best model for test task CHEMBL1963934 at epoch 12 with val loss 0.46984338760375977


 40% 12/30 [00:03<00:04,  3.79it/s][A[ANew best model for test task CHEMBL1963934 at epoch 13 with val loss 0.4624263346195221


 43% 13/30 [00:03<00:04,  3.80it/s][A[ANew best model for test task CHEMBL1963934 at epoch 14 with val loss 0.45537734031677246


 47% 14/30 [00:03<00:04,  3.80it/s][A[ANew best model for test task CHEMBL1963934 at epoch 15 with val loss 0.44891464710235596


 50% 15/30 [00:04<00:08,  1.79it/s][A[ANew best model for test task CHEMBL1963934 at epoch 16 with val loss 0.44281160831451416


 53% 16/30 [00:05<00:06,  2.12it/s][A[ANew best model for test task CHEMBL1963934 at epoch 17 with val loss 0.4370694160461426


 57% 17/30 [00:05<00:05,  2.44it/s][A[ANew best model for test task CHEMBL1963934 at epoch 18 with val loss 0.4317004084587097


 60% 18/30 [00:05<00:04,  2.74it/s][A[ANew best model for test task CHEMBL1963934 at epoch 19 with val loss 0.42673736810684204


 63% 19/30 [00:06<00:03,  2.98it/s][A[ANew best model for test task CHEMBL1963934 at epoch 20 with val loss 0.42209678888320923


 67% 20/30 [00:06<00:03,  3.18it/s][A[ANew best model for test task CHEMBL1963934 at epoch 21 with val loss 0.4176737070083618


 70% 21/30 [00:06<00:02,  3.34it/s][A[ANew best model for test task CHEMBL1963934 at epoch 22 with val loss 0.4134063422679901


 73% 22/30 [00:06<00:02,  3.47it/s][A[ANew best model for test task CHEMBL1963934 at epoch 23 with val loss 0.4092940390110016


 77% 23/30 [00:07<00:01,  3.55it/s][A[ANew best model for test task CHEMBL1963934 at epoch 24 with val loss 0.4054766297340393


 80% 24/30 [00:07<00:01,  3.59it/s][A[ANew best model for test task CHEMBL1963934 at epoch 25 with val loss 0.4018821120262146


 83% 25/30 [00:07<00:01,  3.62it/s][A[ANew best model for test task CHEMBL1963934 at epoch 26 with val loss 0.39862191677093506


 87% 26/30 [00:07<00:01,  3.66it/s][A[ANew best model for test task CHEMBL1963934 at epoch 27 with val loss 0.3954668641090393


 90% 27/30 [00:08<00:00,  3.69it/s][A[ANew best model for test task CHEMBL1963934 at epoch 28 with val loss 0.3925533890724182


 93% 28/30 [00:08<00:00,  3.68it/s][A[ANew best model for test task CHEMBL1963934 at epoch 29 with val loss 0.38971978425979614


 97% 29/30 [00:08<00:00,  3.67it/s][A[ANew best model for test task CHEMBL1963934 at epoch 30 with val loss 0.3870752453804016


100% 30/30 [00:09<00:00,  1.75it/s][A[A100% 30/30 [00:09<00:00,  3.02it/s]
Finished early stopping for task CHEMBL1963934, beginning testing


  0% 0/1 [00:00<?, ?it/s][A[A100% 1/1 [00:00<00:00, 39.81it/s]

100% 1/1 [00:10<00:00, 10.08s/it][A100% 1/1 [00:10<00:00, 10.08s/it]
 73% 19/26 [09:57<04:22, 37.55s/it]
  0% 0/1 [00:00<?, ?it/s][AMeta testing on task: CHEMBL2028077
/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  0% 0/30 [00:00<?, ?it/s][A[ANew best model for test task CHEMBL2028077 at epoch 1 with val loss 0.7799741625785828


  3% 1/30 [00:00<00:13,  2.09it/s][A[ANew best model for test task CHEMBL2028077 at epoch 2 with val loss 0.7756918668746948


  7% 2/30 [00:00<00:13,  2.09it/s][A[ANew best model for test task CHEMBL2028077 at epoch 3 with val loss 0.7717381119728088


 10% 3/30 [00:01<00:13,  2.06it/s][A[ANew best model for test task CHEMBL2028077 at epoch 4 with val loss 0.7682260870933533


 13% 4/30 [00:01<00:12,  2.06it/s][A[ANew best model for test task CHEMBL2028077 at epoch 5 with val loss 0.7649635672569275


 17% 5/30 [00:02<00:12,  2.08it/s][A[ANew best model for test task CHEMBL2028077 at epoch 6 with val loss 0.7619242072105408


 20% 6/30 [00:03<00:18,  1.29it/s][A[ANew best model for test task CHEMBL2028077 at epoch 7 with val loss 0.7590979337692261


 23% 7/30 [00:04<00:15,  1.47it/s][A[ANew best model for test task CHEMBL2028077 at epoch 8 with val loss 0.7565935850143433


 27% 8/30 [00:04<00:13,  1.62it/s][A[ANew best model for test task CHEMBL2028077 at epoch 9 with val loss 0.7543013691902161


 30% 9/30 [00:05<00:11,  1.76it/s][A[ANew best model for test task CHEMBL2028077 at epoch 10 with val loss 0.7520882487297058


 33% 10/30 [00:05<00:10,  1.87it/s][A[ANew best model for test task CHEMBL2028077 at epoch 11 with val loss 0.7500854730606079


 37% 11/30 [00:06<00:09,  1.94it/s][A[ANew best model for test task CHEMBL2028077 at epoch 12 with val loss 0.7481854557991028


 40% 12/30 [00:07<00:14,  1.26it/s][A[ANew best model for test task CHEMBL2028077 at epoch 13 with val loss 0.7463976144790649


 43% 13/30 [00:08<00:11,  1.44it/s][A[ANew best model for test task CHEMBL2028077 at epoch 14 with val loss 0.7446919083595276


 47% 14/30 [00:08<00:10,  1.59it/s][A[ANew best model for test task CHEMBL2028077 at epoch 15 with val loss 0.7431609630584717


 50% 15/30 [00:09<00:08,  1.69it/s][A[ANew best model for test task CHEMBL2028077 at epoch 16 with val loss 0.741671621799469


 53% 16/30 [00:09<00:07,  1.77it/s][A[ANew best model for test task CHEMBL2028077 at epoch 17 with val loss 0.7402879595756531


 57% 17/30 [00:10<00:07,  1.86it/s][A[ANew best model for test task CHEMBL2028077 at epoch 18 with val loss 0.7389975190162659


 60% 18/30 [00:11<00:09,  1.23it/s][A[ANew best model for test task CHEMBL2028077 at epoch 19 with val loss 0.7378445267677307


 63% 19/30 [00:12<00:07,  1.39it/s][A[ANew best model for test task CHEMBL2028077 at epoch 20 with val loss 0.7367293834686279


 67% 20/30 [00:12<00:06,  1.52it/s][A[ANew best model for test task CHEMBL2028077 at epoch 21 with val loss 0.7356823682785034


 70% 21/30 [00:13<00:05,  1.64it/s][A[ANew best model for test task CHEMBL2028077 at epoch 22 with val loss 0.734627902507782


 73% 22/30 [00:13<00:04,  1.75it/s][A[ANew best model for test task CHEMBL2028077 at epoch 23 with val loss 0.733646035194397


 77% 23/30 [00:13<00:03,  1.84it/s][A[ANew best model for test task CHEMBL2028077 at epoch 24 with val loss 0.7327181100845337


 80% 24/30 [00:15<00:04,  1.21it/s][A[ANew best model for test task CHEMBL2028077 at epoch 25 with val loss 0.731856644153595


 83% 25/30 [00:15<00:03,  1.36it/s][A[ANew best model for test task CHEMBL2028077 at epoch 26 with val loss 0.7310259938240051


 87% 26/30 [00:16<00:02,  1.52it/s][A[ANew best model for test task CHEMBL2028077 at epoch 27 with val loss 0.7302544713020325


 90% 27/30 [00:16<00:01,  1.65it/s][A[ANew best model for test task CHEMBL2028077 at epoch 28 with val loss 0.7294885516166687


 93% 28/30 [00:17<00:01,  1.76it/s][A[ANew best model for test task CHEMBL2028077 at epoch 29 with val loss 0.7288001179695129


 97% 29/30 [00:17<00:00,  1.83it/s][A[ANew best model for test task CHEMBL2028077 at epoch 30 with val loss 0.7280927896499634


100% 30/30 [00:18<00:00,  1.90it/s][A[A100% 30/30 [00:18<00:00,  1.63it/s]
Finished early stopping for task CHEMBL2028077, beginning testing


  0% 0/1 [00:00<?, ?it/s][A[A100% 1/1 [00:00<00:00, 17.25it/s]

100% 1/1 [00:18<00:00, 18.58s/it][A100% 1/1 [00:18<00:00, 18.58s/it]
 77% 20/26 [10:16<03:11, 31.86s/it]
  0% 0/1 [00:00<?, ?it/s][AMeta testing on task: CHEMBL2095143
/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  0% 0/30 [00:00<?, ?it/s][A[ANew best model for test task CHEMBL2095143 at epoch 1 with val loss 0.8000643849372864


  3% 1/30 [00:01<00:41,  1.43s/it][A[ANew best model for test task CHEMBL2095143 at epoch 2 with val loss 0.7956492900848389


  7% 2/30 [00:01<00:31,  1.12s/it][A[ANew best model for test task CHEMBL2095143 at epoch 3 with val loss 0.7922755479812622


 10% 3/30 [00:02<00:24,  1.11it/s][A[ANew best model for test task CHEMBL2095143 at epoch 4 with val loss 0.7885134816169739


 13% 4/30 [00:02<00:19,  1.33it/s][A[ANew best model for test task CHEMBL2095143 at epoch 5 with val loss 0.7848649024963379


 17% 5/30 [00:03<00:16,  1.54it/s][A[ANew best model for test task CHEMBL2095143 at epoch 6 with val loss 0.781352698802948


 20% 6/30 [00:03<00:13,  1.75it/s][A[ANew best model for test task CHEMBL2095143 at epoch 7 with val loss 0.7781475782394409


 23% 7/30 [00:03<00:12,  1.91it/s][A[ANew best model for test task CHEMBL2095143 at epoch 8 with val loss 0.7754169702529907


 27% 8/30 [00:04<00:10,  2.03it/s][A[ANew best model for test task CHEMBL2095143 at epoch 9 with val loss 0.7726538181304932


 30% 9/30 [00:05<00:16,  1.30it/s][A[ANew best model for test task CHEMBL2095143 at epoch 10 with val loss 0.7701361179351807


 33% 10/30 [00:06<00:13,  1.52it/s][A[ANew best model for test task CHEMBL2095143 at epoch 11 with val loss 0.767928957939148


 37% 11/30 [00:06<00:11,  1.71it/s][A[ANew best model for test task CHEMBL2095143 at epoch 12 with val loss 0.766178548336029


 40% 12/30 [00:06<00:09,  1.90it/s][A[ANew best model for test task CHEMBL2095143 at epoch 13 with val loss 0.7643225789070129


 43% 13/30 [00:07<00:08,  2.02it/s][A[ANew best model for test task CHEMBL2095143 at epoch 14 with val loss 0.7624111175537109


 47% 14/30 [00:07<00:07,  2.11it/s][A[ANew best model for test task CHEMBL2095143 at epoch 15 with val loss 0.7610225081443787


 50% 15/30 [00:08<00:06,  2.18it/s][A[ANew best model for test task CHEMBL2095143 at epoch 16 with val loss 0.7594443559646606


 53% 16/30 [00:08<00:06,  2.26it/s][A[ANew best model for test task CHEMBL2095143 at epoch 17 with val loss 0.7577280402183533


 57% 17/30 [00:09<00:09,  1.37it/s][A[ANew best model for test task CHEMBL2095143 at epoch 18 with val loss 0.756199836730957


 60% 18/30 [00:10<00:07,  1.59it/s][A[ANew best model for test task CHEMBL2095143 at epoch 19 with val loss 0.7550063133239746


 63% 19/30 [00:10<00:06,  1.77it/s][A[ANew best model for test task CHEMBL2095143 at epoch 20 with val loss 0.7536641359329224


 67% 20/30 [00:11<00:05,  1.91it/s][A[ANew best model for test task CHEMBL2095143 at epoch 21 with val loss 0.7524833679199219


 70% 21/30 [00:11<00:04,  2.03it/s][A[ANew best model for test task CHEMBL2095143 at epoch 22 with val loss 0.7514824867248535


 73% 22/30 [00:12<00:03,  2.14it/s][A[ANew best model for test task CHEMBL2095143 at epoch 23 with val loss 0.7504725456237793


 77% 23/30 [00:12<00:03,  2.24it/s][A[ANew best model for test task CHEMBL2095143 at epoch 24 with val loss 0.749329686164856


 80% 24/30 [00:12<00:02,  2.30it/s][A[ANew best model for test task CHEMBL2095143 at epoch 25 with val loss 0.7484060525894165


 83% 25/30 [00:13<00:02,  2.33it/s][A[ANew best model for test task CHEMBL2095143 at epoch 26 with val loss 0.7477572560310364


 87% 26/30 [00:13<00:01,  2.39it/s][A[ANew best model for test task CHEMBL2095143 at epoch 27 with val loss 0.7471478581428528


 90% 27/30 [00:15<00:02,  1.39it/s][A[ANew best model for test task CHEMBL2095143 at epoch 28 with val loss 0.7464771866798401


 93% 28/30 [00:15<00:01,  1.60it/s][A[ANew best model for test task CHEMBL2095143 at epoch 29 with val loss 0.7456791400909424


 97% 29/30 [00:15<00:00,  1.79it/s][A[ANew best model for test task CHEMBL2095143 at epoch 30 with val loss 0.7450114488601685


100% 30/30 [00:16<00:00,  1.94it/s][A[A100% 30/30 [00:16<00:00,  1.84it/s]
Finished early stopping for task CHEMBL2095143, beginning testing


  0% 0/1 [00:00<?, ?it/s][A[A100% 1/1 [00:00<00:00, 26.43it/s]
Warning: Found a task with targets all 0s or all 1s

100% 1/1 [00:16<00:00, 16.43s/it][A100% 1/1 [00:16<00:00, 16.43s/it]
 81% 21/26 [10:32<02:16, 27.23s/it]
  0% 0/1 [00:00<?, ?it/s][AMeta testing on task: CHEMBL2098499


  0% 0/30 [00:00<?, ?it/s][A[ANew best model for test task CHEMBL2098499 at epoch 1 with val loss 0.7451425790786743


  3% 1/30 [00:00<00:07,  3.83it/s][A[AVal loss: 0.7454056739807129


  7% 2/30 [00:00<00:06,  4.13it/s][A[AVal loss: 0.7454512715339661


 10% 3/30 [00:00<00:06,  4.34it/s][A[AVal loss: 0.746192455291748


 13% 4/30 [00:00<00:05,  4.40it/s][A[AVal loss: 0.7466092109680176


 17% 5/30 [00:01<00:05,  4.45it/s][A[AVal loss: 0.74783855676651


 20% 6/30 [00:01<00:05,  4.50it/s][A[AVal loss: 0.7483787536621094


 23% 7/30 [00:01<00:05,  4.53it/s][A[AVal loss: 0.7486383318901062


 27% 8/30 [00:02<00:11,  1.92it/s][A[AVal loss: 0.7488036751747131


 30% 9/30 [00:02<00:08,  2.34it/s][A[AVal loss: 0.7491669654846191


 33% 10/30 [00:03<00:07,  2.76it/s][A[AVal loss: 0.750265896320343


 37% 11/30 [00:03<00:06,  3.15it/s][A[AVal loss: 0.7508790493011475


 40% 12/30 [00:03<00:05,  3.52it/s][A[AVal loss: 0.7509662508964539


 43% 13/30 [00:03<00:04,  3.85it/s][A[AVal loss: 0.7508783340454102


 47% 14/30 [00:04<00:03,  4.08it/s][A[AVal loss: 0.7504346966743469


 50% 15/30 [00:04<00:03,  4.21it/s][A[AVal loss: 0.7505369186401367


 53% 16/30 [00:04<00:03,  4.31it/s][A[AVal loss: 0.7507089972496033


 57% 17/30 [00:04<00:03,  4.33it/s][A[AVal loss: 0.7507811784744263


 60% 18/30 [00:04<00:02,  4.40it/s][A[AVal loss: 0.7516528964042664


 63% 19/30 [00:05<00:02,  4.51it/s][A[AVal loss: 0.7521145343780518


 67% 20/30 [00:05<00:02,  4.58it/s][A[AVal loss: 0.752259373664856


 70% 21/30 [00:05<00:01,  4.66it/s][A[AVal loss: 0.7526384592056274


 73% 22/30 [00:05<00:01,  4.68it/s][A[AVal loss: 0.7532920837402344


 77% 23/30 [00:06<00:03,  1.96it/s][A[AVal loss: 0.7537158131599426


 80% 24/30 [00:07<00:02,  2.36it/s][A[AVal loss: 0.7537713646888733


 83% 25/30 [00:07<00:01,  2.77it/s][A[AVal loss: 0.7541643381118774


 87% 26/30 [00:07<00:01,  3.13it/s][A[AVal loss: 0.7545996904373169


 90% 27/30 [00:07<00:00,  3.43it/s][A[AVal loss: 0.7550387978553772


 93% 28/30 [00:08<00:00,  3.70it/s][A[AVal loss: 0.7551337480545044


 97% 29/30 [00:08<00:00,  3.95it/s][A[AVal loss: 0.7548934817314148


100% 30/30 [00:08<00:00,  4.17it/s][A[A100% 30/30 [00:08<00:00,  3.55it/s]
Finished early stopping for task CHEMBL2098499, beginning testing


  0% 0/1 [00:00<?, ?it/s][A[A100% 1/1 [00:00<00:00, 43.42it/s]

100% 1/1 [00:08<00:00,  8.59s/it][A100% 1/1 [00:08<00:00,  8.59s/it]
 85% 22/26 [10:41<01:26, 21.64s/it]
  0% 0/1 [00:00<?, ?it/s][AMeta testing on task: CHEMBL2114797
/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  0% 0/30 [00:00<?, ?it/s][A[ANew best model for test task CHEMBL2114797 at epoch 1 with val loss 0.7463735342025757


  3% 1/30 [00:00<00:09,  2.92it/s][A[AVal loss: 0.7488834261894226


  7% 2/30 [00:00<00:09,  3.05it/s][A[AVal loss: 0.7512235641479492


 10% 3/30 [00:00<00:08,  3.15it/s][A[AVal loss: 0.7534900307655334


 13% 4/30 [00:01<00:08,  3.20it/s][A[AVal loss: 0.7555988430976868


 17% 5/30 [00:01<00:07,  3.29it/s][A[AVal loss: 0.7577590346336365


 20% 6/30 [00:02<00:14,  1.66it/s][A[AVal loss: 0.7597765922546387


 23% 7/30 [00:03<00:11,  1.96it/s][A[AVal loss: 0.7615548968315125


 27% 8/30 [00:03<00:09,  2.25it/s][A[AVal loss: 0.7632808685302734


 30% 9/30 [00:03<00:08,  2.51it/s][A[AVal loss: 0.764814555644989


 33% 10/30 [00:03<00:07,  2.73it/s][A[AVal loss: 0.7660892009735107


 37% 11/30 [00:04<00:06,  2.90it/s][A[AVal loss: 0.7676891088485718


 40% 12/30 [00:04<00:05,  3.03it/s][A[AVal loss: 0.769228458404541


 43% 13/30 [00:04<00:05,  3.10it/s][A[AVal loss: 0.7705245018005371


 47% 14/30 [00:05<00:04,  3.23it/s][A[AVal loss: 0.7717141509056091


 50% 15/30 [00:05<00:04,  3.28it/s][A[AVal loss: 0.7727726101875305


 53% 16/30 [00:05<00:04,  3.26it/s][A[AVal loss: 0.773858368396759


 57% 17/30 [00:06<00:04,  3.23it/s][A[AVal loss: 0.774875819683075


 60% 18/30 [00:06<00:03,  3.23it/s][A[AVal loss: 0.7758050560951233


 63% 19/30 [00:07<00:06,  1.66it/s][A[AVal loss: 0.7764789462089539


 67% 20/30 [00:07<00:05,  1.95it/s][A[AVal loss: 0.7774248123168945


 70% 21/30 [00:08<00:04,  2.24it/s][A[AVal loss: 0.7781375050544739


 73% 22/30 [00:08<00:03,  2.48it/s][A[AVal loss: 0.7789168953895569


 77% 23/30 [00:08<00:02,  2.68it/s][A[AVal loss: 0.7793704271316528


 80% 24/30 [00:09<00:02,  2.85it/s][A[AVal loss: 0.7797624468803406


 83% 25/30 [00:09<00:01,  3.01it/s][A[AVal loss: 0.7800910472869873


 87% 26/30 [00:09<00:01,  3.08it/s][A[AVal loss: 0.780514657497406


 90% 27/30 [00:10<00:00,  3.11it/s][A[AVal loss: 0.7807544469833374


 93% 28/30 [00:10<00:00,  3.12it/s][A[AVal loss: 0.781163215637207


 97% 29/30 [00:10<00:00,  3.13it/s][A[AVal loss: 0.7816022634506226


100% 30/30 [00:12<00:00,  1.63it/s][A[A100% 30/30 [00:12<00:00,  2.50it/s]
Finished early stopping for task CHEMBL2114797, beginning testing


  0% 0/1 [00:00<?, ?it/s][A[A100% 1/1 [00:00<00:00, 27.81it/s]

100% 1/1 [00:12<00:00, 12.16s/it][A100% 1/1 [00:12<00:00, 12.16s/it]
 88% 23/26 [10:53<00:56, 18.80s/it]
  0% 0/1 [00:00<?, ?it/s][AMeta testing on task: CHEMBL3215116
/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  0% 0/30 [00:00<?, ?it/s][A[ANew best model for test task CHEMBL3215116 at epoch 1 with val loss 0.7604543566703796


  3% 1/30 [00:00<00:11,  2.47it/s][A[ANew best model for test task CHEMBL3215116 at epoch 2 with val loss 0.7601336240768433


  7% 2/30 [00:00<00:11,  2.48it/s][A[ANew best model for test task CHEMBL3215116 at epoch 3 with val loss 0.7598196268081665


 10% 3/30 [00:01<00:10,  2.47it/s][A[ANew best model for test task CHEMBL3215116 at epoch 4 with val loss 0.7595064640045166


 13% 4/30 [00:01<00:10,  2.50it/s][A[ANew best model for test task CHEMBL3215116 at epoch 5 with val loss 0.7592154741287231


 17% 5/30 [00:02<00:10,  2.45it/s][A[ANew best model for test task CHEMBL3215116 at epoch 6 with val loss 0.7589584589004517


 20% 6/30 [00:02<00:09,  2.44it/s][A[ANew best model for test task CHEMBL3215116 at epoch 7 with val loss 0.7586785554885864


 23% 7/30 [00:03<00:16,  1.41it/s][A[ANew best model for test task CHEMBL3215116 at epoch 8 with val loss 0.7584348917007446


 27% 8/30 [00:04<00:13,  1.62it/s][A[ANew best model for test task CHEMBL3215116 at epoch 9 with val loss 0.7582310438156128


 30% 9/30 [00:04<00:11,  1.81it/s][A[ANew best model for test task CHEMBL3215116 at epoch 10 with val loss 0.7579876780509949


 33% 10/30 [00:05<00:10,  1.97it/s][A[ANew best model for test task CHEMBL3215116 at epoch 11 with val loss 0.7577537298202515


 37% 11/30 [00:05<00:09,  2.11it/s][A[ANew best model for test task CHEMBL3215116 at epoch 12 with val loss 0.7575488090515137


 40% 12/30 [00:05<00:08,  2.20it/s][A[ANew best model for test task CHEMBL3215116 at epoch 13 with val loss 0.7573446035385132


 43% 13/30 [00:06<00:07,  2.26it/s][A[ANew best model for test task CHEMBL3215116 at epoch 14 with val loss 0.7571573257446289


 47% 14/30 [00:06<00:06,  2.29it/s][A[ANew best model for test task CHEMBL3215116 at epoch 15 with val loss 0.7569793462753296


 50% 15/30 [00:08<00:10,  1.37it/s][A[ANew best model for test task CHEMBL3215116 at epoch 16 with val loss 0.7568260431289673


 53% 16/30 [00:08<00:08,  1.59it/s][A[ANew best model for test task CHEMBL3215116 at epoch 17 with val loss 0.7566468119621277


 57% 17/30 [00:08<00:07,  1.78it/s][A[ANew best model for test task CHEMBL3215116 at epoch 18 with val loss 0.7564972639083862


 60% 18/30 [00:09<00:06,  1.95it/s][A[ANew best model for test task CHEMBL3215116 at epoch 19 with val loss 0.7563327550888062


 63% 19/30 [00:09<00:05,  2.06it/s][A[ANew best model for test task CHEMBL3215116 at epoch 20 with val loss 0.7561724185943604


 67% 20/30 [00:10<00:04,  2.20it/s][A[ANew best model for test task CHEMBL3215116 at epoch 21 with val loss 0.7560316920280457


 70% 21/30 [00:10<00:04,  2.24it/s][A[ANew best model for test task CHEMBL3215116 at epoch 22 with val loss 0.7558958530426025


 73% 22/30 [00:11<00:05,  1.35it/s][A[ANew best model for test task CHEMBL3215116 at epoch 23 with val loss 0.755763053894043


 77% 23/30 [00:12<00:04,  1.57it/s][A[ANew best model for test task CHEMBL3215116 at epoch 24 with val loss 0.755645215511322


 80% 24/30 [00:12<00:03,  1.76it/s][A[ANew best model for test task CHEMBL3215116 at epoch 25 with val loss 0.7555011510848999


 83% 25/30 [00:13<00:02,  1.93it/s][A[ANew best model for test task CHEMBL3215116 at epoch 26 with val loss 0.7553743124008179


 87% 26/30 [00:13<00:01,  2.05it/s][A[ANew best model for test task CHEMBL3215116 at epoch 27 with val loss 0.7552534937858582


 90% 27/30 [00:13<00:01,  2.17it/s][A[ANew best model for test task CHEMBL3215116 at epoch 28 with val loss 0.7551447153091431


 93% 28/30 [00:14<00:00,  2.24it/s][A[ANew best model for test task CHEMBL3215116 at epoch 29 with val loss 0.7550543546676636


 97% 29/30 [00:15<00:00,  1.35it/s][A[ANew best model for test task CHEMBL3215116 at epoch 30 with val loss 0.7549514770507812


100% 30/30 [00:16<00:00,  1.56it/s][A[A100% 30/30 [00:16<00:00,  1.85it/s]
Finished early stopping for task CHEMBL3215116, beginning testing


  0% 0/1 [00:00<?, ?it/s][A[A100% 1/1 [00:00<00:00, 28.04it/s]

100% 1/1 [00:16<00:00, 16.39s/it][A100% 1/1 [00:16<00:00, 16.39s/it]
 92% 24/26 [11:09<00:36, 18.08s/it]
  0% 0/1 [00:00<?, ?it/s][AMeta testing on task: CHEMBL3215176
/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.
  warnings.warn("The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad "


  0% 0/30 [00:00<?, ?it/s][A[ANew best model for test task CHEMBL3215176 at epoch 1 with val loss 0.8050594329833984


  3% 1/30 [00:00<00:07,  3.63it/s][A[ANew best model for test task CHEMBL3215176 at epoch 2 with val loss 0.8030121326446533


  7% 2/30 [00:00<00:07,  3.68it/s][A[ANew best model for test task CHEMBL3215176 at epoch 3 with val loss 0.8013286590576172


 10% 3/30 [00:00<00:07,  3.70it/s][A[ANew best model for test task CHEMBL3215176 at epoch 4 with val loss 0.7994605302810669


 13% 4/30 [00:01<00:07,  3.70it/s][A[ANew best model for test task CHEMBL3215176 at epoch 5 with val loss 0.7975953221321106


 17% 5/30 [00:01<00:06,  3.69it/s][A[ANew best model for test task CHEMBL3215176 at epoch 6 with val loss 0.7960783839225769


 20% 6/30 [00:01<00:06,  3.72it/s][A[ANew best model for test task CHEMBL3215176 at epoch 7 with val loss 0.7944017648696899


 23% 7/30 [00:01<00:06,  3.72it/s][A[ANew best model for test task CHEMBL3215176 at epoch 8 with val loss 0.7928759455680847


 27% 8/30 [00:02<00:05,  3.69it/s][A[ANew best model for test task CHEMBL3215176 at epoch 9 with val loss 0.7913779616355896


 30% 9/30 [00:02<00:05,  3.65it/s][A[ANew best model for test task CHEMBL3215176 at epoch 10 with val loss 0.7898122668266296


 33% 10/30 [00:02<00:05,  3.65it/s][A[ANew best model for test task CHEMBL3215176 at epoch 11 with val loss 0.7887064814567566


 37% 11/30 [00:02<00:05,  3.62it/s][A[ANew best model for test task CHEMBL3215176 at epoch 12 with val loss 0.7877299189567566


 40% 12/30 [00:04<00:10,  1.74it/s][A[ANew best model for test task CHEMBL3215176 at epoch 13 with val loss 0.7864367365837097


 43% 13/30 [00:04<00:08,  2.06it/s][A[ANew best model for test task CHEMBL3215176 at epoch 14 with val loss 0.785308301448822


 47% 14/30 [00:04<00:06,  2.39it/s][A[ANew best model for test task CHEMBL3215176 at epoch 15 with val loss 0.7845959663391113


 50% 15/30 [00:05<00:05,  2.67it/s][A[ANew best model for test task CHEMBL3215176 at epoch 16 with val loss 0.7837195992469788


 53% 16/30 [00:05<00:04,  2.93it/s][A[ANew best model for test task CHEMBL3215176 at epoch 17 with val loss 0.7829391956329346


 57% 17/30 [00:05<00:04,  3.15it/s][A[ANew best model for test task CHEMBL3215176 at epoch 18 with val loss 0.7820047736167908


 60% 18/30 [00:05<00:03,  3.26it/s][A[ANew best model for test task CHEMBL3215176 at epoch 19 with val loss 0.781212329864502


 63% 19/30 [00:06<00:03,  3.33it/s][A[ANew best model for test task CHEMBL3215176 at epoch 20 with val loss 0.7804033160209656


 67% 20/30 [00:06<00:02,  3.42it/s][A[ANew best model for test task CHEMBL3215176 at epoch 21 with val loss 0.7792431712150574


 70% 21/30 [00:06<00:02,  3.47it/s][A[ANew best model for test task CHEMBL3215176 at epoch 22 with val loss 0.7784480452537537


 73% 22/30 [00:06<00:02,  3.51it/s][A[ANew best model for test task CHEMBL3215176 at epoch 23 with val loss 0.7775729298591614


 77% 23/30 [00:07<00:01,  3.58it/s][A[ANew best model for test task CHEMBL3215176 at epoch 24 with val loss 0.7768941521644592


 80% 24/30 [00:07<00:01,  3.62it/s][A[ANew best model for test task CHEMBL3215176 at epoch 25 with val loss 0.776287853717804


 83% 25/30 [00:07<00:01,  3.64it/s][A[ANew best model for test task CHEMBL3215176 at epoch 26 with val loss 0.7759344577789307


 87% 26/30 [00:09<00:02,  1.73it/s][A[ANew best model for test task CHEMBL3215176 at epoch 27 with val loss 0.7752313613891602


 90% 27/30 [00:09<00:01,  2.07it/s][A[ANew best model for test task CHEMBL3215176 at epoch 28 with val loss 0.7746707201004028


 93% 28/30 [00:09<00:00,  2.40it/s][A[ANew best model for test task CHEMBL3215176 at epoch 29 with val loss 0.7741782069206238


 97% 29/30 [00:09<00:00,  2.66it/s][A[ANew best model for test task CHEMBL3215176 at epoch 30 with val loss 0.7734470963478088


100% 30/30 [00:10<00:00,  2.87it/s][A[A100% 30/30 [00:10<00:00,  2.95it/s]
Finished early stopping for task CHEMBL3215176, beginning testing


  0% 0/1 [00:00<?, ?it/s][A[A100% 1/1 [00:00<00:00, 36.00it/s]
Warning: Found a task with targets all 0s or all 1s

100% 1/1 [00:10<00:00, 10.31s/it][A100% 1/1 [00:10<00:00, 10.31s/it]
 96% 25/26 [11:20<00:15, 15.75s/it]
  0% 0/1 [00:00<?, ?it/s][AMeta testing on task: CHEMBL918058


  0% 0/30 [00:00<?, ?it/s][A[ANew best model for test task CHEMBL918058 at epoch 1 with val loss 0.8181967735290527


  3% 1/30 [00:00<00:09,  2.96it/s][A[ANew best model for test task CHEMBL918058 at epoch 2 with val loss 0.8129161596298218


  7% 2/30 [00:00<00:09,  3.03it/s][A[ANew best model for test task CHEMBL918058 at epoch 3 with val loss 0.8083840608596802


 10% 3/30 [00:00<00:08,  3.09it/s][A[ANew best model for test task CHEMBL918058 at epoch 4 with val loss 0.804535984992981


 13% 4/30 [00:01<00:08,  3.13it/s][A[ANew best model for test task CHEMBL918058 at epoch 5 with val loss 0.8005273938179016


 17% 5/30 [00:01<00:07,  3.17it/s][A[ANew best model for test task CHEMBL918058 at epoch 6 with val loss 0.7969014644622803


 20% 6/30 [00:01<00:07,  3.18it/s][A[ANew best model for test task CHEMBL918058 at epoch 7 with val loss 0.7934379577636719


 23% 7/30 [00:02<00:07,  3.19it/s][A[ANew best model for test task CHEMBL918058 at epoch 8 with val loss 0.7903984189033508


 27% 8/30 [00:02<00:06,  3.20it/s][A[ANew best model for test task CHEMBL918058 at epoch 9 with val loss 0.7878376841545105


 30% 9/30 [00:02<00:06,  3.18it/s][A[ANew best model for test task CHEMBL918058 at epoch 10 with val loss 0.7853816747665405


 33% 10/30 [00:04<00:12,  1.62it/s][A[ANew best model for test task CHEMBL918058 at epoch 11 with val loss 0.7830575108528137


 37% 11/30 [00:04<00:10,  1.89it/s][A[ANew best model for test task CHEMBL918058 at epoch 12 with val loss 0.7808716893196106


 40% 12/30 [00:04<00:08,  2.16it/s][A[ANew best model for test task CHEMBL918058 at epoch 13 with val loss 0.7791282534599304


 43% 13/30 [00:05<00:07,  2.39it/s][A[ANew best model for test task CHEMBL918058 at epoch 14 with val loss 0.7772179841995239


 47% 14/30 [00:05<00:06,  2.59it/s][A[ANew best model for test task CHEMBL918058 at epoch 15 with val loss 0.7753930687904358


 50% 15/30 [00:05<00:05,  2.75it/s][A[ANew best model for test task CHEMBL918058 at epoch 16 with val loss 0.7740005254745483


 53% 16/30 [00:06<00:04,  2.86it/s][A[ANew best model for test task CHEMBL918058 at epoch 17 with val loss 0.772422194480896


 57% 17/30 [00:06<00:04,  2.94it/s][A[ANew best model for test task CHEMBL918058 at epoch 18 with val loss 0.7710058093070984


 60% 18/30 [00:06<00:03,  3.02it/s][A[ANew best model for test task CHEMBL918058 at epoch 19 with val loss 0.7698196172714233


 63% 19/30 [00:06<00:03,  3.09it/s][A[ANew best model for test task CHEMBL918058 at epoch 20 with val loss 0.7685819268226624


 67% 20/30 [00:07<00:03,  3.07it/s][A[ANew best model for test task CHEMBL918058 at epoch 21 with val loss 0.7672517895698547


 70% 21/30 [00:07<00:02,  3.05it/s][A[ANew best model for test task CHEMBL918058 at epoch 22 with val loss 0.7661572694778442


 73% 22/30 [00:07<00:02,  3.05it/s][A[ANew best model for test task CHEMBL918058 at epoch 23 with val loss 0.7652567028999329


 77% 23/30 [00:09<00:04,  1.61it/s][A[ANew best model for test task CHEMBL918058 at epoch 24 with val loss 0.7642679214477539


 80% 24/30 [00:09<00:03,  1.87it/s][A[ANew best model for test task CHEMBL918058 at epoch 25 with val loss 0.7633617520332336


 83% 25/30 [00:09<00:02,  2.14it/s][A[ANew best model for test task CHEMBL918058 at epoch 26 with val loss 0.762438952922821


 87% 26/30 [00:10<00:01,  2.37it/s][A[ANew best model for test task CHEMBL918058 at epoch 27 with val loss 0.7615180015563965


 90% 27/30 [00:10<00:01,  2.54it/s][A[ANew best model for test task CHEMBL918058 at epoch 28 with val loss 0.7605897784233093


 93% 28/30 [00:10<00:00,  2.67it/s][A[ANew best model for test task CHEMBL918058 at epoch 29 with val loss 0.7599946856498718


 97% 29/30 [00:11<00:00,  2.76it/s][A[ANew best model for test task CHEMBL918058 at epoch 30 with val loss 0.7594611644744873


100% 30/30 [00:11<00:00,  2.87it/s][A[A100% 30/30 [00:11<00:00,  2.60it/s]
Finished early stopping for task CHEMBL918058, beginning testing


  0% 0/1 [00:00<?, ?it/s][A[A100% 1/1 [00:00<00:00, 36.60it/s]
Warning: Found a task with targets all 0s or all 1s

100% 1/1 [00:11<00:00, 11.66s/it][A100% 1/1 [00:11<00:00, 11.66s/it]
100% 26/26 [11:31<00:00, 14.52s/it]100% 26/26 [11:31<00:00, 26.61s/it]
Took 691.789458990097 seconds to complete meta testing
Model test auc = 0.556639
1-fold cross validation
Seed 0 ==> test auc = 0.556639
Overall test auc = 0.556639 +/- 0.000000
/home/ec2-user/molecule-metalearning/chemprop/chemprop/utils.py:52: RuntimeWarning: Mean of empty slice
  results_dict[task_name]['avg_score'] = np.nanmean(all_scores[:, task_num])
/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1665: RuntimeWarning: Degrees of freedom <= 0 for slice.
  keepdims=keepdims)
Total running time was 6616.262433052063 seconds

Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Meta learning, so output of FFN is 1
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Meta learning, so output of FFN is 1
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Meta learning, so output of FFN is 1
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Meta learning, so output of FFN is 1
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Meta learning, so output of FFN is 1
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Meta learning, so output of FFN is 1
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Meta learning, so output of FFN is 1
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Meta learning, so output of FFN is 1
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Meta learning, so output of FFN is 1
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda
Meta learning, so output of FFN is 1
Loading pretrained parameter "encoder.encoder.cached_zero_vector".
Loading pretrained parameter "encoder.encoder.W_i.weight".
Loading pretrained parameter "encoder.encoder.W_h.weight".
Loading pretrained parameter "encoder.encoder.W_o.weight".
Loading pretrained parameter "encoder.encoder.W_o.bias".
Loading pretrained parameter "ffn.1.weight".
Loading pretrained parameter "ffn.1.bias".
Loading pretrained parameter "ffn.4.weight".
Loading pretrained parameter "ffn.4.bias".
Moving model to cuda

wandb: Waiting for W&B process to finish, PID 13377
wandb: Program ended successfully.
wandb: Run summary:
wandb:                                  _runtime 6618.048875808716
wandb:                                     Epoch 29
wandb:                                _timestamp 1595911557.1734061
wandb:                                     _step 44460
wandb:             CHEMBL1033994_adaptation_loss 0.6055331230163574
wandb:             CHEMBL1119333_adaptation_loss 0.3617022633552551
wandb:             CHEMBL1217000_adaptation_loss 0.5926439166069031
wandb:             CHEMBL1243965_adaptation_loss 0.5374957323074341
wandb:             CHEMBL1243966_adaptation_loss 0.653547465801239
wandb:             CHEMBL1243967_adaptation_loss 0.4434334635734558
wandb:             CHEMBL1243968_adaptation_loss 0.3663344085216522
wandb:             CHEMBL1243970_adaptation_loss 0.32267120480537415
wandb:             CHEMBL1243972_adaptation_loss 0.0963103175163269
wandb:             CHEMBL1243976_adaptation_loss 0.4908522665500641
wandb:             CHEMBL1246087_adaptation_loss 0.641838014125824
wandb:             CHEMBL1246088_adaptation_loss 0.6939116716384888
wandb:             CHEMBL1613762_adaptation_loss 0.19923003017902374
wandb:             CHEMBL1613779_adaptation_loss 0.7489487528800964
wandb:             CHEMBL1613785_adaptation_loss 0.7315071821212769
wandb:             CHEMBL1613787_adaptation_loss 0.6146453022956848
wandb:             CHEMBL1613807_adaptation_loss 0.28174811601638794
wandb:             CHEMBL1613813_adaptation_loss 0.8317769765853882
wandb:             CHEMBL1613814_adaptation_loss 0.6533639430999756
wandb:             CHEMBL1613817_adaptation_loss 0.35791677236557007
wandb:             CHEMBL1613853_adaptation_loss 0.9747731685638428
wandb:             CHEMBL1613861_adaptation_loss 0.41825082898139954
wandb:             CHEMBL1613864_adaptation_loss 0.37450748682022095
wandb:             CHEMBL1613867_adaptation_loss 0.21914245188236237
wandb:             CHEMBL1613870_adaptation_loss 0.48242613673210144
wandb:             CHEMBL1613871_adaptation_loss 0.1815609186887741
wandb:             CHEMBL1613874_adaptation_loss 0.7324902415275574
wandb:             CHEMBL1613876_adaptation_loss 0.6511728167533875
wandb:             CHEMBL1613884_adaptation_loss 0.3894149661064148
wandb:             CHEMBL1613890_adaptation_loss 0.9268874526023865
wandb:             CHEMBL1613897_adaptation_loss 0.6738450527191162
wandb:             CHEMBL1613898_adaptation_loss 0.5787786841392517
wandb:                           batch_meta_loss 0.42761579155921936
wandb:                                     GNorm 0.12856638772313364
wandb:                                     PNorm 44.1541777095191
wandb:             CHEMBL1613904_adaptation_loss 0.9850160479545593
wandb:             CHEMBL1613907_adaptation_loss 0.13354644179344177
wandb:             CHEMBL1613926_adaptation_loss 0.6555922031402588
wandb:             CHEMBL1613928_adaptation_loss 0.7341952323913574
wandb:             CHEMBL1613929_adaptation_loss 0.4958483874797821
wandb:             CHEMBL1613941_adaptation_loss 0.42888593673706055
wandb:             CHEMBL1613942_adaptation_loss 0.6105759143829346
wandb:             CHEMBL1613947_adaptation_loss 0.8308651447296143
wandb:             CHEMBL1613949_adaptation_loss 0.7040956616401672
wandb:             CHEMBL1613950_adaptation_loss 0.676612377166748
wandb:             CHEMBL1613955_adaptation_loss 0.6589743494987488
wandb:             CHEMBL1613962_adaptation_loss 0.442627876996994
wandb:             CHEMBL1613967_adaptation_loss 0.9050970077514648
wandb:             CHEMBL1613981_adaptation_loss 0.6223222613334656
wandb:             CHEMBL1613991_adaptation_loss 0.42019715905189514
wandb:             CHEMBL1613997_adaptation_loss 0.3855423033237457
wandb:             CHEMBL1614001_adaptation_loss 0.23905132710933685
wandb:             CHEMBL1614004_adaptation_loss 0.3407667577266693
wandb:             CHEMBL1614016_adaptation_loss 0.25241389870643616
wandb:             CHEMBL1614030_adaptation_loss 0.6822347044944763
wandb:             CHEMBL1614034_adaptation_loss 0.18783311545848846
wandb:             CHEMBL1614035_adaptation_loss 0.46242719888687134
wandb:             CHEMBL1614049_adaptation_loss 0.7041117548942566
wandb:             CHEMBL1614053_adaptation_loss 0.7266068458557129
wandb:             CHEMBL1614063_adaptation_loss 0.2873276472091675
wandb:             CHEMBL1614065_adaptation_loss 0.25962039828300476
wandb:             CHEMBL1614066_adaptation_loss 0.22206340730190277
wandb:             CHEMBL1614069_adaptation_loss 0.73053377866745
wandb:             CHEMBL1614072_adaptation_loss 1.0599554777145386
wandb:             CHEMBL1614084_adaptation_loss 0.42303672432899475
wandb:             CHEMBL1614091_adaptation_loss 0.28715085983276367
wandb:             CHEMBL1614092_adaptation_loss 0.7425202131271362
wandb:             CHEMBL1614097_adaptation_loss 0.6418312788009644
wandb:             CHEMBL1614098_adaptation_loss 0.3129684627056122
wandb:             CHEMBL1614104_adaptation_loss 0.3534926474094391
wandb:             CHEMBL1614105_adaptation_loss 0.16596736013889313
wandb:             CHEMBL1614109_adaptation_loss 0.804398238658905
wandb:             CHEMBL1614128_adaptation_loss 0.3261200785636902
wandb:             CHEMBL1614131_adaptation_loss 0.24542047083377838
wandb:             CHEMBL1614132_adaptation_loss 0.6664587259292603
wandb:             CHEMBL1614138_adaptation_loss 0.5725324749946594
wandb:             CHEMBL1614155_adaptation_loss 0.6028730869293213
wandb:             CHEMBL1614158_adaptation_loss 0.5362660884857178
wandb:             CHEMBL1614167_adaptation_loss 0.4664485454559326
wandb:             CHEMBL1614171_adaptation_loss 0.22445712983608246
wandb:             CHEMBL1614175_adaptation_loss 0.8095849752426147
wandb:             CHEMBL1614185_adaptation_loss 0.6164278388023376
wandb:             CHEMBL1614197_adaptation_loss 0.11045198887586594
wandb:             CHEMBL1614199_adaptation_loss 0.690038800239563
wandb:             CHEMBL1614215_adaptation_loss 0.8137496709823608
wandb:             CHEMBL1614216_adaptation_loss 0.7055121064186096
wandb:             CHEMBL1614218_adaptation_loss 0.7710019946098328
wandb:             CHEMBL1614225_adaptation_loss 0.646437406539917
wandb:             CHEMBL1614244_adaptation_loss 0.7290956377983093
wandb:             CHEMBL1614247_adaptation_loss 0.7074564099311829
wandb:             CHEMBL1614252_adaptation_loss 0.7319182753562927
wandb:             CHEMBL1614255_adaptation_loss 0.7366510033607483
wandb:             CHEMBL1614272_adaptation_loss 0.6751707792282104
wandb:             CHEMBL1614276_adaptation_loss 0.7166868448257446
wandb:             CHEMBL1614287_adaptation_loss 0.4567505121231079
wandb:             CHEMBL1614288_adaptation_loss 0.5423123240470886
wandb:             CHEMBL1614290_adaptation_loss 0.17519956827163696
wandb:             CHEMBL1614295_adaptation_loss 0.10900413990020752
wandb:             CHEMBL1614301_adaptation_loss 0.5835524797439575
wandb:             CHEMBL1614304_adaptation_loss 0.6661474704742432
wandb:             CHEMBL1614309_adaptation_loss 0.5822877287864685
wandb:             CHEMBL1614311_adaptation_loss 0.5771835446357727
wandb:             CHEMBL1614314_adaptation_loss 0.28917691111564636
wandb:             CHEMBL1614319_adaptation_loss 0.6649008989334106
wandb:             CHEMBL1614320_adaptation_loss 0.6800426840782166
wandb:             CHEMBL1614321_adaptation_loss 0.6840977072715759
wandb:             CHEMBL1614328_adaptation_loss 0.5884299278259277
wandb:             CHEMBL1614329_adaptation_loss 0.7595007419586182
wandb:             CHEMBL1614344_adaptation_loss 0.5435467958450317
wandb:             CHEMBL1614356_adaptation_loss 0.5780760645866394
wandb:             CHEMBL1614363_adaptation_loss 0.5584408640861511
wandb:             CHEMBL1614385_adaptation_loss 0.4086717665195465
wandb:             CHEMBL1614388_adaptation_loss 0.7882578372955322
wandb:             CHEMBL1614393_adaptation_loss 0.6986654996871948
wandb:             CHEMBL1614395_adaptation_loss 0.36161181330680847
wandb:             CHEMBL1614403_adaptation_loss 0.3846137523651123
wandb:             CHEMBL1614423_adaptation_loss 0.7144479155540466
wandb:             CHEMBL1614425_adaptation_loss 0.8571057915687561
wandb:             CHEMBL1614433_adaptation_loss 0.9298983216285706
wandb:             CHEMBL1614434_adaptation_loss 0.5785203576087952
wandb:             CHEMBL1614456_adaptation_loss 0.2958616316318512
wandb:             CHEMBL1614466_adaptation_loss 0.3685940206050873
wandb:             CHEMBL1614469_adaptation_loss 0.39680907130241394
wandb:             CHEMBL1614477_adaptation_loss 0.7891169786453247
wandb:             CHEMBL1614478_adaptation_loss 0.17524121701717377
wandb:             CHEMBL1614480_adaptation_loss 0.5186355113983154
wandb:             CHEMBL1614484_adaptation_loss 0.4059563875198364
wandb:             CHEMBL1614492_adaptation_loss 0.725051760673523
wandb:             CHEMBL1614499_adaptation_loss 0.686611533164978
wandb:             CHEMBL1614503_adaptation_loss 0.6543670892715454
wandb:             CHEMBL1614504_adaptation_loss 0.5131900906562805
wandb:             CHEMBL1614509_adaptation_loss 0.5661352872848511
wandb:             CHEMBL1614512_adaptation_loss 0.663435161113739
wandb:             CHEMBL1614514_adaptation_loss 0.5632592439651489
wandb:             CHEMBL1614515_adaptation_loss 0.5444139242172241
wandb:             CHEMBL1614516_adaptation_loss 0.26098331809043884
wandb:             CHEMBL1614522_adaptation_loss 0.6074115633964539
wandb:             CHEMBL1614524_adaptation_loss 0.337356835603714
wandb:             CHEMBL1614528_adaptation_loss 0.6019075512886047
wandb:             CHEMBL1614547_adaptation_loss 0.6193346977233887
wandb:             CHEMBL1614548_adaptation_loss 0.4489862620830536
wandb:             CHEMBL1614549_adaptation_loss 0.4845609962940216
wandb:             CHEMBL1614550_adaptation_loss 0.43592119216918945
wandb:             CHEMBL1614554_adaptation_loss 0.6558542251586914
wandb:             CHEMBL1676103_adaptation_loss 0.8551594018936157
wandb:             CHEMBL1737860_adaptation_loss 0.16853336989879608
wandb:             CHEMBL1737863_adaptation_loss 0.46396544575691223
wandb:             CHEMBL1737865_adaptation_loss 0.744147777557373
wandb:             CHEMBL1737868_adaptation_loss 0.5610827207565308
wandb:             CHEMBL1737910_adaptation_loss 0.700356125831604
wandb:             CHEMBL1737912_adaptation_loss 0.5011129975318909
wandb:             CHEMBL1737942_adaptation_loss 0.6833094358444214
wandb:             CHEMBL1737951_adaptation_loss 0.6910409331321716
wandb:             CHEMBL1737961_adaptation_loss 0.3476113975048065
wandb:             CHEMBL1737966_adaptation_loss 0.48538893461227417
wandb:             CHEMBL1737967_adaptation_loss 0.4452807605266571
wandb:             CHEMBL1737977_adaptation_loss 0.24017854034900665
wandb:             CHEMBL1737978_adaptation_loss 0.5194970965385437
wandb:             CHEMBL1737979_adaptation_loss 0.3733147382736206
wandb:             CHEMBL1738025_adaptation_loss 0.8108941316604614
wandb:             CHEMBL1738040_adaptation_loss 0.5982537269592285
wandb:             CHEMBL1738043_adaptation_loss 0.719127357006073
wandb:             CHEMBL1738079_adaptation_loss 0.18480435013771057
wandb:             CHEMBL1738080_adaptation_loss 0.6486831903457642
wandb:             CHEMBL1738091_adaptation_loss 0.5460473895072937
wandb:             CHEMBL1738097_adaptation_loss 0.6969704627990723
wandb:             CHEMBL1738164_adaptation_loss 0.5858330130577087
wandb:             CHEMBL1738171_adaptation_loss 0.5736554861068726
wandb:             CHEMBL1738183_adaptation_loss 0.8506529331207275
wandb:             CHEMBL1738197_adaptation_loss 0.629294216632843
wandb:             CHEMBL1738242_adaptation_loss 0.4479970932006836
wandb:             CHEMBL1738249_adaptation_loss 0.5753208994865417
wandb:             CHEMBL1738253_adaptation_loss 0.2942807078361511
wandb:             CHEMBL1738319_adaptation_loss 0.6911422610282898
wandb:             CHEMBL1738325_adaptation_loss 0.3175830543041229
wandb:             CHEMBL1738362_adaptation_loss 0.532666802406311
wandb:             CHEMBL1738369_adaptation_loss 0.7135612368583679
wandb:             CHEMBL1738371_adaptation_loss 0.5761152505874634
wandb:             CHEMBL1738391_adaptation_loss 0.7303375005722046
wandb:             CHEMBL1738400_adaptation_loss 0.5859371423721313
wandb:             CHEMBL1738402_adaptation_loss 0.6498762369155884
wandb:             CHEMBL1738407_adaptation_loss 0.5411137938499451
wandb:             CHEMBL1738408_adaptation_loss 0.1369890421628952
wandb:             CHEMBL1738414_adaptation_loss 0.30180272459983826
wandb:             CHEMBL1738418_adaptation_loss 0.6842056512832642
wandb:             CHEMBL1738422_adaptation_loss 0.23873315751552582
wandb:             CHEMBL1738424_adaptation_loss 0.5820662975311279
wandb:             CHEMBL1738430_adaptation_loss 0.496425598859787
wandb:             CHEMBL1738438_adaptation_loss 0.15811821818351746
wandb:             CHEMBL1738482_adaptation_loss 0.304354727268219
wandb:             CHEMBL1738485_adaptation_loss 0.5512238144874573
wandb:             CHEMBL1738494_adaptation_loss 0.37146517634391785
wandb:             CHEMBL1738495_adaptation_loss 0.1736527681350708
wandb:             CHEMBL1738497_adaptation_loss 0.6547425389289856
wandb:             CHEMBL1738502_adaptation_loss 0.36824190616607666
wandb:             CHEMBL1738510_adaptation_loss 0.6802133321762085
wandb:             CHEMBL1738512_adaptation_loss 0.26892825961112976
wandb:             CHEMBL1738513_adaptation_loss 0.5966185927391052
wandb:             CHEMBL1738552_adaptation_loss 0.6172987818717957
wandb:             CHEMBL1738575_adaptation_loss 0.5081599950790405
wandb:             CHEMBL1738578_adaptation_loss 0.5060323476791382
wandb:             CHEMBL1738579_adaptation_loss 0.7143043279647827
wandb:             CHEMBL1738593_adaptation_loss 0.5288457870483398
wandb:             CHEMBL1738599_adaptation_loss 0.5290643572807312
wandb:             CHEMBL1738602_adaptation_loss 0.6713507771492004
wandb:             CHEMBL1738610_adaptation_loss 0.6409379839897156
wandb:             CHEMBL1738611_adaptation_loss 0.21473117172718048
wandb:             CHEMBL1738632_adaptation_loss 0.3914366066455841
wandb:             CHEMBL1738633_adaptation_loss 0.20248712599277496
wandb:             CHEMBL1738639_adaptation_loss 0.6217350363731384
wandb:             CHEMBL1738642_adaptation_loss 0.6139944791793823
wandb:             CHEMBL1738670_adaptation_loss 0.28794118762016296
wandb:             CHEMBL1738673_adaptation_loss 0.3432425856590271
wandb:             CHEMBL1738679_adaptation_loss 0.6682165861129761
wandb:             CHEMBL1738682_adaptation_loss 0.6699510216712952
wandb:             CHEMBL1794296_adaptation_loss 0.7157067060470581
wandb:             CHEMBL1794303_adaptation_loss 0.19491994380950928
wandb:             CHEMBL1794320_adaptation_loss 0.32747122645378113
wandb:             CHEMBL1794327_adaptation_loss 0.489236056804657
wandb:             CHEMBL1794336_adaptation_loss 0.7547935247421265
wandb:             CHEMBL1794356_adaptation_loss 0.2856098711490631
wandb:             CHEMBL1794365_adaptation_loss 0.3004581034183502
wandb:             CHEMBL1794383_adaptation_loss 0.3354056477546692
wandb:             CHEMBL1794387_adaptation_loss 0.6644858121871948
wandb:             CHEMBL1794393_adaptation_loss 0.6991986036300659
wandb:             CHEMBL1794396_adaptation_loss 0.2757171094417572
wandb:             CHEMBL1794410_adaptation_loss 0.47323286533355713
wandb:             CHEMBL1794413_adaptation_loss 0.20515625178813934
wandb:             CHEMBL1794445_adaptation_loss 0.4549526274204254
wandb:             CHEMBL1794452_adaptation_loss 0.37955427169799805
wandb:             CHEMBL1794457_adaptation_loss 0.3338736295700073
wandb:             CHEMBL1794460_adaptation_loss 0.5112965703010559
wandb:             CHEMBL1794467_adaptation_loss 0.606777548789978
wandb:             CHEMBL1794475_adaptation_loss 0.6981760859489441
wandb:             CHEMBL1794484_adaptation_loss 0.6889364719390869
wandb:             CHEMBL1794494_adaptation_loss 0.3010401427745819
wandb:             CHEMBL1794497_adaptation_loss 0.17119702696800232
wandb:             CHEMBL1794499_adaptation_loss 0.7332888841629028
wandb:             CHEMBL1794508_adaptation_loss 0.39247429370880127
wandb:             CHEMBL1794516_adaptation_loss 0.27678415179252625
wandb:             CHEMBL1794522_adaptation_loss 0.6558443307876587
wandb:             CHEMBL1794528_adaptation_loss 0.5055546760559082
wandb:             CHEMBL1794531_adaptation_loss 0.3560641407966614
wandb:             CHEMBL1794548_adaptation_loss 0.4141065776348114
wandb:             CHEMBL1794566_adaptation_loss 0.2559017241001129
wandb:             CHEMBL1794570_adaptation_loss 0.4367389678955078
wandb:             CHEMBL1794571_adaptation_loss 0.7859264612197876
wandb:             CHEMBL1794573_adaptation_loss 0.3312986493110657
wandb:             CHEMBL1794574_adaptation_loss 0.16646665334701538
wandb:             CHEMBL1794578_adaptation_loss 0.7325253486633301
wandb:             CHEMBL1794581_adaptation_loss 0.5875797271728516
wandb:             CHEMBL1863510_adaptation_loss 0.20985335111618042
wandb:             CHEMBL1863512_adaptation_loss 0.15729950368404388
wandb:             CHEMBL1909084_adaptation_loss 0.24443405866622925
wandb:             CHEMBL1909086_adaptation_loss 0.40142425894737244
wandb:             CHEMBL1909087_adaptation_loss 0.3885749876499176
wandb:             CHEMBL1909088_adaptation_loss 0.4410362243652344
wandb:             CHEMBL1909089_adaptation_loss 0.4325510263442993
wandb:             CHEMBL1909090_adaptation_loss 0.4039143919944763
wandb:             CHEMBL1909091_adaptation_loss 0.22374756634235382
wandb:             CHEMBL1909093_adaptation_loss 0.20219659805297852
wandb:             CHEMBL1909094_adaptation_loss 0.3736327290534973
wandb:             CHEMBL1909095_adaptation_loss 0.2499227523803711
wandb:             CHEMBL1909097_adaptation_loss 0.2085208147764206
wandb:             CHEMBL1909102_adaptation_loss 0.29425448179244995
wandb:             CHEMBL1909103_adaptation_loss 0.19538694620132446
wandb:             CHEMBL1909104_adaptation_loss 0.49282076954841614
wandb:             CHEMBL1909105_adaptation_loss 0.3038066029548645
wandb:             CHEMBL1909106_adaptation_loss 0.19977787137031555
wandb:             CHEMBL1909107_adaptation_loss 0.2152433544397354
wandb:             CHEMBL1909108_adaptation_loss 0.3450264632701874
wandb:             CHEMBL1909110_adaptation_loss 0.39519035816192627
wandb:             CHEMBL1909111_adaptation_loss 0.3590051233768463
wandb:             CHEMBL1909114_adaptation_loss 0.2931951880455017
wandb:             CHEMBL1909115_adaptation_loss 0.22921320796012878
wandb:             CHEMBL1909116_adaptation_loss 0.21172374486923218
wandb:             CHEMBL1909121_adaptation_loss 0.3591453433036804
wandb:             CHEMBL1909123_adaptation_loss 0.19064350426197052
wandb:             CHEMBL1909124_adaptation_loss 0.2570064663887024
wandb:             CHEMBL1909130_adaptation_loss 0.2414867877960205
wandb:             CHEMBL1909131_adaptation_loss 0.25701573491096497
wandb:             CHEMBL1909132_adaptation_loss 0.261488139629364
wandb:             CHEMBL1909134_adaptation_loss 0.2613356113433838
wandb:             CHEMBL1909135_adaptation_loss 0.23218028247356415
wandb:             CHEMBL1909136_adaptation_loss 0.3709002733230591
wandb:             CHEMBL1909138_adaptation_loss 0.2648507058620453
wandb:             CHEMBL1909140_adaptation_loss 0.22301732003688812
wandb:             CHEMBL1909142_adaptation_loss 0.23642627894878387
wandb:             CHEMBL1909145_adaptation_loss 0.23100784420967102
wandb:             CHEMBL1909148_adaptation_loss 0.22733542323112488
wandb:             CHEMBL1909150_adaptation_loss 0.313690721988678
wandb:             CHEMBL1909156_adaptation_loss 0.25057947635650635
wandb:             CHEMBL1909157_adaptation_loss 0.25830331444740295
wandb:             CHEMBL1909158_adaptation_loss 0.1065654531121254
wandb:             CHEMBL1909159_adaptation_loss 0.319602370262146
wandb:             CHEMBL1909165_adaptation_loss 0.16370265185832977
wandb:             CHEMBL1909169_adaptation_loss 0.1925337314605713
wandb:             CHEMBL1909170_adaptation_loss 0.29829666018486023
wandb:             CHEMBL1909171_adaptation_loss 0.3806082010269165
wandb:             CHEMBL1909172_adaptation_loss 0.21717454493045807
wandb:             CHEMBL1909173_adaptation_loss 0.34141024947166443
wandb:             CHEMBL1909174_adaptation_loss 0.3700188994407654
wandb:             CHEMBL1909180_adaptation_loss 0.14528454840183258
wandb:             CHEMBL1909181_adaptation_loss 0.12964050471782684
wandb:             CHEMBL1909182_adaptation_loss 0.23427577316761017
wandb:             CHEMBL1909186_adaptation_loss 0.13142690062522888
wandb:             CHEMBL1909190_adaptation_loss 0.12906594574451447
wandb:             CHEMBL1909191_adaptation_loss 0.2663804292678833
wandb:             CHEMBL1909200_adaptation_loss 0.17652565240859985
wandb:             CHEMBL1909201_adaptation_loss 0.23028653860092163
wandb:             CHEMBL1909203_adaptation_loss 0.10594639182090759
wandb:             CHEMBL1909204_adaptation_loss 0.24717944860458374
wandb:             CHEMBL1909205_adaptation_loss 0.15004095435142517
wandb:             CHEMBL1909206_adaptation_loss 0.10968856513500214
wandb:             CHEMBL1909213_adaptation_loss 0.13260182738304138
wandb:             CHEMBL1909215_adaptation_loss 0.32138964533805847
wandb:             CHEMBL1963686_adaptation_loss 0.6909440755844116
wandb:             CHEMBL1963687_adaptation_loss 0.6884461641311646
wandb:             CHEMBL1963688_adaptation_loss 0.6387993693351746
wandb:             CHEMBL1963689_adaptation_loss 0.36995741724967957
wandb:             CHEMBL1963690_adaptation_loss 0.7143841981887817
wandb:             CHEMBL1963691_adaptation_loss 0.7162225842475891
wandb:             CHEMBL1963692_adaptation_loss 0.622819185256958
wandb:             CHEMBL1963693_adaptation_loss 0.4889521598815918
wandb:             CHEMBL1963694_adaptation_loss 0.3879544734954834
wandb:             CHEMBL1963695_adaptation_loss 0.6528963446617126
wandb:             CHEMBL1963696_adaptation_loss 0.6419984698295593
wandb:             CHEMBL1963697_adaptation_loss 0.4465072453022003
wandb:             CHEMBL1963698_adaptation_loss 1.2426186800003052
wandb:             CHEMBL1963699_adaptation_loss 0.4381794333457947
wandb:             CHEMBL1963701_adaptation_loss 0.7251087427139282
wandb:             CHEMBL1963702_adaptation_loss 0.5152396559715271
wandb:             CHEMBL1963703_adaptation_loss 0.6647796630859375
wandb:             CHEMBL1963704_adaptation_loss 0.710148811340332
wandb:             CHEMBL1963706_adaptation_loss 0.6126704216003418
wandb:             CHEMBL1963707_adaptation_loss 0.627641499042511
wandb:             CHEMBL1963708_adaptation_loss 0.799211859703064
wandb:             CHEMBL1963710_adaptation_loss 0.6164648532867432
wandb:             CHEMBL1963711_adaptation_loss 0.23998118937015533
wandb:             CHEMBL1963712_adaptation_loss 0.5533877015113831
wandb:             CHEMBL1963714_adaptation_loss 0.4950605034828186
wandb:             CHEMBL1963715_adaptation_loss 0.8138294219970703
wandb:             CHEMBL1963716_adaptation_loss 0.28956303000450134
wandb:             CHEMBL1963717_adaptation_loss 0.6670866012573242
wandb:             CHEMBL1963718_adaptation_loss 0.5901719331741333
wandb:             CHEMBL1963719_adaptation_loss 0.657211184501648
wandb:             CHEMBL1963720_adaptation_loss 0.5351895093917847
wandb:             CHEMBL1963721_adaptation_loss 0.6698365211486816
wandb:             CHEMBL1963722_adaptation_loss 0.6485719084739685
wandb:             CHEMBL1963723_adaptation_loss 0.6877174377441406
wandb:             CHEMBL1963724_adaptation_loss 0.5557112693786621
wandb:             CHEMBL1963725_adaptation_loss 0.5872458219528198
wandb:             CHEMBL1963727_adaptation_loss 0.642889678478241
wandb:             CHEMBL1963728_adaptation_loss 0.25542059540748596
wandb:             CHEMBL1963731_adaptation_loss 0.6296175718307495
wandb:             CHEMBL1963733_adaptation_loss 0.4993528127670288
wandb:             CHEMBL1963734_adaptation_loss 0.4124988317489624
wandb:             CHEMBL1963735_adaptation_loss 0.6096893548965454
wandb:             CHEMBL1963736_adaptation_loss 0.3583946228027344
wandb:             CHEMBL1963737_adaptation_loss 0.4376964867115021
wandb:             CHEMBL1963738_adaptation_loss 0.6904393434524536
wandb:             CHEMBL1963739_adaptation_loss 0.38206660747528076
wandb:             CHEMBL1963740_adaptation_loss 0.62287437915802
wandb:             CHEMBL1963742_adaptation_loss 0.5244931578636169
wandb:             CHEMBL1963743_adaptation_loss 0.5002086758613586
wandb:             CHEMBL1963744_adaptation_loss 0.6064085364341736
wandb:             CHEMBL1963745_adaptation_loss 0.58027583360672
wandb:             CHEMBL1963746_adaptation_loss 0.5823783874511719
wandb:             CHEMBL1963747_adaptation_loss 0.4468100965023041
wandb:             CHEMBL1963748_adaptation_loss 0.6752259135246277
wandb:             CHEMBL1963749_adaptation_loss 0.6784889101982117
wandb:             CHEMBL1963750_adaptation_loss 0.7068109512329102
wandb:             CHEMBL1963751_adaptation_loss 0.5001038908958435
wandb:             CHEMBL1963752_adaptation_loss 0.5201864242553711
wandb:             CHEMBL1963753_adaptation_loss 0.31879299879074097
wandb:             CHEMBL1963754_adaptation_loss 0.6013565063476562
wandb:             CHEMBL1963756_adaptation_loss 0.6295637488365173
wandb:             CHEMBL1963757_adaptation_loss 0.8810177445411682
wandb:             CHEMBL1963758_adaptation_loss 0.4517977833747864
wandb:             CHEMBL1963759_adaptation_loss 0.35313135385513306
wandb:             CHEMBL1963760_adaptation_loss 0.43490904569625854
wandb:             CHEMBL1963761_adaptation_loss 0.3226892948150635
wandb:             CHEMBL1963763_adaptation_loss 0.5006961822509766
wandb:             CHEMBL1963764_adaptation_loss 0.8831531405448914
wandb:             CHEMBL1963765_adaptation_loss 0.3083454966545105
wandb:             CHEMBL1963766_adaptation_loss 0.5312045812606812
wandb:             CHEMBL1963767_adaptation_loss 0.41133755445480347
wandb:             CHEMBL1963768_adaptation_loss 0.5265237092971802
wandb:             CHEMBL1963770_adaptation_loss 0.3588278889656067
wandb:             CHEMBL1963771_adaptation_loss 0.6845707297325134
wandb:             CHEMBL1963772_adaptation_loss 0.6126416325569153
wandb:             CHEMBL1963773_adaptation_loss 0.6470922827720642
wandb:             CHEMBL1963775_adaptation_loss 0.5182366967201233
wandb:             CHEMBL1963776_adaptation_loss 0.3938452899456024
wandb:             CHEMBL1963777_adaptation_loss 0.49358314275741577
wandb:             CHEMBL1963778_adaptation_loss 0.597977340221405
wandb:             CHEMBL1963779_adaptation_loss 0.6471476554870605
wandb:             CHEMBL1963780_adaptation_loss 0.28390148282051086
wandb:             CHEMBL1963782_adaptation_loss 0.5527049899101257
wandb:             CHEMBL1963783_adaptation_loss 0.6637872457504272
wandb:             CHEMBL1963785_adaptation_loss 0.5729544758796692
wandb:             CHEMBL1963786_adaptation_loss 0.5926443934440613
wandb:             CHEMBL1963787_adaptation_loss 0.5441070199012756
wandb:             CHEMBL1963789_adaptation_loss 0.6644729971885681
wandb:             CHEMBL1963790_adaptation_loss 0.7310968041419983
wandb:             CHEMBL1963791_adaptation_loss 0.6748013496398926
wandb:             CHEMBL1963792_adaptation_loss 0.39813295006752014
wandb:             CHEMBL1963793_adaptation_loss 0.4908178150653839
wandb:             CHEMBL1963794_adaptation_loss 0.9053174257278442
wandb:             CHEMBL1963795_adaptation_loss 0.6044750809669495
wandb:             CHEMBL1963796_adaptation_loss 0.5589956045150757
wandb:             CHEMBL1963797_adaptation_loss 0.3655010759830475
wandb:             CHEMBL1963798_adaptation_loss 0.40419822931289673
wandb:             CHEMBL1963799_adaptation_loss 0.651828408241272
wandb:             CHEMBL1963800_adaptation_loss 0.6264551877975464
wandb:             CHEMBL1963801_adaptation_loss 0.5166632533073425
wandb:             CHEMBL1963802_adaptation_loss 0.6376022100448608
wandb:             CHEMBL1963803_adaptation_loss 0.4524155855178833
wandb:             CHEMBL1963804_adaptation_loss 0.9529650211334229
wandb:             CHEMBL1963805_adaptation_loss 0.7933174967765808
wandb:             CHEMBL1963806_adaptation_loss 0.5900893807411194
wandb:             CHEMBL1963807_adaptation_loss 0.632935643196106
wandb:             CHEMBL1963809_adaptation_loss 0.40947842597961426
wandb:             CHEMBL1963810_adaptation_loss 0.6392309665679932
wandb:             CHEMBL1963811_adaptation_loss 0.6360898613929749
wandb:             CHEMBL1963812_adaptation_loss 0.6965972781181335
wandb:             CHEMBL1963813_adaptation_loss 0.3435186743736267
wandb:             CHEMBL1963814_adaptation_loss 0.6277337074279785
wandb:             CHEMBL1963815_adaptation_loss 0.5728921890258789
wandb:             CHEMBL1963816_adaptation_loss 0.4171003997325897
wandb:             CHEMBL1963817_adaptation_loss 0.5807681679725647
wandb:             CHEMBL1963818_adaptation_loss 0.6501049399375916
wandb:             CHEMBL1963819_adaptation_loss 0.6809775233268738
wandb:             CHEMBL1963820_adaptation_loss 0.44675424695014954
wandb:             CHEMBL1963821_adaptation_loss 0.6475719213485718
wandb:             CHEMBL1963823_adaptation_loss 0.6983564496040344
wandb:             CHEMBL1963824_adaptation_loss 0.6238737106323242
wandb:             CHEMBL1963825_adaptation_loss 0.6546029448509216
wandb:             CHEMBL1963826_adaptation_loss 0.7811183929443359
wandb:             CHEMBL1963827_adaptation_loss 0.6131411790847778
wandb:             CHEMBL1963828_adaptation_loss 0.33423981070518494
wandb:             CHEMBL1963829_adaptation_loss 0.3283654451370239
wandb:             CHEMBL1963831_adaptation_loss 0.6999329328536987
wandb:             CHEMBL1963832_adaptation_loss 0.5948388576507568
wandb:             CHEMBL1963833_adaptation_loss 0.3440154194831848
wandb:             CHEMBL1963834_adaptation_loss 0.7281034588813782
wandb:             CHEMBL1963836_adaptation_loss 0.5358994603157043
wandb:             CHEMBL1963837_adaptation_loss 0.49350252747535706
wandb:             CHEMBL1963838_adaptation_loss 0.5834223628044128
wandb:             CHEMBL1963846_adaptation_loss 0.6057416796684265
wandb:             CHEMBL1963867_adaptation_loss 0.46308040618896484
wandb:             CHEMBL1963893_adaptation_loss 0.3625423312187195
wandb:             CHEMBL1963898_adaptation_loss 0.6833192110061646
wandb:             CHEMBL1963907_adaptation_loss 0.6142625212669373
wandb:             CHEMBL1963910_adaptation_loss 0.5250932574272156
wandb:             CHEMBL1963915_adaptation_loss 0.5021330118179321
wandb:             CHEMBL1963916_adaptation_loss 0.7184329032897949
wandb:             CHEMBL1963918_adaptation_loss 0.5960570573806763
wandb:             CHEMBL1963930_adaptation_loss 0.2223799228668213
wandb:             CHEMBL1963933_adaptation_loss 0.2809993028640747
wandb:             CHEMBL1963937_adaptation_loss 0.611280620098114
wandb:             CHEMBL1963938_adaptation_loss 0.3347175717353821
wandb:             CHEMBL1963940_adaptation_loss 0.568228006362915
wandb:             CHEMBL1963947_adaptation_loss 0.24204039573669434
wandb:             CHEMBL1963966_adaptation_loss 0.32798293232917786
wandb:             CHEMBL1963968_adaptation_loss 0.6773021221160889
wandb:             CHEMBL1963969_adaptation_loss 0.49290281534194946
wandb:             CHEMBL1963971_adaptation_loss 0.3960462510585785
wandb:             CHEMBL1963974_adaptation_loss 0.5012522339820862
wandb:             CHEMBL1963983_adaptation_loss 0.41824036836624146
wandb:             CHEMBL1964000_adaptation_loss 0.5811771750450134
wandb:             CHEMBL1964005_adaptation_loss 0.29952484369277954
wandb:             CHEMBL1964010_adaptation_loss 0.6405734419822693
wandb:             CHEMBL1964015_adaptation_loss 0.5134877562522888
wandb:             CHEMBL1964023_adaptation_loss 0.5522130131721497
wandb:             CHEMBL1964081_adaptation_loss 0.6702663898468018
wandb:             CHEMBL1964095_adaptation_loss 0.6898033618927002
wandb:             CHEMBL1964096_adaptation_loss 0.4370709955692291
wandb:             CHEMBL1964100_adaptation_loss 0.33167028427124023
wandb:             CHEMBL1964101_adaptation_loss 0.6157772541046143
wandb:             CHEMBL1964102_adaptation_loss 0.49710386991500854
wandb:             CHEMBL1964103_adaptation_loss 0.6762217283248901
wandb:             CHEMBL1964104_adaptation_loss 0.6355426907539368
wandb:             CHEMBL1964105_adaptation_loss 0.6535636186599731
wandb:             CHEMBL1964106_adaptation_loss 0.44349437952041626
wandb:             CHEMBL1964108_adaptation_loss 0.12168373912572861
wandb:             CHEMBL1964111_adaptation_loss 0.552695095539093
wandb:             CHEMBL1964112_adaptation_loss 0.3229201138019562
wandb:             CHEMBL1964114_adaptation_loss 0.6148735880851746
wandb:             CHEMBL1964115_adaptation_loss 0.6284264326095581
wandb:             CHEMBL1964116_adaptation_loss 0.6242979168891907
wandb:             CHEMBL1964117_adaptation_loss 0.6450626850128174
wandb:             CHEMBL1964118_adaptation_loss 0.6111041307449341
wandb:             CHEMBL1964119_adaptation_loss 0.5742365717887878
wandb:             CHEMBL2028073_adaptation_loss 0.5062651038169861
wandb:             CHEMBL2028074_adaptation_loss 0.667547345161438
wandb:             CHEMBL2028075_adaptation_loss 0.8514331579208374
wandb:             CHEMBL2028076_adaptation_loss 0.39972251653671265
wandb:             CHEMBL2114715_adaptation_loss 0.5448318719863892
wandb:             CHEMBL2114716_adaptation_loss 0.4938160479068756
wandb:             CHEMBL2114719_adaptation_loss 0.5003944635391235
wandb:             CHEMBL2114725_adaptation_loss 0.7335488200187683
wandb:             CHEMBL2114727_adaptation_loss 0.45163294672966003
wandb:             CHEMBL2114728_adaptation_loss 0.39749911427497864
wandb:             CHEMBL2114742_adaptation_loss 0.6879904270172119
wandb:             CHEMBL2114748_adaptation_loss 0.6445549130439758
wandb:             CHEMBL2114752_adaptation_loss 0.4620370864868164
wandb:             CHEMBL2114753_adaptation_loss 0.2516648769378662
wandb:             CHEMBL2114761_adaptation_loss 0.7856760621070862
wandb:             CHEMBL2114764_adaptation_loss 0.644761860370636
wandb:             CHEMBL2114771_adaptation_loss 0.7588086724281311
wandb:             CHEMBL2114791_adaptation_loss 0.252398818731308
wandb:             CHEMBL2114811_adaptation_loss 0.318200945854187
wandb:             CHEMBL2114814_adaptation_loss 0.6945632696151733
wandb:             CHEMBL2114816_adaptation_loss 0.6825826168060303
wandb:             CHEMBL2114818_adaptation_loss 0.2777411937713623
wandb:             CHEMBL2114820_adaptation_loss 0.4424704611301422
wandb:             CHEMBL2114821_adaptation_loss 0.6737154722213745
wandb:             CHEMBL2114823_adaptation_loss 0.5104056000709534
wandb:             CHEMBL2114825_adaptation_loss 0.42555803060531616
wandb:             CHEMBL2114827_adaptation_loss 0.128287672996521
wandb:             CHEMBL2114829_adaptation_loss 0.616844654083252
wandb:             CHEMBL2114830_adaptation_loss 0.53658127784729
wandb:             CHEMBL2114839_adaptation_loss 0.546538233757019
wandb:             CHEMBL2114842_adaptation_loss 0.5318581461906433
wandb:             CHEMBL2114844_adaptation_loss 0.5965938568115234
wandb:             CHEMBL2114847_adaptation_loss 0.48020392656326294
wandb:             CHEMBL2114850_adaptation_loss 0.2792247533798218
wandb:             CHEMBL2114852_adaptation_loss 0.3193494975566864
wandb:             CHEMBL2114857_adaptation_loss 0.769054651260376
wandb:             CHEMBL2114858_adaptation_loss 0.6415814161300659
wandb:             CHEMBL2114863_adaptation_loss 0.6107999086380005
wandb:             CHEMBL2114865_adaptation_loss 0.267373263835907
wandb:             CHEMBL2114872_adaptation_loss 0.3536909520626068
wandb:             CHEMBL2114874_adaptation_loss 0.59101402759552
wandb:             CHEMBL2114882_adaptation_loss 0.24394556879997253
wandb:             CHEMBL2114896_adaptation_loss 0.7151296734809875
wandb:             CHEMBL2114899_adaptation_loss 0.18098239600658417
wandb:             CHEMBL2114909_adaptation_loss 0.24365952610969543
wandb:             CHEMBL2114916_adaptation_loss 0.5552731156349182
wandb:             CHEMBL2114926_adaptation_loss 0.7124329805374146
wandb:             CHEMBL2114928_adaptation_loss 0.30566948652267456
wandb:             CHEMBL2114930_adaptation_loss 0.224068745970726
wandb:             CHEMBL2114931_adaptation_loss 0.2618626654148102
wandb:             CHEMBL2114932_adaptation_loss 0.22899344563484192
wandb:             CHEMBL2354206_adaptation_loss 0.34295597672462463
wandb:             CHEMBL2354207_adaptation_loss 0.3609093725681305
wandb:             CHEMBL2354217_adaptation_loss 0.5677397847175598
wandb:             CHEMBL2354227_adaptation_loss 0.5093613862991333
wandb:             CHEMBL2354228_adaptation_loss 0.713477611541748
wandb:             CHEMBL2354248_adaptation_loss 0.13824345171451569
wandb:             CHEMBL2354256_adaptation_loss 0.364212304353714
wandb:             CHEMBL2354269_adaptation_loss 0.395105242729187
wandb:             CHEMBL2354274_adaptation_loss 0.3373577296733856
wandb:             CHEMBL2354276_adaptation_loss 0.4974272549152374
wandb:             CHEMBL2354289_adaptation_loss 0.6520968675613403
wandb:             CHEMBL2354292_adaptation_loss 0.4337673485279083
wandb:             CHEMBL2354303_adaptation_loss 0.4480467140674591
wandb:             CHEMBL2354305_adaptation_loss 0.493610680103302
wandb:             CHEMBL2354308_adaptation_loss 0.2923218309879303
wandb:             CHEMBL2378059_adaptation_loss 0.295655757188797
wandb:             CHEMBL2449559_adaptation_loss 0.2964826822280884
wandb:             CHEMBL3214794_adaptation_loss 0.5754735469818115
wandb:             CHEMBL3214801_adaptation_loss 0.5278192758560181
wandb:             CHEMBL3214812_adaptation_loss 0.6476289629936218
wandb:             CHEMBL3214816_adaptation_loss 0.514086127281189
wandb:             CHEMBL3214851_adaptation_loss 0.5828276872634888
wandb:             CHEMBL3214906_adaptation_loss 0.711639404296875
wandb:             CHEMBL3214907_adaptation_loss 0.692206621170044
wandb:             CHEMBL3214929_adaptation_loss 0.595641016960144
wandb:             CHEMBL3214930_adaptation_loss 0.5421820282936096
wandb:             CHEMBL3214958_adaptation_loss 1.8946430683135986
wandb:             CHEMBL3214959_adaptation_loss 0.5890220999717712
wandb:             CHEMBL3214970_adaptation_loss 0.49682703614234924
wandb:             CHEMBL3214992_adaptation_loss 0.570396900177002
wandb:             CHEMBL3214993_adaptation_loss 0.5504295825958252
wandb:             CHEMBL3214997_adaptation_loss 0.6930179595947266
wandb:             CHEMBL3215006_adaptation_loss 0.549189567565918
wandb:             CHEMBL3215013_adaptation_loss 0.3596550524234772
wandb:             CHEMBL3215025_adaptation_loss 0.3799416124820709
wandb:             CHEMBL3215034_adaptation_loss 0.756921112537384
wandb:             CHEMBL3215092_adaptation_loss 0.705036461353302
wandb:             CHEMBL3215096_adaptation_loss 0.678086519241333
wandb:             CHEMBL3215112_adaptation_loss 0.2966929078102112
wandb:             CHEMBL3215128_adaptation_loss 0.4367108941078186
wandb:             CHEMBL3215154_adaptation_loss 0.7541118860244751
wandb:             CHEMBL3215157_adaptation_loss 0.3450642228126526
wandb:             CHEMBL3215158_adaptation_loss 0.5357354283332825
wandb:             CHEMBL3215171_adaptation_loss 0.19545233249664307
wandb:             CHEMBL3215185_adaptation_loss 0.2494097203016281
wandb:             CHEMBL3215187_adaptation_loss 0.3418331742286682
wandb:             CHEMBL3215216_adaptation_loss 0.3448001742362976
wandb:             CHEMBL3215220_adaptation_loss 0.40192562341690063
wandb:             CHEMBL3215227_adaptation_loss 0.6018527150154114
wandb:             CHEMBL3215228_adaptation_loss 0.3690798878669739
wandb:             CHEMBL3215276_adaptation_loss 0.27753230929374695
wandb:             CHEMBL3215277_adaptation_loss 0.6114892363548279
wandb:             CHEMBL3215288_adaptation_loss 0.636624276638031
wandb:              CHEMBL829401_adaptation_loss 0.2072475403547287
wandb:              CHEMBL830839_adaptation_loss 0.18010129034519196
wandb:              CHEMBL830842_adaptation_loss 0.2072475403547287
wandb:              CHEMBL914418_adaptation_loss 0.382021963596344
wandb:             CHEMBL1614259_adaptation_loss 0.6254074573516846
wandb:             CHEMBL1614336_adaptation_loss 0.7193521857261658
wandb:             CHEMBL1794350_adaptation_loss 0.36877280473709106
wandb:             CHEMBL1794438_adaptation_loss 0.7022861242294312
wandb:             CHEMBL1909109_adaptation_loss 0.3610123097896576
wandb:             CHEMBL1909112_adaptation_loss 0.2077605128288269
wandb:             CHEMBL1909139_adaptation_loss 0.29003283381462097
wandb:             CHEMBL1909141_adaptation_loss 0.34670114517211914
wandb:             CHEMBL1909143_adaptation_loss 0.29202353954315186
wandb:             CHEMBL1909184_adaptation_loss 0.1853419989347458
wandb:             CHEMBL1909210_adaptation_loss 0.3048848509788513
wandb:             CHEMBL1909214_adaptation_loss 0.11810216307640076
wandb:             CHEMBL1963729_adaptation_loss 0.28394800424575806
wandb:             CHEMBL1963781_adaptation_loss 0.2668544054031372
wandb:             CHEMBL1963808_adaptation_loss 0.6061033606529236
wandb:             CHEMBL1963822_adaptation_loss 0.33322665095329285
wandb:             CHEMBL1964022_adaptation_loss 0.6423384547233582
wandb:             CHEMBL2114737_adaptation_loss 0.35109686851501465
wandb:             CHEMBL3214944_adaptation_loss 0.8044698238372803
wandb:             CHEMBL3215078_adaptation_loss 0.6243336796760559
wandb:                            meta_val_score 0.6588391867930243
wandb:   meta_test_CHEMBL1614170_adaptation_loss 0.6346984505653381
wandb:    meta_test_CHEMBL1614170_epoch_val_loss 0.7240599989891052
wandb:   meta_test_CHEMBL1614202_adaptation_loss 0.15845172107219696
wandb:    meta_test_CHEMBL1614202_epoch_val_loss 0.34750062227249146
wandb:   meta_test_CHEMBL1614359_adaptation_loss 0.7490482330322266
wandb:    meta_test_CHEMBL1614359_epoch_val_loss 0.7310552000999451
wandb:   meta_test_CHEMBL1738019_adaptation_loss 0.6217223405838013
wandb:    meta_test_CHEMBL1738019_epoch_val_loss 0.5564230680465698
wandb:   meta_test_CHEMBL1738021_adaptation_loss 0.6170655488967896
wandb:    meta_test_CHEMBL1738021_epoch_val_loss 0.4712318480014801
wandb:   meta_test_CHEMBL1738131_adaptation_loss 0.6514643430709839
wandb:    meta_test_CHEMBL1738131_epoch_val_loss 0.7548515796661377
wandb:   meta_test_CHEMBL1738202_adaptation_loss 0.2603387236595154
wandb:    meta_test_CHEMBL1738202_epoch_val_loss 0.4375447630882263
wandb:   meta_test_CHEMBL1794355_adaptation_loss 0.2810572683811188
wandb:    meta_test_CHEMBL1794355_epoch_val_loss 0.377359539270401
wandb:   meta_test_CHEMBL1794358_adaptation_loss 0.27710282802581787
wandb:    meta_test_CHEMBL1794358_epoch_val_loss 0.7483403086662292
wandb:   meta_test_CHEMBL1794567_adaptation_loss 0.3584686517715454
wandb:    meta_test_CHEMBL1794567_epoch_val_loss 0.42164573073387146
wandb:   meta_test_CHEMBL1909085_adaptation_loss 0.22483566403388977
wandb:    meta_test_CHEMBL1909085_epoch_val_loss 0.7238863408565521
wandb:   meta_test_CHEMBL1909092_adaptation_loss 0.18848668038845062
wandb:    meta_test_CHEMBL1909092_epoch_val_loss 0.7241645753383636
wandb:   meta_test_CHEMBL1909192_adaptation_loss 0.040179021656513214
wandb:    meta_test_CHEMBL1909192_epoch_val_loss 0.714213490486145
wandb:   meta_test_CHEMBL1909209_adaptation_loss 0.3859512209892273
wandb:    meta_test_CHEMBL1909209_epoch_val_loss 0.7310746014118195
wandb:   meta_test_CHEMBL1909211_adaptation_loss 0.09523136168718338
wandb:    meta_test_CHEMBL1909211_epoch_val_loss 0.7474049627780914
wandb:   meta_test_CHEMBL1909212_adaptation_loss 0.04374946281313896
wandb:    meta_test_CHEMBL1909212_epoch_val_loss 0.7139926850795746
wandb:   meta_test_CHEMBL1963705_adaptation_loss 0.714076042175293
wandb:    meta_test_CHEMBL1963705_epoch_val_loss 0.6695496439933777
wandb:   meta_test_CHEMBL1963741_adaptation_loss 0.5352129340171814
wandb:    meta_test_CHEMBL1963741_epoch_val_loss 0.7394067943096161
wandb:   meta_test_CHEMBL1963934_adaptation_loss 0.3144017457962036
wandb:    meta_test_CHEMBL1963934_epoch_val_loss 0.3870752453804016
wandb:   meta_test_CHEMBL2028077_adaptation_loss 0.10974563658237457
wandb:    meta_test_CHEMBL2028077_epoch_val_loss 0.7280927896499634
wandb:   meta_test_CHEMBL2095143_adaptation_loss 0.26207488775253296
wandb:    meta_test_CHEMBL2095143_epoch_val_loss 0.7450114488601685
wandb:   meta_test_CHEMBL2098499_adaptation_loss 0.41158241033554077
wandb:    meta_test_CHEMBL2098499_epoch_val_loss 0.7548934817314148
wandb:   meta_test_CHEMBL2114797_adaptation_loss 0.8224555850028992
wandb:    meta_test_CHEMBL2114797_epoch_val_loss 0.7816022634506226
wandb:   meta_test_CHEMBL3215116_adaptation_loss 0.42788049578666687
wandb:    meta_test_CHEMBL3215116_epoch_val_loss 0.7549514770507812
wandb:   meta_test_CHEMBL3215176_adaptation_loss 0.29075130820274353
wandb:    meta_test_CHEMBL3215176_epoch_val_loss 0.7734470963478088
wandb:    meta_test_CHEMBL918058_adaptation_loss 0.393704891204834
wandb:     meta_test_CHEMBL918058_epoch_val_loss 0.7594611644744873
wandb:                                total_time 6616.262433052063
wandb: Syncing 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)
wandb: - 4.21MB of 4.21MB uploaded (0.00MB deduped)wandb: \ 4.21MB of 4.21MB uploaded (0.00MB deduped)wandb:                                                                                
wandb: Synced maml_test: https://app.wandb.ai/apappu97/molecule-metalearning-chemprop/runs/10lguxd9
